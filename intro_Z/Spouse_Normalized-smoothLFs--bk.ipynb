{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n",
    "print(len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 2625\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### Discrete ##########\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,0)\n",
    "    \n",
    "# def LF_familial_relationship(c):\n",
    "#     return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x.decode('utf-8')).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,0)\n",
    "\n",
    "\n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    global LF_Threshold\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "# def LF_other_relationship_left_window(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "#     for ow in other:\n",
    "#         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "#     return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def LF_Three_Lists_Left_Window(c):\n",
    "#     global softmax_Threshold\n",
    "#     c1,s1 = LF_husband_wife_left_window(c)\n",
    "#     c2,s2 = LF_family_left_window(c)\n",
    "#     c3,s3 = LF_other_relationship_left_window(c)\n",
    "#     sc = np.array([s1,s2,s3])\n",
    "#     c = [c1,c2,c3]\n",
    "#     sharp_param = 1.5\n",
    "#     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "#     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "#     #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "#     if s1==s2 or s3==s1:\n",
    "#         return (0,0)\n",
    "#     return c[np.argmax(prob_sc)],1\n",
    "\n",
    "# def LF_Three_Lists_Between_Words(c):\n",
    "#     global softmax_Threshold\n",
    "#     c1,s1 = LF_husband_wife(c)\n",
    "#     c2,s2 = LF_familial_relationship(c)\n",
    "#     c3,s3 = LF_other_relationship(c)\n",
    "#     sc = np.array([s1,s2,s3])\n",
    "#     c = [c1,c2,c3]\n",
    "#     sharp_param = 1.5\n",
    "    \n",
    "#     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "#     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "#     #print 'BW:',s1,s2,s3,prob_sc\n",
    "#     if s1==s2 or s3==s1:\n",
    "#         return (0,0)\n",
    "#     return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 9-5-2018, 10:41:55\n",
      "500data points labelled in 0.7188430507977803 mins\n",
      "1000data points labelled in 1.3335495034853617 mins\n",
      "1500data points labelled in 1.917067794005076 mins\n",
      "2000data points labelled in 2.519752260049184 mins\n",
      "2500data points labelled in 3.1769800662994383 mins\n",
      "500data points labelled in 4.174797224998474 mins\n",
      "1000data points labelled in 4.797278384367625 mins\n",
      "1500data points labelled in 5.556176539262136 mins\n",
      "2000data points labelled in 6.174566467603047 mins\n",
      "2500data points labelled in 6.850887068112692 mins\n",
      "3000data points labelled in 7.6337612748146055 mins\n",
      "3500data points labelled in 8.271688584486643 mins\n",
      "4000data points labelled in 8.86154446999232 mins\n",
      "4500data points labelled in 9.401108086109161 mins\n",
      "5000data points labelled in 10.001064256827037 mins\n",
      "5500data points labelled in 10.652975161870321 mins\n",
      "6000data points labelled in 11.262283476193746 mins\n",
      "6500data points labelled in 11.85253625313441 mins\n",
      "7000data points labelled in 12.50683133204778 mins\n",
      "7500data points labelled in 13.094375010331472 mins\n",
      "8000data points labelled in 13.716650235652924 mins\n",
      "8500data points labelled in 14.281550602118175 mins\n",
      "9000data points labelled in 14.971553881963095 mins\n",
      "9500data points labelled in 15.57162874142329 mins\n",
      "10000data points labelled in 16.21110211610794 mins\n",
      "10500data points labelled in 16.86411997079849 mins\n",
      "11000data points labelled in 17.50747369925181 mins\n",
      "11500data points labelled in 18.0820143977801 mins\n",
      "12000data points labelled in 18.754645995299022 mins\n",
      "12500data points labelled in 19.329470952351887 mins\n",
      "13000data points labelled in 19.94033740758896 mins\n",
      "13500data points labelled in 20.581616592407226 mins\n",
      "14000data points labelled in 21.171934326489765 mins\n",
      "14500data points labelled in 21.763719876607258 mins\n",
      "15000data points labelled in 22.431539726257324 mins\n",
      "15500data points labelled in 22.935143303871154 mins\n",
      "16000data points labelled in 23.565833016236624 mins\n",
      "16500data points labelled in 24.203933664162953 mins\n",
      "17000data points labelled in 24.749000080426534 mins\n",
      "17500data points labelled in 25.358984434604643 mins\n",
      "18000data points labelled in 25.93876881202062 mins\n",
      "18500data points labelled in 26.685788782437644 mins\n",
      "19000data points labelled in 27.294497899214427 mins\n",
      "19500data points labelled in 27.912985666592917 mins\n",
      "20000data points labelled in 28.575929319858552 mins\n",
      "20500data points labelled in 29.122294485569 mins\n",
      "21000data points labelled in 29.714926477273305 mins\n",
      "21500data points labelled in 30.330261317888894 mins\n",
      "22000data points labelled in 30.933784091472624 mins\n",
      "--- 1877.3884024620056 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "test_L_S = dev_L_S\n",
    "true_labels = gold_labels_dev\n",
    "# dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with smooth lf normalizer\n",
    "\n",
    "def train_unl_s(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s,name = \"s_\")\n",
    "        print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "    \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),\\\n",
    "                           -tf.ones_like(v))\n",
    "            print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda c: iskequalsy(l,c)*s_ ,np.array([-1,1],dtype=np.float64),name=\"pout\")\n",
    "       \n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,name=\"t_pout\")\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            print(\"intsy\",out1)\n",
    "            return out1\n",
    "        \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),np.arange(NoOfClasses,dtype=np.float64))\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                   np.array([-1,1],dtype=np.float64),name=\"zy\")\n",
    "    \n",
    "        print(\"zy\",zy)\n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        \n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        \n",
    "        print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(lsp         ))\n",
    "\n",
    "\n",
    "        tf.summary.scalar('un-normloss', normloss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t,a = sess.run([summary_merged,train_step,normloss,thetas,alphas])\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)                      \n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(true_labels,pl))\n",
    "                print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "            predictAndPrint(pl)\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"macro\"))\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fd25ac04198>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fd25ac04198>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fd25ac04198>\n",
      "s_ Tensor(\"s_/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 17269.481424912243\n",
      "[ 0.21854224 -0.08479684  0.11044418  0.10150883  0.20736169  0.14126437\n",
      "  0.27011953  0.13032326  0.24545013  0.16383871]\n",
      "[[1.11655224 0.81322589 1.00869145 0.99975078 1.10541844 1.04173094\n",
      "  1.16829253 1.03206668 1.14718796 1.06559154]]\n",
      "acc 0.9154228855721394\n",
      "(0.304, 0.20105820105820105, 0.24203821656050958, None)\n",
      "\n",
      "1 loss 17051.9668146155\n",
      "[ 0.21953985 -0.08380813  0.11133243  0.10239708  0.20834567  0.14118797\n",
      "  0.27103515  0.12943494  0.24456181  0.16295039]\n",
      "[[1.11555996 0.81223717 1.00781951 0.99887387 1.10445859 1.04212559\n",
      "  1.16737688 1.0329293  1.14804454 1.06646417]]\n",
      "acc 0.9189765458422174\n",
      "(0.32432432432432434, 0.19047619047619047, 0.24, None)\n",
      "\n",
      "2 loss 16833.432488383332\n",
      "[ 0.22053747 -0.08281934  0.11222335  0.103288    0.20932979  0.14110969\n",
      "  0.27195251  0.12854396  0.24367084  0.16205941]\n",
      "[[1.11456766 0.81124837 1.00694486 0.99799423 1.10349834 1.04252171\n",
      "  1.16645933 1.03379469 1.14890395 1.06733954]]\n",
      "acc 0.9203980099502488\n",
      "(0.3333333333333333, 0.18518518518518517, 0.23809523809523808, None)\n",
      "\n",
      "3 loss 16613.888258766427\n",
      "[ 0.22153509 -0.08183048  0.11311686  0.10418151  0.21031407  0.14102955\n",
      "  0.2728716   0.12765037  0.24277725  0.16116583]\n",
      "[[1.11357535 0.81025949 1.00606757 0.99711191 1.10253768 1.04291927\n",
      "  1.1655399  1.03466278 1.14976614 1.0682176 ]]\n",
      "acc 0.92181947405828\n",
      "(0.3465346534653465, 0.18518518518518517, 0.24137931034482754, None)\n",
      "\n",
      "4 loss 16393.33909748394\n",
      "[ 0.22253273 -0.08084155  0.11401294  0.10507759  0.2112985   0.14094758\n",
      "  0.27379238  0.12675423  0.24188112  0.16026969]\n",
      "[[1.11258302 0.80927054 1.00518771 0.99622699 1.10157662 1.04331825\n",
      "  1.16461863 1.03553351 1.15063104 1.06909828]]\n",
      "acc 0.9228855721393034\n",
      "(0.35106382978723405, 0.1746031746031746, 0.23321554770318023, None)\n",
      "\n",
      "[ 0.22253273 -0.08084155  0.11401294  0.10507759  0.2112985   0.14094758\n",
      "  0.27379238  0.12675423  0.24188112  0.16026969]\n",
      "[[1.11258302 0.80927054 1.00518771 0.99622699 1.10157662 1.04331825\n",
      "  1.16461863 1.03553351 1.15063104 1.06909828]]\n",
      "{0: 2720, 1: 94}\n",
      "acc 0.9228855721393034\n",
      "acc 0.9228855721393034\n",
      "[[2564   61]\n",
      " [ 156   33]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x230.4 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.35106382978723405, 0.1746031746031746, 0.23321554770318023, None)\n",
      "(0.6468554443053818, 0.5756825396825397, 0.5963084286691767, None)\n",
      "acc 0.9228855721393034\n",
      "[[2564   61]\n",
      " [ 156   33]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x230.4 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.17</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2814.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1-score  precision  recall  support\n",
       "0                0.96       0.94    0.98   2625.0\n",
       "1                0.23       0.35    0.17    189.0\n",
       "avg / total      0.91       0.90    0.92   2814.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels=train_unl_s(0.001/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12))\n",
    "Results = predictAndPrint(predicted_labels)\n",
    "Results.to_csv(\"results.csv\")\n",
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with smooth lf normalizer\n",
    "\n",
    "def train_nl_s(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s,name = \"s_\")\n",
    "        print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "    \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),\\\n",
    "                           -tf.ones_like(v))\n",
    "            print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda c: iskequalsy(l,c)*s_ ,np.array([-1,1],dtype=np.float64),name=\"pout\")\n",
    "       \n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,name=\"t_pout\")\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            print(\"intsy\",out1)\n",
    "            return out1\n",
    "        \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),np.arange(NoOfClasses,dtype=np.float64))\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                   np.array([-1,1],dtype=np.float64),name=\"zy\")\n",
    "    \n",
    "        print(\"zy\",zy)\n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        \n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        \n",
    "        print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "\n",
    "\n",
    "        tf.summary.scalar('un-normloss', normloss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,normloss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(true_labels,pl))\n",
    "                print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "            predictAndPrint(pl)\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"macro\"))\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fd25a781710>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fd25a781710>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fd25a781710>\n",
      "s_ Tensor(\"s_/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 197296.44998323973\n",
      "acc 0.9157782515991472\n",
      "(0.3064516129032258, 0.20105820105820105, 0.24281150159744405, None)\n",
      "\n",
      "1 loss 197013.82014176098\n",
      "acc 0.9193319118692252\n",
      "(0.32727272727272727, 0.19047619047619047, 0.24080267558528426, None)\n",
      "\n",
      "2 loss 196731.0041609472\n",
      "acc 0.9207533759772566\n",
      "(0.33653846153846156, 0.18518518518518517, 0.2389078498293515, None)\n",
      "\n",
      "3 loss 196448.01619032279\n",
      "acc 0.92181947405828\n",
      "(0.3465346534653465, 0.18518518518518517, 0.24137931034482754, None)\n",
      "\n",
      "4 loss 196164.86469011274\n",
      "acc 0.9225302061122956\n",
      "(0.34408602150537637, 0.1693121693121693, 0.22695035460992907, None)\n",
      "\n",
      "[ 0.22253658 -0.08081851  0.11428546  0.10535121  0.21132734  0.14061589\n",
      "  0.27395202  0.12666942  0.24181331  0.1601904 ]\n",
      "[[1.11256395 0.8092538  1.00488821 0.99593253 1.1014759  1.04332127\n",
      "  1.16438472 1.03553573 1.15062739 1.069098  ]]\n",
      "{0: 2721, 1: 93}\n",
      "acc 0.9225302061122956\n",
      "acc 0.9225302061122956\n",
      "[[2564   61]\n",
      " [ 157   32]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x230.4 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.34408602150537637, 0.1693121693121693, 0.22695035460992907, None)\n",
      "(0.6431933231378407, 0.573037037037037, 0.593086101360333, None)\n",
      "acc 0.9225302061122956\n",
      "[[2564   61]\n",
      " [ 157   32]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x230.4 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.17</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2814.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1-score  precision  recall  support\n",
       "0                0.96       0.94    0.98   2625.0\n",
       "1                0.23       0.34    0.17    189.0\n",
       "avg / total      0.91       0.90    0.92   2814.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels=train_nl_s(0.001/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12))\n",
    "Results = predictAndPrint(predicted_labels)\n",
    "Results.to_csv(\"results.csv\")\n",
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc88a7e48>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc88a7e48>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc88a7e48>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "ints <function ints at 0x7f6cb38b0b70>\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 155967.12925270078\n",
      "[0.31803372 0.01467325 0.21122707 0.20228131 0.30685932 0.24027927\n",
      " 0.37013872 0.22994369 0.34494849 0.26342436]\n",
      "[[1.1173599  0.81386525 1.00874661 0.99981828 1.10619331 1.04192594\n",
      "  1.1689777  1.03176479 1.14673924 1.06518368]]\n",
      "{0: 2684, 1: 130}\n",
      "(0.2923076923076923, 0.20105820105820105, 0.23824451410658307, None)\n",
      "\n",
      "1 loss 155806.64309580828\n",
      "[0.31851834 0.01512748 0.21290258 0.20394629 0.30733661 0.23921635\n",
      " 0.37107504 0.2286678  0.34354886 0.26211327]\n",
      "[[1.11717674 0.81351884 1.00793096 0.99900979 1.10601014 1.04251703\n",
      "  1.16874998 1.03232873 1.14714946 1.06564999]]\n",
      "{0: 2699, 1: 115}\n",
      "(0.33043478260869563, 0.20105820105820105, 0.25, None)\n",
      "\n",
      "2 loss 155644.34438454875\n",
      "[0.31899846 0.0155772  0.2145861  0.20561918 0.30780974 0.23814988\n",
      " 0.3720149  0.22738024 0.3421359  0.2607901 ]\n",
      "[[1.11699515 0.81317531 1.0071128  0.99819889 1.10582837 1.04311178\n",
      "  1.16852284 1.03289968 1.1475654  1.06612224]]\n",
      "{0: 2709, 1: 105}\n",
      "(0.3333333333333333, 0.18518518518518517, 0.23809523809523808, None)\n",
      "\n",
      "3 loss 155480.2334294139\n",
      "[0.31947406 0.01602241 0.21627754 0.2072999  0.30827867 0.23707995\n",
      " 0.37295828 0.22608108 0.3407097  0.25945493]\n",
      "[[1.11681513 0.81283466 1.00629218 0.99738567 1.10564799 1.04371015\n",
      "  1.16829631 1.03347764 1.14798707 1.06660045]]\n",
      "{0: 2711, 1: 103}\n",
      "(0.33980582524271846, 0.18518518518518517, 0.23972602739726026, None)\n",
      "\n",
      "4 loss 155314.3115504323\n",
      "[0.31994514 0.01646309 0.21797683 0.20898837 0.30874341 0.23600668\n",
      " 0.37390514 0.22477039 0.33927033 0.25810783]\n",
      "[[1.11663667 0.8124969  1.0054692  0.9965702  1.10546901 1.04431209\n",
      "  1.16807041 1.03406263 1.1484145  1.06708462]]\n",
      "{0: 2721, 1: 93}\n",
      "(0.34408602150537637, 0.1693121693121693, 0.22695035460992907, None)\n",
      "\n",
      "[0.31994514 0.01646309 0.21797683 0.20898837 0.30874341 0.23600668\n",
      " 0.37390514 0.22477039 0.33927033 0.25810783]\n",
      "[[1.11663667 0.8124969  1.0054692  0.9965702  1.10546901 1.04431209\n",
      "  1.16807041 1.03406263 1.1484145  1.06708462]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.169\n",
      "Neg. class accuracy: 0.977\n",
      "Precision            0.344\n",
      "Recall               0.169\n",
      "F1                   0.227\n",
      "----------------------------------------\n",
      "TP: 32 | FP: 61 | TN: 2564 | FN: 157\n",
      "========================================\n",
      "\n",
      "{0: 2721, 1: 93}\n",
      "acc 0.9225302061122956\n",
      "(array([0.94230062, 0.34408602]), array([0.9767619 , 0.16931217]), array([0.95922185, 0.22695035]), array([2625,  189]))\n",
      "(0.6431933231378407, 0.573037037037037, 0.593086101360333, None)\n",
      "[[2564   61]\n",
      " [ 157   32]]\n",
      "prec: tp/(tp+fp) 0.34408602150537637 recall: tp/(tp+fn) 0.1693121693121693\n",
      "(0.34408602150537637, 0.1693121693121693, 0.22695035460992907, None)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## smooth LFs\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: ls_*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t = tf.squeeze(thetas)\n",
    "    t_k =  k*t\n",
    "    \n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    def ints(y):\n",
    "        return alphas+((tf.exp((t_k*y)*(1-alphas))-1)/(t_k*y))\n",
    "    \n",
    "    print(\"ints\",ints)\n",
    "    \n",
    "#     zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                   np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.MomentumOptimizer(0.0000001,0.002).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "           \n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            drawPRcurve(np.array(gold_labels_dev),np.array(m[1::].flatten()),it)\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7f4ebb70>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7f4ebb70>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7f4ebb70>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "ints <function ints at 0x7f6c7dd8dae8>\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 155897.12885737582\n",
      "[0.31974384 0.01637783 0.21163194 0.2026973  0.30848131 0.23960285\n",
      " 0.37093495 0.22923914 0.34437169 0.2627566 ]\n",
      "[[1.11538467 0.81213514 1.00752008 0.99857126 1.10443769 1.04300815\n",
      "  1.16726315 1.03305008 1.14814347 1.06659289]]\n",
      "{0: 2693, 1: 121}\n",
      "(0.3140495867768595, 0.20105820105820105, 0.2451612903225807, None)\n",
      "\n",
      "1 loss 155595.9783639421\n",
      "[0.32194153 0.01853981 0.21371203 0.20477805 0.31058288 0.23786984\n",
      " 0.372661   0.22725771 0.34239525 0.26077696]\n",
      "[[1.11322809 0.81005796 1.00547271 0.99651101 1.10250009 1.04467831\n",
      "  1.16532408 1.03490703 1.14996613 1.06847898]]\n",
      "{0: 2711, 1: 103}\n",
      "(0.33980582524271846, 0.18518518518518517, 0.23972602739726026, None)\n",
      "\n",
      "2 loss 155292.64514795056\n",
      "[0.32413843 0.02070088 0.21579742 0.20686406 0.31268226 0.23614165\n",
      " 0.37438158 0.22526637 0.34040827 0.25878722]\n",
      "[[1.11107303 0.80798181 1.00342009 0.99444538 1.10056544 1.04634593\n",
      "  1.16339058 1.0367761  1.15180161 1.070377  ]]\n",
      "{0: 2723, 1: 91}\n",
      "(0.34065934065934067, 0.164021164021164, 0.2214285714285714, None)\n",
      "\n",
      "3 loss 154987.21985621194\n",
      "[0.32633448 0.02286102 0.21788795 0.20895518 0.31477922 0.23441834\n",
      " 0.3760966  0.22326549 0.33841116 0.25678777]\n",
      "[[1.10891955 0.80590668 1.00136247 0.99237462 1.09863406 1.04801094\n",
      "  1.16146287 1.03865677 1.15364937 1.07228645]]\n",
      "{0: 2730, 1: 84}\n",
      "(0.34523809523809523, 0.15343915343915343, 0.21245421245421245, None)\n",
      "\n",
      "4 loss 154679.78048190873\n",
      "[0.32852965 0.02502019 0.2199835  0.21105127 0.31687351 0.23270002\n",
      " 0.37780599 0.22125544 0.33640433 0.25477898]\n",
      "[[1.10676768 0.80383258 0.9993001  0.99029896 1.09670633 1.04967324\n",
      "  1.15954118 1.04054853 1.15550886 1.07420687]]\n",
      "{0: 2733, 1: 81}\n",
      "(0.345679012345679, 0.14814814814814814, 0.2074074074074074, None)\n",
      "\n",
      "[0.32852965 0.02502019 0.2199835  0.21105127 0.31687351 0.23270002\n",
      " 0.37780599 0.22125544 0.33640433 0.25477898]\n",
      "[[1.10676768 0.80383258 0.9993001  0.99029896 1.09670633 1.04967324\n",
      "  1.15954118 1.04054853 1.15550886 1.07420687]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.148\n",
      "Neg. class accuracy: 0.98\n",
      "Precision            0.346\n",
      "Recall               0.148\n",
      "F1                   0.207\n",
      "----------------------------------------\n",
      "TP: 28 | FP: 53 | TN: 2572 | FN: 161\n",
      "========================================\n",
      "\n",
      "{0: 2733, 1: 81}\n",
      "acc 0.923951670220327\n",
      "(array([0.94109038, 0.34567901]), array([0.97980952, 0.14814815]), array([0.96005972, 0.20740741]), array([2625,  189]))\n",
      "(0.6433846946104538, 0.563978835978836, 0.5837335655924681, None)\n",
      "[[2572   53]\n",
      " [ 161   28]]\n",
      "prec: tp/(tp+fp) 0.345679012345679 recall: tp/(tp+fn) 0.14814814814814814\n",
      "(0.345679012345679, 0.14814814814814814, 0.2074074074074074, None)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucXWV97/HPdy7J5E6uBJJIkIRbgtyCQC2XI2qBWqi1IqhVrIVqS6un1nM8r1pLvZy29mgvR7ygWK2tWPS0ntSieFQkaAGJBpAAgZBwSUgggSQkk8xkJvM7f/zW3rMzmczskNmzZ89836/Xfs26PHvt39ozs37reZ61nqWIwMzMDKCp3gGYmdnI4aRgZmZlTgpmZlbmpGBmZmVOCmZmVuakYGZmZU4KNqQkvVXS96oo9zlJfzocMQ0HSVdL+nHFfEhaVM+YzF4KJ4UGJWm8pJskPSlpp6T7JF0yQPmrJe2TtEvSi0X51w91XBHxzxHxuirKvTsiPjrUnw/lA3J7sa8bJX1KUnMtPmu0k3SspB5Jn+1n3WF/z5JmSPq3YjtPSnrLAGU/IOnB4u99vaQP9Fn/UUm/kNQt6fpDicN6OSk0rhbgaeACYBrwIeAWSQsHeM9dETEZOAK4qSg/vW8hSS1DHu3wO7XY1wuANwO/Xed4htwwJbq3A9uAN0sa38/60vd8EfAW4JpD3P4NwF7gSOCtwGclLTlIWRXxTAcuBq6TdGXF+rXAfwP+4xBjsApOCg0qItoj4vqIeCIieiLi28B64Mwq3tsDfAmYABwn6UJJGyT9d0mbgX8AkPT6okaxXdJ/SnpFaRuSFkj6V0lbJD0v6dPF8nIzitLfSHquqJ38QtLSYt2XJX2sYnvXSFor6QVJyyUdXbEuJL1b0mNFLDdIUpXf01rgJ8BpFdubVtSyNhVnuB+rPMAWsTxcnJE+JOmMYvkHJT1esfwN1cTQV3F2/A+SnpG0TdK3+n53ffZ9UcV39llJt0pqB/5Y0uY+sb9B0gPFdFNFzM9LukXSjEOIs3QQ/hDQBfzawcpGxCPAncDSQ9j+JOCNwJ9GxK6I+DGwHPitg3zGJyLi5xHRHRFrgP8LvKpi/Vci4jvAzmpjsAM5KYwSko4EjgdWV1G2BfgdYBfwWLF4LjADOAa4VtLpZOL4XWAm8HlgubLZqhn4NvAksBCYB3y9n496HXB+Edc04Arg+X7ieTXwF8X6o4rt9t3e64GzgFcU5X5lsP0stn0icB55FlnyZaAbWAScXsT5O0X5NwHXkwfDqcBlFTE/XmxrGvDnwD9JOqqaOPr4KjARWALMAf7mEN77FuDjwBTg74B24NV91n+tmP4D4NfJ2tLR5Bn/DaWCkh4YqLkG+GVgPvm7uAV4x8EKSjqZ/G5WFfPfLhJ4f69vF287HuiOiEcrNnU/+b0MqEhY51HF37sdoojwq8FfQCvwfeDzA5S5mjwQbge2AncDrynWXUhW4dsqyn8W+GifbawhDzDnAluAloN8zo+L6VcDjwLnAE19yn0Z+FgxfRPwiYp1k8kz04XFfAC/XLH+FuCDA+xrAC+SB8wAbgbGF+uOBDqBCRXlrwJuL6ZvA95b5fd+H3B53/2uiGFRP+85CugBpg/03fW3neI7+8c+6z8GfKmYnlLs8zHF/MPARX0+u6u/39tB9u+LwLeK6XOL987p53veRibMj/X9PQ+y/fOAzX2WXQP8qIr3/jmZQMb3s+6fgOtr9f822l+uKTQ4SU3kmede4LpBit8dEUdExKyIOCcivl+xbktEdFTMHwO8v/IMD1hAnnEuAJ6MiO6BPiwifgh8mjw7fU7SjZKm9lP0aLJ2UHrfLvLsfF5Fmc0V07vJxIGk1cqOzl2Szqsoc0ZR5s3A2cCkiv1qBTZV7NfnyTN2in17vL/9kfT2iua07WRTyayBvoN+LABeiIhth/i+kqf7zH8N+I2ivf83gJ9HROm7PAb4t4p4Hwb2kYlxQJImAG8C/hkgIu4CniJrIpXOiIjpEXFcRHwosmmyWrvI2lilqQzS/CPpOrIm96sR0XkIn2dVcFJoYEUV+ibyn/yNEdF1GJvrO1zu08DHiyRSek2MiJuLdS9TFR3SEfH3EXEmcDLZXPCBfoo9Qx7AgHJb80xgYxXbXxIRk4vXnX3WRUTcAtwFfLhivzqBWRX7NTUillSsP67v50g6BvgCmXhnRsQRwINk5+eheBqYIemIfta1k81Kpc+c20+Z/X5PEfEQmVAvYf+mo9JnXdLnd9gWEYN+r8AbyAP0Z4p+i81kkj5oE1IlSd+pSNZ9X98pij0KtEhaXPHWUxmgSUjSbwMfJGtAG6qJxQ6Nk0Jj+yxwEvBrEbFniLf9BeDdks4uOownSfpVSVOAnwKbgL8slrdJelXfDUg6q3h/K3nA6yCbTvq6GXinpNOKM97/CdwTEU8M0b78JXCNpLkRsQn4HvBJSVOLztjjJF1QlP0i2YF7ZrHfi4qEMIk8IG8p9u2dHEKnaknx+d8hD7bTJbVKOr9YfT+wpPge2si+jWp8DXgv2X/zjYrlnwM+XsSPpNmSLq9ym+8g+5ROITvpTyM7dU+VdMpgb46ISyqSdd/XJUWZduBfgY8Uf0evAi4na74HkPRW8m/jtRGxrp/1rcX31kQmmzb5UuRD5qTQoIp/9N8l/1k3V5yFvXUoth8RK8n23U+TbcZryTZvImIfeSXKIrJJYQPZTNPXVDK5bCPPZp8H/rqfz/o+8KfA/yGTzXHAlX3LHca+/AJYQW8t5e3AOOChIrZvku3tRMQ3yI7cr5HNGN8CZhRn5J8kax3PkgfLn7zEkH6LbJ9/BHgOeF/x2Y8CHyH7hx4DfnywDfRxM9nX88OI2Fqx/O/Iq3m+J2kn2Y90dmll0fR2wN+LpHnkJaZ/GxGbK14/A75LlbWFKv0eeRXcc8V+vCciVhdxnCdpV0XZj5E1yHsr/t4/V7H+C8Aeso/oT4rpfq9ksoNThB+yY2ZmyTUFMzMrc1IwM7MyJwUzMytzUjAzs7KGG/hs1qxZsXDhwnqHYWbWUH72s59tjYjZg5VruKSwcOFCVq5cWe8wzMwaiqQnBy/l5iMzM6vgpGBmZmVOCmZmVuakYGZmZU4KZmZW5qRgZmZlNUsKkr6kfDbvgwdZL0l/r3wu7wMqnoNrZmb1U8uawpeBiwdYfwmwuHhdSz4boCo9Pfu/zMxsaNTs5rWIWCFp4QBFLiefNxvA3ZKOkHRU8RCSg9q1C+68c/9lU6bAGa5nmJkdtnre0TyP/Z83u6FYdkBSkHQtWZtg9uyFbNgAKh6CuGsXRMCSJTB+fM1jNjMb1RpimIuIuBG4EeCEE5bFokXQUkS+aRM83fdR5mZm9pLU8+qjjcCCivn5VPGgdjMzq516JoXlwNuLq5DOAXYM1p9gZma1VbPmI0k3AxcCsyRtAP4MaAWIiM8BtwKXkg+E3w28s1axmJlZdWp59dFVg6wP4Pdr9flmZnbofEezmZmVOSmYmVmZk4KZmZU5KZiZWZmTgpmZlTkpmJlZmZOCmZmVOSmYmVmZk4KZmZU5KZiZWZmTgpmZlTkpmJlZmZOCmZmVOSmYmVmZk4KZmZU5KZiZWZmTgpmZlTkpmJlZmZOCmZmVOSmYmVmZk4KZmZU5KZiZWZmTgpmZlTkpmJlZmZOCmZmVOSmYmVmZk4KZmZW11DuAw9XZCd3dsHEjjB8Pzc0wZw40Od2ZmR2yhk8Ke/ZARwfccQe0tuay88+Hl72svnGZmTWiUXE+3dEBRx8Ns2fD1q2ZKMzM7NA1fE2hpK0tm4xaRs0emZkNv1FRUzAzs6FR06Qg6WJJayStlfTBfta/TNLtklZJekDSpbWMx8zMBlazpCCpGbgBuAQ4GbhK0sl9in0IuCUiTgeuBD5Tq3jMzGxwtawpvBJYGxHrImIv8HXg8j5lAphaTE8DnqlhPGZmNohaJoV5wNMV8xuKZZWuB94maQNwK/AH/W1I0rWSVkpauWPHllrEamZm1L+j+SrgyxExH7gU+KqkA2KKiBsjYllELJs2bfawB2lmNlbUMilsBBZUzM8vllV6F3ALQETcBbQBs2oYk5mZDaCWSeFeYLGkYyWNIzuSl/cp8xRwEYCkk8ik4PYhM7M6qVlSiIhu4DrgNuBh8iqj1ZI+Iumyotj7gWsk3Q/cDFwdEVGrmMzMbGA1vf83Im4lO5Arl324Yvoh4FW1jMHMzKpX745mMzMbQZwUzMyszEnBzMzKRmVS2LkzH75jZmaHZlQlhY6OTAYPPwx33gn79tU7IjOzxtLwSWHxYli6FCZNgp6efMDOE0/A6tWuLZiZHaqGfyRNS0v/j95sbh7+WMzMGl3D1xTMzGzoOCmYmVmZk4KZmZU5KZiZWZmTgpmZlTX81UeVpk+HCRNg8mTo6qp3NGZmjWdU1RSam+HCC2HBgkGLmplZP0ZVUjAzs8PjpGBmZmVOCmZmVuakYGZmZU4KZmZW5qRgZmZlTgpmZlY2qm5eG8gLL8CaNXlT21FH5XMYzMxsf2MmKXR2wlNPwZYtsGGDk4KZWX/GVPNRd3cOgdHdXe9IzMxGpjGVFMzMbGBjIin09OTLzMwGNur7FLq6YNUqaG+HiHpHY2Y2so3KmsLu3ZkA1q6FrVthxw5Ytw7a2qC1td7RmZmNXKMyKXR0ZM1gxQr4z//MZdOmwbHHQsuorxuZmb10ozIpQDYbNTdncjAzs+qM2qQAINU7AjOzxlJ1Y4qkecAxle+JiBW1CGqodHXlTWtmZladqpKCpL8C3gw8BOwrFgcwYFKQdDHwd0Az8MWI+Mt+ylwBXF9s7/6IeEu1wQ+mpyeTwtat2clsZmYDq7am8OvACRFR9Xm3pGbgBuC1wAbgXknLI+KhijKLgf8BvCoitkmaU33og+vpgRdfhOOPhylThnLLZmajU7V9CuuAQ72Y85XA2ohYFxF7ga8Dl/cpcw1wQ0RsA4iI5w7xM6oycyaMG1eLLZuZjS7V1hR2A/dJ+gFQri1ExB8O8J55wNMV8xuAs/uUOR5A0k/IJqbrI+K7VcZkZmZDrNqksLx41eLzFwMXAvOBFZJOiYjtlYUkXQtcC3DkkS8bdKOLF+f9CDt2wK5dQx6zmdmoVVVSiIivSBpHcWYPrImIrkHethFYUDE/v1hWaQNwT7Gt9ZIeJZPEvX0+/0bgRoATTlg26GAVLS2ZGFatGqykmZlVqqpPQdKFwGNkx/FngEclnT/I2+4FFks6tkgoV3JgbeNbZC0BSbPIpLOu2uDNzGxoVdt89EngdRGxBkDS8cDNwJkHe0NEdEu6DriN7C/4UkSslvQRYGVELC/WvU5S6VLXD0TE8y99d/bX1uarjszMDkW1SaG1lBAAIuJRSYNejRQRtwK39ln24YrpAP6oeA25k06qxVbNzEavapPCSklfBP6pmH8rsLI2IZmZWb1UmxTeA/w+ULoE9U6yb8HMzEaRaq8+6gQ+VbzMzGyUGjApSLolIq6Q9AtybKL9RMQrahaZmZkNu8FqCu8tfr6+1oGYmVn9DXifQkRsKia3Ak9HxJPAeOBU4Jkax2ZmZsOs2gHxVgBtxTMVvgf8FvDlWgVlZmb1UW1SUETsBn4D+ExEvAlYUruwzMysHqpOCpLOJe9P+I9iWXNtQjIzs3qpNim8j3wYzr8VQ1W8HLi9dmGZmVk9VHufwh3AHRXz6+i9kc3MzEaJwe5T+NuIeJ+kf6f/+xQuq1lkZmY27AarKXy1+Pm/ah2ImZnV34BJISJ+VkyuBPZERA+ApGbyfgUzMxtFqu1o/gEwsWJ+AvD9oQ/HzMzqqdqk0BYR5acdF9MTByhvZmYNqNqk0C7pjNKMpDOBPbUJyczM6qXa5ym8D/iGpGcAAXOBN9csKjMzq4tq71O4V9KJwAnFojUR0VW7sGpn715ob4cXXoDOTti6FSZNgunT82VmNpZVlRQkTSSfo3xMRFwjabGkEyLi27UNb+j19EB3N6xYkfMvvAAdHTBnDvzmb9Y3NjOzequ2T+EfgL3AucX8RuBjNYmoxiJg+3YYPx6efRY2boSurlxmZjbWVZsUjouITwBdAMWIqapZVMNgzhw44oicVkPviZnZ0Kk2KeyVNIFiqAtJxwGdNYvKzMzqotqrj/4M+C6wQNI/A68Crq5VUMNl2rT82dpa3zjMzEaKQZOCJAGPkA/YOYdsNnpvRGytcWw1N3s2XHQRPPPMwH0K3d1ZpqcHJk+GWbOGL0Yzs+E0aFKIiJB0a0ScQu8DdkaN8eOhaZBGtF274JFH4KmnYOJEuOIKaKm2jmVm1kCq7VP4uaSzahrJCNfZCXv25BVLK1bAhg31jsjMbOhVe757NvA2SU8A7WQTUkTEK2oV2Eixdm02LfX0ZI1izx74+c9h926YP7/e0ZmZDa1qk8Kv1DSKEWzz5mw6Gjcum4z27cs7oPfty2alnp5c19ZW70jNzA7fYE9eawPeDSwCfgHcFBHdwxFYrcydm2f71erpgQkT4MQT4eGHc5mUd0Hffz/s2JHz55+f/Q2+58HMGtlgNYWvkDes3QlcApwMvLfWQdXS7Nn5qrR7dyaK1avhmGPyCqNNm7Jjee/e3gP9iSfmTW+lK5F27cr3NDXl/NFHw5IlWXMwM2tEgyWFk4urjpB0E/DT2odUH3v2wA9+AMuWwS/9UiaKhx7Kexgq73yeOROefhp27oSpU7PZaMcOePDB7Hx+4QU49VRftmpmjWmwpFAeCTUiujWK20Y6O/Nmtr17e5dFwPHHQ3Pz/mW7uuDFF7O5qLU1y40blyOu7tqVNREnBTNrRINdknqqpBeL107gFaVpSS8OtnFJF0taI2mtpA8OUO6NkkLSskPdgaG2Zw/cdx8899zBy0RkEjnppGxSmjgxk0JPjzuczayxDVhTiIjmgdYPRFIzcAPwWmADcK+k5RHxUJ9yU8h+inte6mcNpc5OePJJeP75g3ccNzX19htMmQIXXphXKUUMa6hmZkOu2pvXXopXAmsjYl1E7AW+DlzeT7mPAn8FdNQwlgEtWLB/5/O+fXDUUbB4cf93Oy9bBueeu/+yuXPh9NNrG6eZWa3VMinMA56umN9QLCsrnvu8ICIGHD5D0rWSVkpauWPHliEPdPJkOKN4AnVnlWO/Tpo05GGYmdVdLZPCgCQ1AZ8C3j9Y2Yi4MSKWRcSyadNmD1b8JenpyYSweXPewezLSs1sLKplUtgILKiYn18sK5kCLAV+VAyfcQ6wvJ6dzT09+fzmpUt7L0M1MxtLapkU7gUWSzpW0jjgSmB5aWVE7IiIWRGxMCIWAncDl0XEyhrGVJW+l6CamY0VNRsAuriv4TrgNqAZ+FJErJb0EWBlRCwfeAvDq9ShPH784W9r+3a45568n+Hoo+HlLz/8bZqZDYeaPhUgIm4Fbu2z7MMHKXthLWMZTHNzdjZX29E8kD17ciiMrVvzLmcnBTNrFH5UTIVJkw7vqqLu7uyX2Lo1L2udPDmXmZk1irpdfTQadXfnMBcbN2ZyKT2drb09x0favbu+8ZmZDcY1hSFUSgJtbbBwITz2WA6x/cADOVCelIPltbf3vqe1NcuO4mGlzKyBOCkMocmTYdGi3iaozs5MCrt3w+OPZ01ixgxYsyY7offty2VXXHHgcN5mZvXgpDDE+nYqd3RkZ/PEidkB3dGRTUxnn51XKT38cI64OmNGDsnd3p41jkWLfGmsmQ0/J4UaisjnLpx1Vj6wZ+vWrDF0dWVzUXt71iZ+9jNYvz6bnR58MGsPXV2ZGDzqqpkNJyeFGpo8OUdRbWvL+yC6umDLlt4rkkq1h0cfzbJz5uTzHDo6MlFs3gwXXeT+BjMbPr76qIaOPRbOOy+n587Nn2eeCa99bU7PmJEH/QULskYxfTrMm5e1h6eeyqalrq6scWzblsN5V3ZSm5kNNdcUhsnkyXDppQcuHz8e5s/PpDBrViaAzZuzs3rPnuyM3rUrm5W2bMnyl1zi/gYzqw3XFEaAI47I5zM0NeXB/pxzem98W7UqXy+8AJs25WWulY8MNTMbSk4KI1RHRzYjrV6djwctPfjHQ3qbWS05KYxQs2dnU1JTU9YaFi70lUhmVnvuUxihZs2CV7+63lGY2VjjmoKZmZU5KZiZWZmTgpmZlTkpmJlZmZOCmZmVOSmYmVmZk4KZmZX5PoUGEpHPgH766RwDqbUVjjzS4yCZ2dBxUmgg27fnuEe/+EVONzfD+ecf+GAfM7OXykmhwezenUNqR+RIqrt21TsiMxtNnBQayOLFOSDeMcdkMnjkkRw5VcoEMXMmTJtW7yjNrJE5KTSQlhY47riclnIU1Y0b8xnQe/fC0UfD5ZfXN0Yza2xOCg1q0qQcTvv553OY7enTM0E880w+1rOpKfsdXnwxE8jcudk5bWY2ECeFBtXSsv8oqg8+mI/s/N738rkLU6dmTeLxx/NhPWedBWecUb94zawxOCmMEscdB2vXwnPPZTKA7Hdoasp+iJ076xufmTUGJ4VRYsIEOOUUuOuu3r6HSZPyvob77ssmJDOzwTgpjDLnnrv/fE9PfeIws8bkYS7MzKzMSWGM6OjIy1bNzAZS06Qg6WJJayStlfTBftb/kaSHJD0g6QeSjqllPGNR5XhJd9wBe/bUOyIzG8lqlhQkNQM3AJcAJwNXSTq5T7FVwLKIeAXwTeATtYpnrGppyRrCpk2walXe6NbZ6b4GM+tfLWsKrwTWRsS6iNgLfB3Y737biLg9InYXs3cD82sYz5gkwS//Mpx0Uk7//Ofw3e/CihV5s5uZWaVaXn00D3i6Yn4DcPYA5d8FfKe/FZKuBa4FOPLIlw1VfGPKtGnZr/Doo3nvQmtr3v38qlcd/D0TJ8Ls2cMXo5nV34i4JFXS24BlwAX9rY+IG4EbAU44YVkMY2ijxvTp8NrXZkLYti1vdHv+efjRj/ov392d/REnnQTHHpvjKpnZ6FfLpLARWFAxP79Yth9JrwH+BLggIjprGM+Y19qaP2fOzBrDjh2wbFn/ZdesyZrEzp15l/Qb3jB8cZpZ/dSyT+FeYLGkYyWNA64EllcWkHQ68Hngsoh4roax2EGMG9f/a8kSOO20HEPpuefggQey+cnMRreaJYWI6AauA24DHgZuiYjVkj4i6bKi2F8Dk4FvSLpP0vKDbM6GkJQjpg7UX9DUlI/6jMiRVn/4Q1i3bvhiNLP6qGmfQkTcCtzaZ9mHK6ZfU8vPt4M788zqyp1+Ojz7LKxfn0N1m9no5juabVB+mpvZ2DEirj6yxtLenq+mpryqae/ebGZqa8tlZta4nBRsUFFcBPzkkzlMRukO6Z6erEW0tmbTUnNzPj+6Lykf/DNx4vDGbWaHzknBBrV3b155tGlT3gXd3JyJ4Kmneg/0nZ3Zef3EEwe+v6MDZs2CE06ApUtz6A0zG5n872mDmjYNpkzJBDBvHkyenGf/J54IXV35gJ+HHsqEcN55+783Au65Jwfk27w575FYsKDfj6mb3buzM73RTZqUz+c2OxxOClaV/q5WamnpPes//vhMHs3NB5Y791zYsiWfFz0SB+Lbti2fTrd9e70jeem6uvK7X7o0E3FE1ubGjcsmv66u/F0tWZIJvtb27Om9Wi0ia5HNzf3/fdjI4qRgQ6KlJWsR/WlqytpEpZ6eHEpjqOzZk01YUnZ+H2oTVXt7DufRN85GsX599vn89Kf53e7blwdiKZv/Sgfotra8KbGvfft6+45Kurp6H+Pa0ZFltm3L7be3759cdu7sTfidnfnas6c3CXR09PY/jRuXy6WcnjIlP7v0GUcckc2N48YN7Xdk1XFSsLpYvTprD30PRIdj27Y8qJx8cj6v+lA1Nzduf8fixdks19aW86VEUDoA794N99+fzXwvvphNgV1deSBvacmynX0GmSkl2ebmXN/Rke/t7s73NjX1Xm3W2Zll2tpy3e7d2cw4ezZs3ZpDqvT05PrSNkvvb2nJZNzRkdsePz6TwlFH5R31kD/Hj8/9Gjcup0v7um9fbru0TTs8DfovYI2mdBb56KN5kOjoyAPUULaBR2RneEdHTi9ZMraaK0oHSTjwLLutLQ+4Gzbkd1QaNbezM/siSpcUl2pKe/fm2FczZ+aZe0R+l+PG5QF6y5Zsqqo8CJcO6H2VrkgrPfApIhPD9u0ZQ3t7JoHW1lz25JOwa1fG2d2dnztjRk63tfXWgmbOzOnu7t6fpcuiW1vzjnwpkxNkzWXixFw/cWImI6m3NmTJScGGRWdnNjGsX599C93d+Y9/7rlD9xkRmXC2boXbb88D19SpeSAqnfUuWtR71tnTM3bOLJuaei8CKPUvSHmgbG/PA2epualkyZKDb29+P08+GSwBl2oIJTNnHlhmypRMEFu39l7QsH59/u20t2eZ7u6siUycmL/D6dNzfufO3tpCc3O+v6UlE0Vra28NZ8KE3qvmJkzoHe+rVIuZOzf/Rhq11ni4xuhu23CbMyebOCZNyvn16/MfdChJcPbZeSXRfffBY4/1nkGWhgJfty4PFtOnZ6Jobc1//r5NJ6NZabRcyIPiSOtHmTBh/yvUTj/9wDKl2sbBPPts1jq2bMkENmFCJprOzrxKrtR8Vvq8pqbezvkZM/Lkob/PHQucFGxYSPDyl/fOb9lSm7P00pne2Wf33k8xYUL+w993X7aJb9iQy5ua8sAwblwmj717fYNdoxisyefII/N1wgm9y/prqozIRNHUlLWOxx/Pe3F6erKmNBY7u50UrC6WLq3t9qdP339eGvjMr9SBaWOLtH9fzJIlvZcnr1iR/SHTp2cNt9S8NNo5KZjhhGC9Zs/OJxOWbrpsbc1l8+cP3M8yWjgpmJlVmDcvX6U+iWeeyZ/PPps3aVb2yYxGTgpmZv044oh8LV4MjzyS/VF33JFJoXTp7ZQp2cQ0adLoaVpyUjAzG8T8+fDww3nH+PTp2dy4fXsmhSefzP6GSZPg6KMzkQzHUCK14qRgZjaIyZPhrLP2X9bVBQ8+CGvW5JVuEXml29y5cNll/W+nETgpmJm9BK2t+1/R1t6ew8k/+WQ2NU2e3Jg3RzopmJkNgUmTstnohRfyctZJk7Kpadas7G+IyCRRGiqkNO5Xf0PGxijKAAAGx0lEQVSD1JOTgpnZEDnqqKwlPP5477hKpeFDSq9S8iiNNyX1JgkpR+uNyD6LqVN7O7DHjx+e56U7KZiZDZHm5hylt2T37hxipXRz5NatOdTK7Nl513TpGR6zZ2cykXLQyM7OHH6lu7t3QL85c+DCC2u/D04KZmY10nfYlKlT9x/upa9NmzIZtLdngmhp6R34b6jHCjsYJwUzsxHiqKPy5+zZ+y/fty9HFR4ODdg3bmZmteKkYGZmZU4KZmZW5qRgZmZlTgpmZlbmpGBmZmVOCmZmVuakYGZmZU4KZmZWVtOkIOliSWskrZX0wX7Wj5f0L8X6eyQtrGU8ZmY2sJolBUnNwA3AJcDJwFWSTu5T7F3AtohYBPwN8Fe1isfMzAZXy7GPXgmsjYh1AJK+DlwOPFRR5nLg+mL6m8CnJSmiNNJ4/zo7c/RAM7OxYN++4fusWiaFecDTFfMbgLMPViYiuiXtAGYCWysLSboWuLaY23vBBVMer03IjaBrOrRuq3cU9TOW938s7zt4/zunQdemw9jAMdUUaohRUiPiRuBGAEkrI3Yuq3NIdZP73+H9H4PG8r6D9z/3P2q+/7XsaN4ILKiYn18s67eMpBZgGvB8DWMyM7MB1DIp3AsslnSspHHAlcDyPmWWA+8opn8T+OFg/QlmZlY7NWs+KvoIrgNuA5qBL0XEakkfAVZGxHLgJuCrktYCL5CJYzA31irmBuH9H7vG8r6D939Y9l8+MTczsxLf0WxmZmVOCmZmVjZik8JYHyKjiv3/I0kPSXpA0g8kVXUNciMYbN8ryr1RUkgaVZcpVrP/kq4ofv+rJX1tuGOspSr+9l8m6XZJq4q//0vrEWctSPqSpOckPXiQ9ZL098V384CkM4Y8iIgYcS+yY/px4OXAOOB+4OQ+ZX4P+FwxfSXwL/WOe5j3/78AE4vp94yW/a9m34tyU4AVwN3AsnrHPcy/+8XAKmB6MT+n3nEP8/7fCLynmD4ZeKLecQ/h/p8PnAE8eJD1lwLfAQScA9wz1DGM1JpCeYiMiNgLlIbIqHQ58JVi+pvARZI0jDHW0qD7HxG3R8TuYvZu8j6Q0aCa3z3AR8mxsjqGM7hhUM3+XwPcEBHbACLiuWGOsZaq2f8AphbT04BnhjG+moqIFeSVmAdzOfCPke4GjpB01FDGMFKTQn9DZMw7WJmI6AZKQ2SMBtXsf6V3kWcPo8Gg+15UmRdExH8MZ2DDpJrf/fHA8ZJ+IuluSRcPW3S1V83+Xw+8TdIG4FbgD4YntBHhUI8Nh6whhrmwg5P0NmAZcEG9YxkOkpqATwFX1zmUemohm5AuJGuIKySdEhHb6xrV8LkK+HJEfFLSueS9TksjoqfegY0GI7WmMNaHyKhm/5H0GuBPgMsionOYYqu1wfZ9CrAU+JGkJ8h21eWjqLO5mt/9BmB5RHRFxHrgUTJJjAbV7P+7gFsAIuIuoA2YNSzR1V9Vx4bDMVKTwlgfImPQ/Zd0OvB5MiGMpjblAfc9InZExKyIWBgRC8n+lMsiYmV9wh1y1fztf4usJSBpFtmctG44g6yhavb/KeAiAEknkUlhy7BGWT/LgbcXVyGdA+yIiMMZOfUAI7L5KGo3REZDqHL//xqYDHyj6F9/KiIuq1vQQ6TKfR+1qtz/24DXSXoI2Ad8ICJGRS25yv1/P/AFSf+V7HS+erScEEq6mUz4s4o+kz8DWgEi4nNkH8qlwFpgN/DOIY9hlHyXZmY2BEZq85GZmdWBk4KZmZU5KZiZWZmTgpmZlTkpmJlZmZOCWR+S9km6T9KDkv5d0hFDvP2rJX26mL5e0h8P5fbNDoeTgtmB9kTEaRGxlLwH5vfrHZDZcHFSMBvYXVQMOCbpA5LuLcay//OK5W8vlt0v6avFsl8rnvWxStL3JR1Zh/jNDsmIvKPZbCSQ1EwOp3BTMf86coyhV5Lj2S+XdD455taHgF+KiK2SZhSb+DFwTkSEpN8B/ht5N67ZiOWkYHagCZLuI2sIDwP/r1j+uuK1qpifTCaJU4FvRMRWgIgojYc/H/iXYrz7ccD64Qnf7KVz85HZgfZExGnAMWSNoNSnIOAviv6G0yJiUUTcNMB2/jfw6Yg4BfhdcuA2sxHNScHsIIon2/0h8P5iePbbgN+WNBlA0jxJc4AfAm+SNLNYXmo+mkbvsMbvwKwBuPnIbAARsUrSA8BVEfHVYqjmu4qRaXcBbytG8fw4cIekfWTz0tXkE8K+IWkbmTiOrcc+mB0Kj5JqZmZlbj4yM7MyJwUzMytzUjAzszInBTMzK3NSMDOzMicFMzMrc1IwM7Oy/w8k78FP692QVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## smooth LFs #other configs\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: ls_*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    def ints(y):\n",
    "        return alphas+((tf.exp((t_k*y)*(1-alphas))-1)/(t_k*y))\n",
    "    \n",
    "    print(\"ints\",ints)\n",
    "    \n",
    "#     zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                   np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(0.0000001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#     train_step = tf.train.MomentumOptimizer(0.0000001,0.002).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "           \n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            drawPRcurve(np.array(gold_labels_dev),np.array(m[1::].flatten()),it)\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-weight: 0.0\n",
      "0 loss -1389905.435892547\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 1.55314453e+00 -8.50306967e+00  6.63025697e+01  5.69882534e+01\n",
      "  -1.21856102e+01  1.73452451e+02  1.10987785e-01  6.05973130e+01\n",
      "   5.55920061e+01  1.63989427e+01]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.534\n",
      "Neg. class accuracy: 0.927\n",
      "Precision            0.345\n",
      "Recall               0.534\n",
      "F1                   0.419\n",
      "----------------------------------------\n",
      "TP: 101 | FP: 192 | TN: 2433 | FN: 88\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.900497512437811\n",
      "(array([0.96509322, 0.3447099 ]), array([0.92685714, 0.53439153]), array([0.94558881, 0.41908714]), array([2625,  189]))\n",
      "(0.6549015572941557, 0.7306243386243386, 0.6823379718848624, None)\n",
      "[[2433  192]\n",
      " [  88  101]]\n",
      "prec: tp/(tp+fp) 0.3447098976109215 recall: tp/(tp+fn) 0.5343915343915344\n",
      "(0.3447098976109215, 0.5343915343915344, 0.4190871369294606, None)\n",
      "z-weight: 0.1111111111111111\n",
      "0 loss -1000437.1982126305\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[  5.44463511   4.92828952  68.3755654   57.74597935   4.31089735\n",
      "  159.13217342   4.35300309  -2.29900223  -2.34528548  -4.2143449 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 0.2222222222222222\n",
      "0 loss -706893.8379170188\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[  3.48935945   3.64977188  62.36537449  51.13269642   3.51019556\n",
      "  140.23218683   3.26200745  -2.63824574  -2.69232002  -3.57701276]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 0.3333333333333333\n",
      "0 loss -441329.3626789301\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[  2.60448977   2.57288613  53.95040729  42.04799799   2.49713579\n",
      "  116.36014164   2.60694953  -2.27229424  -2.34146449  -2.63106009]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 0.4444444444444444\n",
      "0 loss -218104.77948797474\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 2.16338236  2.11878902 42.00014609 29.26101301  2.03164064 87.9603782\n",
      "   2.17627209 -1.85351314 -1.94616762 -2.13383894]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 0.5555555555555556\n",
      "0 loss -54711.69309558433\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 1.91198629  1.87386207 27.14533017 14.45505704  1.79166598 56.27769755\n",
      "   1.92172604 -1.6088025  -1.71071947 -1.87925477]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 0.6666666666666666\n",
      "0 loss 39138.140043263535\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 1.7106152   1.67805026  9.28278821  4.23247638  1.60324542 26.59673277\n",
      "   1.71790132 -1.42611098 -1.52294653 -1.68124332]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 0.7777777777777777\n",
      "0 loss 85802.78999218742\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 1.15978603  1.14153428  2.17093924  1.88471541  1.08650701 12.84175112\n",
      "   1.16326949 -0.92265452 -1.00711048 -1.13788444]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 0.8888888888888888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 118270.01096733002\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.6833814   0.67489471  1.16327265  1.08791289  0.63841563  7.31378212\n",
      "   0.68492701 -0.4667125  -0.56059337 -0.66732256]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "z-weight: 1.0\n",
      "0 loss 144103.45632156145\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183281  0.3968114   0.69874604  0.66187815  0.37243063  4.19841686\n",
      "   0.40312403 -0.18403703 -0.29869983 -0.39304135]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## varying z weight \n",
    "\n",
    "def train(z_weight):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        \n",
    "        zw = tf.convert_to_tensor(z_weight,dtype=tf.float64)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "#         print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - zw*logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "\n",
    "for w in np.linspace(0,1,10):\n",
    "    print()\n",
    "    print(\"z-weight:\",w)\n",
    "    train(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 177328.68299044456\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.54\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.354\n",
      "Recall               0.54\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 102 | FP: 186 | TN: 2439 | FN: 87\n",
      "========================================\n",
      "\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n"
     ]
    }
   ],
   "source": [
    "# set learning rate 0.1/len(train_L_S)  --marked\n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "#         print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "      \n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#         reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#         print(\"reg\",reg_losses)\n",
    "\n",
    "#         totalloss = normloss + tf.reduce_sum(reg_losses)\n",
    "        \n",
    "#         global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#         starter_learning_rate = 0.1/len(train_L_S)\n",
    "#         learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "#                                                10, 0.95)\n",
    "\n",
    "\n",
    "#         train_step = tf.train.AdamOptimizer(learning_rate).minimize(totalloss, global_step=global_step) \n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(0.1/len(train_L_S)).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(5):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "#             cf = confusion_matrix(gold_labels_dev,pl)\n",
    "#             print(cf)\n",
    "#             print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 177328.68299044456\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "5 loss 158604.93738094255\n",
      "[[0.54997522 0.38551056 0.71208165 0.68224501 0.60391497 1.3682661\n",
      "  0.58642215 0.89307    0.94179888 0.7020998 ]]\n",
      "(0.35051546391752575, 0.5396825396825397, 0.42499999999999993, None)\n",
      "\n",
      "6 loss 157032.75110433032\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "7 loss 155668.8970006794\n",
      "[[0.4314503  0.38839979 0.70424996 0.67037562 0.52367733 1.4280809\n",
      "  0.42606297 0.77043883 0.80142626 0.51281886]]\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "8 loss 154481.67664693782\n",
      "[[0.39067389 0.37902675 0.69806429 0.66205327 0.48793706 1.45887735\n",
      "  0.37175861 0.71320089 0.73512861 0.4199123 ]]\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "9 loss 153446.87729269065\n",
      "[[0.3546431  0.36275345 0.68942781 0.65117459 0.45284473 1.49068915\n",
      "  0.33247148 0.65941213 0.67218014 0.32889969]]\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n",
      "\n",
      "[[0.3546431  0.36275345 0.68942781 0.65117459 0.45284473 1.49068915\n",
      "  0.33247148 0.65941213 0.67218014 0.32889969]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.561\n",
      "Neg. class accuracy: 0.921\n",
      "Precision            0.339\n",
      "Recall               0.561\n",
      "F1                   0.422\n",
      "----------------------------------------\n",
      "TP: 106 | FP: 207 | TN: 2418 | FN: 83\n",
      "========================================\n",
      "\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n"
     ]
    }
   ],
   "source": [
    "# set learning rate 0.1/len(train_L_S), run 10 iterations\n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "#         print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "      \n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#         reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#         print(\"reg\",reg_losses)\n",
    "\n",
    "#         totalloss = normloss + tf.reduce_sum(reg_losses)\n",
    "        \n",
    "#         global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#         starter_learning_rate = 0.1/len(train_L_S)\n",
    "#         learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "#                                                10, 0.95)\n",
    "\n",
    "\n",
    "#         train_step = tf.train.AdamOptimizer(learning_rate).minimize(totalloss, global_step=global_step) \n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(0.1/len(train_L_S)).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(10):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "#             cf = confusion_matrix(gold_labels_dev,pl)\n",
    "#             print(cf)\n",
    "#             print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 177328.68299044456\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "5 loss 158604.93738094255\n",
      "[[0.54997522 0.38551056 0.71208165 0.68224501 0.60391497 1.3682661\n",
      "  0.58642215 0.89307    0.94179888 0.7020998 ]]\n",
      "(0.35051546391752575, 0.5396825396825397, 0.42499999999999993, None)\n",
      "\n",
      "6 loss 157032.75110433032\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "7 loss 155668.8970006794\n",
      "[[0.4314503  0.38839979 0.70424996 0.67037562 0.52367733 1.4280809\n",
      "  0.42606297 0.77043883 0.80142626 0.51281886]]\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "8 loss 154481.67664693782\n",
      "[[0.39067389 0.37902675 0.69806429 0.66205327 0.48793706 1.45887735\n",
      "  0.37175861 0.71320089 0.73512861 0.4199123 ]]\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "9 loss 153446.87729269065\n",
      "[[0.3546431  0.36275345 0.68942781 0.65117459 0.45284473 1.49068915\n",
      "  0.33247148 0.65941213 0.67218014 0.32889969]]\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n",
      "\n",
      "10 loss 152548.81576950828\n",
      "[[0.3228263  0.34281758 0.67875204 0.63822859 0.41888411 1.52333766\n",
      "  0.3020918  0.60913597 0.61277898 0.24064353]]\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 151777.05782060773\n",
      "[[0.29540707 0.32165781 0.66655386 0.62380273 0.38668169 1.55659623\n",
      "  0.27742105 0.56230756 0.55701578 0.15647785]]\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss 151121.9679241542\n",
      "[[0.27235149 0.30085188 0.65330747 0.6084251  0.35673569 1.59025571\n",
      "  0.25725406 0.51880115 0.50491631 0.07840616]]\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss 150572.4023227708\n",
      "[[0.25361337 0.28149093 0.63946432 0.5925888  0.32946785 1.62411754\n",
      "  0.24130367 0.47842506 0.45642398 0.00909516]]\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "14 loss 150114.45362566956\n",
      "[[ 0.23919651  0.2643783   0.62546829  0.57676995  0.30525161  1.65798876\n",
      "   0.22955256  0.44092173  0.41138899 -0.0487575 ]]\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n",
      "\n",
      "[[ 0.23919651  0.2643783   0.62546829  0.57676995  0.30525161  1.65798876\n",
      "   0.22955256  0.44092173  0.41138899 -0.0487575 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.916\n",
      "Precision            0.326\n",
      "Recall               0.566\n",
      "F1                   0.414\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 221 | TN: 2404 | FN: 82\n",
      "========================================\n",
      "\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n"
     ]
    }
   ],
   "source": [
    "# set learning rate 0.1/len(train_L_S), run 15 iterations\n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "#         print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "      \n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#         reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#         print(\"reg\",reg_losses)\n",
    "\n",
    "#         totalloss = normloss + tf.reduce_sum(reg_losses)\n",
    "        \n",
    "#         global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#         starter_learning_rate = 0.1/len(train_L_S)\n",
    "#         learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "#                                                10, 0.95)\n",
    "\n",
    "\n",
    "#         train_step = tf.train.AdamOptimizer(learning_rate).minimize(totalloss, global_step=global_step) \n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(0.1/len(train_L_S)).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(15):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "#             cf = confusion_matrix(gold_labels_dev,pl)\n",
    "#             print(cf)\n",
    "#             print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 144103.45632156145\n",
      "[[ 0.40183281  0.3968114   0.69874604  0.66187815  0.37243063  4.19841686\n",
      "   0.40312403 -0.18403703 -0.29869983 -0.39304135]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "1 loss 143856.8505697892\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "2 loss 143856.85044932846\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "3 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "4 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "5 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "6 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "7 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "8 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "9 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "10 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "11 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "12 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "13 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "14 loss 143856.850449328\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "# set learning rate 0.01, run 15 iterations\n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "#         print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "      \n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#         reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#         print(\"reg\",reg_losses)\n",
    "\n",
    "#         totalloss = normloss + tf.reduce_sum(reg_losses)\n",
    "        \n",
    "#         global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#         starter_learning_rate = 0.1/len(train_L_S)\n",
    "#         learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "#                                                10, 0.95)\n",
    "\n",
    "\n",
    "#         train_step = tf.train.AdamOptimizer(learning_rate).minimize(totalloss, global_step=global_step) \n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(15):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "#             print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "#             cf = confusion_matrix(gold_labels_dev,pl)\n",
    "#             print(cf)\n",
    "#             print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ccb9ea3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ccb9ea3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ccb9ea3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 153003.94588533268\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.13442    0.12574913 0.16954309 0.16251167 0.11668317 0.73244594\n",
      "  0.1351565  0.13010459 0.10139554 0.0060327 ]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3282208588957055, 0.5661375661375662, 0.4155339805825243, None)\n",
      "\n",
      "1 loss 152646.04605004212\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.12330404  0.12198669  0.18282961  0.17390827  0.1147601   0.78123146\n",
      "   0.11924576  0.12021881  0.08790548 -0.03962962]]\n",
      "{0: 2485, 1: 329}\n",
      "(0.3252279635258359, 0.5661375661375662, 0.4131274131274132, None)\n",
      "\n",
      "2 loss 152422.96683145966\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.11828394  0.11783191  0.19277459  0.18196136  0.11177749  0.82989491\n",
      "   0.1159449   0.11244512  0.07728241 -0.06754417]]\n",
      "{0: 2475, 1: 339}\n",
      "(0.31563421828908556, 0.5661375661375662, 0.40530303030303033, None)\n",
      "\n",
      "3 loss 152208.89081903518\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.11829031  0.11593551  0.20154064  0.18898966  0.10996611  0.87811366\n",
      "   0.11714622  0.10580061  0.06830557 -0.08437371]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "4 loss 151997.6442902714\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.12057198  0.11600648  0.2098237   0.19569857  0.10949831  0.92576698\n",
      "   0.11999464  0.09988879  0.06045043 -0.09513655]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "5 loss 151787.80605201106\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.12378746  0.11741665  0.21787371  0.20231517  0.11012214  0.97279923\n",
      "   0.12346672  0.09450903  0.05343491 -0.10266716]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "6 loss 151579.44330749047\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.1273702   0.11966727  0.22578245  0.20890451  0.11155022  1.01918021\n",
      "   0.1271678   0.08954077  0.04708091 -0.10844932]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "7 loss 151373.02440227015\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.13108411  0.1224267   0.23358317  0.21547669  0.11354951  1.06489106\n",
      "   0.13093837  0.0849007   0.04126168 -0.11325942]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "8 loss 151169.08883022165\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.13482772  0.12548647  0.2412866   0.22202398  0.11594795  1.10991901\n",
      "   0.13470972  0.08052599  0.03588043 -0.11751151]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "9 loss 150968.14576807074\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.13855473  0.12871744  0.24889456  0.22853394  0.11862234  1.1542553\n",
      "   0.13844962  0.07636702  0.0308605  -0.12143095]]\n",
      "{0: 2462, 1: 352}\n",
      "(0.3039772727272727, 0.5661375661375662, 0.39556377079482435, None)\n",
      "\n",
      "10 loss 150770.64294441266\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.14224227  0.13203928  0.25640534  0.23499395  0.12148507  1.19789414\n",
      "   0.14214139  0.07238376  0.02614005 -0.1251424 ]]\n",
      "{0: 2462, 1: 352}\n",
      "(0.3039772727272727, 0.5661375661375662, 0.39556377079482435, None)\n",
      "\n",
      "11 loss 150576.95895242243\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.14587791  0.13540138  0.26381593  0.24139269  0.12447353  1.24083226\n",
      "   0.1457755   0.06854362  0.02166882 -0.12871551]]\n",
      "{0: 2462, 1: 352}\n",
      "(0.3039772727272727, 0.5661375661375662, 0.39556377079482435, None)\n",
      "\n",
      "12 loss 150387.40413078957\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.14945437  0.13877125  0.27112304  0.24772047  0.12754259  1.28306859\n",
      "   0.14934613  0.06482005  0.01740581 -0.13218925]]\n",
      "{0: 2462, 1: 352}\n",
      "(0.3039772727272727, 0.5661375661375662, 0.39556377079482435, None)\n",
      "\n",
      "13 loss 150202.2248266979\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.15296712  0.14212761  0.27832353  0.25396923  0.13065938  1.32460395\n",
      "   0.15284963  0.06119145  0.01331754 -0.1355853 ]]\n",
      "{0: 2462, 1: 352}\n",
      "(0.3039772727272727, 0.5661375661375662, 0.39556377079482435, None)\n",
      "\n",
      "14 loss 150021.6090182211\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.15641334  0.14545627  0.28541459  0.26013241  0.13379966  1.3654409\n",
      "   0.15628376  0.05764032  0.00937666 -0.13891555]]\n",
      "{0: 2462, 1: 352}\n",
      "(0.3039772727272727, 0.5661375661375662, 0.39556377079482435, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.15641334  0.14545627  0.28541459  0.26013241  0.13379966  1.3654409\n",
      "   0.15628376  0.05764032  0.00937666 -0.13891555]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.907\n",
      "Precision            0.304\n",
      "Recall               0.566\n",
      "F1                   0.396\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 245 | TN: 2380 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2462, 1: 352}\n",
      "acc 0.8837953091684435\n",
      "(array([0.96669374, 0.30397727]), array([0.90666667, 0.56613757]), array([0.9357185 , 0.39556377]), array([2625,  189]))\n",
      "(0.6353355088250499, 0.7364021164021164, 0.6656411344636595, None)\n",
      "[[2380  245]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3039772727272727 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3039772727272727, 0.5661375661375662, 0.39556377079482435, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with snorkel thetas and train lr 0.1/len(train_L_S)\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[0.07472098,\\\n",
    "                            0.07514459,  0.11910277,0.11186369,0.07306518,0.69216714,\\\n",
    "                            0.07467749,0.16012659, 0.13682546,0.08183363]])),\\\n",
    "                            dtype=tf.float64)\n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.1/len(train_L_S)).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(15):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c802230b8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c802230b8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c802230b8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 144059.2118620104\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.4018328   0.39681139  0.69874603  0.66187815  0.37243063  4.19841682\n",
      "   0.40312402 -0.18403703 -0.29869982 -0.39304135]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "1 loss 143856.85057093657\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "2 loss 143856.85044932846\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "3 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "4 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "5 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "6 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "7 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "8 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "9 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "10 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "11 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "12 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "13 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "14 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with snorkel thetas and train lr 0.01\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[0.07472098,\\\n",
    "                            0.07514459,  0.11910277,0.11186369,0.07306518,0.69216714,\\\n",
    "                            0.07467749,0.16012659, 0.13682546,0.08183363]])),\\\n",
    "                            dtype=tf.float64)\n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(15):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 144095.64128461378\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.4018328   0.3968114   0.69874604  0.66187815  0.37243063  4.19841685\n",
      "   0.40312403 -0.18403703 -0.29869983 -0.39304135]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "1 loss 143856.85056997798\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "2 loss 143856.85044932846\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "3 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "4 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with old network thetas and train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[1.0,1.0,1.0,\\\n",
    "                             1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]])),\\\n",
    "                    dtype=tf.float64)\n",
    "                             \n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7f03d278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7f03d278>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7f03d278>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 175275.6814735655\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.90127828 0.90195067 0.91350089 0.91134562 0.9025506  1.10751084\n",
      "  0.90111003 1.07146054 1.05045067 1.06563759]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 168637.70927636613\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.80330076 0.80510356 0.83558855 0.83009435 0.80684569 1.17871899\n",
      "  0.8028484  1.08257735 1.05684583 1.0239013 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 164210.74424741804\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.70623106 0.71033036 0.77314068 0.76285001 0.71463531 1.23812979\n",
      "  0.70520303 1.0551735  1.02092044 0.93690229]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 161177.37476752856\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.61134109 0.62111844 0.73326168 0.71766302 0.63142406 1.28493919\n",
      "  0.6088062  1.00500967 0.96278816 0.84293452]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 159032.32019255578\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.52413018 0.54797009 0.71545462 0.69566016 0.56857192 1.32152685\n",
      "  0.51644194 0.94461342 0.89561784 0.74749777]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "5 loss 157400.9998213487\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.45991489 0.500905   0.7096552  0.68679916 0.5287394  1.3527859\n",
      "  0.44041014 0.88131391 0.82622595 0.65213805]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "6 loss 156036.5090475551\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.41687846 0.46564352 0.70511517 0.67943862 0.49657377 1.38344917\n",
      "  0.39203674 0.81948928 0.75855728 0.55759824]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "7 loss 154835.4980578535\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.38020337 0.4317733  0.69824669 0.66969011 0.46467657 1.41510106\n",
      "  0.3564875  0.76066558 0.69404478 0.46424328]]\n",
      "{0: 2519, 1: 295}\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n",
      "\n",
      "8 loss 153775.92212339625\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34747099 0.3990684  0.68914614 0.65769771 0.43307227 1.44769316\n",
      "  0.3261805  0.70506517 0.63299308 0.37251423]]\n",
      "{0: 2519, 1: 295}\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n",
      "\n",
      "9 loss 152849.76610468762\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3184846  0.36816165 0.67824434 0.64395605 0.40237908 1.4810294\n",
      "  0.29968831 0.65274113 0.57555962 0.28311117]]\n",
      "{0: 2501, 1: 313}\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n",
      "\n",
      "10 loss 152049.43972991276\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.29303674 0.33947731 0.66593966 0.62891312 0.37307948 1.51492752\n",
      "  0.27656747 0.60370175 0.52185227 0.19713172]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 151366.50267830203\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.27102445 0.31333076 0.6526126  0.61298874 0.34557563 1.54921382\n",
      "  0.25676892 0.55790052 0.47191027 0.11626145]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss 150790.98204181276\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.25252664 0.29001934 0.63864975 0.59660153 0.32024725 1.5837141\n",
      "  0.24048158 0.51522116 0.42568028 0.04288963]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss 150310.46632411194\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.23775316  0.26985053  0.62445489  0.58018169  0.29747197  1.61825004\n",
      "   0.22797545  0.47547096  0.38300259 -0.0201966 ]]\n",
      "{0: 2486, 1: 328}\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n",
      "\n",
      "14 loss 149909.63291909563\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.22683191  0.25307285  0.6104294   0.56415043  0.27757831  1.65264788\n",
      "   0.21930361  0.43839923  0.34362788 -0.07088195]]\n",
      "{0: 2486, 1: 328}\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.22683191  0.25307285  0.6104294   0.56415043  0.27757831  1.65264788\n",
      "   0.21930361  0.43839923  0.34362788 -0.07088195]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.916\n",
      "Precision            0.326\n",
      "Recall               0.566\n",
      "F1                   0.414\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 221 | TN: 2404 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2486, 1: 328}\n",
      "acc 0.8923240938166311\n",
      "(array([0.96701529, 0.32621951]), array([0.91580952, 0.56613757]), array([0.9407161, 0.4139265]), array([2625,  189]))\n",
      "(0.6466173988972391, 0.7409735449735451, 0.6773213007784249, None)\n",
      "[[2404  221]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.32621951219512196 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with old network thetas and train and lr 0.1/len(train_L_S)\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[1.0,1.0,1.0,\\\n",
    "                             1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]])),\\\n",
    "                    dtype=tf.float64)\n",
    "                             \n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.1/len(train_L_S)).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(15):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 153548.14977017298\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.convert_to_tensor(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "            0.11186369,0.07306518,0.69216714,0.07467749,0.16012659, 0.13682546,0.08183363]]))\n",
    "\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(1):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "#                     _,ls = sess.run([train_step,normloss])\n",
    "                    ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 179552.66617224828\n",
      "[[1.         1.         1.         1.         1.         1.02750979\n",
      "  1.         1.0218145  1.         1.        ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.         1.         1.         1.         1.         1.02750979\n",
      "  1.         1.0218145  1.         1.        ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.545\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.353\n",
      "Recall               0.545\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 103 | FP: 189 | TN: 2436 | FN: 86\n",
      "========================================\n",
      "\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(array([0.96590008, 0.35273973]), array([0.928     , 0.54497354]), array([0.94657082, 0.42827443]), array([2625,  189]))\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "[[2436  189]\n",
      " [  86  103]]\n",
      "prec: tp/(tp+fp) 0.3527397260273973 recall: tp/(tp+fn) 0.544973544973545\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "## Objective value on thetas from old network\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.convert_to_tensor(np.array([[1.0,1.0,1.0,1.0,1.0,1.02750979,\\\n",
    "                             1.0,1.0218145,1.0,1.0]]))\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(1):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "#                     _,ls = sess.run([train_step,normloss])\n",
    "                    ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 145767.7856663791\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "1 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "2 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "3 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "4 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "5 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "6 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "7 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "8 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "9 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "10 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "11 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "12 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "13 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "14 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "15 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "16 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "17 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "18 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "19 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "21 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "22 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "23 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "24 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "25 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "26 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "27 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "28 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "29 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "acc 0.7949538024164889\n",
      "(array([0.93353091, 0.07079646]), array([0.84      , 0.16931217]), array([0.88429918, 0.09984399]), array([2625,  189]))\n",
      "(0.5021636830944227, 0.5046560846560846, 0.49207158581109633, None)\n",
      "[[2205  420]\n",
      " [ 157   32]]\n",
      "prec: tp/(tp+fp) 0.07079646017699115 recall: tp/(tp+fn) 0.1693121693121693\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n"
     ]
    }
   ],
   "source": [
    "## same network that didn't train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(30):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 145767.7856663791\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "1 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "2 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "3 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "4 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "5 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "6 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "7 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "8 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "9 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "10 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "11 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "12 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "13 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "14 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "15 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "16 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "17 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "18 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "19 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "21 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "22 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "23 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "24 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "25 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "26 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "27 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "28 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "29 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "acc 0.7949538024164889\n",
      "(array([0.93353091, 0.07079646]), array([0.84      , 0.16931217]), array([0.88429918, 0.09984399]), array([2625,  189]))\n",
      "(0.5021636830944227, 0.5046560846560846, 0.49207158581109633, None)\n",
      "[[2205  420]\n",
      " [ 157   32]]\n",
      "prec: tp/(tp+fp) 0.07079646017699115 recall: tp/(tp+fn) 0.1693121693121693\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n"
     ]
    }
   ],
   "source": [
    "## same network that didn't train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(30):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 7 4 1 0 6 3 2 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([ 0.4751682, 0.46430319 , 0.77729748 , 0.69961045 , 0.43660742,  4.98316919,\n",
    "   0.4786732 , -0.29070728, -0.31361022, -0.41560446])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00024597518522208474\n",
      "[1.         1.         1.         1.         1.         1.00531229\n",
      " 1.         1.         1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "0 -0.9839007408883389\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002477972163026618\n",
      "[1.         1.         1.         1.         1.         1.01071759\n",
      " 1.         1.00531229 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "1 -0.9911888652106471\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.00024967684220414605\n",
      "[1.         1.         1.         1.         1.         1.01621772\n",
      " 1.         1.01071759 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "2 -0.9987073688165843\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002516160600819831\n",
      "[1.         1.         1.         1.         1.         1.0218145\n",
      " 1.         1.01621772 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "3 -1.0064642403279322\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002536169307750545\n",
      "[1.         1.         1.         1.         1.         1.02750979\n",
      " 1.         1.0218145  1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "4 -1.014467723100218\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00024597518522208474\n",
      "[1.         1.         1.         1.         1.         1.00531229\n",
      " 1.         1.         1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "0 -0.9839007408883389\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002477972163026618\n",
      "[1.         1.         1.         1.         1.         1.01071759\n",
      " 1.         1.00531229 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "1 -0.9911888652106471\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.00024967684220414605\n",
      "[1.         1.         1.         1.         1.         1.01621772\n",
      " 1.         1.01071759 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "2 -0.9987073688165843\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002516160600819831\n",
      "[1.         1.         1.         1.         1.         1.0218145\n",
      " 1.         1.01621772 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "3 -1.0064642403279322\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002536169307750545\n",
      "[1.         1.         1.         1.         1.         1.02750979\n",
      " 1.         1.0218145  1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "4 -1.014467723100218\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network 2 to get thetas \n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
