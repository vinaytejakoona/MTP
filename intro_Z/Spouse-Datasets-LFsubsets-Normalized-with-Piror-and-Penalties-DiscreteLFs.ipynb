{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n",
    "print(len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 2625\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Discrete ##########\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,0)\n",
    "    \n",
    "def LF_familial_relationship(c):\n",
    "    return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x.decode('utf-8')).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,0)\n",
    "\n",
    "\n",
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Continuous ################\n",
    "\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "#     get_text_between, get_tagged_text,\n",
    "# )\n",
    "\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     global LF_Threshold\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for sw in spouses:\n",
    "#         sc=max(sc,get_similarity(word_vectors,sw))\n",
    "#     return (1,sc)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     global LF_Threshold\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for sw in spouses:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for sw in spouses:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "#     return(1,max(sc_1,sc_2))\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     global LF_Threshold\n",
    "#     word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "#     sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "#     if 'and' in get_between_tokens(c):\n",
    "#         return (1,sc)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_familial_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for fw in family:\n",
    "#         sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for fw in family:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for fw in family:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,max(sc_1,sc_2))\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for ow in other:\n",
    "#         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# # def LF_other_relationship_left_window(c):\n",
    "# #     sc = 0\n",
    "# #     word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "# #     for ow in other:\n",
    "# #         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "# #     return (-1,sc) \n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # # Read in known spouse pairs and save as set of tuples\n",
    "# # with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "# #     known_spouses = set(\n",
    "# #         tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "# #     )\n",
    "# # # Last name pairs for known spouses\n",
    "# # last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # def LF_Three_Lists_Left_Window(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife_left_window(c)\n",
    "# #     c2,s2 = LF_family_left_window(c)\n",
    "# #     c3,s3 = LF_other_relationship_left_window(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "\n",
    "# # def LF_Three_Lists_Between_Words(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife(c)\n",
    "# #     c2,s2 = LF_familial_relationship(c)\n",
    "# #     c3,s3 = LF_other_relationship(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "    \n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'BW:',s1,s2,s3,prob_sc\n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "    \n",
    "\n",
    "def drawLossVsF1(y_loss,x_f1s,text,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x_f1s, y_loss)\n",
    "\n",
    "    plt.xlabel('f1-score')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(title)\n",
    "    \n",
    "    for i, txt in enumerate(text):\n",
    "        ax.annotate(txt, (x_f1s[i],y_loss[i]))\n",
    "        \n",
    "    plt.savefig(title+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "(2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "LF_l = np.array([\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "])\n",
    "\n",
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))\n",
    "\n",
    "import numpy as np\n",
    "# dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def createSubsetOfDataAndLFs(train_L_S,dev_L_S,LF_l,data_subset_indices,LF_subset_indices):\n",
    "    global NoOfLFs\n",
    "    SizeOfDataSubSet = 10000\n",
    "    NoOfLFsInLFSubset = len(LF_subset_indices)\n",
    "    NoOfLFs = NoOfLFsInLFSubset\n",
    "#     train_L_S = train_L_S[data_subset_indices,:]\n",
    "    train_L_S = train_L_S[:,:,LF_subset_indices]\n",
    "    print(data_subset_indices.shape,LF_subset_indices.shape)\n",
    "    dev_L_S = dev_L_S[:,:,LF_subset_indices]\n",
    "    LF_l = LF_l[LF_subset_indices]\n",
    "    return train_L_S,dev_L_S,LF_l\n",
    "\n",
    "def train_nl_penalties(LF_subset_indices,lr=0.1/len(train_L_S),it=5,th=tf.truncated_normal_initializer(0,0.1,seed)):\n",
    "    global train_L_S,dev_L_S,LF_l\n",
    "    print(LF_subset_indices)\n",
    "    \n",
    "    data_subset_indices = np.array(random.sample(range(train_L_S.shape[0]), SizeOfDataSubSet))\n",
    "\n",
    "    train_L_S,dev_L_S,LF_l = createSubsetOfDataAndLFs(train_L_S,dev_L_S,LF_l,data_subset_indices,LF_subset_indices)\n",
    "    print(train_L_S.shape,dev_L_S.shape,LF_l.shape)\n",
    "\n",
    "    train_nl(lr,it,th) \n",
    "\n",
    "    train_nl_p(lr,it,th) \n",
    "\n",
    "    train_nl_p2(lr,it,th)\n",
    "\n",
    "    train_nl_p3(lr,it,th) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "seed = 12 \n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "(10000,) (10,)\n",
      "(22276, 2, 10) (2814, 2, 10) (10,)\n",
      "0 loss 154550.32722203954\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.03625473 -0.09201922  0.04087745  0.03838884  0.04881964  0.07631783\n",
      "   0.07385835  0.01965096  0.09410612 -0.01521934]]\n",
      "{0: 2492, 1: 322}\n",
      "(0.32298136645962733, 0.5502645502645502, 0.4070450097847358, None)\n",
      "\n",
      "1 loss 154394.2097362968\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.02376233 -0.00917943  0.04839996  0.04515221  0.02658253  0.12737847\n",
      "   0.02282324  0.01706548  0.05779753 -0.02330113]]\n",
      "{0: 2486, 1: 328}\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n",
      "\n",
      "2 loss 154320.63792672456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.02578081  0.02102967  0.05121235  0.04709643  0.02268402  0.18204747\n",
      "   0.02531712  0.01957862  0.03959555 -0.02290153]]\n",
      "{0: 2485, 1: 329}\n",
      "(0.3252279635258359, 0.5661375661375662, 0.4131274131274132, None)\n",
      "\n",
      "3 loss 154246.7173934166\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.03209256  0.02864914  0.05808933  0.05337554  0.02591457  0.23659913\n",
      "   0.03165313  0.02208744  0.03011233 -0.02747112]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "4 loss 154155.20364583188\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.03842209  0.03442046  0.0670751   0.06168969  0.03033175  0.29084456\n",
      "   0.03801262  0.02459282  0.02499466 -0.03254361]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.03842209  0.03442046  0.0670751   0.06168969  0.03033175  0.29084456\n",
      "   0.03801262  0.02459282  0.02499466 -0.03254361]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.907\n",
      "Precision            0.305\n",
      "Recall               0.566\n",
      "F1                   0.396\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 244 | TN: 2381 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2463, 1: 351}\n",
      "acc 0.8841506751954513\n",
      "(array([0.96670727, 0.3048433 ]), array([0.90704762, 0.56613757]), array([0.93592767, 0.3962963 ]), array([2625,  189]))\n",
      "(0.6357752862015956, 0.7365925925925927, 0.6661119846261356, None)\n",
      "[[2381  244]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.30484330484330485 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "0 loss 157574.7348655809\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 3.83943366e-02 -8.59176793e-02  4.12140554e-02  3.87969377e-02\n",
      "   4.94520347e-02  7.63448781e-02  7.43057986e-02  1.95670645e-02\n",
      "   9.40633695e-02  5.01784904e-06]]\n",
      "{0: 2492, 1: 322}\n",
      "(0.32919254658385094, 0.5608465608465608, 0.41487279843444225, None)\n",
      "\n",
      "1 loss 155215.51969439027\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.48005963e-02 2.43615010e-04 5.03492442e-02 4.70117231e-02\n",
      "  2.79224024e-02 1.26737596e-01 2.41703044e-02 1.56798558e-02\n",
      "  5.65577570e-02 8.61844142e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "2 loss 154322.58291672444\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.77687137e-02 2.25366351e-02 5.46633369e-02 5.03081359e-02\n",
      "  2.50382561e-02 1.80879939e-01 2.72792878e-02 1.69925746e-02\n",
      "  3.69843154e-02 2.72648967e-05]]\n",
      "{0: 2478, 1: 336}\n",
      "(0.31845238095238093, 0.5661375661375662, 0.4076190476190476, None)\n",
      "\n",
      "3 loss 154250.1440012092\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[3.43848963e-02 3.09483804e-02 6.19408185e-02 5.68594522e-02\n",
      "  2.84000132e-02 2.35087505e-01 3.40275329e-02 1.88589847e-02\n",
      "  2.68273317e-02 8.72434321e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "4 loss 154160.5610998261\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[4.12179267e-02 3.72599430e-02 7.14021436e-02 6.55816067e-02\n",
      "  3.32506935e-02 2.88978421e-01 4.08993327e-02 2.07114449e-02\n",
      "  2.11020578e-02 1.91588116e-05]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[4.12179267e-02 3.72599430e-02 7.14021436e-02 6.55816067e-02\n",
      "  3.32506935e-02 2.88978421e-01 4.08993327e-02 2.07114449e-02\n",
      "  2.11020578e-02 1.91588116e-05]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.908\n",
      "Precision            0.307\n",
      "Recall               0.566\n",
      "F1                   0.399\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 241 | TN: 2384 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2466, 1: 348}\n",
      "acc 0.8852167732764747\n",
      "(array([0.96674777, 0.30747126]), array([0.90819048, 0.56613757]), array([0.9365547 , 0.39851024]), array([2625,  189]))\n",
      "(0.6371095170176468, 0.7371640211640211, 0.66753247323297, None)\n",
      "[[2384  241]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3074712643678161 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b5d8f98>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b5d8f98>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b5d8f98>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 157574.4721340865\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.0354697  -0.08592353  0.0400991   0.03764402  0.04799072  0.07676354\n",
      "   0.07374134  0.02015336  0.09448555 -0.01471016]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.3271604938271605, 0.5608465608465608, 0.41325536062378165, None)\n",
      "\n",
      "1 loss 155235.79723322837\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.75151102e-02 8.71633543e-05 4.84614629e-02 4.53447049e-02\n",
      "  2.80230961e-02 1.27837740e-01 2.71435257e-02 1.73664396e-02\n",
      "  5.81361727e-02 2.60496767e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "2 loss 154321.87328954018\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.84728219e-02 2.24880551e-02 5.45227003e-02 5.03624325e-02\n",
      "  2.59301625e-02 1.81742097e-01 2.78787145e-02 1.77951298e-02\n",
      "  3.78663783e-02 2.14057427e-05]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3282208588957055, 0.5661375661375662, 0.4155339805825243, None)\n",
      "\n",
      "3 loss 154248.95583177728\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[3.46917178e-02 3.12286661e-02 6.20969624e-02 5.71151974e-02\n",
      "  2.88711714e-02 2.35915842e-01 3.43234542e-02 1.94356666e-02\n",
      "  2.74667415e-02 1.13359329e-05]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 loss 154159.0631150702\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 4.14473160e-02  3.74993703e-02  7.16429435e-02  6.58696507e-02\n",
      "   3.35547255e-02  2.89791883e-01  4.11253641e-02  2.11622138e-02\n",
      "   2.15834607e-02 -1.72112189e-07]]\n",
      "{0: 2463, 1: 351}\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 4.14473160e-02  3.74993703e-02  7.16429435e-02  6.58696507e-02\n",
      "   3.35547255e-02  2.89791883e-01  4.11253641e-02  2.11622138e-02\n",
      "   2.15834607e-02 -1.72112189e-07]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.907\n",
      "Precision            0.305\n",
      "Recall               0.566\n",
      "F1                   0.396\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 244 | TN: 2381 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2463, 1: 351}\n",
      "acc 0.8841506751954513\n",
      "(array([0.96670727, 0.3048433 ]), array([0.90704762, 0.56613757]), array([0.93592767, 0.3962963 ]), array([2625,  189]))\n",
      "(0.6357752862015956, 0.7365925925925927, 0.6661119846261356, None)\n",
      "[[2381  244]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.30484330484330485 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.30484330484330485, 0.5661375661375662, 0.3962962962962963, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d61710550>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d61710550>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d61710550>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 298660.34564764914\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.21653715 -0.08654023  0.10857346  0.09968567  0.20529357  0.14084332\n",
      "   0.26813308  0.13021673  0.2449088   0.16463597]]\n",
      "{0: 2504, 1: 310}\n",
      "(0.3387096774193548, 0.5555555555555556, 0.4208416833667335, None)\n",
      "\n",
      "1 loss 289042.0469504977\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.31502918 0.01238471 0.20573015 0.19709201 0.30337098 0.23893134\n",
      "  0.36658743 0.22742358 0.34142501 0.26439191]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "2 loss 280575.551214702\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.41259523 0.11077149 0.2981076  0.29018884 0.39932393 0.33562165\n",
      "  0.46428189 0.32275444 0.43593797 0.36389651]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "3 loss 273273.9007485553\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.50754095 0.20811311 0.38105607 0.37448033 0.48877859 0.43111459\n",
      "  0.55988146 0.41639413 0.52873351 0.46308132]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "4 loss 267086.37229447044\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.59068958 0.3034698  0.45034013 0.44536065 0.55887603 0.5254595\n",
      "  0.63923058 0.50834897 0.61981736 0.56181253]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.59068958 0.3034698  0.45034013 0.44536065 0.55887603 0.5254595\n",
      "  0.63923058 0.50834897 0.61981736 0.56181253]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.561\n",
      "Neg. class accuracy: 0.92\n",
      "Precision            0.335\n",
      "Recall               0.561\n",
      "F1                   0.42\n",
      "----------------------------------------\n",
      "TP: 106 | FP: 210 | TN: 2415 | FN: 83\n",
      "========================================\n",
      "\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(array([0.96677342, 0.33544304]), array([0.92      , 0.56084656]), array([0.94280695, 0.41980198]), array([2625,  189]))\n",
      "(0.6511082283548357, 0.7404232804232804, 0.6813044646256545, None)\n",
      "[[2415  210]\n",
      " [  83  106]]\n",
      "prec: tp/(tp+fp) 0.33544303797468356 recall: tp/(tp+fn) 0.5608465608465608\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n"
     ]
    }
   ],
   "source": [
    "# Use full dataset and all LFs\n",
    "LF_subset_indices = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "train_nl_penalties(LF_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "(10000,) (10,)\n",
      "(22276, 2, 10) (2814, 2, 10) (10,)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.54\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.354\n",
      "Recall               0.54\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 102 | FP: 186 | TN: 2439 | FN: 87\n",
      "========================================\n",
      "\n",
      "{0: 2526, 1: 288}\n",
      "acc 0.9029850746268657\n",
      "(array([0.96555819, 0.35416667]), array([0.92914286, 0.53968254]), array([0.94700058, 0.42767296]), array([2625,  189]))\n",
      "(0.6598624307205068, 0.7344126984126984, 0.6873367691930126, None)\n",
      "[[2439  186]\n",
      " [  87  102]]\n",
      "prec: tp/(tp+fp) 0.3541666666666667 recall: tp/(tp+fn) 0.5396825396825397\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.54\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.354\n",
      "Recall               0.54\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 102 | FP: 186 | TN: 2439 | FN: 87\n",
      "========================================\n",
      "\n",
      "{0: 2526, 1: 288}\n",
      "acc 0.9029850746268657\n",
      "(array([0.96555819, 0.35416667]), array([0.92914286, 0.53968254]), array([0.94700058, 0.42767296]), array([2625,  189]))\n",
      "(0.6598624307205068, 0.7344126984126984, 0.6873367691930126, None)\n",
      "[[2439  186]\n",
      " [  87  102]]\n",
      "prec: tp/(tp+fp) 0.3541666666666667 recall: tp/(tp+fn) 0.5396825396825397\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b0e4320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b0e4320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b0e4320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.54\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.354\n",
      "Recall               0.54\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 102 | FP: 186 | TN: 2439 | FN: 87\n",
      "========================================\n",
      "\n",
      "{0: 2526, 1: 288}\n",
      "acc 0.9029850746268657\n",
      "(array([0.96555819, 0.35416667]), array([0.92914286, 0.53968254]), array([0.94700058, 0.42767296]), array([2625,  189]))\n",
      "(0.6598624307205068, 0.7344126984126984, 0.6873367691930126, None)\n",
      "[[2439  186]\n",
      " [  87  102]]\n",
      "prec: tp/(tp+fp) 0.3541666666666667 recall: tp/(tp+fn) 0.5396825396825397\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b2de048>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b2de048>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b2de048>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 245068.43749177322\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02051008 0.72589385 0.95136584 0.93762739 1.01335664 1.12975561\n",
      "  1.07147025 1.1126708  1.22644629 1.15909523]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 240366.05178711584\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.93078459 0.71178833 0.93448554 0.91716702 0.93885492 1.21319719\n",
      "  0.97744814 1.17751891 1.2852895  1.24110288]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 238331.30684385338\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.90272714 0.77810441 0.95124265 0.93379064 0.92132506 1.29162979\n",
      "  0.91533631 1.22144329 1.31675336 1.27867582]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 237122.33535813304\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.91844167 0.83688517 0.97196485 0.95478206 0.92329816 1.3677358\n",
      "  0.9219655  1.25681765 1.3385256  1.29091086]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "4 loss 236085.88252938355\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.93017044 0.88011831 0.99141606 0.97430726 0.92808305 1.44206806\n",
      "  0.93240418 1.28751566 1.35616905 1.29462401]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.93017044 0.88011831 0.99141606 0.97430726 0.92808305 1.44206806\n",
      "  0.93240418 1.28751566 1.35616905 1.29462401]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.545\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.353\n",
      "Recall               0.545\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 103 | FP: 189 | TN: 2436 | FN: 86\n",
      "========================================\n",
      "\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(array([0.96590008, 0.35273973]), array([0.928     , 0.54497354]), array([0.94657082, 0.42827443]), array([2625,  189]))\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "[[2436  189]\n",
      " [  86  103]]\n",
      "prec: tp/(tp+fp) 0.3527397260273973 recall: tp/(tp+fn) 0.544973544973545\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# Use full dataset and all LFs\n",
    "LF_subset_indices = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "train_nl_penalties(LF_subset_indices,th=tf.truncated_normal_initializer(1,0.1,seed) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "(10000,) (10,)\n",
      "(22276, 2, 10) (2814, 2, 10) (10,)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.54\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.354\n",
      "Recall               0.54\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 102 | FP: 186 | TN: 2439 | FN: 87\n",
      "========================================\n",
      "\n",
      "{0: 2526, 1: 288}\n",
      "acc 0.9029850746268657\n",
      "(array([0.96555819, 0.35416667]), array([0.92914286, 0.53968254]), array([0.94700058, 0.42767296]), array([2625,  189]))\n",
      "(0.6598624307205068, 0.7344126984126984, 0.6873367691930126, None)\n",
      "[[2439  186]\n",
      " [  87  102]]\n",
      "prec: tp/(tp+fp) 0.3541666666666667 recall: tp/(tp+fn) 0.5396825396825397\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.54\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.354\n",
      "Recall               0.54\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 102 | FP: 186 | TN: 2439 | FN: 87\n",
      "========================================\n",
      "\n",
      "{0: 2526, 1: 288}\n",
      "acc 0.9029850746268657\n",
      "(array([0.96555819, 0.35416667]), array([0.92914286, 0.53968254]), array([0.94700058, 0.42767296]), array([2625,  189]))\n",
      "(0.6598624307205068, 0.7344126984126984, 0.6873367691930126, None)\n",
      "[[2439  186]\n",
      " [  87  102]]\n",
      "prec: tp/(tp+fp) 0.3541666666666667 recall: tp/(tp+fn) 0.5396825396825397\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b0e4320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b0e4320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b0e4320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.54\n",
      "Neg. class accuracy: 0.929\n",
      "Precision            0.354\n",
      "Recall               0.54\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 102 | FP: 186 | TN: 2439 | FN: 87\n",
      "========================================\n",
      "\n",
      "{0: 2526, 1: 288}\n",
      "acc 0.9029850746268657\n",
      "(array([0.96555819, 0.35416667]), array([0.92914286, 0.53968254]), array([0.94700058, 0.42767296]), array([2625,  189]))\n",
      "(0.6598624307205068, 0.7344126984126984, 0.6873367691930126, None)\n",
      "[[2439  186]\n",
      " [  87  102]]\n",
      "prec: tp/(tp+fp) 0.3541666666666667 recall: tp/(tp+fn) 0.5396825396825397\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b2de048>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b2de048>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b2de048>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 245068.43749177322\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02051008 0.72589385 0.95136584 0.93762739 1.01335664 1.12975561\n",
      "  1.07147025 1.1126708  1.22644629 1.15909523]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 240366.05178711584\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.93078459 0.71178833 0.93448554 0.91716702 0.93885492 1.21319719\n",
      "  0.97744814 1.17751891 1.2852895  1.24110288]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 238331.30684385338\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.90272714 0.77810441 0.95124265 0.93379064 0.92132506 1.29162979\n",
      "  0.91533631 1.22144329 1.31675336 1.27867582]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 237122.33535813304\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.91844167 0.83688517 0.97196485 0.95478206 0.92329816 1.3677358\n",
      "  0.9219655  1.25681765 1.3385256  1.29091086]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "4 loss 236085.88252938355\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.93017044 0.88011831 0.99141606 0.97430726 0.92808305 1.44206806\n",
      "  0.93240418 1.28751566 1.35616905 1.29462401]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.93017044 0.88011831 0.99141606 0.97430726 0.92808305 1.44206806\n",
      "  0.93240418 1.28751566 1.35616905 1.29462401]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.545\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.353\n",
      "Recall               0.545\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 103 | FP: 189 | TN: 2436 | FN: 86\n",
      "========================================\n",
      "\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(array([0.96590008, 0.35273973]), array([0.928     , 0.54497354]), array([0.94657082, 0.42827443]), array([2625,  189]))\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "[[2436  189]\n",
      " [  86  103]]\n",
      "prec: tp/(tp+fp) 0.3527397260273973 recall: tp/(tp+fn) 0.544973544973545\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# Use full dataset and all LFs\n",
    "LF_subset_indices = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "train_nl_penalties(LF_subset_indices,th=tf.truncated_normal_initializer(1,0.1,seed) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 7 8 9]\n",
      "(10000,) (6,)\n",
      "(10000, 2, 6) (2814, 2, 6) (6,)\n",
      "0 loss 41641.76504616807\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.04529264 -0.09905075  0.03875814 -0.02447436  0.03313103 -0.01251145]]\n",
      "{0: 2390, 1: 424}\n",
      "(0.22641509433962265, 0.5079365079365079, 0.31321370309951063, None)\n",
      "\n",
      "1 loss 41595.04026404794\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-1.12811658e-04 -1.85190406e-02  1.06663502e-02 -5.73847509e-03\n",
      "  -2.45000975e-03  3.99037312e-05]]\n",
      "{0: 2217, 1: 597}\n",
      "(0.1574539363484087, 0.4973544973544973, 0.23918575063613226, None)\n",
      "\n",
      "2 loss 41588.887845803336\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-2.36561654e-17 -4.50008643e-17 -4.28936544e-17  5.90739548e-17\n",
      "   6.11807467e-17  2.60414069e-17]]\n",
      "{0: 2814}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loss 41588.830833630505\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 3.10903081e-17  4.82762452e-17 -2.59143103e-17  4.91687711e-18\n",
      "  -2.88735145e-18  4.44713718e-17]]\n",
      "{0: 2799, 1: 15}\n",
      "(0.26666666666666666, 0.021164021164021163, 0.0392156862745098, None)\n",
      "\n",
      "4 loss 41588.830833634966\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 6.66033802e-17  1.14761024e-16  6.06776279e-17 -4.18142729e-17\n",
      "  -8.41424476e-17  3.50142153e-17]]\n",
      "{0: 2202, 1: 612}\n",
      "(0.1650326797385621, 0.5343915343915344, 0.25218476903870163, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 6.66033802e-17  1.14761024e-16  6.06776279e-17 -4.18142729e-17\n",
      "  -8.41424476e-17  3.50142153e-17]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0952\n",
      "Neg. class accuracy: 0.909\n",
      "Precision            0.07\n",
      "Recall               0.0952\n",
      "F1                   0.0807\n",
      "----------------------------------------\n",
      "TP: 18 | FP: 239 | TN: 2386 | FN: 171\n",
      "========================================\n",
      "\n",
      "{0: 2202, 1: 612}\n",
      "acc 0.7871357498223169\n",
      "(array([0.96003633, 0.16503268]), array([0.80533333, 0.53439153]), array([0.87590636, 0.25218477]), array([2625,  189]))\n",
      "(0.5625345051735499, 0.6698624338624339, 0.5640455645483543, None)\n",
      "[[2114  511]\n",
      " [  88  101]]\n",
      "prec: tp/(tp+fp) 0.1650326797385621 recall: tp/(tp+fn) 0.5343915343915344\n",
      "(0.1650326797385621, 0.5343915343915344, 0.25218476903870163, None)\n",
      "0 loss 43002.655596927034\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 5.05514970e-02 -8.63328855e-02  4.87706366e-02  4.88520190e-06\n",
      "   3.37271773e-02  5.53260207e-05]]\n",
      "{0: 2556, 1: 258}\n",
      "(0.3488372093023256, 0.47619047619047616, 0.40268456375838924, None)\n",
      "\n",
      "1 loss 41967.30351234111\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[5.58885697e-05 8.22785263e-05 1.16045885e-02 3.47465367e-03\n",
      "  4.56075547e-03 2.31505709e-03]]\n",
      "{0: 2545, 1: 269}\n",
      "(0.35687732342007433, 0.5079365079365079, 0.4192139737991266, None)\n",
      "\n",
      "2 loss 41588.84704491809\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[2.74844718e-04 2.60321001e-04 3.50275769e-05 2.20554809e-04\n",
      "  1.94582836e-04 1.03894496e-04]]\n",
      "{0: 2593, 1: 221}\n",
      "(0.38461538461538464, 0.4497354497354497, 0.41463414634146345, None)\n",
      "\n",
      "3 loss 41588.83105067487\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.35692987e-04 1.35431146e-04 1.73598114e-04 2.84856300e-04\n",
      "  2.81456656e-04 1.07065547e-05]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "4 loss 41588.83100874175\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[2.76215500e-05 2.70155816e-05 3.52194591e-05 5.31472088e-05\n",
      "  4.13025071e-05 1.72126112e-05]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[2.76215500e-05 2.70155816e-05 3.52194591e-05 5.31472088e-05\n",
      "  4.13025071e-05 1.72126112e-05]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.455\n",
      "Neg. class accuracy: 0.947\n",
      "Precision            0.384\n",
      "Recall               0.455\n",
      "F1                   0.416\n",
      "----------------------------------------\n",
      "TP: 86 | FP: 138 | TN: 2487 | FN: 103\n",
      "========================================\n",
      "\n",
      "{0: 2590, 1: 224}\n",
      "acc 0.9143567874911158\n",
      "(array([0.96023166, 0.38392857]), array([0.94742857, 0.45502646]), array([0.95378715, 0.41646489]), array([2625,  189]))\n",
      "(0.6720801158301158, 0.7012275132275132, 0.6851260217430164, None)\n",
      "[[2487  138]\n",
      " [ 103   86]]\n",
      "prec: tp/(tp+fp) 0.38392857142857145 recall: tp/(tp+fn) 0.455026455026455\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5bffff60>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5bffff60>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5bffff60>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 43000.63130243052\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.04446025 -0.08636132  0.03611076 -0.02164246  0.03420167 -0.0099945 ]]\n",
      "{0: 2390, 1: 424}\n",
      "(0.22641509433962265, 0.5079365079365079, 0.31321370309951063, None)\n",
      "\n",
      "1 loss 41966.733989915076\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[3.81553795e-05 7.87308348e-05 3.33471024e-03 2.95218112e-04\n",
      "  1.54727638e-03 8.43212793e-04]]\n",
      "{0: 2545, 1: 269}\n",
      "(0.35687732342007433, 0.5079365079365079, 0.4192139737991266, None)\n",
      "\n",
      "2 loss 41588.83167274608\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.48443530e-04 1.15109539e-04 1.82449415e-04 3.05086179e-04\n",
      "  1.82960285e-04 9.40974418e-05]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "3 loss 41588.83102246899\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[7.24080209e-05 6.72642220e-05 1.02099734e-04 5.46730147e-05\n",
      "  2.13805595e-04 3.47919333e-05]]\n",
      "{0: 2556, 1: 258}\n",
      "(0.37209302325581395, 0.5079365079365079, 0.42953020134228187, None)\n",
      "\n",
      "4 loss 41588.83102868187\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.00015718 0.00015185 0.0002266  0.0001243  0.00024096 0.00030744]]\n",
      "{0: 2560, 1: 254}\n",
      "(0.37401574803149606, 0.5026455026455027, 0.4288939051918736, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.00015718 0.00015185 0.0002266  0.0001243  0.00024096 0.00030744]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.503\n",
      "Neg. class accuracy: 0.939\n",
      "Precision            0.374\n",
      "Recall               0.503\n",
      "F1                   0.429\n",
      "----------------------------------------\n",
      "TP: 95 | FP: 159 | TN: 2466 | FN: 94\n",
      "========================================\n",
      "\n",
      "{0: 2560, 1: 254}\n",
      "acc 0.910092395167022\n",
      "(array([0.96328125, 0.37401575]), array([0.93942857, 0.5026455 ]), array([0.9512054 , 0.42889391]), array([2625,  189]))\n",
      "(0.668648499015748, 0.721037037037037, 0.6900496526923688, None)\n",
      "[[2466  159]\n",
      " [  94   95]]\n",
      "prec: tp/(tp+fp) 0.37401574803149606 recall: tp/(tp+fn) 0.5026455026455027\n",
      "(0.37401574803149606, 0.5026455026455027, 0.4288939051918736, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d610c0b00>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d610c0b00>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d610c0b00>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 81502.1506470213\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.21666989 -0.08647327  0.10881792  0.09944611  0.20484656  0.14030441]]\n",
      "{0: 2568, 1: 246}\n",
      "(0.37398373983739835, 0.48677248677248675, 0.4229885057471264, None)\n",
      "\n",
      "1 loss 78833.59265509271\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.31546659 0.01260159 0.20749485 0.19653671 0.30100983 0.23885532]]\n",
      "{0: 2562, 1: 252}\n",
      "(0.373015873015873, 0.4973544973544973, 0.4263038548752835, None)\n",
      "\n",
      "2 loss 76445.04474176883\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.41399234 0.11148472 0.30517841 0.29088138 0.39358324 0.33701439]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "3 loss 74339.8757578947\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.51211409 0.2100796  0.40116541 0.38154931 0.48141777 0.43453633]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "4 loss 72515.79407187033\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.6096446  0.30826555 0.49455496 0.4679286  0.56384456 0.53102825]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.6096446  0.30826555 0.49455496 0.4679286  0.56384456 0.53102825]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.503\n",
      "Neg. class accuracy: 0.94\n",
      "Precision            0.375\n",
      "Recall               0.503\n",
      "F1                   0.43\n",
      "----------------------------------------\n",
      "TP: 95 | FP: 158 | TN: 2467 | FN: 94\n",
      "========================================\n",
      "\n",
      "{0: 2561, 1: 253}\n",
      "acc 0.9104477611940298\n",
      "(array([0.96329559, 0.37549407]), array([0.93980952, 0.5026455 ]), array([0.95140764, 0.42986425]), array([2625,  189]))\n",
      "(0.6693948294036575, 0.7212275132275132, 0.6906359446682941, None)\n",
      "[[2467  158]\n",
      " [  94   95]]\n",
      "prec: tp/(tp+fp) 0.37549407114624506 recall: tp/(tp+fn) 0.5026455026455027\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n"
     ]
    }
   ],
   "source": [
    "# for picking lfs randomly\n",
    "# LF_subset_indices = np.array(random.sample(range(train_L_S.shape[2]), NoOfLFsInLFSubset))\n",
    "LF_subset_indices = np.array([0,1,2,7,8,9])\n",
    "train_nl_penalties(LF_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 7 8]\n",
      "(10000,) (6,)\n",
      "(10000, 2, 6) (2814, 2, 6) (6,)\n",
      "0 loss 41638.92673303141\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.08449117 -0.11044035  0.02008056 -0.0598917   0.04483325  0.00326031]]\n",
      "{0: 927, 1: 1887}\n",
      "(0.041865394806571275, 0.41798941798941797, 0.07610789980732177, None)\n",
      "\n",
      "1 loss 41588.54307785565\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.0130182  -0.05790362 -0.01682119 -0.11221936  0.01801132  0.00857004]]\n",
      "{0: 981, 1: 1833}\n",
      "(0.03600654664484452, 0.3492063492063492, 0.06528189910979229, None)\n",
      "\n",
      "2 loss 41561.165261243885\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-0.03361289 -0.05100757 -0.02642908 -0.16474845  0.01094241  0.00962209]]\n",
      "{0: 1073, 1: 1741}\n",
      "(0.031016657093624354, 0.2857142857142857, 0.055958549222797936, None)\n",
      "\n",
      "3 loss 41534.27115888344\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-0.05748832 -0.06126314 -0.03491478 -0.21682487  0.00836414  0.01079066]]\n",
      "{0: 1073, 1: 1741}\n",
      "(0.031016657093624354, 0.2857142857142857, 0.055958549222797936, None)\n",
      "\n",
      "4 loss 41500.89314666141\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-0.07610907 -0.07437539 -0.04345855 -0.26838919  0.00742192  0.01240415]]\n",
      "{0: 1073, 1: 1741}\n",
      "(0.031016657093624354, 0.2857142857142857, 0.055958549222797936, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-0.07610907 -0.07437539 -0.04345855 -0.26838919  0.00742192  0.01240415]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.286\n",
      "Neg. class accuracy: 0.357\n",
      "Precision            0.031\n",
      "Recall               0.286\n",
      "F1                   0.056\n",
      "----------------------------------------\n",
      "TP: 54 | FP: 1687 | TN: 938 | FN: 135\n",
      "========================================\n",
      "\n",
      "{0: 1073, 1: 1741}\n",
      "acc 0.35252309879175553\n",
      "(array([0.87418453, 0.03101666]), array([0.35733333, 0.28571429]), array([0.50730124, 0.05595855]), array([2625,  189]))\n",
      "(0.45260059322528373, 0.32152380952380955, 0.281629896569214, None)\n",
      "[[ 938 1687]\n",
      " [ 135   54]]\n",
      "prec: tp/(tp+fp) 0.031016657093624354 recall: tp/(tp+fn) 0.2857142857142857\n",
      "(0.031016657093624354, 0.2857142857142857, 0.055958549222797936, None)\n",
      "0 loss 43000.823058711794\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 9.27775986e-02 -8.63474764e-02  2.95543858e-02  1.00579691e-04\n",
      "   4.58297423e-02  5.67719028e-06]]\n",
      "{0: 2529, 1: 285}\n",
      "(0.3263157894736842, 0.49206349206349204, 0.3924050632911393, None)\n",
      "\n",
      "1 loss 41969.496308810434\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.0402914  0.00016482 0.00515594 0.03690218 0.01403275 0.00387977]]\n",
      "{0: 2492, 1: 322}\n",
      "(0.32919254658385094, 0.5608465608465608, 0.41487279843444225, None)\n",
      "\n",
      "2 loss 41582.76552859085\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.03518776 0.0170737  0.01876827 0.09056569 0.00109783 0.00017042]]\n",
      "{0: 2468, 1: 346}\n",
      "(0.3063583815028902, 0.5608465608465608, 0.3962616822429907, None)\n",
      "\n",
      "3 loss 41567.72466146975\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[4.71385273e-02 3.95411097e-02 2.56671838e-02 1.43559209e-01\n",
      "  8.60056401e-05 5.57710322e-05]]\n",
      "{0: 2468, 1: 346}\n",
      "(0.3063583815028902, 0.5608465608465608, 0.3962616822429907, None)\n",
      "\n",
      "4 loss 41545.05630176133\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[6.24097648e-02 5.60241633e-02 3.46139590e-02 1.95634171e-01\n",
      "  1.28714325e-04 9.47384210e-05]]\n",
      "{0: 2468, 1: 346}\n",
      "(0.3063583815028902, 0.5608465608465608, 0.3962616822429907, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[6.24097648e-02 5.60241633e-02 3.46139590e-02 1.95634171e-01\n",
      "  1.28714325e-04 9.47384210e-05]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.561\n",
      "Neg. class accuracy: 0.909\n",
      "Precision            0.306\n",
      "Recall               0.561\n",
      "F1                   0.396\n",
      "----------------------------------------\n",
      "TP: 106 | FP: 240 | TN: 2385 | FN: 83\n",
      "========================================\n",
      "\n",
      "{0: 2468, 1: 346}\n",
      "acc 0.8852167732764747\n",
      "(array([0.96636953, 0.30635838]), array([0.90857143, 0.56084656]), array([0.93657962, 0.39626168]), array([2625,  189]))\n",
      "(0.6363639557433414, 0.7347089947089946, 0.6664206506640047, None)\n",
      "[[2385  240]\n",
      " [  83  106]]\n",
      "prec: tp/(tp+fp) 0.3063583815028902 recall: tp/(tp+fn) 0.5608465608465608\n",
      "(0.3063583815028902, 0.5608465608465608, 0.3962616822429907, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d61502f60>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d61502f60>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d61502f60>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 42996.88540189842\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.08221045 -0.08639693  0.01519509 -0.05599833  0.04707566  0.00761433]]\n",
      "{0: 922, 1: 1892}\n",
      "(0.04386892177589852, 0.43915343915343913, 0.07976934166266218, None)\n",
      "\n",
      "1 loss 42038.517306912494\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.01759791 -0.00821615 -0.00264197 -0.00821669  0.00807286  0.00438483]]\n",
      "{0: 918, 1: 1896}\n",
      "(0.07647679324894514, 0.7671957671957672, 0.13908872901678657, None)\n",
      "\n",
      "2 loss 41593.665841055445\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.0002709  0.00012876 0.00016443 0.00022069 0.00028082 0.00027784]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "3 loss 41587.956959245195\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.48087067e-02 1.36927138e-02 8.62858199e-03 4.72300864e-02\n",
      "  9.72527403e-05 2.41753706e-04]]\n",
      "{0: 2468, 1: 346}\n",
      "(0.3063583815028902, 0.5608465608465608, 0.3962616822429907, None)\n",
      "\n",
      "4 loss 41579.93137698078\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[3.21154029e-02 2.96819520e-02 1.81273031e-02 1.02009041e-01\n",
      "  6.91678811e-05 5.58430801e-05]]\n",
      "{0: 2468, 1: 346}\n",
      "(0.3063583815028902, 0.5608465608465608, 0.3962616822429907, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[3.21154029e-02 2.96819520e-02 1.81273031e-02 1.02009041e-01\n",
      "  6.91678811e-05 5.58430801e-05]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.561\n",
      "Neg. class accuracy: 0.909\n",
      "Precision            0.306\n",
      "Recall               0.561\n",
      "F1                   0.396\n",
      "----------------------------------------\n",
      "TP: 106 | FP: 240 | TN: 2385 | FN: 83\n",
      "========================================\n",
      "\n",
      "{0: 2468, 1: 346}\n",
      "acc 0.8852167732764747\n",
      "(array([0.96636953, 0.30635838]), array([0.90857143, 0.56084656]), array([0.93657962, 0.39626168]), array([2625,  189]))\n",
      "(0.6363639557433414, 0.7347089947089946, 0.6664206506640047, None)\n",
      "[[2385  240]\n",
      " [  83  106]]\n",
      "prec: tp/(tp+fp) 0.3063583815028902 recall: tp/(tp+fn) 0.5608465608465608\n",
      "(0.3063583815028902, 0.5608465608465608, 0.3962616822429907, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59754dd8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59754dd8>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59754dd8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 81470.2300283042\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.21656558 -0.08643751  0.10873977  0.10021223  0.20424723  0.13959845]]\n",
      "{0: 2539, 1: 275}\n",
      "(0.3381818181818182, 0.49206349206349204, 0.40086206896551724, None)\n",
      "\n",
      "1 loss 78712.99664044699\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.31448979 0.01246084 0.20749486 0.19792972 0.29855022 0.23507732]]\n",
      "{0: 2482, 1: 332}\n",
      "(0.32831325301204817, 0.5767195767195767, 0.41842610364683297, None)\n",
      "\n",
      "2 loss 76178.5170127415\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.41012431 0.11037914 0.30576254 0.29322878 0.38798031 0.32662548]]\n",
      "{0: 2482, 1: 332}\n",
      "(0.32831325301204817, 0.5767195767195767, 0.41842610364683297, None)\n",
      "\n",
      "3 loss 73881.050688587\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.5021969  0.20668208 0.40329983 0.3861756  0.47207982 0.41370951]]\n",
      "{0: 2482, 1: 332}\n",
      "(0.32831325301204817, 0.5767195767195767, 0.41842610364683297, None)\n",
      "\n",
      "4 loss 71823.0791359423\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.58956301 0.30076434 0.4997803  0.47697056 0.55088509 0.49617201]]\n",
      "{0: 2482, 1: 332}\n",
      "(0.32831325301204817, 0.5767195767195767, 0.41842610364683297, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.58956301 0.30076434 0.4997803  0.47697056 0.55088509 0.49617201]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.915\n",
      "Precision            0.328\n",
      "Recall               0.577\n",
      "F1                   0.418\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 223 | TN: 2402 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2482, 1: 332}\n",
      "acc 0.8923240938166311\n",
      "(array([0.96776793, 0.32831325]), array([0.91504762, 0.57671958]), array([0.94066967, 0.4184261 ]), array([2625,  189]))\n",
      "(0.6480405910507461, 0.7458835978835978, 0.6795478863642428, None)\n",
      "[[2402  223]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.32831325301204817 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.32831325301204817, 0.5767195767195767, 0.41842610364683297, None)\n"
     ]
    }
   ],
   "source": [
    "LF_subset_indices = np.array([2,3,4,5,7,8])\n",
    "train_nl_penalties(LF_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 6 7 8 9]\n",
      "(10000,) (9,)\n",
      "(22276, 2, 9) (2814, 2, 9) (9,)\n",
      "0 loss 139171.781832511\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 0.05144432 -0.09275716  0.04617867  0.0424805   0.06674175  0.02729831\n",
      "   0.1055682   0.00126014  0.05632088]]\n",
      "{0: 2514, 1: 300}\n",
      "(0.3233333333333333, 0.5132275132275133, 0.3967280163599182, None)\n",
      "\n",
      "1 loss 138996.60347260014\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 0.00136959 -0.00913628  0.03101953  0.02755143  0.00709781  0.00101656\n",
      "   0.05405614  0.00637396 -0.00092416]]\n",
      "{0: 2481, 1: 333}\n",
      "(0.3183183183183183, 0.5608465608465608, 0.40613026819923365, None)\n",
      "\n",
      "2 loss 138967.96302785305\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 0.00068522  0.00072692  0.00771056  0.00653462  0.00065158  0.00057692\n",
      "   0.0132444   0.00478876 -0.00060508]]\n",
      "{0: 2475, 1: 339}\n",
      "(0.3185840707964602, 0.5714285714285714, 0.409090909090909, None)\n",
      "\n",
      "3 loss 138964.97624060838\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 2.86949724e-17 -2.41639038e-17  3.81694722e-17  4.22141026e-17\n",
      "  -1.21977714e-16 -1.99381838e-17 -5.75125668e-18 -1.49701890e-19\n",
      "  -4.15033041e-17]]\n",
      "{0: 2483, 1: 331}\n",
      "(0.2809667673716012, 0.49206349206349204, 0.35769230769230764, None)\n",
      "\n",
      "4 loss 138964.91934802078\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 2.57778862e-12 -5.01402602e-12 -7.35060385e-11  1.95462909e-11\n",
      "  -3.27699377e-11  2.54852260e-12 -4.44813080e-12 -1.09464139e-10\n",
      "  -1.78280272e-11]]\n",
      "{0: 2390, 1: 424}\n",
      "(0.03773584905660377, 0.08465608465608465, 0.05220228384991843, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 2.57778862e-12 -5.01402602e-12 -7.35060385e-11  1.95462909e-11\n",
      "  -3.27699377e-11  2.54852260e-12 -4.44813080e-12 -1.09464139e-10\n",
      "  -1.78280272e-11]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0847\n",
      "Neg. class accuracy: 0.845\n",
      "Precision            0.0377\n",
      "Recall               0.0847\n",
      "F1                   0.0522\n",
      "----------------------------------------\n",
      "TP: 16 | FP: 408 | TN: 2217 | FN: 173\n",
      "========================================\n",
      "\n",
      "{0: 2390, 1: 424}\n",
      "acc 0.7935323383084577\n",
      "(array([0.92761506, 0.03773585]), array([0.84457143, 0.08465608]), array([0.88414756, 0.05220228]), array([2625,  189]))\n",
      "(0.48267545590905503, 0.4646137566137566, 0.4681749205889672, None)\n",
      "[[2217  408]\n",
      " [ 173   16]]\n",
      "prec: tp/(tp+fp) 0.03773584905660377 recall: tp/(tp+fn) 0.08465608465608465\n",
      "(0.03773584905660377, 0.08465608465608465, 0.05220228384991843, None)\n",
      "0 loss 142197.94527833574\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 0.05105895 -0.08610337  0.04538103  0.04171132  0.06590322  0.025959\n",
      "   0.10595731  0.00192954  0.05643951]]\n",
      "{0: 2514, 1: 300}\n",
      "(0.3233333333333333, 0.5132275132275133, 0.3967280163599182, None)\n",
      "\n",
      "1 loss 139820.7902508103\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[1.28237811e-03 5.15373963e-05 2.86277905e-02 2.52780963e-02\n",
      "  5.95664445e-03 1.28402059e-03 5.59044026e-02 7.88796769e-03\n",
      "  1.85865123e-05]]\n",
      "{0: 2504, 1: 310}\n",
      "(0.3258064516129032, 0.5343915343915344, 0.40480961923847697, None)\n",
      "\n",
      "2 loss 138967.9934182389\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[9.16657941e-04 9.85340659e-04 7.63561519e-03 6.59173845e-03\n",
      "  9.28287419e-04 7.76288055e-04 1.39046215e-02 4.61479181e-03\n",
      "  3.92781896e-05]]\n",
      "{0: 2492, 1: 322}\n",
      "(0.33540372670807456, 0.5714285714285714, 0.42270058708414876, None)\n",
      "\n",
      "3 loss 138964.98161202046\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[2.34069795e-05 2.51087127e-05 6.97836596e-05 6.66357582e-05\n",
      "  2.49884898e-05 1.21727563e-04 9.65261900e-05 7.01128279e-05\n",
      "  1.07543414e-04]]\n",
      "{0: 2500, 1: 314}\n",
      "(0.3343949044585987, 0.5555555555555556, 0.4174950298210735, None)\n",
      "\n",
      "4 loss 138964.919594797\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[3.15904199e-05 3.17580110e-05 9.24549681e-05 8.85565188e-05\n",
      "  3.10103132e-05 3.46282516e-05 1.19425768e-04 1.16288977e-04\n",
      "  1.06762660e-04]]\n",
      "{0: 2496, 1: 318}\n",
      "(0.33647798742138363, 0.5661375661375662, 0.4220907297830375, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[3.15904199e-05 3.17580110e-05 9.24549681e-05 8.85565188e-05\n",
      "  3.10103132e-05 3.46282516e-05 1.19425768e-04 1.16288977e-04\n",
      "  1.06762660e-04]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.92\n",
      "Precision            0.336\n",
      "Recall               0.566\n",
      "F1                   0.422\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 211 | TN: 2414 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2496, 1: 318}\n",
      "acc 0.8958777540867093\n",
      "(array([0.96714744, 0.33647799]), array([0.91961905, 0.56613757]), array([0.94278461, 0.42209073]), array([2625,  189]))\n",
      "(0.6518127116594098, 0.7428783068783069, 0.682437671081716, None)\n",
      "[[2414  211]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.33647798742138363 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.33647798742138363, 0.5661375661375662, 0.4220907297830375, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 9), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 9) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(9,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b63a4a8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b63a4a8>\n",
      "<tf.Variable 'alphas:0' shape=(9,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 9), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 9), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5b63a4a8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 9), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 9), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(9,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 142197.94527833574\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 0.05105895 -0.08610337  0.04538103  0.04171132  0.06590322  0.025959\n",
      "   0.10595731  0.00192954  0.05643951]]\n",
      "{0: 2514, 1: 300}\n",
      "(0.3233333333333333, 0.5132275132275133, 0.3967280163599182, None)\n",
      "\n",
      "1 loss 139820.7902508103\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[1.28237811e-03 5.15373963e-05 2.86277905e-02 2.52780963e-02\n",
      "  5.95664445e-03 1.28402059e-03 5.59044026e-02 7.88796769e-03\n",
      "  1.85865123e-05]]\n",
      "{0: 2504, 1: 310}\n",
      "(0.3258064516129032, 0.5343915343915344, 0.40480961923847697, None)\n",
      "\n",
      "2 loss 138967.9934182389\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[9.16657941e-04 9.85340659e-04 7.63561519e-03 6.59173845e-03\n",
      "  9.28287419e-04 7.76288055e-04 1.39046215e-02 4.61479181e-03\n",
      "  3.92781896e-05]]\n",
      "{0: 2492, 1: 322}\n",
      "(0.33540372670807456, 0.5714285714285714, 0.42270058708414876, None)\n",
      "\n",
      "3 loss 138964.98161202046\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[2.34069795e-05 2.51087127e-05 6.97836596e-05 6.66357582e-05\n",
      "  2.49884898e-05 1.21727563e-04 9.65261900e-05 7.01128279e-05\n",
      "  1.07543414e-04]]\n",
      "{0: 2500, 1: 314}\n",
      "(0.3343949044585987, 0.5555555555555556, 0.4174950298210735, None)\n",
      "\n",
      "4 loss 138964.919594797\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[3.15904199e-05 3.17580110e-05 9.24549681e-05 8.85565188e-05\n",
      "  3.10103132e-05 3.46282516e-05 1.19425768e-04 1.16288977e-04\n",
      "  1.06762660e-04]]\n",
      "{0: 2496, 1: 318}\n",
      "(0.33647798742138363, 0.5661375661375662, 0.4220907297830375, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[3.15904199e-05 3.17580110e-05 9.24549681e-05 8.85565188e-05\n",
      "  3.10103132e-05 3.46282516e-05 1.19425768e-04 1.16288977e-04\n",
      "  1.06762660e-04]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.92\n",
      "Precision            0.336\n",
      "Recall               0.566\n",
      "F1                   0.422\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 211 | TN: 2414 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2496, 1: 318}\n",
      "acc 0.8958777540867093\n",
      "(array([0.96714744, 0.33647799]), array([0.91961905, 0.56613757]), array([0.94278461, 0.42209073]), array([2625,  189]))\n",
      "(0.6518127116594098, 0.7428783068783069, 0.682437671081716, None)\n",
      "[[2414  211]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.33647798742138363 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.33647798742138363, 0.5661375661375662, 0.4220907297830375, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 9), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 9) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(9,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d595b2d68>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d595b2d68>\n",
      "<tf.Variable 'alphas:0' shape=(9,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 9), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 9), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d595b2d68>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nls Tensor(\"mul:0\", shape=(?, 9), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 9), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(9,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 269085.54387308477\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[ 0.2163905  -0.08665923  0.10842293  0.09954735  0.2051548   0.1402791\n",
      "   0.26774418  0.13053799  0.24643691]]\n",
      "{0: 2510, 1: 304}\n",
      "(0.3256578947368421, 0.5238095238095238, 0.40162271805273836, None)\n",
      "\n",
      "1 loss 260535.2979800912\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[0.31459739 0.0120591  0.20537105 0.196775   0.30292011 0.23876022\n",
      "  0.3648114  0.22877185 0.34644475]]\n",
      "{0: 2496, 1: 318}\n",
      "(0.33647798742138363, 0.5661375661375662, 0.4220907297830375, None)\n",
      "\n",
      "2 loss 253407.048282886\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[0.41145053 0.11002074 0.29681645 0.28907013 0.39769805 0.33656052\n",
      "  0.46043947 0.32589753 0.44625498]]\n",
      "{0: 2495, 1: 319}\n",
      "(0.335423197492163, 0.5661375661375662, 0.421259842519685, None)\n",
      "\n",
      "3 loss 247675.99959120058\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[0.50278105 0.20626214 0.37576942 0.36967947 0.4807063  0.43275585\n",
      "  0.5547349  0.42196014 0.54579404]]\n",
      "{0: 2495, 1: 319}\n",
      "(0.335423197492163, 0.5661375661375662, 0.421259842519685, None)\n",
      "\n",
      "4 loss 243201.8055994961\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[0.55706697 0.29869116 0.4362613  0.43190921 0.52909042 0.52147739\n",
      "  0.64746687 0.51677512 0.64488911]]\n",
      "{0: 2495, 1: 319}\n",
      "(0.335423197492163, 0.5661375661375662, 0.421259842519685, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471]\n",
      "[[0.55706697 0.29869116 0.4362613  0.43190921 0.52909042 0.52147739\n",
      "  0.64746687 0.51677512 0.64488911]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.919\n",
      "Precision            0.335\n",
      "Recall               0.566\n",
      "F1                   0.421\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 212 | TN: 2413 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2495, 1: 319}\n",
      "acc 0.8955223880597015\n",
      "(array([0.96713427, 0.3354232 ]), array([0.9192381 , 0.56613757]), array([0.94257813, 0.42125984]), array([2625,  189]))\n",
      "(0.6512787330146186, 0.7426878306878306, 0.6819189837598425, None)\n",
      "[[2413  212]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.335423197492163 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.335423197492163, 0.5661375661375662, 0.421259842519685, None)\n"
     ]
    }
   ],
   "source": [
    "# Use full dataset all but no_spouse_in_sentence\n",
    "LF_subset_indices = np.array([0,1,2,3,4,6,7,8,9])\n",
    "train_nl_penalties(LF_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized training with different params\n",
    "\n",
    "def train_nl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty reduce_sum(max(0,-theta))\n",
    "\n",
    "def train_nl_p(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) +\\\n",
    "                        tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty2  -tf.minimum( tf.reduce_min(theta),0)\n",
    "\n",
    "def train_nl_p2(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     -tf.minimum( tf.reduce_min(thetas),0.0)\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty3 sum(log(1+e^(-x-pk)))\n",
    "\n",
    "def train_nl_p3(lr,ep,th,pk=0):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     +tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized loss with prior from other LFs\n",
    "\n",
    "def train_nlp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        \n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        \n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "               \n",
    "        \n",
    "        pout = tf.map_fn(lambda li: tf.map_fn(lambda lij:li*lij,li ),l)\n",
    "#         print(\"nls\",nls_)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        \n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        \n",
    "\n",
    "        sumy = tf.reduce_sum(t_pout-logz,axis=1)\n",
    "        print(\"sumy\",sumy)\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(t_pout-logz,axis=1) ))\n",
    "\n",
    "        \n",
    "        def index_along_every_row(array, index):\n",
    "            N, _ = array.shape\n",
    "            return array[np.arange(N), index]\n",
    "\n",
    "        #Best LF\n",
    "        blf = tf.argmax(t_pout,axis=1)\n",
    "        print(\"blf\",blf)\n",
    "        print(\"normloss\",normloss)\n",
    "        \n",
    "        \n",
    "        marginals = tf.py_func(index_along_every_row, [tf.squeeze(t_pout), tf.squeeze(blf)], [tf.float64])[0]\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict1 = tf.gather(k,tf.squeeze(blf))\n",
    "        \n",
    "        predict = tf.where(tf.equal(predict1,1),tf.ones_like(predict1),tf.zeros_like(predict1))\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "                print(b)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "            print(b)\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 10)\n",
      "(2814, 2)\n",
      "(22276, 10)\n",
      "(22276, 2)\n"
     ]
    }
   ],
   "source": [
    "#input L_S:train_L_S, K: no of classes\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def get_maj_prior(L_S,K):\n",
    "    maj_prior = []\n",
    "    \n",
    "    print(L_S[:,0,:].shape)\n",
    "    for row in np.nditer(L_S[:,0,:],flags=['external_loop'], order='C'):\n",
    "        p = np.ones(K)/K\n",
    "        unique, counts = np.unique(row, return_counts=True)\n",
    "        unique = [int(x) for x in unique]\n",
    "        rc = dict(zip(unique, counts))\n",
    "        tnz = np.count_nonzero(row)\n",
    "        if -1 in rc:\n",
    "            p[0] = rc[-1]\n",
    "        if 1 in rc:\n",
    "            p[1] = rc[1]\n",
    "        p = softmax(p)\n",
    "        maj_prior.append(p)\n",
    "    return np.array(maj_prior)\n",
    "\n",
    "dev_maj_pl=get_maj_prior(dev_L_S,2)\n",
    "print(dev_maj_pl.shape)\n",
    "\n",
    "\n",
    "train_maj_pl=get_maj_prior(train_L_S,2)\n",
    "print(train_maj_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized loss with majority prior\n",
    "\n",
    "\n",
    "\n",
    "def train_nlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout-logz),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Normalized loss with majority bias un-normalized\n",
    "\n",
    "def train_unlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
