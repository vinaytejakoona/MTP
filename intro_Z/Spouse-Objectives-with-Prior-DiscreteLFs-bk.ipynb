{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n",
    "print(len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 2625\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### Discrete ##########\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,0)\n",
    "    \n",
    "# def LF_familial_relationship(c):\n",
    "#     return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x.decode('utf-8')).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,0)\n",
    "\n",
    "\n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    global LF_Threshold\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "# def LF_other_relationship_left_window(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "#     for ow in other:\n",
    "#         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "#     return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def LF_Three_Lists_Left_Window(c):\n",
    "#     global softmax_Threshold\n",
    "#     c1,s1 = LF_husband_wife_left_window(c)\n",
    "#     c2,s2 = LF_family_left_window(c)\n",
    "#     c3,s3 = LF_other_relationship_left_window(c)\n",
    "#     sc = np.array([s1,s2,s3])\n",
    "#     c = [c1,c2,c3]\n",
    "#     sharp_param = 1.5\n",
    "#     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "#     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "#     #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "#     if s1==s2 or s3==s1:\n",
    "#         return (0,0)\n",
    "#     return c[np.argmax(prob_sc)],1\n",
    "\n",
    "# def LF_Three_Lists_Between_Words(c):\n",
    "#     global softmax_Threshold\n",
    "#     c1,s1 = LF_husband_wife(c)\n",
    "#     c2,s2 = LF_familial_relationship(c)\n",
    "#     c3,s3 = LF_other_relationship(c)\n",
    "#     sc = np.array([s1,s2,s3])\n",
    "#     c = [c1,c2,c3]\n",
    "#     sharp_param = 1.5\n",
    "    \n",
    "#     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "#     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "#     #print 'BW:',s1,s2,s3,prob_sc\n",
    "#     if s1==s2 or s3==s1:\n",
    "#         return (0,0)\n",
    "#     return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 9-5-2018, 10:41:55\n",
      "500data points labelled in 0.7188430507977803 mins\n",
      "1000data points labelled in 1.3335495034853617 mins\n",
      "1500data points labelled in 1.917067794005076 mins\n",
      "2000data points labelled in 2.519752260049184 mins\n",
      "2500data points labelled in 3.1769800662994383 mins\n",
      "500data points labelled in 4.174797224998474 mins\n",
      "1000data points labelled in 4.797278384367625 mins\n",
      "1500data points labelled in 5.556176539262136 mins\n",
      "2000data points labelled in 6.174566467603047 mins\n",
      "2500data points labelled in 6.850887068112692 mins\n",
      "3000data points labelled in 7.6337612748146055 mins\n",
      "3500data points labelled in 8.271688584486643 mins\n",
      "4000data points labelled in 8.86154446999232 mins\n",
      "4500data points labelled in 9.401108086109161 mins\n",
      "5000data points labelled in 10.001064256827037 mins\n",
      "5500data points labelled in 10.652975161870321 mins\n",
      "6000data points labelled in 11.262283476193746 mins\n",
      "6500data points labelled in 11.85253625313441 mins\n",
      "7000data points labelled in 12.50683133204778 mins\n",
      "7500data points labelled in 13.094375010331472 mins\n",
      "8000data points labelled in 13.716650235652924 mins\n",
      "8500data points labelled in 14.281550602118175 mins\n",
      "9000data points labelled in 14.971553881963095 mins\n",
      "9500data points labelled in 15.57162874142329 mins\n",
      "10000data points labelled in 16.21110211610794 mins\n",
      "10500data points labelled in 16.86411997079849 mins\n",
      "11000data points labelled in 17.50747369925181 mins\n",
      "11500data points labelled in 18.0820143977801 mins\n",
      "12000data points labelled in 18.754645995299022 mins\n",
      "12500data points labelled in 19.329470952351887 mins\n",
      "13000data points labelled in 19.94033740758896 mins\n",
      "13500data points labelled in 20.581616592407226 mins\n",
      "14000data points labelled in 21.171934326489765 mins\n",
      "14500data points labelled in 21.763719876607258 mins\n",
      "15000data points labelled in 22.431539726257324 mins\n",
      "15500data points labelled in 22.935143303871154 mins\n",
      "16000data points labelled in 23.565833016236624 mins\n",
      "16500data points labelled in 24.203933664162953 mins\n",
      "17000data points labelled in 24.749000080426534 mins\n",
      "17500data points labelled in 25.358984434604643 mins\n",
      "18000data points labelled in 25.93876881202062 mins\n",
      "18500data points labelled in 26.685788782437644 mins\n",
      "19000data points labelled in 27.294497899214427 mins\n",
      "19500data points labelled in 27.912985666592917 mins\n",
      "20000data points labelled in 28.575929319858552 mins\n",
      "20500data points labelled in 29.122294485569 mins\n",
      "21000data points labelled in 29.714926477273305 mins\n",
      "21500data points labelled in 30.330261317888894 mins\n",
      "22000data points labelled in 30.933784091472624 mins\n",
      "--- 1877.3884024620056 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "    \n",
    "\n",
    "def drawLossVsF1(y_loss,x_f1s,text,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x_f1s, y_loss)\n",
    "\n",
    "    plt.xlabel('f1-score')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(title)\n",
    "    \n",
    "    for i, txt in enumerate(text):\n",
    "        ax.annotate(txt, (x_f1s[i],y_loss[i]))\n",
    "        \n",
    "    plt.savefig(title+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized loss with prior from other LFs\n",
    "\n",
    "def train_nlp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        \n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        \n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "               \n",
    "        \n",
    "        pout = tf.map_fn(lambda li: tf.map_fn(lambda lij:li*lij,li ),l)\n",
    "#         print(\"nls\",nls_)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        \n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        \n",
    "\n",
    "        sumy = tf.reduce_sum(t_pout-logz,axis=1)\n",
    "        print(\"sumy\",sumy)\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(t_pout-logz,axis=1) ))\n",
    "\n",
    "        \n",
    "        def index_along_every_row(array, index):\n",
    "            N, _ = array.shape\n",
    "            return array[np.arange(N), index]\n",
    "\n",
    "        #Best LF\n",
    "        blf = tf.argmax(t_pout,axis=1)\n",
    "        print(\"blf\",blf)\n",
    "        print(\"normloss\",normloss)\n",
    "        \n",
    "        \n",
    "        marginals = tf.py_func(index_along_every_row, [tf.squeeze(t_pout), tf.squeeze(blf)], [tf.float64])[0]\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict1 = tf.gather(k,tf.squeeze(blf))\n",
    "        \n",
    "        predict = tf.where(tf.equal(predict1,1),tf.ones_like(predict1),tf.zeros_like(predict1))\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "                print(b)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "            print(b)\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cb33fe278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cb33fe278>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cb33fe278>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"Sum_1:0\", shape=(?, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 2013128.8891574587\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[1.01869193 0.71557711 0.91125614 0.90227968 1.00758283 1.12364657\n",
      "  1.07030801 1.09970523 1.20222222 1.13217179]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "1 loss 1949188.521442267\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.91983684 0.61704297 0.81328829 0.804232   0.90882362 1.1353606\n",
      "  0.97138712 1.06032208 1.14009408 1.03651902]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "2 loss 1898049.864934474\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.82098162 0.51867251 0.71588567 0.70669635 0.81012091 1.07481351\n",
      "  0.87243628 0.97904733 1.05218136 0.93618352]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "3 loss 1853910.2943624642\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.72239588 0.42100099 0.61990766 0.61048509 0.71179248 0.99303804\n",
      "  0.77369255 0.88907487 0.95900911 0.83587022]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "4 loss 1816568.540433177\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.62434737 0.32532591 0.52747044 0.51758373 0.61424517 0.90400988\n",
      "  0.67533815 0.79571932 0.86367741 0.73563685]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-406-cc0e441067d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-405-6f7f89e7e8c3>\u001b[0m in \u001b[0;36mtrain_nlp\u001b[0;34m(lr, ep, th)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Initialize an iterator over the validation dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_init_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarginals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7fcdc8d0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7fcdc8d0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7fcdc8d0>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"Sum_1:0\", shape=(?, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 2013128.8891574587\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[1.01869193 0.71557711 0.91125614 0.90227968 1.00758283 1.12364657\n",
      "  1.07030801 1.09970523 1.20222222 1.13217179]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "1 loss 1949188.521442267\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.91983684 0.61704297 0.81328829 0.804232   0.90882362 1.1353606\n",
      "  0.97138712 1.06032208 1.14009408 1.03651902]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "2 loss 1898049.864934474\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.82098162 0.51867251 0.71588567 0.70669635 0.81012091 1.07481351\n",
      "  0.87243628 0.97904733 1.05218136 0.93618352]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "3 loss 1853910.2943624642\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.72239588 0.42100099 0.61990766 0.61048509 0.71179248 0.99303804\n",
      "  0.77369255 0.88907487 0.95900911 0.83587022]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "4 loss 1816568.540433177\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.62434737 0.32532591 0.52747044 0.51758373 0.61424517 0.90400988\n",
      "  0.67533815 0.79571932 0.86367741 0.73563685]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-404-7fc2dfa8bfc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print blf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-403-e44df1238c94>\u001b[0m in \u001b[0;36mtrain_nlp\u001b[0;34m(lr, ep, th)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Initialize an iterator over the validation dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_init_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarginals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "#print blf\n",
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 10)\n",
      "(2814, 2)\n",
      "(22276, 10)\n",
      "(22276, 2)\n"
     ]
    }
   ],
   "source": [
    "#input L_S:train_L_S, K: no of classes\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def get_maj_prior(L_S,K):\n",
    "    maj_prior = []\n",
    "    \n",
    "    print(L_S[:,0,:].shape)\n",
    "    for row in np.nditer(L_S[:,0,:],flags=['external_loop'], order='C'):\n",
    "        p = np.ones(K)/K\n",
    "        unique, counts = np.unique(row, return_counts=True)\n",
    "        unique = [int(x) for x in unique]\n",
    "        rc = dict(zip(unique, counts))\n",
    "        tnz = np.count_nonzero(row)\n",
    "        if -1 in rc:\n",
    "            p[0] = rc[-1]\n",
    "        if 1 in rc:\n",
    "            p[1] = rc[1]\n",
    "        p = softmax(p)\n",
    "        maj_prior.append(p)\n",
    "    return np.array(maj_prior)\n",
    "\n",
    "dev_maj_pl=get_maj_prior(dev_L_S,2)\n",
    "print(dev_maj_pl.shape)\n",
    "\n",
    "\n",
    "train_maj_pl=get_maj_prior(train_L_S,2)\n",
    "print(train_maj_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Normalized loss with majority prior\n",
    "\n",
    "\n",
    "\n",
    "def train_nlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout-logz),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eb1be0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eb1be0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eb1be0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 408964.17801724526\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01864878 0.71548178 0.91071576 0.90178499 1.00748712 1.11280642\n",
      "  1.07028735 1.10642092 1.18568422 1.12757612]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 395866.0696287779\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.91918559 0.61613422 0.81128926 0.80236173 0.90802752 1.00655596\n",
      "  0.97080806 1.00032437 1.08259992 1.02218264]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 384524.60108956933\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.8198474  0.51695871 0.7120017  0.70307867 0.80869426 0.90341696\n",
      "  0.87144795 0.89703403 0.98053296 0.9193616 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 374466.47881153744\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72068344 0.41804226 0.61291245 0.60399618 0.70953748 0.80139241\n",
      "  0.77225224 0.79495402 0.87897956 0.81746155]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "4 loss 365751.6824111813\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.62176833 0.31954721 0.51411781 0.50521268 0.61063372 0.69992109\n",
      "  0.67328743 0.69345504 0.7777644  0.71605413]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.62176833 0.31954721 0.51411781 0.50521268 0.61063372 0.69992109\n",
      "  0.67328743 0.69345504 0.7777644  0.71605413]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.545\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.353\n",
      "Recall               0.545\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 103 | FP: 189 | TN: 2436 | FN: 86\n",
      "========================================\n",
      "\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(array([0.96590008, 0.35273973]), array([0.928     , 0.54497354]), array([0.94657082, 0.42827443]), array([2625,  189]))\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "[[2436  189]\n",
      " [  86  103]]\n",
      "prec: tp/(tp+fp) 0.3527397260273973 recall: tp/(tp+fn) 0.544973544973545\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "train_nlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Normalized loss with majority bias un-normalized\n",
    "\n",
    "\n",
    "\n",
    "def train_unlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6ec940>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6ec940>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6ec940>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "1 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "2 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "3 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "4 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.561\n",
      "Neg. class accuracy: 0.92\n",
      "Precision            0.335\n",
      "Recall               0.561\n",
      "F1                   0.42\n",
      "----------------------------------------\n",
      "TP: 106 | FP: 210 | TN: 2415 | FN: 83\n",
      "========================================\n",
      "\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(array([0.96677342, 0.33544304]), array([0.92      , 0.56084656]), array([0.94280695, 0.41980198]), array([2625,  189]))\n",
      "(0.6511082283548357, 0.7404232804232804, 0.6813044646256545, None)\n",
      "[[2415  210]\n",
      " [  83  106]]\n",
      "prec: tp/(tp+fp) 0.33544303797468356 recall: tp/(tp+fn) 0.5608465608465608\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n"
     ]
    }
   ],
   "source": [
    "train_unlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with different params\n",
    "\n",
    "def train_nl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 153548.14977017298\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "dev loss 19382.46646189899\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "un-norma thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 154340.00504922983\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "dev loss 19477.32881134378\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "[(153548.14977017298, 19382.46646189899, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)), (154340.00504922983, 19477.32881134378, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value Normalized\n",
    "\n",
    "def getNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getNLObjValue(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "                0.11186369,0.07306518,0.69216714,0.07467749,0.16012659,\\\n",
    "                    0.13682546,0.08183363]])))\n",
    " \n",
    "            \n",
    "print(\" un-norma thetas ep7 \")\n",
    "\n",
    "# l_f1s.append(getNLObjValue(np.array([[1.0,1.0,1.0,1.0,1.0,1.02750979,\\\n",
    "#                              1.0,1.0218145,1.0,1.0]])))\n",
    "\n",
    "l_f1s.append(getNLObjValue(np.array([[0.33293226,0.01940464,0.42274838,0.39655883,\\\n",
    "    0.31731244,0.84775084,0.37180681,0.46009105,0.5502137,0.32638473]])))\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc866f9e8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc866f9e8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc866f9e8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "5 loss 158604.93738094255\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.54997522 0.38551056 0.71208165 0.68224501 0.60391497 1.3682661\n",
      "  0.58642215 0.89307    0.94179888 0.7020998 ]]\n",
      "{0: 2523, 1: 291}\n",
      "(0.35051546391752575, 0.5396825396825397, 0.42499999999999993, None)\n",
      "\n",
      "6 loss 157032.75110433032\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "7 loss 155668.8970006794\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.4314503  0.38839979 0.70424996 0.67037562 0.52367733 1.4280809\n",
      "  0.42606297 0.77043883 0.80142626 0.51281886]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "8 loss 154481.67664693782\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.39067389 0.37902675 0.69806429 0.66205327 0.48793706 1.45887735\n",
      "  0.37175861 0.71320089 0.73512861 0.4199123 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "9 loss 153446.87729269065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3546431  0.36275345 0.68942781 0.65117459 0.45284473 1.49068915\n",
      "  0.33247148 0.65941213 0.67218014 0.32889969]]\n",
      "{0: 2501, 1: 313}\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n",
      "\n",
      "10 loss 152548.81576950828\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3228263  0.34281758 0.67875204 0.63822859 0.41888411 1.52333766\n",
      "  0.3020918  0.60913597 0.61277898 0.24064353]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 151777.05782060773\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.29540707 0.32165781 0.66655386 0.62380273 0.38668169 1.55659623\n",
      "  0.27742105 0.56230756 0.55701578 0.15647785]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss 151121.9679241542\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.27235149 0.30085188 0.65330747 0.6084251  0.35673569 1.59025571\n",
      "  0.25725406 0.51880115 0.50491631 0.07840616]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss 150572.4023227708\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.25361337 0.28149093 0.63946432 0.5925888  0.32946785 1.62411754\n",
      "  0.24130367 0.47842506 0.45642398 0.00909516]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "14 loss 150114.45362566956\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.23919651  0.2643783   0.62546829  0.57676995  0.30525161  1.65798876\n",
      "   0.22955256  0.44092173  0.41138899 -0.0487575 ]]\n",
      "{0: 2486, 1: 328}\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.23919651  0.2643783   0.62546829  0.57676995  0.30525161  1.65798876\n",
      "   0.22955256  0.44092173  0.41138899 -0.0487575 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.916\n",
      "Precision            0.326\n",
      "Recall               0.566\n",
      "F1                   0.414\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 221 | TN: 2404 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2486, 1: 328}\n",
      "acc 0.8923240938166311\n",
      "(array([0.96701529, 0.32621951]), array([0.91580952, 0.56613757]), array([0.9407161, 0.4139265]), array([2625,  189]))\n",
      "(0.6466173988972391, 0.7409735449735451, 0.6773213007784249, None)\n",
      "[[2404  221]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.32621951219512196 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n"
     ]
    }
   ],
   "source": [
    "# init random thetas\n",
    "\n",
    "train_nl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Normalized loss plot loss vs f1s\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "y_loss=[153548.149,179552.66,157032.751,150114.45]\n",
    "x_f1s =[0.432,0.428,0.43,0.414]\n",
    "text=[\"snorkel-thetas\",\"old-unNormalized-thetas\",\"normalized-trained-thetas-ep7\",\\\n",
    "      \"normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"Spouse-Normalized-Loss\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -18693.37512233427\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "dev loss -2375.8227752836674\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "normalized thetas ep6 f10.43\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -29153.740301843533\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "dev loss -3721.7512709843986\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.354\n",
      "Recall               0.55\n",
      "F1                   0.431\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 190 | TN: 2435 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2520, 1: 294}\n",
      "acc 0.90227434257285\n",
      "(array([0.96626984, 0.3537415 ]), array([0.92761905, 0.55026455]), array([0.94655005, 0.43064182]), array([2625,  189]))\n",
      "(0.6600056689342404, 0.7389417989417989, 0.6885959352685174, None)\n",
      "[[2435  190]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35374149659863946 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "[(-18693.37512233427, -2375.8227752836674, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)), (-29153.740301843533, -3721.7512709843986, (0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas Unnormalized # remove logz from obj\n",
    "\n",
    "def getUNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getUNLObjValue(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "                0.11186369,0.07306518,0.69216714,0.07467749,0.16012659,\\\n",
    "                    0.13682546,0.08183363]])))\n",
    " \n",
    "            \n",
    "print(\"normalized thetas ep6 f10.43\")\n",
    "\n",
    "l_f1s.append(getUNLObjValue(np.array([[0.48198891,0.38912505,0.70829843,0.67643395,0.56100246,\\\n",
    "        1.39815618,0.49929654,0.83071361,0.8706048,0.6070296 ]])))\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -27046.447774325923\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00304454 1.00650418 1.02847709 1.02590827 1.00626986 1.10374278\n",
      "  1.00047081 1.05014972 1.02497607 1.00775472]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "1 loss -28052.143411860863\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00609458 1.01281986 1.05679467 1.05166221 1.01210828 1.18026413\n",
      "  1.0008722  1.07854839 1.05000733 1.01580513]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "2 loss -29084.10636456966\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00913348 1.01894236 1.08511902 1.07741817 1.01748748 1.25689478\n",
      "  1.00117106 1.10696165 1.07504049 1.02383491]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "3 loss -30139.25214077889\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01215851 1.02486468 1.11344882 1.10317555 1.02241298 1.33361627\n",
      "  1.00138832 1.13538697 1.10007485 1.03184426]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "4 loss -31214.8692847093\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01516631 1.03057898 1.14178284 1.12893381 1.02689378 1.41041477\n",
      "  1.0015406  1.16382207 1.1251098  1.03983336]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "5 loss -32308.533016622125\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01815264 1.03607611 1.17012004 1.15469247 1.03094158 1.48727984\n",
      "  1.00164101 1.19226496 1.15014486 1.04780238]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "6 loss -33418.08609820088\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02111228 1.04134521 1.19845945 1.18045107 1.03456996 1.56420354\n",
      "  1.00169983 1.22071391 1.1751796  1.05575152]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "7 loss -34541.61763052727\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02403881 1.04637338 1.22680022 1.20620922 1.03779353 1.64117968\n",
      "  1.00172501 1.24916742 1.20021365 1.06368097]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "8 loss -35677.44092887072\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02692445 1.05114559 1.2551416  1.23196655 1.04062737 1.71820341\n",
      "  1.00172258 1.27762423 1.22524668 1.07159092]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "9 loss -36824.07134223953\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02975997 1.05564493 1.28348296 1.25772274 1.04308654 1.79527078\n",
      "  1.00169666 1.30608329 1.2502784  1.07948156]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "10 loss -37980.20466766686\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03253478 1.05985339 1.3118237  1.28347749 1.04518599 1.87237845\n",
      "  1.00164896 1.33454373 1.27530856 1.0873531 ]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "11 loss -39144.69663656382\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03523736 1.06375291 1.34016334 1.30923055 1.0469405  1.94952346\n",
      "  1.00157637 1.36300482 1.30033694 1.09520574]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "12 loss -40316.54381636862\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03785615 1.06732708 1.36850145 1.33498169 1.0483649  2.02670308\n",
      "  1.00146078 1.39146599 1.32536332 1.10303971]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "13 loss -41494.86615522479\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04038111 1.07056283 1.39683767 1.36073069 1.0494743  2.10391463\n",
      "  1.00124898 1.41992675 1.35038755 1.11085522]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "14 loss -42678.891105944065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04280545 1.07345216 1.42517169 1.38647739 1.05028432 2.18115548\n",
      "  1.0009374  1.44838672 1.37540947 1.11865252]]\n",
      "{0: 2519, 1: 295}\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04280545 1.07345216 1.42517169 1.38647739 1.05028432 2.18115548\n",
      "  1.0009374  1.44838672 1.37540947 1.11865252]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.927\n",
      "Precision            0.353\n",
      "Recall               0.55\n",
      "F1                   0.43\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 191 | TN: 2434 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2519, 1: 295}\n",
      "acc 0.9019189765458422\n",
      "(array([0.96625645, 0.35254237]), array([0.9272381 , 0.55026455]), array([0.94634526, 0.42975207]), array([2625,  189]))\n",
      "(0.659399411926982, 0.7387513227513227, 0.6880486613626724, None)\n",
      "[[2434  191]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.3525423728813559 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n"
     ]
    }
   ],
   "source": [
    "#init with old network thetas\n",
    "# train_unl(0.01,15,np.array([[1.0,1.0,1.0,\\\n",
    "#                 1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]]))   \n",
    "\n",
    "train_unl(0.1/len(train_L_S),15,tf.constant_initializer(np.array([[1.0,1.0,1.0,\\\n",
    "                1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## UN-Normalized loss plot loss vs f1s\n",
    "    \n",
    "y_loss=[-18693.37,-29153.74,-33418.08,-42678.89]\n",
    "x_f1s =[0.432,0.431,0.43,0.43]\n",
    "text=[\"snorkel-thetas\",\"normalized-thetas-ep7\",\"Un-normalized-trained-thetas-ep7\",\"Un-normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"Spouse-Un-Normalized-Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -16494.182287841937\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32011886 0.01742789 0.2357877  0.22487695 0.31165927 0.31422477\n",
      "  0.3704413  0.2596181  0.37190215 0.27239461]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "1 loss -16916.06023599083\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32240975 0.01967812 0.26207358 0.24913283 0.31554645 0.38878112\n",
      "  0.3713858  0.28807452 0.39742105 0.28029567]]\n",
      "{0: 2525, 1: 289}\n",
      "(0.35294117647058826, 0.5396825396825397, 0.42677824267782427, None)\n",
      "\n",
      "2 loss -17416.749670163787\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32452451 0.02109609 0.28853598 0.27350093 0.31813751 0.46426523\n",
      "  0.37184652 0.31660673 0.42290434 0.28811702]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3493150684931507, 0.5396825396825397, 0.42411642411642414, None)\n",
      "\n",
      "3 loss -17991.625112567406\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32647211 0.02179288 0.31514827 0.29796438 0.31959997 0.54035557\n",
      "  0.3721248  0.34520201 0.44836835 0.29587051]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "4 loss -18635.534044730877\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32826902 0.02187884 0.34189006 0.32251053 0.32010328 0.61685632\n",
      "  0.3722757  0.37385273 0.47382477 0.3035663 ]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "5 loss -19343.04096959059\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32993333 0.02145398 0.36874542 0.34712976 0.31980289 0.69364125\n",
      "  0.37231688 0.4025537  0.49928154 0.31121285]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "6 loss -20108.635126544148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33148239 0.02060487 0.39570164 0.37181452 0.31883431 0.77062559\n",
      "  0.37212214 0.43130096 0.52474361 0.31881709]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "7 loss -20926.887016926226\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "8 loss -21792.56392621291\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33429748 0.01791428 0.44987709 0.42135783 0.31533315 0.9249758\n",
      "  0.37143819 0.48892073 0.5756929  0.33392039]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "9 loss -22700.711796781943\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33559094 0.01618427 0.47708055 0.44620747 0.3129758  1.00227118\n",
      "  0.37102895 0.51778678 0.60118119 0.34142788]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "10 loss -23646.708630970148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33682366 0.01425623 0.50435256 0.4711043  0.31030577 1.07961598\n",
      "  0.37058621 0.54668595 0.62667775 0.34891031]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "11 loss -24626.294582069993\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33800499 0.01216444 0.53168768 0.49604533 0.30737681 1.15699511\n",
      "  0.37011559 0.57561492 0.65218124 0.35637022]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "12 loss -25635.583678344366\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33914272 0.00993705 0.55908108 0.52102789 0.30423308 1.23439775\n",
      "  0.36962181 0.60457039 0.67769008 0.36380974]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "13 loss -26671.061594641385\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34024335 0.00759722 0.58652841 0.54604955 0.30091092 1.31181615\n",
      "  0.3691088  0.63354905 0.70320251 0.37123061]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "14 loss -27729.573303400488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "# init random thetas\n",
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -16494.182287841937\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32011886 0.01742789 0.2357877  0.22487695 0.31165927 0.31422477\n",
      "  0.3704413  0.2596181  0.37190215 0.27239461]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "1 loss -16916.06023599083\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32240975 0.01967812 0.26207358 0.24913283 0.31554645 0.38878112\n",
      "  0.3713858  0.28807452 0.39742105 0.28029567]]\n",
      "{0: 2525, 1: 289}\n",
      "(0.35294117647058826, 0.5396825396825397, 0.42677824267782427, None)\n",
      "\n",
      "2 loss -17416.749670163787\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32452451 0.02109609 0.28853598 0.27350093 0.31813751 0.46426523\n",
      "  0.37184652 0.31660673 0.42290434 0.28811702]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3493150684931507, 0.5396825396825397, 0.42411642411642414, None)\n",
      "\n",
      "3 loss -17991.625112567406\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32647211 0.02179288 0.31514827 0.29796438 0.31959997 0.54035557\n",
      "  0.3721248  0.34520201 0.44836835 0.29587051]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "4 loss -18635.534044730877\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32826902 0.02187884 0.34189006 0.32251053 0.32010328 0.61685632\n",
      "  0.3722757  0.37385273 0.47382477 0.3035663 ]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "5 loss -19343.04096959059\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32993333 0.02145398 0.36874542 0.34712976 0.31980289 0.69364125\n",
      "  0.37231688 0.4025537  0.49928154 0.31121285]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "6 loss -20108.635126544148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33148239 0.02060487 0.39570164 0.37181452 0.31883431 0.77062559\n",
      "  0.37212214 0.43130096 0.52474361 0.31881709]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "7 loss -20926.887016926226\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "8 loss -21792.56392621291\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33429748 0.01791428 0.44987709 0.42135783 0.31533315 0.9249758\n",
      "  0.37143819 0.48892073 0.5756929  0.33392039]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "9 loss -22700.711796781943\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33559094 0.01618427 0.47708055 0.44620747 0.3129758  1.00227118\n",
      "  0.37102895 0.51778678 0.60118119 0.34142788]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "10 loss -23646.708630970148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33682366 0.01425623 0.50435256 0.4711043  0.31030577 1.07961598\n",
      "  0.37058621 0.54668595 0.62667775 0.34891031]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "11 loss -24626.294582069993\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33800499 0.01216444 0.53168768 0.49604533 0.30737681 1.15699511\n",
      "  0.37011559 0.57561492 0.65218124 0.35637022]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "12 loss -25635.583678344366\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33914272 0.00993705 0.55908108 0.52102789 0.30423308 1.23439775\n",
      "  0.36962181 0.60457039 0.67769008 0.36380974]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "13 loss -26671.061594641385\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34024335 0.00759722 0.58652841 0.54604955 0.30091092 1.31181615\n",
      "  0.3691088  0.63354905 0.70320251 0.37123061]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "14 loss -27729.573303400488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "# LFS init random thetas\n",
    "\n",
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 7 4 1 0 6 3 2 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([ 0.4751682, 0.46430319 , 0.77729748 , 0.69961045 , 0.43660742,  4.98316919,\n",
    "   0.4786732 , -0.29070728, -0.31361022, -0.41560446])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00019475777320921748\n",
      "[0.2        0.2        0.20397272 0.20397272 0.2        0.2\n",
      " 0.2        0.19602728 0.19602728 0.19602728]\n",
      "[1.         1.         0.99818703 0.99876917 1.         1.\n",
      " 1.         1.00189171 1.00123147 1.00159078]\n",
      "2714 100\n",
      "0  dm  (0.6518128224023582, 0.583047619047619, 0.604245316341007, None)\n",
      "0  db  (0.36, 0.19047619047619047, 0.24913494809688577, None)\n",
      "\n",
      "-7.1199749476605e+32\n",
      "[ 2.39523991e-01  4.44712440e-01  1.00592853e+00  1.00306476e+00\n",
      "  7.93729267e-01 -2.47683784e+10  8.30695122e-01 -1.40683111e+17\n",
      " -1.37206290e+17 -1.36481844e+17]\n",
      "[9.68596811e-01 8.16929334e-01 8.01833673e-01 8.35694159e-01\n",
      " 6.32314677e-01 2.47683784e+10 9.20080431e-01 1.40683111e+17\n",
      " 1.37206290e+17 1.36481844e+17]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.6513554689606083e+67\n",
      "[ 2.87203003e-01  7.29222600e-01  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -4.86866279e+20  8.94995005e-01 -2.71479292e+34\n",
      " -2.64769995e+34 -2.63372016e+34]\n",
      "[9.31534051e-01 6.56469425e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 4.86866279e+20 9.15642897e-01 2.71479292e+34\n",
      " 2.64769995e+34 2.63372016e+34]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-9.873188985162394e+101\n",
      "[ 3.24040405e-01  8.92774981e-01  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -8.57801313e+30  8.94995005e-01 -5.23879560e+51\n",
      " -5.10932482e+51 -5.08234771e+51]\n",
      "[9.03578251e-01 6.06795047e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 8.57801313e+30 9.15642897e-01 5.23879560e+51\n",
      " 5.10932482e+51 5.08234771e+51]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-3.6766047358767233e+136\n",
      "[ 4.03004051e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -1.52645882e+41  9.22433705e-01 -1.01094191e+69\n",
      " -9.85957646e+68 -9.80751814e+68]\n",
      "[8.45939120e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 1.52645882e+41 9.12767335e-01 1.01094191e+69\n",
      " 9.85957646e+68 9.80751814e+68]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-1.3691039849622358e+171\n",
      "[ 4.52881733e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -8.87619724e+51  9.68003629e-01 -1.95083683e+86\n",
      " -1.90262415e+86 -1.89257834e+86]\n",
      "[8.11376459e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 8.87619724e+51 9.09801058e-01 1.95083683e+86\n",
      " 1.90262415e+86 1.89257834e+86]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "0 -2.516208931471449e+194\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "-1.2643949596623685e+189\n",
      "[ 4.92911523e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -4.18977398e+57  9.86196450e-01 -1.33557247e+96\n",
      " -1.30256533e+96 -1.29568782e+96]\n",
      "[7.84826492e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 4.18977398e+57 9.09252111e-01 1.33557247e+96\n",
      " 1.30256533e+96 1.29568782e+96]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-2.3895655466420104e+225\n",
      "[ 5.24003463e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -1.25083611e+068  1.00438011e+000 -2.57728356e+113\n",
      " -2.51358897e+113 -2.50031728e+113]\n",
      "[7.65011828e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 1.25083611e+068 9.09066965e-001 2.57728356e+113\n",
      " 2.51358897e+113 2.50031728e+113]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-8.898328613657434e+259\n",
      "[ 5.61785684e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -2.45873958e+078  1.00438011e+000 -4.97344076e+130\n",
      " -4.85052791e+130 -4.82491724e+130]\n",
      "[7.41972292e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 2.45873958e+078 9.09066965e-001 4.97344076e+130\n",
      " 4.85052791e+130 4.82491724e+130]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-3.313583602170098e+294\n",
      "[ 5.91204610e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -4.33201092e+088  1.00438011e+000 -9.59735801e+147\n",
      " -9.36017038e+147 -9.31074891e+147]\n",
      "[7.24887158e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 4.33201092e+088 9.09066965e-001 9.59735801e+147\n",
      " 9.36017038e+147 9.31074891e+147]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/ipykernel_launcher.py:60: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "1 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "2 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "3 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "4 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "                print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "                print(c,\" dm \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "                print(c,\" db \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "  \n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00024597518522208474\n",
      "[1.         1.         1.         1.         1.         1.00531229\n",
      " 1.         1.         1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "0 -0.9839007408883389\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002477972163026618\n",
      "[1.         1.         1.         1.         1.         1.01071759\n",
      " 1.         1.00531229 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "1 -0.9911888652106471\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.00024967684220414605\n",
      "[1.         1.         1.         1.         1.         1.01621772\n",
      " 1.         1.01071759 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "2 -0.9987073688165843\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002516160600819831\n",
      "[1.         1.         1.         1.         1.         1.0218145\n",
      " 1.         1.01621772 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "3 -1.0064642403279322\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002536169307750545\n",
      "[1.         1.         1.         1.         1.         1.02750979\n",
      " 1.         1.0218145  1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "4 -1.014467723100218\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network 2 to get thetas \n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
