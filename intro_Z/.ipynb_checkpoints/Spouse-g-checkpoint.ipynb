{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "session = SnorkelSession()\n",
    "dev_cands = session.query(Spouse).filter(Spouse.split == 1).all()\n",
    "test_cands = session.query(Spouse).filter(Spouse.split == 2).all()\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "gold_labels_test = [L[0,0] for L in L_gold_test]\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2625, 1: 189}\n",
      "{0: 2484, 1: 218}\n",
      "2814 2702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(gold_labels_dev, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "unique, counts = np.unique(gold_labels_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(len(gold_labels_dev),len(gold_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# import gensim.matutils as gm\n",
    "\n",
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# # Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "# model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "# wordvec_unavailable= set()\n",
    "# def write_to_file(wordvec_unavailable):\n",
    "#     with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "#         for word in wordvec_unavailable:\n",
    "#             f.write(word+\"\\n\")\n",
    "\n",
    "# def preprocess(tokens):\n",
    "#     btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "#     btw_words = [word for word in btw_words if word.isalpha()]\n",
    "#     return btw_words\n",
    "\n",
    "# def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "#     word_vectors= []\n",
    "#     for word in btw_words:\n",
    "#         try:\n",
    "#             word_v = np.array(model[word])\n",
    "#             word_v = word_v.reshape(len(word_v),1)\n",
    "#             #print(word_v.shape)\n",
    "#             word_vectors.append(model[word])\n",
    "#         except:\n",
    "#             wordvec_unavailable.add(word)\n",
    "#     return word_vectors\n",
    "\n",
    "# def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "#     similarity = 0\n",
    "#     target_word_vector = 0\n",
    "#     try:\n",
    "#         target_word_vector = model[target_word]\n",
    "#     except:\n",
    "#         wordvec_unavailable.add(target_word+\" t\")\n",
    "#         return similarity\n",
    "#     target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "#     for wv in word_vectors:\n",
    "#         wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "#         similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "#     return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### Discrete ##########\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "#     get_text_between, get_tagged_text,\n",
    "# )\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,0)\n",
    "    \n",
    "# def LF_familial_relationship(c):\n",
    "#     return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x.decode('utf-8')).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,0)\n",
    "\n",
    "\n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Continuous ################\n",
    "\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "#     get_text_between, get_tagged_text,\n",
    "# )\n",
    "\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     global LF_Threshold\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for sw in spouses:\n",
    "#         sc=max(sc,get_similarity(word_vectors,sw))\n",
    "#     return (1,sc)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     global LF_Threshold\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for sw in spouses:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for sw in spouses:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "#     return(1,max(sc_1,sc_2))\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     global LF_Threshold\n",
    "#     word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "#     sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "#     if 'and' in get_between_tokens(c):\n",
    "#         return (1,sc)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_familial_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for fw in family:\n",
    "#         sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for fw in family:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for fw in family:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,max(sc_1,sc_2))\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for ow in other:\n",
    "#         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# # def LF_other_relationship_left_window(c):\n",
    "# #     sc = 0\n",
    "# #     word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "# #     for ow in other:\n",
    "# #         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "# #     return (-1,sc) \n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     s = s.decode(\"utf-8\") \n",
    "#     return ''.join(c for c in s if (ord(c) < 128))\n",
    "\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'r') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # def LF_Three_Lists_Left_Window(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife_left_window(c)\n",
    "# #     c2,s2 = LF_family_left_window(c)\n",
    "# #     c3,s3 = LF_other_relationship_left_window(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "\n",
    "# # def LF_Three_Lists_Between_Words(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife(c)\n",
    "# #     c2,s2 = LF_familial_relationship(c)\n",
    "# #     c3,s3 = LF_other_relationship(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "    \n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'BW:',s1,s2,s3,prob_sc\n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "    \n",
    "output shape : [NoOfDataPoints,2,NoOfLFs]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# import numpy as np\n",
    "# start_time = time.time()\n",
    "\n",
    "# lt = time.localtime()\n",
    "\n",
    "# print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "# # dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "# # np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "# # train_L_S = get_L_S_Tensor(train_cands)\n",
    "# # np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "\n",
    "# # test_L_S = get_L_S_Tensor(test_cands)\n",
    "# # np.save(\"test_L_S_smooth\",np.array(test_L_S))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LF_l = [\n",
    "#     1,1,1,1,1,-1,1,-1,-1,-1\n",
    "# ]\n",
    "# import numpy as np\n",
    "# dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "# test_L_S = np.load(\"test_L_S_smooth.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "# # dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "# # train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "# # true_labels = gold_labels_test\n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(test_L_S.shape,dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2702, 2, 10) (2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "test_L_S = np.load(\"test_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "print(test_L_S.shape,dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32\n",
    "seed = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LF_l = [\n",
    "#     1,1,1,1,1,-1,1,-1,-1,-1\n",
    "# ]\n",
    "\n",
    "# def merge(a,b):\n",
    "#     c = []\n",
    "#     for i in range(len(a)):\n",
    "#         ci = []\n",
    "#         ci_l = a[i,0,:].tolist()+b[i,0,:].tolist()\n",
    "#         ci_s = a[i,1,:].tolist()+b[i,1,:].tolist()\n",
    "#         ci.append(ci_l)\n",
    "#         ci.append(ci_s)\n",
    "#         c.append(ci)\n",
    "#     return c\n",
    "# import numpy as np\n",
    "# dev_L_S_s = np.load(\"dev_L_S_smooth.npy\")\n",
    "# train_L_S_s = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "# dev_L_S_d = np.load(\"dev_L_S_discrete.npy\")\n",
    "# train_L_S_d = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "# dev_L_S = np.array(merge(dev_L_S_d,dev_L_S_s))\n",
    "# train_L_S = np.array(merge(train_L_S_d,train_L_S_s))\n",
    "\n",
    "# LF_l = LF_l + LF_l\n",
    "# print(len(LF_l))\n",
    "\n",
    "\n",
    "# print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LF_l)\n",
    "NoOfClasses = 2\n",
    "print(len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized model with smooth LFs + penalties\n",
    "\n",
    "def train(lr,ep,th,af,batch_size=32,LF_acc=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
    "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (to set a max value for alpha)\n",
    "    ## norm : use normalization or not\n",
    "    ## Gamma : penalty tuning parameter\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        test_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=af,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "        \n",
    "        \n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    \n",
    "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
    "        \n",
    "        LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
    "        \n",
    "#         print(\"k\",k)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "        \n",
    "        s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "        print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "        if(smooth):\n",
    "            pout = tf.map_fn(lambda c: l*c*s_ ,pcl,name=\"pout\")\n",
    "        else:\n",
    "            pout = tf.map_fn(lambda c: l*c ,pcl,name=\"pout\")\n",
    "\n",
    "        print(\"pout\",pout)    \n",
    "\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                           name=\"t_pout\")\n",
    "    \n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            print(\"intsy\",out1)\n",
    "            return out1\n",
    "                \n",
    "\n",
    "        if(smooth):\n",
    "            #smooth normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "        else:\n",
    "            #discrete normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "            \n",
    "        ### for precision penalty\n",
    "        def softplus(j):\n",
    "            Lj = tf.map_fn(lambda li : tf.gather(li,j),l)\n",
    "            print(\"sft Lj\",Lj)\n",
    "            kj = tf.gather(k,j)\n",
    "            print(\"sft kj\",kj)\n",
    "            aj = tf.gather(LF_a,j)\n",
    "            print(\"sft aj\",aj)\n",
    "            indices = tf.where(tf.equal(Lj,kj))\n",
    "            print(\"sft indices\",indices)\n",
    "            li_lij_eq_kj = tf.gather(l,tf.squeeze(indices,1))\n",
    "            print(\"sft l_ij_eq_kj\",li_lij_eq_kj)\n",
    "            prec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            print(\"prec_z\",prec_z)\n",
    "            prec_t_pout = (tf.matmul(li_lij_eq_kj*kj, thetas,transpose_b=True))/prec_z\n",
    "            print(\"prec_t_pout\",prec_t_pout)\n",
    "            f =  tf.reduce_sum(aj - prec_t_pout)\n",
    "            print(\"f\",f)\n",
    "            sft = tf.nn.softplus(f,name=\"sft\")\n",
    "            print(\"sft\",sft)\n",
    "            return sft\n",
    "        \n",
    "        plogsft = tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))\n",
    "        psft  =  tf.reduce_sum(tf.map_fn(lambda j: softplus(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))\n",
    "        psftg = tf.gradients(psft, [thetas])\n",
    "        plogsftg = tf.gradients(plogsft, [thetas])\n",
    "        \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        \n",
    "        print(\"zy\",zy)\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        \n",
    "        print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk))))\n",
    "        elif(penalty == 4):\n",
    "            print(\"precision penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 5):\n",
    "            print(\"precision log(softplus) penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "  \n",
    "        else:\n",
    "            print(\"normalized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "       \n",
    "            \n",
    "        print(\"loss\",loss)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        \n",
    "              \n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,loss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "                print(\"dev set\")\n",
    "                sess.run(dev_init_op)\n",
    "                if(penalty == 4):\n",
    "                    sm,s,sg,a,t,m,pl = sess.run([summary_merged,psft,psftg,alphas,thetas,marginals,predict])\n",
    "                    print(\"softplus\",s)\n",
    "                    print(\"softplus-grad\",sg)\n",
    "                elif(penalty == 5):\n",
    "                    sm,s,sg,a,t,m,pl = sess.run([summary_merged,plogsft,plogsftg,alphas,thetas,marginals,predict])\n",
    "                    print(\"log-softplus\",s)\n",
    "                    print(\"log-softplus-grad\",sg)\n",
    "                else:\n",
    "                    sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "                if(smooth):\n",
    "                    print(\"alphas\",a)\n",
    "                print(\"thetas\",t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "                print(\"test set\")\n",
    "                sess.run(test_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_test,pl))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_test),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "                \n",
    "#             # Initialize an iterator over the validation dataset.\n",
    "#             sess.run(dev_init_op)\n",
    "#             a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "#             print(t)\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "# #             predictAndPrint(pl)\n",
    "#             print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 189, -1: 2625}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7.10732054e-04, 2.48756219e-03, 3.30490405e-02, 2.87846482e-02,\n",
       "       6.75195451e-03, 5.99502488e-01, 3.55366027e-04, 1.12651031e-01,\n",
       "       7.46268657e-02, 7.46268657e-03])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "def get_LF_acc(L_S,true_labels):\n",
    "    #L_S : a numpy array of [NoOfDataPoints,2,NoOfLFs] \n",
    "    #true_labels : numpy array [NoOfDataPoints]\n",
    "    \n",
    "    tl = [-1 if x==0 else x for x in true_labels]\n",
    "    unique, counts = np.unique(tl, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "\n",
    "    # take only labels \n",
    "    L_S = L_S[:,0,:]\n",
    "    #L_S shape [NoOfDataPoints,NoOfLFs]\n",
    "    LF_acc = []\n",
    "    for i in range(L_S.shape[1]):\n",
    "#         print(accuracy_score(L_S[:,i],tl,normalize=False),accuracy_score(L_S[:,i],tl))\n",
    "        LF_acc.append(accuracy_score(L_S[:,i],tl))\n",
    "#         unique, counts = np.unique(L_S[:,i], return_counts=True)\n",
    "#         print(i,dict(zip(unique, counts)))\n",
    "#         print(precision_score(L_S[:,i],tl,labels=[LF_l[i]],average='macro'))\n",
    "#         LF_acc.append(precision_score(L_S[:,i],tl,labels=[LF_l[i]],average='macro'))\n",
    "    return np.array(LF_acc)\n",
    "                      \n",
    "get_LF_acc(dev_L_S,gold_labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 32\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 191004.97299376698\n",
      "dev set\n",
      "softplus 1114.5490857133777\n",
      "softplus-grad [array([[ 0.23517041,  0.20697423,  0.1888159 ,  0.1945504 ,  0.23371264,\n",
      "        -0.23982406,  0.23999047, -0.08100806, -0.0585641 , -0.01672878]])]\n",
      "thetas [[1.05046739 0.74753509 0.94381144 0.93478734 1.03929744 1.10701521\n",
      "  1.10204145 1.08907541 1.20153912 1.11674167]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "1 loss 185851.3357691606\n",
      "dev set\n",
      "softplus 1114.4536481119978\n",
      "softplus-grad [array([[ 0.23088801,  0.19598473,  0.17580128,  0.18247895,  0.22910384,\n",
      "        -0.2295067 ,  0.23686027, -0.04489648, -0.01947002,  0.02708203]])]\n",
      "thetas [[0.989175   0.68753803 0.88489167 0.87550729 0.97804014 1.16788496\n",
      "  1.04054882 1.12098156 1.22092219 1.10928555]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "2 loss 182156.79791380258\n",
      "dev set\n",
      "softplus 1114.369820967678\n",
      "softplus-grad [array([[ 0.21981494,  0.17897015,  0.15717893,  0.16464977,  0.21773637,\n",
      "        -0.21410155,  0.22682463, -0.01101177,  0.0152753 ,  0.06183958]])]\n",
      "thetas [[0.93212529 0.63277197 0.83153932 0.8216209  0.92105164 1.22506825\n",
      "  0.98316046 1.11924539 1.19400962 1.03269697]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "3 loss 179133.15308558205\n",
      "dev set\n",
      "softplus 1114.2900535297008\n",
      "softplus-grad [array([[ 0.20773856,  0.16130946,  0.1379851 ,  0.14619316,  0.20537174,\n",
      "        -0.19982639,  0.21574414,  0.01603952,  0.04254434,  0.08952335]])]\n",
      "thetas [[0.87727238 0.58112036 0.78167621 0.7710377  0.86626633 1.28056282\n",
      "  0.92784622 1.08552717 1.13591773 0.9439649 ]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "4 loss 176477.80983678324\n",
      "dev set\n",
      "softplus 1114.2131420975243\n",
      "softplus-grad [array([[ 0.19266947,  0.14103285,  0.11633055,  0.12519889,  0.19001815,\n",
      "        -0.18409281,  0.20162947,  0.03905393,  0.06635054,  0.11461066]])]\n",
      "thetas [[0.82391423 0.53197891 0.73478342 0.72319797 0.81297609 1.33505686\n",
      "  0.8738989  1.03145839 1.06695089 0.85910137]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "batch-size: 64\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 190372.69727217496\n",
      "dev set\n",
      "softplus 1114.6027651130664\n",
      "softplus-grad [array([[ 0.22725929,  0.20275936,  0.185949  ,  0.19111567,  0.22598195,\n",
      "        -0.23475479,  0.23144922, -0.09206826, -0.0718528 , -0.0341934 ]])]\n",
      "thetas [[1.08326091 0.7800262  0.97562564 0.96672174 1.07208203 1.0751333\n",
      "  1.13489453 1.0636114  1.17828451 1.09574942]]\n",
      "{0: 2515, 1: 299}\n",
      "acc 0.900497512437811\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "test set\n",
      "{0: 2334, 1: 368}\n",
      "acc 0.8926720947446336\n",
      "(0.40217391304347827, 0.6788990825688074, 0.5051194539249146, None)\n",
      "\n",
      "1 loss 187293.52151850145\n",
      "dev set\n",
      "softplus 1114.5484308492494\n",
      "softplus-grad [array([[ 0.23502903,  0.20678956,  0.18859637,  0.19433818,  0.23356879,\n",
      "        -0.23944643,  0.23985699, -0.08045113, -0.05798325, -0.01617563]])]\n",
      "thetas [[1.05043632 0.74748445 0.9433447  0.93436749 1.03925815 1.10783073\n",
      "  1.10202368 1.09075346 1.20333657 1.11785309]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "2 loss 184729.33969632062\n",
      "dev set\n",
      "softplus 1114.4984225855274\n",
      "softplus-grad [array([[ 0.23484715,  0.20311167,  0.18379609,  0.19004378,  0.23321675,\n",
      "        -0.23600619,  0.24027397, -0.06296519, -0.0387336 ,  0.00611116]])]\n",
      "thetas [[1.01911884 0.71668899 0.91286358 0.9037764  1.00795142 1.13919716\n",
      "  1.07062574 1.11142712 1.21932428 1.12446288]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "3 loss 182606.69042783175\n",
      "dev set\n",
      "softplus 1114.4528599284886\n",
      "softplus-grad [array([[ 0.23036633,  0.19541143,  0.17516497,  0.181859  ,  0.22857878,\n",
      "        -0.22849907,  0.23634821, -0.04370007, -0.01834441,  0.02790769]])]\n",
      "thetas [[0.98918175 0.68753185 0.88409298 0.87485839 0.97803304 1.16934824\n",
      "  1.0405712  1.12402813 1.22321346 1.10854156]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "4 loss 180787.06206774316\n",
      "dev set\n",
      "softplus 1114.4100388896275\n",
      "softplus-grad [array([[ 0.22521402,  0.18722182,  0.16611923,  0.17322916,  0.2232764 ,\n",
      "        -0.2208469 ,  0.23172403, -0.0260669 , -0.00025974,  0.04612966]])]\n",
      "thetas [[0.96028087 0.65965734 0.85667682 0.84725822 0.94915569 1.19863041\n",
      "  1.01151813 1.12741864 1.21327464 1.07259371]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "batch-size: 128\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 190146.08489934003\n",
      "dev set\n",
      "softplus 1114.6307307713184\n",
      "softplus-grad [array([[ 0.22009445,  0.19749407,  0.18146018,  0.18631852,  0.21891066,\n",
      "        -0.22891073,  0.22396112, -0.09497341, -0.07599453, -0.04061807]])]\n",
      "thetas [[1.10019132 0.79688462 0.99229921 0.98339829 1.08901607 1.05852355\n",
      "  1.15184303 1.04817553 1.16332452 1.08130898]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 188469.08664866403\n",
      "dev set\n",
      "softplus 1114.602514845143\n",
      "softplus-grad [array([[ 0.22729005,  0.20277337,  0.18595115,  0.19111929,  0.22601202,\n",
      "        -0.23472354,  0.23148312, -0.09197893, -0.07174843, -0.03406951]])]\n",
      "thetas [[1.08321859 0.77997465 0.9754585  0.96654926 1.07204042 1.07544932\n",
      "  1.13485901 1.06392956 1.17869108 1.0962146 ]]\n",
      "{0: 2515, 1: 299}\n",
      "acc 0.900497512437811\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "test set\n",
      "{0: 2334, 1: 368}\n",
      "acc 0.8926720947446336\n",
      "(0.40217391304347827, 0.6788990825688074, 0.5051194539249146, None)\n",
      "\n",
      "2 loss 186923.59539151718\n",
      "dev set\n",
      "softplus 1114.5748068570276\n",
      "softplus-grad [array([[ 0.23225688,  0.20584151,  0.1882903 ,  0.19375576,  0.23088595,\n",
      "        -0.23816296,  0.23677372, -0.08698591, -0.06557496, -0.02573283]])]\n",
      "thetas [[1.06660095 0.76347058 0.95902728 0.95010105 1.05542284 1.09207458\n",
      "  1.11822306 1.07847531 1.1924716  1.10883295]]\n",
      "{0: 2517, 1: 297}\n",
      "acc 0.900497512437811\n",
      "(0.3468013468013468, 0.544973544973545, 0.4238683127572016, None)\n",
      "\n",
      "test set\n",
      "{0: 2335, 1: 367}\n",
      "acc 0.8930421909696521\n",
      "(0.4032697547683924, 0.6788990825688074, 0.505982905982906, None)\n",
      "\n",
      "3 loss 185511.86991136323\n",
      "dev set\n",
      "softplus 1114.5479839301652\n",
      "softplus-grad [array([[ 0.23496346,  0.20669419,  0.18848056,  0.19422737,  0.23350213,\n",
      "        -0.23924241,  0.23979688, -0.08013545, -0.05764378, -0.01584714]])]\n",
      "thetas [[1.05035964 0.74739885 0.94303886 0.93408827 1.0391837  1.10836928\n",
      "  1.10195531 1.09161209 1.2043412  1.11845168]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "4 loss 184230.11968958564\n",
      "dev set\n",
      "softplus 1114.5223077855605\n",
      "softplus-grad [array([[ 0.23560636,  0.20554828,  0.18673966,  0.19274997,  0.23405799,\n",
      "        -0.23821113,  0.24074586, -0.07176276, -0.04831742, -0.00485161]])]\n",
      "thetas [[1.03450252 0.73177198 0.92750954 0.91852692 1.02333076 1.12432312\n",
      "  1.08606291 1.10311869 1.21390727 1.12414477]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "batch-size: 512\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 190070.44850940816\n",
      "dev set\n",
      "softplus 1114.6519822379898\n",
      "softplus-grad [array([[ 0.21335829,  0.19218944,  0.17677854,  0.18139707,  0.21224542,\n",
      "        -0.22309178,  0.21698148, -0.09592944, -0.07792587, -0.04431223]])]\n",
      "thetas [[1.11315418 0.80982564 1.00517418 0.9962463  1.10198603 1.04571119\n",
      "  1.16481441 1.03559505 1.15073478 1.06906923]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 189623.9927740934\n",
      "dev set\n",
      "softplus 1114.6448598916072\n",
      "softplus-grad [array([[ 0.21573591,  0.19408838,  0.17846517,  0.18316386,  0.21459938,\n",
      "        -0.22516788,  0.21944058, -0.09572615, -0.07739619, -0.04318275]])]\n",
      "thetas [[1.10879929 0.80547346 1.00082993 0.99190382 1.09762966 1.05005652\n",
      "  1.16045869 1.03988815 1.15500875 1.07333001]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 189185.14832040507\n",
      "dev set\n",
      "softplus 1114.6377221743362\n",
      "softplus-grad [array([[ 0.21799754,  0.19586917,  0.18003601,  0.18481455,  0.21683729,\n",
      "        -0.22711523,  0.22178408, -0.09540157, -0.07674685, -0.04193599]])]\n",
      "thetas [[1.10446047 0.80114007 0.99650318 0.98757763 1.09328986 1.05439209\n",
      "  1.15611881 1.0441236  1.15920788 1.07749902]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 188754.19211160485\n",
      "dev set\n",
      "softplus 1114.6305804394765\n",
      "softplus-grad [array([[ 0.22013601,  0.19752543,  0.18148509,  0.18634307,  0.21895202,\n",
      "        -0.22892735,  0.22400467, -0.09495293, -0.07597562, -0.0405713 ]])]\n",
      "thetas [[1.10014063 0.79682845 0.99219785 0.98327227 1.08896926 1.05871299\n",
      "  1.15179753 1.0482981  1.16332926 1.08156515]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 188331.4252256391\n",
      "dev set\n",
      "softplus 1114.623443272481\n",
      "softplus-grad [array([[ 0.22214576,  0.19905216,  0.18280768,  0.18774461,  0.22093801,\n",
      "        -0.23059918,  0.22609669, -0.09437808, -0.07508086, -0.03908816]])]\n",
      "thetas [[1.09584107 0.79254002 0.98791575 0.97898971 1.08466911 1.0630172\n",
      "  1.14749614 1.05240869 1.16736916 1.08552011]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "batch-size: 1024\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 190084.50226686918\n",
      "dev set\n",
      "softplus 1114.655552107259\n",
      "softplus-grad [array([[ 0.21212236,  0.19119297,  0.17588962,  0.18046754,  0.21102136,\n",
      "        -0.2220023 ,  0.21570482, -0.0959879 , -0.07814935, -0.04483547]])]\n",
      "thetas [[1.11534728 0.81201759 1.00736348 0.99843053 1.10418014 1.04353035\n",
      "  1.16700834 1.03340718 1.14853992 1.06690917]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 189858.75514250092\n",
      "dev set\n",
      "softplus 1114.6519898492122\n",
      "softplus-grad [array([[ 0.21335587,  0.19218749,  0.17677686,  0.18139504,  0.21224305,\n",
      "        -0.22308913,  0.21697901, -0.09593015, -0.07792752, -0.04431267]])]\n",
      "thetas [[1.11315953 0.80983025 1.00517941 0.99624764 1.10199172 1.04571306\n",
      "  1.16482036 1.03558387 1.15071385 1.06907757]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 189634.90520075132\n",
      "dev set\n",
      "softplus 1114.6484203101566\n",
      "softplus-grad [array([[ 0.21456228,  0.19315414,  0.17763672,  0.1822951 ,  0.21343762,\n",
      "        -0.22414548,  0.21822624, -0.09584263, -0.07767616, -0.04376046]])]\n",
      "thetas [[1.11097479 0.80764652 1.00299849 0.99406748 1.09980649 1.04789462\n",
      "  1.16263536 1.0377479  1.15287076 1.0712272 ]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 189412.90728015162\n",
      "dev set\n",
      "softplus 1114.6448460452714\n",
      "softplus-grad [array([[ 0.21574024,  0.19409175,  0.17846809,  0.18316662,  0.21460373,\n",
      "        -0.22517013,  0.21944512, -0.09572494, -0.07739496, -0.04317896]])]\n",
      "thetas [[1.10879401 0.80546743 1.00082192 0.99189145 1.09762534 1.0500736\n",
      "  1.16045426 1.03989812 1.15500982 1.07335454]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 189192.86654528618\n",
      "dev set\n",
      "softplus 1114.6412687919878\n",
      "softplus-grad [array([[ 0.21688869,  0.19499937,  0.17927011,  0.1840087 ,  0.21574031,\n",
      "        -0.22616214,  0.22063458, -0.09557669, -0.07708364, -0.0425681 ]])]\n",
      "thetas [[1.10661774 0.80329352 0.99865041 0.98972034 1.09544875 1.05224919\n",
      "  1.15827758 1.04203387 1.15713046 1.07545783]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "batch-size: 2048\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 190099.40944124435\n",
      "dev set\n",
      "softplus 1114.6573360913671\n",
      "softplus-grad [array([[ 0.21149368,  0.19068383,  0.17543454,  0.1799922 ,  0.21039859,\n",
      "        -0.22144599,  0.21505579, -0.09600606, -0.07825015, -0.04508652]])]\n",
      "thetas [[1.11644554 0.81311549 1.00846079 0.99952619 1.10527864 1.04243593\n",
      "  1.16810682 1.03230828 1.14743646 1.06581937]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 189985.87889610248\n",
      "dev set\n",
      "softplus 1114.6555544971825\n",
      "softplus-grad [array([[ 0.21212161,  0.19119235,  0.17588911,  0.18046692,  0.21102061,\n",
      "        -0.22200158,  0.21570405, -0.09598816, -0.07815005, -0.04483561]])]\n",
      "thetas [[1.11534873 0.81201871 1.00736522 0.99843098 1.10418159 1.04353065\n",
      "  1.16700993 1.03340366 1.14853112 1.06691208]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 189872.82387557707\n",
      "dev set\n",
      "softplus 1114.653770502334\n",
      "softplus-grad [array([[ 0.21274302,  0.19169412,  0.17633704,  0.180935  ,  0.21163612,\n",
      "        -0.2225498 ,  0.21634583, -0.09596284, -0.07804253, -0.04457724]])]\n",
      "thetas [[1.11425249 0.81092261 1.00627032 0.99733631 1.10308516 1.04462511\n",
      "  1.16591362 1.0344966  1.14962242 1.06800153]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 189760.202788305\n",
      "dev set\n",
      "softplus 1114.6519845448713\n",
      "softplus-grad [array([[ 0.21335773,  0.19218897,  0.17677818,  0.18139628,  0.21224491,\n",
      "        -0.22309045,  0.21698094, -0.09593005, -0.0779276 , -0.04431148]])]\n",
      "thetas [[1.113157   0.80982739 1.00517625 0.99624239 1.1019895  1.04571916\n",
      "  1.16481804 1.03558668 1.15070998 1.06908662]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 189648.04097859588\n",
      "dev set\n",
      "softplus 1114.6501970694103\n",
      "softplus-grad [array([[ 0.21396552,  0.19267674,  0.17721237,  0.18185058,  0.21284678,\n",
      "        -0.22362335,  0.21760916, -0.09588977, -0.07780524, -0.0440384 ]])]\n",
      "thetas [[1.11206243 0.80873326 1.0040832  0.99514943 1.1008948  1.0468126\n",
      "  1.16372337 1.03667358 1.15179343 1.0701666 ]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "batch-size: 4096\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 190109.96797280968\n",
      "dev set\n",
      "softplus 1114.6581547200951\n",
      "softplus-grad [array([[ 0.21120272,  0.19044771,  0.17522329,  0.17977167,  0.21011033,\n",
      "        -0.22118803,  0.2147555 , -0.09601192, -0.07829388, -0.04519936]])]\n",
      "thetas [[1.11695004 0.81361994 1.00896526 1.00003012 1.10578317 1.04193265\n",
      "  1.16861139 1.03180258 1.1469298  1.06531725]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 190049.06125400894\n",
      "dev set\n",
      "softplus 1114.6572099479113\n",
      "softplus-grad [array([[ 0.21153843,  0.19072013,  0.17546706,  0.18002612,  0.21044291,\n",
      "        -0.22148574,  0.21510199, -0.09600527, -0.07824363, -0.04506896]])]\n",
      "thetas [[1.11636737 0.81303723 1.00838361 0.99944854 1.10520035 1.04251389\n",
      "  1.1680287  1.03238393 1.1475105  1.06589852]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 189988.89682882637\n",
      "dev set\n",
      "softplus 1114.6562665867266\n",
      "softplus-grad [array([[ 0.21187159,  0.19099005,  0.17570843,  0.18027816,  0.21077295,\n",
      "        -0.22178075,  0.21544591, -0.09599657, -0.07819141, -0.04493676]])]\n",
      "thetas [[1.11578611 0.81245595 1.00780352 0.99886848 1.10461894 1.04309359\n",
      "  1.16744742 1.03296336 1.14808899 1.06647778]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 189928.93411281198\n",
      "dev set\n",
      "softplus 1114.6553228746998\n",
      "softplus-grad [array([[ 0.21220281,  0.191258  ,  0.17594788,  0.18052826,  0.21110104,\n",
      "        -0.22207361,  0.2157879 , -0.09598579, -0.07813714, -0.04480251]])]\n",
      "thetas [[1.1152052  0.81187504 1.00722385 0.99828881 1.10403789 1.04367294\n",
      "  1.16686649 1.03354196 1.14866642 1.06705595]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 189869.0994157487\n",
      "dev set\n",
      "softplus 1114.6543783954583\n",
      "softplus-grad [array([[ 0.21253224,  0.19152409,  0.17618548,  0.18077653,  0.21142734,\n",
      "        -0.22236443,  0.21612812, -0.09597292, -0.07808078, -0.04466616]])]\n",
      "thetas [[1.11462437 0.81129424 1.00664431 0.99770927 1.10345692 1.04425223\n",
      "  1.16628565 1.03411995 1.14924303 1.06763314]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in [32,64,128,512,1024,2048,4096]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.0001,5,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.array([-1,1],dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 32\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 178090.7014773185\n",
      "dev set\n",
      "log-softplus 15.84190004544293\n",
      "log-softplus-grad [array([[ 1.08413565e-02,  7.37359793e-03,  4.11554615e-03,\n",
      "         5.07109139e-03,  5.19145341e-03,  7.77791546e-05,\n",
      "         1.09163331e-02,  1.74457981e-04, -7.54027203e-04,\n",
      "        -2.26774032e-03]])]\n",
      "thetas [[1.05046765 0.74753536 0.94381179 0.9347877  1.03929769 1.10701485\n",
      "  1.1020417  1.08907549 1.20153924 1.11674146]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "1 loss 172937.52652820025\n",
      "dev set\n",
      "log-softplus 15.839703096091505\n",
      "log-softplus-grad [array([[ 0.00952881,  0.0055091 ,  0.00161939,  0.00274753,  0.00286186,\n",
      "         0.00251619,  0.00960693,  0.00251267,  0.00132146, -0.00055264]])]\n",
      "thetas [[0.98917596 0.687539   0.88489302 0.87550859 0.97804113 1.16788346\n",
      "  1.04054977 1.12098184 1.22092292 1.10928662]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "2 loss 169243.43417698075\n",
      "dev set\n",
      "log-softplus 15.838356365960125\n",
      "log-softplus-grad [array([[ 0.00815309,  0.00368318, -0.00078901,  0.00049237,  0.00058226,\n",
      "         0.00457767,  0.00822799,  0.00432683,  0.00282782,  0.00049233]])]\n",
      "thetas [[0.93212722 0.63277392 0.83154221 0.82162363 0.92105365 1.22506512\n",
      "  0.98316237 1.11924686 1.19401273 1.03270092]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "3 loss 166220.2403593371\n",
      "dev set\n",
      "log-softplus 15.837384388540263\n",
      "log-softplus-grad [array([[ 0.00693874,  0.00205311, -0.00300854, -0.0015762 , -0.00152568,\n",
      "         0.00627889,  0.00700608,  0.00562293,  0.00380882,  0.00107471]])]\n",
      "thetas [[0.87727545 0.58112346 0.78168117 0.77104229 0.86626959 1.28055767\n",
      "  0.92784928 1.08553133 1.13592426 0.94397076]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "4 loss 163565.3596963177\n",
      "dev set\n",
      "log-softplus 15.836741430457252\n",
      "log-softplus-grad [array([[ 0.00576591,  0.00049874, -0.00515618, -0.00357582, -0.00357999,\n",
      "         0.00776315,  0.0058216 ,  0.0065923 ,  0.00449974,  0.00143677]])]\n",
      "thetas [[0.82391856 0.53198323 0.73479094 0.72320484 0.81298073 1.33504945\n",
      "  0.8739032  1.03146607 1.06696088 0.85910892]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "batch-size: 64\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 180158.11529972043\n",
      "dev set\n",
      "log-softplus 15.84349319343119\n",
      "log-softplus-grad [array([[ 0.01108199,  0.0079523 ,  0.00504987,  0.00590528,  0.0060222 ,\n",
      "        -0.00097867,  0.01115255, -0.00087126, -0.00167565, -0.00303679]])]\n",
      "thetas [[1.08326099 0.78002628 0.97562574 0.96672185 1.0720821  1.0751332\n",
      "  1.13489461 1.06361145 1.17828456 1.09574933]]\n",
      "{0: 2515, 1: 299}\n",
      "acc 0.900497512437811\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "test set\n",
      "{0: 2334, 1: 368}\n",
      "acc 0.8926720947446336\n",
      "(0.40217391304347827, 0.6788990825688074, 0.5051194539249146, None)\n",
      "\n",
      "1 loss 177079.20013370345\n",
      "dev set\n",
      "log-softplus 15.841891460724227\n",
      "log-softplus-grad [array([[ 0.01082217,  0.00735233,  0.00409006,  0.00504645,  0.00516844,\n",
      "         0.00010456,  0.0108972 ,  0.00020485, -0.00072431, -0.00224137]])]\n",
      "thetas [[1.05043664 0.74748477 0.94334512 0.9343679  1.03925847 1.10783029\n",
      "  1.102024   1.09075344 1.20333659 1.1178529 ]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "2 loss 174515.2803750105\n",
      "dev set\n",
      "log-softplus 15.840646664674457\n",
      "log-softplus-grad [array([[ 0.01022121,  0.00645642,  0.002865  ,  0.00391213,  0.00403429,\n",
      "         0.00134546,  0.01029872,  0.00141398,  0.00035681, -0.00132736]])]\n",
      "thetas [[1.01911951 0.71668967 0.91286448 0.90377726 1.00795212 1.13919619\n",
      "  1.0706264  1.11142698 1.21932434 1.12446301]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "3 loss 172392.88250833593\n",
      "dev set\n",
      "log-softplus 15.839715205531228\n",
      "log-softplus-grad [array([[ 0.0094861 ,  0.00546666,  0.00157175,  0.00270084,  0.002818  ,\n",
      "         0.00255928,  0.00956413,  0.00256299,  0.00136747, -0.00051943]])]\n",
      "thetas [[0.98918285 0.68753297 0.88409449 0.87485984 0.97803419 1.16934659\n",
      "  1.0405723  1.12402797 1.22321388 1.10854301]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "4 loss 170573.49843917412\n",
      "dev set\n",
      "log-softplus 15.838983935705468\n",
      "log-softplus-grad [array([[8.78493101e-03, 4.53381944e-03, 3.44586752e-04, 1.55165685e-03,\n",
      "        1.65923600e-03, 3.63568185e-03, 8.86190234e-03, 3.53431584e-03,\n",
      "        2.18210536e-03, 6.18910741e-05]])]\n",
      "thetas [[0.96028247 0.65965896 0.85667908 0.84726037 0.94915737 1.19862797\n",
      "  1.01151972 1.12741867 1.21327593 1.07259697]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "batch-size: 128\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 181122.27858457886\n",
      "dev set\n",
      "log-softplus 15.844408703449773\n",
      "log-softplus-grad [array([[ 0.01104669,  0.00809973,  0.0053844 ,  0.00618657,  0.00629978,\n",
      "        -0.00141325,  0.01111441, -0.0013094 , -0.00205237, -0.00333459]])]\n",
      "thetas [[1.10019135 0.79688465 0.99229925 0.98339833 1.0890161  1.05852352\n",
      "  1.15184306 1.04817554 1.16332453 1.08130895]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 179445.42472488937\n",
      "dev set\n",
      "log-softplus 15.84348713203776\n",
      "log-softplus-grad [array([[ 0.01107955,  0.00794866,  0.00504452,  0.00590026,  0.00601771,\n",
      "        -0.00097208,  0.01115015, -0.00086461, -0.00166912, -0.0030305 ]])]\n",
      "thetas [[1.08321869 0.77997475 0.97545863 0.96654939 1.07204052 1.0754492\n",
      "  1.13485911 1.06392954 1.17869106 1.09621452]]\n",
      "{0: 2515, 1: 299}\n",
      "acc 0.900497512437811\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "test set\n",
      "{0: 2334, 1: 368}\n",
      "acc 0.8926720947446336\n",
      "(0.40217391304347827, 0.6788990825688074, 0.5051194539249146, None)\n",
      "\n",
      "2 loss 177900.08236445155\n",
      "dev set\n",
      "log-softplus 15.842641997474054\n",
      "log-softplus-grad [array([[ 0.01099825,  0.00769175,  0.00460364,  0.0055112 ,  0.00563193,\n",
      "        -0.00045592,  0.01107132, -0.00034884, -0.00121555, -0.00265561]])]\n",
      "thetas [[1.06660117 0.76347081 0.95902756 0.95010131 1.05542307 1.0920743\n",
      "  1.11822327 1.0784752  1.19247151 1.1088328 ]]\n",
      "{0: 2517, 1: 297}\n",
      "acc 0.900497512437811\n",
      "(0.3468013468013468, 0.544973544973545, 0.4238683127572016, None)\n",
      "\n",
      "test set\n",
      "{0: 2335, 1: 367}\n",
      "acc 0.8930421909696521\n",
      "(0.4032697547683924, 0.6788990825688074, 0.505982905982906, None)\n",
      "\n",
      "3 loss 176488.50683553412\n",
      "dev set\n",
      "log-softplus 15.84188440888068\n",
      "log-softplus-grad [array([[ 0.01081118,  0.00733968,  0.00407456,  0.00503158,  0.00515446,\n",
      "         0.00012104,  0.01088625,  0.00022271, -0.00070668, -0.00222592]])]\n",
      "thetas [[1.05036001 0.74739923 0.94303933 0.93408872 1.03918409 1.1083688\n",
      "  1.10195568 1.09161185 1.20434101 1.11845151]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n",
      "4 loss 175206.9046460193\n",
      "dev set\n",
      "log-softplus 15.841219252895975\n",
      "log-softplus-grad [array([[ 0.01053627,  0.0069116 ,  0.00347762,  0.00448141,  0.00460518,\n",
      "         0.00073836,  0.01061282,  0.00082844, -0.00016438, -0.00176544]])]\n",
      "thetas [[1.03450308 0.73177255 0.92751025 0.91852761 1.02333134 1.12432239\n",
      "  1.08606347 1.10311828 1.21390697 1.1241447 ]]\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "test set\n",
      "{0: 2339, 1: 363}\n",
      "acc 0.8915618060695781\n",
      "(0.39669421487603307, 0.6605504587155964, 0.49569707401032703, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 512\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 181684.2636210173\n",
      "dev set\n",
      "log-softplus 15.845141400754883\n",
      "log-softplus-grad [array([[ 0.01094721,  0.00814205,  0.00556994,  0.00633112,  0.00644081,\n",
      "        -0.00169027,  0.01101255, -0.00159166, -0.00228949, -0.00351087]])]\n",
      "thetas [[1.11315418 0.80982564 1.00517419 0.99624631 1.10198604 1.04571119\n",
      "  1.16481441 1.03559505 1.15073478 1.06906923]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 181237.85373904457\n",
      "dev set\n",
      "log-softplus 15.844892428104751\n",
      "log-softplus-grad [array([[ 0.01098744,  0.00813448,  0.00551425,  0.00628918,  0.00640019,\n",
      "        -0.00160241,  0.01105359, -0.00150195, -0.00221496, -0.00345647]])]\n",
      "thetas [[1.1087993  0.80547347 1.00082994 0.99190383 1.09762967 1.05005651\n",
      "  1.16045871 1.03988815 1.15500875 1.07333   ]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 180799.05561179982\n",
      "dev set\n",
      "log-softplus 15.844646572476055\n",
      "log-softplus-grad [array([[ 0.01102065,  0.0081201 ,  0.00545176,  0.00624041,  0.0063527 ,\n",
      "        -0.00150912,  0.01108761, -0.00140697, -0.00213527, -0.00339689]])]\n",
      "thetas [[1.1044605  0.80114009 0.9965032  0.98757765 1.09328988 1.05439207\n",
      "  1.15611883 1.04412359 1.15920787 1.077499  ]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 180368.14627079674\n",
      "dev set\n",
      "log-softplus 15.844404271622974\n",
      "log-softplus-grad [array([[ 0.01104669,  0.00809885,  0.00538246,  0.00618479,  0.00629832,\n",
      "        -0.00141047,  0.01111443, -0.00130682, -0.00205048, -0.00333218]])]\n",
      "thetas [[1.10014066 0.79682848 0.99219789 0.98327231 1.0889693  1.05871296\n",
      "  1.15179756 1.04829808 1.16332924 1.08156511]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 179945.42671890304\n",
      "dev set\n",
      "log-softplus 15.84416585565654\n",
      "log-softplus-grad [array([[ 0.01106545,  0.00807067,  0.00530638,  0.00612232,  0.00623704,\n",
      "        -0.00130653,  0.01113396, -0.00120156, -0.00196066, -0.0032624 ]])]\n",
      "thetas [[1.09584112 0.79254007 0.98791581 0.97898977 1.08466916 1.06301715\n",
      "  1.14749619 1.05240864 1.16736911 1.08552007]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "batch-size: 1024\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 181711.35542707265\n",
      "dev set\n",
      "log-softplus 15.845267529781518\n",
      "log-softplus-grad [array([[ 0.01092444,  0.00814334,  0.00559539,  0.00634965,  0.00645868,\n",
      "        -0.00173235,  0.01098935, -0.00163479, -0.00232507, -0.00353622]])]\n",
      "thetas [[1.11534728 0.81201759 1.00736348 0.99843053 1.10418014 1.04353035\n",
      "  1.16700834 1.03340718 1.14853992 1.06690917]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 181485.6332048042\n",
      "dev set\n",
      "log-softplus 15.845141669814938\n",
      "log-softplus-grad [array([[ 0.01094718,  0.00814206,  0.00557   ,  0.00633116,  0.00644086,\n",
      "        -0.00169034,  0.01101251, -0.00159178, -0.00228963, -0.00351089]])]\n",
      "thetas [[1.11315953 0.80983025 1.00517941 0.99624764 1.10199172 1.04571306\n",
      "  1.16482036 1.03558387 1.15071385 1.06907757]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 181261.80825613046\n",
      "dev set\n",
      "log-softplus 15.845016457894559\n",
      "log-softplus-grad [array([[ 0.0109682 ,  0.00813909,  0.00554291,  0.00631096,  0.00642133,\n",
      "        -0.00164695,  0.01103395, -0.00154742, -0.00225287, -0.00348423]])]\n",
      "thetas [[1.11097479 0.80764653 1.00299849 0.99406748 1.0998065  1.04789462\n",
      "  1.16263537 1.0377479  1.15287076 1.0712272 ]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 181039.83548350056\n",
      "dev set\n",
      "log-softplus 15.844891992877276\n",
      "log-softplus-grad [array([[ 0.01098747,  0.00813443,  0.00551411,  0.00628905,  0.00640008,\n",
      "        -0.00160218,  0.01105363, -0.00150174, -0.00221481, -0.00345627]])]\n",
      "thetas [[1.10879402 0.80546744 1.00082193 0.99189146 1.09762535 1.0500736\n",
      "  1.16045427 1.03989811 1.15500982 1.07335453]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 180819.82004631456\n",
      "dev set\n",
      "log-softplus 15.844768341799572\n",
      "log-softplus-grad [array([[ 0.01100498,  0.00812805,  0.0054836 ,  0.00626542,  0.00637711,\n",
      "        -0.00155606,  0.01107154, -0.00145474, -0.00217545, -0.00342701]])]\n",
      "thetas [[1.10661776 0.80329353 0.99865042 0.98972036 1.09544877 1.05224918\n",
      "  1.1582776  1.04203386 1.15713045 1.07545782]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "batch-size: 2048\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 181692.1077065804\n",
      "dev set\n",
      "log-softplus 15.845330885461916\n",
      "log-softplus-grad [array([[ 0.01091241,  0.00814336,  0.0056075 ,  0.0063583 ,  0.00646698,\n",
      "        -0.00175291,  0.01097711, -0.00165587, -0.00234238, -0.00354842]])]\n",
      "thetas [[1.11644554 0.81311549 1.00846079 0.99952619 1.10527864 1.04243593\n",
      "  1.16810682 1.03230828 1.14743646 1.06581937]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 181578.59020028007\n",
      "dev set\n",
      "log-softplus 15.845267609598391\n",
      "log-softplus-grad [array([[ 0.01092443,  0.00814334,  0.00559541,  0.00634967,  0.00645869,\n",
      "        -0.00173238,  0.01098935, -0.00163483, -0.00232512, -0.00353623]])]\n",
      "thetas [[1.11534873 0.81201871 1.00736522 0.99843098 1.10418159 1.04353065\n",
      "  1.16700993 1.03340366 1.14853112 1.06691208]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 181465.5482331414\n",
      "dev set\n",
      "log-softplus 15.845204471854633\n",
      "log-softplus-grad [array([[ 0.01093603,  0.00814291,  0.0055829 ,  0.00634061,  0.00644998,\n",
      "        -0.0017115 ,  0.01100116, -0.00161344, -0.00230752, -0.00352369]])]\n",
      "thetas [[1.11425249 0.81092261 1.00627032 0.99733631 1.10308516 1.04462511\n",
      "  1.16591362 1.0344966  1.14962242 1.06800153]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 181352.94023525715\n",
      "dev set\n",
      "log-softplus 15.845141490389846\n",
      "log-softplus-grad [array([[ 0.01094721,  0.00814205,  0.00556996,  0.00633112,  0.00644084,\n",
      "        -0.00169027,  0.01101254, -0.00159172, -0.00228959, -0.00351083]])]\n",
      "thetas [[1.113157   0.80982739 1.00517625 0.99624239 1.1019895  1.04571916\n",
      "  1.16481804 1.03558668 1.15070998 1.06908662]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 181240.7915541514\n",
      "dev set\n",
      "log-softplus 15.845078682446884\n",
      "log-softplus-grad [array([[ 0.01095795,  0.00814077,  0.0055566 ,  0.00632121,  0.00643126,\n",
      "        -0.0016687 ,  0.01102349, -0.00156965, -0.00227134, -0.00349763]])]\n",
      "thetas [[1.11206244 0.80873326 1.0040832  0.99514943 1.1008948  1.04681259\n",
      "  1.16372337 1.03667358 1.15179343 1.0701666 ]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "batch-size: 4096\n",
      "{1: 189, -1: 2625}\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 10), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "sft Lj Tensor(\"map_1/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_1/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_1/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_1/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_1/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_1/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_1/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_1/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_1/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_1/while/sft:0\", shape=(), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map_2/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map_2/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map_2/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map_2/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map_2/while/Gather_2:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"map_2/while/prec_zy/while/Select:0\", shape=(10,), dtype=float64)\n",
      "prec_z Tensor(\"map_2/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map_2/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map_2/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map_2/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 181668.38169568827\n",
      "dev set\n",
      "log-softplus 15.845360029775122\n",
      "log-softplus-grad [array([[ 0.01090674,  0.00814323,  0.00561291,  0.00636213,  0.00647065,\n",
      "        -0.00176223,  0.01097134, -0.00166544, -0.00235022, -0.00355392]])]\n",
      "thetas [[1.11695004 0.81361994 1.00896526 1.00003012 1.10578317 1.04193265\n",
      "  1.16861139 1.03180258 1.1469298  1.06531725]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "1 loss 181607.48208081242\n",
      "dev set\n",
      "log-softplus 15.845326388745825\n",
      "log-softplus-grad [array([[ 0.01091328,  0.00814338,  0.00560666,  0.00635771,  0.00646641,\n",
      "        -0.00175147,  0.010978  , -0.0016544 , -0.00234119, -0.00354757]])]\n",
      "thetas [[1.11636737 0.81303723 1.00838361 0.99944854 1.10520035 1.04251389\n",
      "  1.1680287  1.03238393 1.1475105  1.06589852]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "2 loss 181547.32468343142\n",
      "dev set\n",
      "log-softplus 15.845292858742829\n",
      "log-softplus-grad [array([[ 0.01091969,  0.00814341,  0.00560031,  0.00635318,  0.00646206,\n",
      "        -0.00174063,  0.01098452, -0.0016433 , -0.00233208, -0.00354114]])]\n",
      "thetas [[1.11578611 0.81245595 1.00780352 0.99886848 1.10461894 1.04309359\n",
      "  1.16744742 1.03296336 1.14808899 1.06647778]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "3 loss 181487.36899197614\n",
      "dev set\n",
      "log-softplus 15.84525937819444\n",
      "log-softplus-grad [array([[ 0.01092598,  0.00814333,  0.00559383,  0.00634853,  0.0064576 ,\n",
      "        -0.0017297 ,  0.01099092, -0.0016321 , -0.00232289, -0.00353463]])]\n",
      "thetas [[1.1152052  0.81187504 1.00722385 0.99828881 1.10403789 1.04367294\n",
      "  1.16686649 1.03354196 1.14866642 1.06705595]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n",
      "4 loss 181427.5413278025\n",
      "dev set\n",
      "log-softplus 15.84522593294874\n",
      "log-softplus-grad [array([[ 0.01093215,  0.00814312,  0.00558724,  0.00634376,  0.00645301,\n",
      "        -0.00171867,  0.01099721, -0.0016208 , -0.0023136 , -0.00352801]])]\n",
      "thetas [[1.11462437 0.81129424 1.00664431 0.99770927 1.10345692 1.04425223\n",
      "  1.16628565 1.03411995 1.14924303 1.06763314]]\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "test set\n",
      "{0: 2321, 1: 381}\n",
      "acc 0.8886010362694301\n",
      "(0.3910761154855643, 0.6834862385321101, 0.4974958263772954, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in [32,64,128,512,1024,2048,4096]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.0001,5,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.array([-1,1],dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
