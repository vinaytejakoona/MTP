{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n",
    "print(len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 2625\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### Discrete ##########\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,0)\n",
    "    \n",
    "# def LF_familial_relationship(c):\n",
    "#     return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x.decode('utf-8')).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,0)\n",
    "\n",
    "\n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    global LF_Threshold\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "# def LF_other_relationship_left_window(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "#     for ow in other:\n",
    "#         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "#     return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # Read in known spouse pairs and save as set of tuples\n",
    "# with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "#     known_spouses = set(\n",
    "#         tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "#     )\n",
    "# # Last name pairs for known spouses\n",
    "# last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# def LF_Three_Lists_Left_Window(c):\n",
    "#     global softmax_Threshold\n",
    "#     c1,s1 = LF_husband_wife_left_window(c)\n",
    "#     c2,s2 = LF_family_left_window(c)\n",
    "#     c3,s3 = LF_other_relationship_left_window(c)\n",
    "#     sc = np.array([s1,s2,s3])\n",
    "#     c = [c1,c2,c3]\n",
    "#     sharp_param = 1.5\n",
    "#     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "#     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "#     #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "#     if s1==s2 or s3==s1:\n",
    "#         return (0,0)\n",
    "#     return c[np.argmax(prob_sc)],1\n",
    "\n",
    "# def LF_Three_Lists_Between_Words(c):\n",
    "#     global softmax_Threshold\n",
    "#     c1,s1 = LF_husband_wife(c)\n",
    "#     c2,s2 = LF_familial_relationship(c)\n",
    "#     c3,s3 = LF_other_relationship(c)\n",
    "#     sc = np.array([s1,s2,s3])\n",
    "#     c = [c1,c2,c3]\n",
    "#     sharp_param = 1.5\n",
    "    \n",
    "#     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "#     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "#     #print 'BW:',s1,s2,s3,prob_sc\n",
    "#     if s1==s2 or s3==s1:\n",
    "#         return (0,0)\n",
    "#     return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 9-5-2018, 10:41:55\n",
      "500data points labelled in 0.7188430507977803 mins\n",
      "1000data points labelled in 1.3335495034853617 mins\n",
      "1500data points labelled in 1.917067794005076 mins\n",
      "2000data points labelled in 2.519752260049184 mins\n",
      "2500data points labelled in 3.1769800662994383 mins\n",
      "500data points labelled in 4.174797224998474 mins\n",
      "1000data points labelled in 4.797278384367625 mins\n",
      "1500data points labelled in 5.556176539262136 mins\n",
      "2000data points labelled in 6.174566467603047 mins\n",
      "2500data points labelled in 6.850887068112692 mins\n",
      "3000data points labelled in 7.6337612748146055 mins\n",
      "3500data points labelled in 8.271688584486643 mins\n",
      "4000data points labelled in 8.86154446999232 mins\n",
      "4500data points labelled in 9.401108086109161 mins\n",
      "5000data points labelled in 10.001064256827037 mins\n",
      "5500data points labelled in 10.652975161870321 mins\n",
      "6000data points labelled in 11.262283476193746 mins\n",
      "6500data points labelled in 11.85253625313441 mins\n",
      "7000data points labelled in 12.50683133204778 mins\n",
      "7500data points labelled in 13.094375010331472 mins\n",
      "8000data points labelled in 13.716650235652924 mins\n",
      "8500data points labelled in 14.281550602118175 mins\n",
      "9000data points labelled in 14.971553881963095 mins\n",
      "9500data points labelled in 15.57162874142329 mins\n",
      "10000data points labelled in 16.21110211610794 mins\n",
      "10500data points labelled in 16.86411997079849 mins\n",
      "11000data points labelled in 17.50747369925181 mins\n",
      "11500data points labelled in 18.0820143977801 mins\n",
      "12000data points labelled in 18.754645995299022 mins\n",
      "12500data points labelled in 19.329470952351887 mins\n",
      "13000data points labelled in 19.94033740758896 mins\n",
      "13500data points labelled in 20.581616592407226 mins\n",
      "14000data points labelled in 21.171934326489765 mins\n",
      "14500data points labelled in 21.763719876607258 mins\n",
      "15000data points labelled in 22.431539726257324 mins\n",
      "15500data points labelled in 22.935143303871154 mins\n",
      "16000data points labelled in 23.565833016236624 mins\n",
      "16500data points labelled in 24.203933664162953 mins\n",
      "17000data points labelled in 24.749000080426534 mins\n",
      "17500data points labelled in 25.358984434604643 mins\n",
      "18000data points labelled in 25.93876881202062 mins\n",
      "18500data points labelled in 26.685788782437644 mins\n",
      "19000data points labelled in 27.294497899214427 mins\n",
      "19500data points labelled in 27.912985666592917 mins\n",
      "20000data points labelled in 28.575929319858552 mins\n",
      "20500data points labelled in 29.122294485569 mins\n",
      "21000data points labelled in 29.714926477273305 mins\n",
      "21500data points labelled in 30.330261317888894 mins\n",
      "22000data points labelled in 30.933784091472624 mins\n",
      "--- 1877.3884024620056 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "    \n",
    "\n",
    "def drawLossVsF1(y_loss,x_f1s,text,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x_f1s, y_loss)\n",
    "\n",
    "    plt.xlabel('f1-score')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(title)\n",
    "    \n",
    "    for i, txt in enumerate(text):\n",
    "        ax.annotate(txt, (x_f1s[i],y_loss[i]))\n",
    "        \n",
    "    plt.savefig(title+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train smooth Normalized \n",
    "\n",
    "def train_SNL(lr,ep,th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.001,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=th,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: ls_*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        def ints(y):\n",
    "            return alphas+((tf.exp((t_k*y)*(1-alphas))-1)/(t_k*y))\n",
    "\n",
    "        print(\"ints\",ints)\n",
    "\n",
    "    #     zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                       np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) -logz ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#         train_step = tf.train.MomentumOptimizer(0.0000001,0.002).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                drawPRcurve(np.array(gold_labels_dev),np.array(m[1::].flatten()),it)\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train smooth unnormalized \n",
    "\n",
    "def train_SUNL(lr,ep,th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=th,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: ls_*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        def ints(y):\n",
    "            return alphas+((tf.exp((t_k*y)*(1-alphas))-1)/(t_k*y))\n",
    "\n",
    "        print(\"ints\",ints)\n",
    "\n",
    "    #     zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                       np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#         train_step = tf.train.AdamOptimizer(lr).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#         train_step = tf.train.MomentumOptimizer(0.0000001,0.002).minimize(normloss) \n",
    "        train_step = tf.train.AdagradOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 drawPRcurve(np.array(gold_labels_dev),np.array(m[1::].flatten()),it)\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ccb937438>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ccb937438>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ccb937438>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "ints <function train_SUNL.<locals>.ints at 0x7f6c7fd80510>\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -21137.741180838464\n",
      "[0.31749949 0.01410638 0.21072908 0.20179368 0.30628062 0.24032793\n",
      " 0.36990044 0.2300379  0.34516436 0.26355322]\n",
      "[[1.11758836 0.81432336 1.00842745 0.99947768 1.10647396 1.04234801\n",
      "  1.16856516 1.03232483 1.14743832 1.06584853]]\n",
      "{0: 2683, 1: 131}\n",
      "(0.2900763358778626, 0.20105820105820105, 0.2375, None)\n",
      "\n",
      "1 loss -21204.279028315163\n",
      "[0.31747928 0.01405996 0.2112156  0.20228021 0.30623867 0.23990551\n",
      " 0.37019028 0.22955094 0.34467742 0.26306626]\n",
      "[[1.11760852 0.81436979 1.00795485 0.99900144 1.10651589 1.04277054\n",
      "  1.16829572 1.03279065 1.14789609 1.06632166]]\n",
      "{0: 2688, 1: 126}\n",
      "(0.30158730158730157, 0.20105820105820105, 0.2412698412698413, None)\n",
      "\n",
      "2 loss -21247.98361784177\n",
      "[0.31746376 0.01402437 0.21158983 0.20265443 0.30620674 0.23958099\n",
      " 0.37041353 0.22917628 0.34430278 0.2626916 ]\n",
      "[[1.11762402 0.81440538 1.00759142 0.99863522 1.10654781 1.04309515\n",
      "  1.16808828 1.03314905 1.14824828 1.06668584]]\n",
      "{0: 2692, 1: 122}\n",
      "(0.3114754098360656, 0.20105820105820105, 0.24437299035369772, None)\n",
      "\n",
      "3 loss -21283.545613538623\n",
      "[0.31745068 0.01399441 0.21190593 0.20297052 0.30618    0.23930717\n",
      " 0.37060232 0.22885975 0.34398628 0.26237508]\n",
      "[[1.11763709 0.81443534 1.00728448 0.99832594 1.10657455 1.04336908\n",
      "  1.16791288 1.03345188 1.14854588 1.06699361]]\n",
      "{0: 2694, 1: 120}\n",
      "(0.31666666666666665, 0.20105820105820105, 0.24595469255663427, None)\n",
      "\n",
      "4 loss -21314.401976265857\n",
      "[0.31743915 0.01396804 0.21218486 0.20324946 0.30615657 0.23906575\n",
      " 0.37076909 0.22858038 0.34370693 0.26209571]\n",
      "[[1.11764861 0.81446172 1.00701364 0.99805303 1.10659798 1.04361061\n",
      "  1.16775794 1.0337192  1.14880861 1.06726531]]\n",
      "{0: 2701, 1: 113}\n",
      "(0.3274336283185841, 0.19576719576719576, 0.24503311258278143, None)\n",
      "\n",
      "5 loss -21342.08348199105\n",
      "[0.31742874 0.01394422 0.21243741 0.203502   0.30613549 0.23884733\n",
      " 0.37092021 0.2283274  0.34345397 0.26184273]\n",
      "[[1.11765902 0.81448554 1.00676845 0.99780595 1.10661905 1.04382913\n",
      "  1.16761752 1.0339613  1.14904658 1.0675114 ]]\n",
      "{0: 2704, 1: 110}\n",
      "(0.32727272727272727, 0.19047619047619047, 0.24080267558528426, None)\n",
      "\n",
      "6 loss -21367.434320135602\n",
      "[0.31741916 0.01392234 0.21266995 0.20373453 0.3061162  0.23864636\n",
      " 0.37105949 0.22809442 0.34322102 0.26160975]\n",
      "[[1.11766859 0.81450742 1.00654268 0.99757845 1.10663834 1.04403021\n",
      "  1.16748812 1.03418428 1.14926578 1.06773807]]\n",
      "{0: 2706, 1: 108}\n",
      "(0.3333333333333333, 0.19047619047619047, 0.24242424242424246, None)\n",
      "\n",
      "7 loss -21390.979394712765\n",
      "[0.31741026 0.01390199 0.21288665 0.20395123 0.30609832 0.23845919\n",
      " 0.37118938 0.22787727 0.3430039  0.26139261]\n",
      "[[1.11767749 0.81452777 1.0063323  0.99736645 1.10665622 1.04421749\n",
      "  1.16736743 1.03439214 1.14947014 1.06794937]]\n",
      "{0: 2708, 1: 106}\n",
      "(0.330188679245283, 0.18518518518518517, 0.23728813559322035, None)\n",
      "\n",
      "8 loss -21413.071697029256\n",
      "[0.31740189 0.01388289 0.21309041 0.20415499 0.3060816  0.23828331\n",
      " 0.37131159 0.22767307 0.34279972 0.26118842]\n",
      "[[1.11768585 0.81454687 1.00613449 0.99716712 1.10667295 1.04439348\n",
      "  1.16725387 1.03458763 1.14966235 1.0681481 ]]\n",
      "{0: 2708, 1: 106}\n",
      "(0.330188679245283, 0.18518518518518517, 0.23728813559322035, None)\n",
      "\n",
      "9 loss -21433.962194362884\n",
      "[0.31739399 0.01386484 0.21328334 0.20434791 0.30606584 0.23811687\n",
      " 0.37142739 0.22747971 0.34260638 0.26099506]\n",
      "[[1.11769375 0.81456491 1.0059472  0.99697839 1.10668871 1.04456002\n",
      "  1.16714627 1.03477277 1.1498444  1.06833632]]\n",
      "{0: 2709, 1: 105}\n",
      "(0.3333333333333333, 0.18518518518518517, 0.23809523809523808, None)\n",
      "\n",
      "10 loss -21453.83691694267\n",
      "[0.31738647 0.01384769 0.21346702 0.20453159 0.3060509  0.23795849\n",
      " 0.37153771 0.22729559 0.34242229 0.26081094]\n",
      "[[1.11770127 0.81458206 1.0057689  0.99679871 1.10670364 1.0447185\n",
      "  1.16704375 1.03494907 1.15001778 1.06851556]]\n",
      "{0: 2710, 1: 104}\n",
      "(0.33653846153846156, 0.18518518518518517, 0.2389078498293515, None)\n",
      "\n",
      "11 loss -21472.83831361634\n",
      "[0.31737929 0.01383132 0.21364268 0.20470725 0.30603668 0.2378071\n",
      " 0.37164327 0.22711948 0.3422462  0.26063483]\n",
      "[[1.11770845 0.81459844 1.00559838 0.99662687 1.10671786 1.04487\n",
      "  1.16694565 1.03511772 1.15018366 1.06868702]]\n",
      "{0: 2710, 1: 104}\n",
      "(0.33653846153846156, 0.18518518518518517, 0.2389078498293515, None)\n",
      "\n",
      "12 loss -21491.07834070765\n",
      "[0.31737241 0.01381562 0.21381132 0.20487589 0.30602309 0.23766183\n",
      " 0.37174468 0.22695039 0.34207714 0.26046575]\n",
      "[[1.11771533 0.81461413 1.00543468 0.99646191 1.10673146 1.04501538\n",
      "  1.16685141 1.03527966 1.15034294 1.06885166]]\n",
      "{0: 2711, 1: 103}\n",
      "(0.33980582524271846, 0.18518518518518517, 0.23972602739726026, None)\n",
      "\n",
      "13 loss -21508.646890139764\n",
      "[0.31736578 0.01380053 0.21397373 0.20503829 0.30601005 0.237522\n",
      " 0.37184238 0.22678754 0.34191432 0.2603029 ]\n",
      "[[1.11772195 0.81462923 1.00527704 0.99630305 1.10674449 1.04515532\n",
      "  1.1667606  1.03543565 1.15049638 1.06901025]]\n",
      "{0: 2711, 1: 103}\n",
      "(0.33980582524271846, 0.18518518518518517, 0.23972602739726026, None)\n",
      "\n",
      "14 loss -21525.61743859177\n",
      "[0.3173594  0.01378598 0.21413057 0.20519512 0.30599751 0.23738702\n",
      " 0.37193679 0.22663026 0.34175706 0.26014563]\n",
      "[[1.11772834 0.81464378 1.00512481 0.99614964 1.10675703 1.0452904\n",
      "  1.16667286 1.03558631 1.15064459 1.06916343]]\n",
      "{0: 2711, 1: 103}\n",
      "(0.33980582524271846, 0.18518518518518517, 0.23972602739726026, None)\n",
      "\n",
      "[0.3173594  0.01378598 0.21413057 0.20519512 0.30599751 0.23738702\n",
      " 0.37193679 0.22663026 0.34175706 0.26014563]\n",
      "[[1.11772834 0.81464378 1.00512481 0.99614964 1.10675703 1.0452904\n",
      "  1.16667286 1.03558631 1.15064459 1.06916343]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.185\n",
      "Neg. class accuracy: 0.974\n",
      "Precision            0.34\n",
      "Recall               0.185\n",
      "F1                   0.24\n",
      "----------------------------------------\n",
      "TP: 35 | FP: 68 | TN: 2557 | FN: 154\n",
      "========================================\n",
      "\n",
      "{0: 2711, 1: 103}\n",
      "acc 0.9211087420042644\n",
      "(array([0.94319439, 0.33980583]), array([0.97409524, 0.18518519]), array([0.9583958 , 0.23972603]), array([2625,  189]))\n",
      "(0.6415001092277776, 0.5796402116402116, 0.5990609147481054, None)\n",
      "[[2557   68]\n",
      " [ 154   35]]\n",
      "prec: tp/(tp+fp) 0.33980582524271846 recall: tp/(tp+fn) 0.18518518518518517\n",
      "(0.33980582524271846, 0.18518518518518517, 0.23972602739726026, None)\n"
     ]
    }
   ],
   "source": [
    "train_SUNL(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00f98128>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00f98128>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00f98128>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "ints <function train_SUNL.<locals>.ints at 0x7f6c81b0b598>\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -21066.28082805327\n",
      "[0.31754011 0.01420369 0.20967624 0.20074089 0.30636805 0.24123763\n",
      " 0.36927509 0.23109125 0.34621807 0.26460668]\n",
      "[[1.11754901 0.81422544 1.00944677 1.00050999 1.10638757 1.04143934\n",
      "  1.16914226 1.03131913 1.14644464 1.0648354 ]]\n",
      "{0: 2678, 1: 136}\n",
      "(0.27941176470588236, 0.20105820105820105, 0.23384615384615384, None)\n",
      "\n",
      "1 loss -21072.831713008985\n",
      "[0.31753808 0.01419903 0.20972455 0.20078919 0.30636376 0.24119553\n",
      " 0.36930373 0.23104294 0.34616976 0.26455837]\n",
      "[[1.11755103 0.8142301  1.00939985 1.00046271 1.10639186 1.04148144\n",
      "  1.16911565 1.03136531 1.14648998 1.06488229]]\n",
      "{0: 2678, 1: 136}\n",
      "(0.27941176470588236, 0.20105820105820105, 0.23384615384615384, None)\n",
      "\n",
      "2 loss -21077.099202172165\n",
      "[0.31753653 0.01419545 0.20976162 0.20082627 0.30636047 0.24116322\n",
      " 0.36932571 0.23100586 0.34613268 0.2645213 ]\n",
      "[[1.11755258 0.81423368 1.00936384 1.00042643 1.10639515 1.04151376\n",
      "  1.16909524 1.03140074 1.14652477 1.06491829]]\n",
      "{0: 2678, 1: 136}\n",
      "(0.27941176470588236, 0.20105820105820105, 0.23384615384615384, None)\n",
      "\n",
      "3 loss -21080.552379070756\n",
      "[0.31753521 0.01419243 0.20979288 0.20085753 0.3063577  0.24113598\n",
      " 0.36934425 0.2309746  0.34610142 0.26449003]\n",
      "[[1.1175539  0.81423671 1.00933348 1.00039584 1.10639792 1.041541\n",
      "  1.16907804 1.03143061 1.14655409 1.06494865]]\n",
      "{0: 2678, 1: 136}\n",
      "(0.27941176470588236, 0.20105820105820105, 0.23384615384615384, None)\n",
      "\n",
      "4 loss -21083.53495054725\n",
      "[0.31753405 0.01418977 0.20982043 0.20088508 0.30635526 0.24111197\n",
      " 0.36936058 0.23094705 0.34607387 0.26446248]\n",
      "[[1.11755505 0.81423937 1.00930673 1.00036889 1.10640035 1.041565\n",
      "  1.16906288 1.03145693 1.14657993 1.0649754 ]]\n",
      "{0: 2678, 1: 136}\n",
      "(0.27941176470588236, 0.20105820105820105, 0.23384615384615384, None)\n",
      "\n",
      "5 loss -21086.200000827805\n",
      "[0.31753301 0.01418736 0.20984533 0.20090998 0.30635306 0.24109027\n",
      " 0.36937535 0.23092213 0.34604896 0.26443757]\n",
      "[[1.1175561  0.81424177 1.00928254 1.00034452 1.10640256 1.04158671\n",
      "  1.16904917 1.03148074 1.1466033  1.06499959]]\n",
      "{0: 2678, 1: 136}\n",
      "(0.27941176470588236, 0.20105820105820105, 0.23384615384615384, None)\n",
      "\n",
      "6 loss -21088.63195344792\n",
      "[0.31753204 0.01418515 0.20986824 0.20093289 0.30635103 0.24107031\n",
      " 0.36938894 0.23089922 0.34602604 0.26441466]\n",
      "[[1.11755706 0.81424398 1.00926029 1.0003221  1.10640458 1.04160666\n",
      "  1.16903656 1.03150263 1.14662479 1.06502184]]\n",
      "{0: 2678, 1: 136}\n",
      "(0.27941176470588236, 0.20105820105820105, 0.23384615384615384, None)\n",
      "\n",
      "7 loss -21090.883302228656\n",
      "[0.31753115 0.01418309 0.20988957 0.20095422 0.30634915 0.24105174\n",
      " 0.36940158 0.23087789 0.34600471 0.26439333]\n",
      "[[1.11755796 0.81424604 1.00923958 1.00030124 1.10640647 1.04162524\n",
      "  1.16902483 1.03152301 1.14664479 1.06504255]]\n",
      "{0: 2679, 1: 135}\n",
      "(0.2814814814814815, 0.20105820105820105, 0.2345679012345679, None)\n",
      "\n",
      "8 loss -21092.989348974083\n",
      "[0.31753031 0.01418116 0.2099096  0.20097425 0.30634738 0.24103429\n",
      " 0.36941347 0.23085786 0.34598468 0.26437329]\n",
      "[[1.1175588  0.81424798 1.00922013 1.00028164 1.10640824 1.04164269\n",
      "  1.1690138  1.03154215 1.14666359 1.06506201]]\n",
      "{0: 2679, 1: 135}\n",
      "(0.2814814814814815, 0.20105820105820105, 0.2345679012345679, None)\n",
      "\n",
      "9 loss -21094.97519257567\n",
      "[0.31752951 0.01417933 0.20992855 0.2009932  0.30634571 0.24101778\n",
      " 0.36942471 0.2308389  0.34596573 0.26435434]\n",
      "[[1.1175596  0.81424981 1.00920173 1.0002631  1.10640991 1.0416592\n",
      "  1.16900337 1.03156026 1.14668136 1.06508042]]\n",
      "{0: 2679, 1: 135}\n",
      "(0.2814814814814815, 0.20105820105820105, 0.2345679012345679, None)\n",
      "\n",
      "10 loss -21096.859437852454\n",
      "[0.31752875 0.01417759 0.20994657 0.20101122 0.30634412 0.24100208\n",
      " 0.3694354  0.23082087 0.3459477  0.26433631]\n",
      "[[1.11756035 0.81425155 1.00918423 1.00024547 1.1064115  1.0416749\n",
      "  1.16899345 1.03157748 1.14669827 1.06509792]]\n",
      "{0: 2679, 1: 135}\n",
      "(0.2814814814814815, 0.20105820105820105, 0.2345679012345679, None)\n",
      "\n",
      "11 loss -21098.656331213442\n",
      "[0.31752803 0.01417592 0.2099638  0.20102844 0.3063426  0.24098708\n",
      " 0.36944562 0.23080365 0.34593047 0.26431908]\n",
      "[[1.11756108 0.81425321 1.0091675  1.00022862 1.10641302 1.0416899\n",
      "  1.16898397 1.03159394 1.14671443 1.06511466]]\n",
      "{0: 2679, 1: 135}\n",
      "(0.2814814814814815, 0.20105820105820105, 0.2345679012345679, None)\n",
      "\n",
      "12 loss -21100.377069604783\n",
      "[0.31752733 0.01417433 0.20998032 0.20104497 0.30634114 0.24097269\n",
      " 0.36945542 0.23078712 0.34591394 0.26430256]\n",
      "[[1.11756177 0.8142548  1.00915146 1.00021246 1.10641448 1.04170429\n",
      "  1.16897487 1.03160973 1.14672993 1.06513071]]\n",
      "{0: 2680, 1: 134}\n",
      "(0.2835820895522388, 0.20105820105820105, 0.23529411764705882, None)\n",
      "\n",
      "13 loss -21102.03064336513\n",
      "[0.31752667 0.0141728  0.20999622 0.20106086 0.30633974 0.24095885\n",
      " 0.36946485 0.23077122 0.34589804 0.26428665]\n",
      "[[1.11756244 0.81425634 1.00913602 1.0001969  1.10641588 1.04171814\n",
      "  1.16896612 1.03162493 1.14674484 1.06514615]]\n",
      "{0: 2680, 1: 134}\n",
      "(0.2835820895522388, 0.20105820105820105, 0.23529411764705882, None)\n",
      "\n",
      "14 loss -21103.624401175544\n",
      "[0.31752602 0.01417132 0.21001156 0.20107621 0.30633838 0.24094549\n",
      " 0.36947396 0.23075587 0.3458827  0.26427131]\n",
      "[[1.11756308 0.81425782 1.00912112 1.00018189 1.10641723 1.0417315\n",
      "  1.16895767 1.03163959 1.14675924 1.06516106]]\n",
      "{0: 2680, 1: 134}\n",
      "(0.2835820895522388, 0.20105820105820105, 0.23529411764705882, None)\n",
      "\n",
      "[0.31752602 0.01417132 0.21001156 0.20107621 0.30633838 0.24094549\n",
      " 0.36947396 0.23075587 0.3458827  0.26427131]\n",
      "[[1.11756308 0.81425782 1.00912112 1.00018189 1.10641723 1.0417315\n",
      "  1.16895767 1.03163959 1.14675924 1.06516106]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.201\n",
      "Neg. class accuracy: 0.963\n",
      "Precision            0.284\n",
      "Recall               0.201\n",
      "F1                   0.235\n",
      "----------------------------------------\n",
      "TP: 38 | FP: 96 | TN: 2529 | FN: 151\n",
      "========================================\n",
      "\n",
      "{0: 2680, 1: 134}\n",
      "acc 0.912224591329069\n",
      "(array([0.94365672, 0.28358209]), array([0.96342857, 0.2010582 ]), array([0.95344015, 0.23529412]), array([2625,  189]))\n",
      "(0.6136194029850747, 0.5822433862433862, 0.5943671342240949, None)\n",
      "[[2529   96]\n",
      " [ 151   38]]\n",
      "prec: tp/(tp+fp) 0.2835820895522388 recall: tp/(tp+fn) 0.20105820105820105\n",
      "(0.2835820895522388, 0.20105820105820105, 0.23529411764705882, None)\n"
     ]
    }
   ],
   "source": [
    "train_SUNL(0.01/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c8406fb00>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c8406fb00>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c8406fb00>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "ints <function train_SNL.<locals>.ints at 0x7f6d0b563158>\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 157065.25301388316\n",
      "[ 0.12845453 -0.17496841  0.02011157  0.01117763  0.11707751  0.03421038\n",
      "  0.17856867  0.02294729  0.13810977  0.05647389]\n",
      "[[1.10770302 0.80458828 1.00009316 0.99112765 1.0970454  1.04904104\n",
      "  1.16046452 1.04013947 1.15523174 1.07371892]]\n",
      "{0: 2738, 1: 76}\n",
      "(0.3684210526315789, 0.14814814814814814, 0.2113207547169811, None)\n",
      "\n",
      "1 loss 154916.81433907387\n",
      "[ 0.13834848 -0.16517113  0.0297387   0.02080575  0.12673965  0.02619915\n",
      "  0.18682074  0.01355895  0.12874637  0.04709356]\n",
      "[[1.09788418 0.79498389 0.99054571 0.98155251 1.08776963 1.05666511\n",
      "  1.15181455 1.04923578 1.16429414 1.08288293]]\n",
      "{0: 2760, 1: 54}\n",
      "(0.42592592592592593, 0.12169312169312169, 0.18930041152263372, None)\n",
      "\n",
      "2 loss 152776.4221907198\n",
      "[ 0.14822741 -0.15539594  0.0394237   0.03049142  0.13634779  0.01833157\n",
      "  0.1949352   0.00407149  0.11927566  0.03761167]\n",
      "[[1.0880859  0.78540342 0.98094082 0.97191919 1.07857902 1.06418978\n",
      "  1.14328186 1.05845656 1.17348506 1.09217244]]\n",
      "{0: 2772, 1: 42}\n",
      "(0.42857142857142855, 0.09523809523809523, 0.15584415584415587, None)\n",
      "\n",
      "3 loss 150657.66423059616\n",
      "[ 0.15808685 -0.1456539   0.04915854  0.04022668  0.14587074  0.01062782\n",
      "  0.20289644 -0.00549889  0.1097157   0.02804496]\n",
      "[[1.07831322 0.77586025 0.9712919  0.96224031 1.06951995 1.07160291\n",
      "  1.13488412 1.06777656 1.18277852 1.10156352]]\n",
      "{0: 2781, 1: 33}\n",
      "(0.45454545454545453, 0.07936507936507936, 0.13513513513513514, None)\n",
      "\n",
      "4 loss 148573.1870570401\n",
      "[ 0.16791983 -0.13596596  0.05893587  0.05000422  0.15525589  0.00311052\n",
      "  0.21068965 -0.01513826  0.10008195  0.0184078 ]\n",
      "[[1.06857359 0.7663793  0.96161066 0.95252705 1.06066204 1.07889202\n",
      "  1.12663918 1.07717458 1.19215271 1.11103602]]\n",
      "{0: 2790, 1: 24}\n",
      "(0.4583333333333333, 0.0582010582010582, 0.10328638497652581, None)\n",
      "\n",
      "[ 0.16791983 -0.13596596  0.05893587  0.05000422  0.15525589  0.00311052\n",
      "  0.21068965 -0.01513826  0.10008195  0.0184078 ]\n",
      "[[1.06857359 0.7663793  0.96161066 0.95252705 1.06066204 1.07889202\n",
      "  1.12663918 1.07717458 1.19215271 1.11103602]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0582\n",
      "Neg. class accuracy: 0.995\n",
      "Precision            0.458\n",
      "Recall               0.0582\n",
      "F1                   0.103\n",
      "----------------------------------------\n",
      "TP: 11 | FP: 13 | TN: 2612 | FN: 178\n",
      "========================================\n",
      "\n",
      "{0: 2790, 1: 24}\n",
      "acc 0.9321250888415068\n",
      "(array([0.93620072, 0.45833333]), array([0.99504762, 0.05820106]), array([0.96472761, 0.10328638]), array([2625,  189]))\n",
      "(0.6972670250896057, 0.5266243386243387, 0.5340069967357237, None)\n",
      "[[2612   13]\n",
      " [ 178   11]]\n",
      "prec: tp/(tp+fp) 0.4583333333333333 recall: tp/(tp+fn) 0.0582010582010582\n",
      "(0.4583333333333333, 0.0582010582010582, 0.10328638497652581, None)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_SNL(0.01/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with different params\n",
    "\n",
    "def train_nl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 153548.14977017298\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "dev loss 19382.46646189899\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "un-norma thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 154340.00504922983\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "dev loss 19477.32881134378\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "[(153548.14977017298, 19382.46646189899, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)), (154340.00504922983, 19477.32881134378, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value Normalized\n",
    "\n",
    "def getNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getNLObjValue(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "                0.11186369,0.07306518,0.69216714,0.07467749,0.16012659,\\\n",
    "                    0.13682546,0.08183363]])))\n",
    " \n",
    "            \n",
    "print(\" un-norma thetas ep7 \")\n",
    "\n",
    "# l_f1s.append(getNLObjValue(np.array([[1.0,1.0,1.0,1.0,1.0,1.02750979,\\\n",
    "#                              1.0,1.0218145,1.0,1.0]])))\n",
    "\n",
    "l_f1s.append(getNLObjValue(np.array([[0.33293226,0.01940464,0.42274838,0.39655883,\\\n",
    "    0.31731244,0.84775084,0.37180681,0.46009105,0.5502137,0.32638473]])))\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc866f9e8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc866f9e8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc866f9e8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "5 loss 158604.93738094255\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.54997522 0.38551056 0.71208165 0.68224501 0.60391497 1.3682661\n",
      "  0.58642215 0.89307    0.94179888 0.7020998 ]]\n",
      "{0: 2523, 1: 291}\n",
      "(0.35051546391752575, 0.5396825396825397, 0.42499999999999993, None)\n",
      "\n",
      "6 loss 157032.75110433032\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "7 loss 155668.8970006794\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.4314503  0.38839979 0.70424996 0.67037562 0.52367733 1.4280809\n",
      "  0.42606297 0.77043883 0.80142626 0.51281886]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "8 loss 154481.67664693782\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.39067389 0.37902675 0.69806429 0.66205327 0.48793706 1.45887735\n",
      "  0.37175861 0.71320089 0.73512861 0.4199123 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "9 loss 153446.87729269065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3546431  0.36275345 0.68942781 0.65117459 0.45284473 1.49068915\n",
      "  0.33247148 0.65941213 0.67218014 0.32889969]]\n",
      "{0: 2501, 1: 313}\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n",
      "\n",
      "10 loss 152548.81576950828\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3228263  0.34281758 0.67875204 0.63822859 0.41888411 1.52333766\n",
      "  0.3020918  0.60913597 0.61277898 0.24064353]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 151777.05782060773\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.29540707 0.32165781 0.66655386 0.62380273 0.38668169 1.55659623\n",
      "  0.27742105 0.56230756 0.55701578 0.15647785]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss 151121.9679241542\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.27235149 0.30085188 0.65330747 0.6084251  0.35673569 1.59025571\n",
      "  0.25725406 0.51880115 0.50491631 0.07840616]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss 150572.4023227708\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.25361337 0.28149093 0.63946432 0.5925888  0.32946785 1.62411754\n",
      "  0.24130367 0.47842506 0.45642398 0.00909516]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "14 loss 150114.45362566956\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.23919651  0.2643783   0.62546829  0.57676995  0.30525161  1.65798876\n",
      "   0.22955256  0.44092173  0.41138899 -0.0487575 ]]\n",
      "{0: 2486, 1: 328}\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.23919651  0.2643783   0.62546829  0.57676995  0.30525161  1.65798876\n",
      "   0.22955256  0.44092173  0.41138899 -0.0487575 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.916\n",
      "Precision            0.326\n",
      "Recall               0.566\n",
      "F1                   0.414\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 221 | TN: 2404 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2486, 1: 328}\n",
      "acc 0.8923240938166311\n",
      "(array([0.96701529, 0.32621951]), array([0.91580952, 0.56613757]), array([0.9407161, 0.4139265]), array([2625,  189]))\n",
      "(0.6466173988972391, 0.7409735449735451, 0.6773213007784249, None)\n",
      "[[2404  221]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.32621951219512196 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n"
     ]
    }
   ],
   "source": [
    "# init random thetas\n",
    "\n",
    "train_nl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Normalized loss plot loss vs f1s\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "y_loss=[153548.149,179552.66,157032.751,150114.45]\n",
    "x_f1s =[0.432,0.428,0.43,0.414]\n",
    "text=[\"snorkel-thetas\",\"old-unNormalized-thetas\",\"normalized-trained-thetas-ep7\",\\\n",
    "      \"normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"Spouse-Normalized-Loss\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -18693.37512233427\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "dev loss -2375.8227752836674\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "normalized thetas ep6 f10.43\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -29153.740301843533\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "dev loss -3721.7512709843986\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.354\n",
      "Recall               0.55\n",
      "F1                   0.431\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 190 | TN: 2435 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2520, 1: 294}\n",
      "acc 0.90227434257285\n",
      "(array([0.96626984, 0.3537415 ]), array([0.92761905, 0.55026455]), array([0.94655005, 0.43064182]), array([2625,  189]))\n",
      "(0.6600056689342404, 0.7389417989417989, 0.6885959352685174, None)\n",
      "[[2435  190]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35374149659863946 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "[(-18693.37512233427, -2375.8227752836674, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)), (-29153.740301843533, -3721.7512709843986, (0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas Unnormalized # remove logz from obj\n",
    "\n",
    "def getUNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getUNLObjValue(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "                0.11186369,0.07306518,0.69216714,0.07467749,0.16012659,\\\n",
    "                    0.13682546,0.08183363]])))\n",
    " \n",
    "            \n",
    "print(\"normalized thetas ep6 f10.43\")\n",
    "\n",
    "l_f1s.append(getUNLObjValue(np.array([[0.48198891,0.38912505,0.70829843,0.67643395,0.56100246,\\\n",
    "        1.39815618,0.49929654,0.83071361,0.8706048,0.6070296 ]])))\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -27046.447774325923\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00304454 1.00650418 1.02847709 1.02590827 1.00626986 1.10374278\n",
      "  1.00047081 1.05014972 1.02497607 1.00775472]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "1 loss -28052.143411860863\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00609458 1.01281986 1.05679467 1.05166221 1.01210828 1.18026413\n",
      "  1.0008722  1.07854839 1.05000733 1.01580513]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "2 loss -29084.10636456966\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00913348 1.01894236 1.08511902 1.07741817 1.01748748 1.25689478\n",
      "  1.00117106 1.10696165 1.07504049 1.02383491]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "3 loss -30139.25214077889\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01215851 1.02486468 1.11344882 1.10317555 1.02241298 1.33361627\n",
      "  1.00138832 1.13538697 1.10007485 1.03184426]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "4 loss -31214.8692847093\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01516631 1.03057898 1.14178284 1.12893381 1.02689378 1.41041477\n",
      "  1.0015406  1.16382207 1.1251098  1.03983336]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "5 loss -32308.533016622125\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01815264 1.03607611 1.17012004 1.15469247 1.03094158 1.48727984\n",
      "  1.00164101 1.19226496 1.15014486 1.04780238]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "6 loss -33418.08609820088\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02111228 1.04134521 1.19845945 1.18045107 1.03456996 1.56420354\n",
      "  1.00169983 1.22071391 1.1751796  1.05575152]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "7 loss -34541.61763052727\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02403881 1.04637338 1.22680022 1.20620922 1.03779353 1.64117968\n",
      "  1.00172501 1.24916742 1.20021365 1.06368097]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "8 loss -35677.44092887072\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02692445 1.05114559 1.2551416  1.23196655 1.04062737 1.71820341\n",
      "  1.00172258 1.27762423 1.22524668 1.07159092]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "9 loss -36824.07134223953\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02975997 1.05564493 1.28348296 1.25772274 1.04308654 1.79527078\n",
      "  1.00169666 1.30608329 1.2502784  1.07948156]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "10 loss -37980.20466766686\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03253478 1.05985339 1.3118237  1.28347749 1.04518599 1.87237845\n",
      "  1.00164896 1.33454373 1.27530856 1.0873531 ]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "11 loss -39144.69663656382\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03523736 1.06375291 1.34016334 1.30923055 1.0469405  1.94952346\n",
      "  1.00157637 1.36300482 1.30033694 1.09520574]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "12 loss -40316.54381636862\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03785615 1.06732708 1.36850145 1.33498169 1.0483649  2.02670308\n",
      "  1.00146078 1.39146599 1.32536332 1.10303971]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "13 loss -41494.86615522479\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04038111 1.07056283 1.39683767 1.36073069 1.0494743  2.10391463\n",
      "  1.00124898 1.41992675 1.35038755 1.11085522]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "14 loss -42678.891105944065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04280545 1.07345216 1.42517169 1.38647739 1.05028432 2.18115548\n",
      "  1.0009374  1.44838672 1.37540947 1.11865252]]\n",
      "{0: 2519, 1: 295}\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04280545 1.07345216 1.42517169 1.38647739 1.05028432 2.18115548\n",
      "  1.0009374  1.44838672 1.37540947 1.11865252]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.927\n",
      "Precision            0.353\n",
      "Recall               0.55\n",
      "F1                   0.43\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 191 | TN: 2434 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2519, 1: 295}\n",
      "acc 0.9019189765458422\n",
      "(array([0.96625645, 0.35254237]), array([0.9272381 , 0.55026455]), array([0.94634526, 0.42975207]), array([2625,  189]))\n",
      "(0.659399411926982, 0.7387513227513227, 0.6880486613626724, None)\n",
      "[[2434  191]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.3525423728813559 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n"
     ]
    }
   ],
   "source": [
    "#init with old network thetas\n",
    "# train_unl(0.01,15,np.array([[1.0,1.0,1.0,\\\n",
    "#                 1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]]))   \n",
    "\n",
    "train_unl(0.1/len(train_L_S),15,tf.constant_initializer(np.array([[1.0,1.0,1.0,\\\n",
    "                1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## UN-Normalized loss plot loss vs f1s\n",
    "    \n",
    "y_loss=[-18693.37,-29153.74,-33418.08,-42678.89]\n",
    "x_f1s =[0.432,0.431,0.43,0.43]\n",
    "text=[\"snorkel-thetas\",\"normalized-thetas-ep7\",\"Un-normalized-trained-thetas-ep7\",\"Un-normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"Spouse-Un-Normalized-Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -16494.182287841937\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32011886 0.01742789 0.2357877  0.22487695 0.31165927 0.31422477\n",
      "  0.3704413  0.2596181  0.37190215 0.27239461]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "1 loss -16916.06023599083\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32240975 0.01967812 0.26207358 0.24913283 0.31554645 0.38878112\n",
      "  0.3713858  0.28807452 0.39742105 0.28029567]]\n",
      "{0: 2525, 1: 289}\n",
      "(0.35294117647058826, 0.5396825396825397, 0.42677824267782427, None)\n",
      "\n",
      "2 loss -17416.749670163787\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32452451 0.02109609 0.28853598 0.27350093 0.31813751 0.46426523\n",
      "  0.37184652 0.31660673 0.42290434 0.28811702]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3493150684931507, 0.5396825396825397, 0.42411642411642414, None)\n",
      "\n",
      "3 loss -17991.625112567406\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32647211 0.02179288 0.31514827 0.29796438 0.31959997 0.54035557\n",
      "  0.3721248  0.34520201 0.44836835 0.29587051]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "4 loss -18635.534044730877\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32826902 0.02187884 0.34189006 0.32251053 0.32010328 0.61685632\n",
      "  0.3722757  0.37385273 0.47382477 0.3035663 ]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "5 loss -19343.04096959059\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32993333 0.02145398 0.36874542 0.34712976 0.31980289 0.69364125\n",
      "  0.37231688 0.4025537  0.49928154 0.31121285]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "6 loss -20108.635126544148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33148239 0.02060487 0.39570164 0.37181452 0.31883431 0.77062559\n",
      "  0.37212214 0.43130096 0.52474361 0.31881709]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "7 loss -20926.887016926226\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "8 loss -21792.56392621291\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33429748 0.01791428 0.44987709 0.42135783 0.31533315 0.9249758\n",
      "  0.37143819 0.48892073 0.5756929  0.33392039]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "9 loss -22700.711796781943\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33559094 0.01618427 0.47708055 0.44620747 0.3129758  1.00227118\n",
      "  0.37102895 0.51778678 0.60118119 0.34142788]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "10 loss -23646.708630970148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33682366 0.01425623 0.50435256 0.4711043  0.31030577 1.07961598\n",
      "  0.37058621 0.54668595 0.62667775 0.34891031]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "11 loss -24626.294582069993\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33800499 0.01216444 0.53168768 0.49604533 0.30737681 1.15699511\n",
      "  0.37011559 0.57561492 0.65218124 0.35637022]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "12 loss -25635.583678344366\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33914272 0.00993705 0.55908108 0.52102789 0.30423308 1.23439775\n",
      "  0.36962181 0.60457039 0.67769008 0.36380974]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "13 loss -26671.061594641385\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34024335 0.00759722 0.58652841 0.54604955 0.30091092 1.31181615\n",
      "  0.3691088  0.63354905 0.70320251 0.37123061]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "14 loss -27729.573303400488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "# init random thetas\n",
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -16494.182287841937\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32011886 0.01742789 0.2357877  0.22487695 0.31165927 0.31422477\n",
      "  0.3704413  0.2596181  0.37190215 0.27239461]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "1 loss -16916.06023599083\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32240975 0.01967812 0.26207358 0.24913283 0.31554645 0.38878112\n",
      "  0.3713858  0.28807452 0.39742105 0.28029567]]\n",
      "{0: 2525, 1: 289}\n",
      "(0.35294117647058826, 0.5396825396825397, 0.42677824267782427, None)\n",
      "\n",
      "2 loss -17416.749670163787\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32452451 0.02109609 0.28853598 0.27350093 0.31813751 0.46426523\n",
      "  0.37184652 0.31660673 0.42290434 0.28811702]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3493150684931507, 0.5396825396825397, 0.42411642411642414, None)\n",
      "\n",
      "3 loss -17991.625112567406\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32647211 0.02179288 0.31514827 0.29796438 0.31959997 0.54035557\n",
      "  0.3721248  0.34520201 0.44836835 0.29587051]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "4 loss -18635.534044730877\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32826902 0.02187884 0.34189006 0.32251053 0.32010328 0.61685632\n",
      "  0.3722757  0.37385273 0.47382477 0.3035663 ]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "5 loss -19343.04096959059\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32993333 0.02145398 0.36874542 0.34712976 0.31980289 0.69364125\n",
      "  0.37231688 0.4025537  0.49928154 0.31121285]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "6 loss -20108.635126544148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33148239 0.02060487 0.39570164 0.37181452 0.31883431 0.77062559\n",
      "  0.37212214 0.43130096 0.52474361 0.31881709]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "7 loss -20926.887016926226\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "8 loss -21792.56392621291\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33429748 0.01791428 0.44987709 0.42135783 0.31533315 0.9249758\n",
      "  0.37143819 0.48892073 0.5756929  0.33392039]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "9 loss -22700.711796781943\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33559094 0.01618427 0.47708055 0.44620747 0.3129758  1.00227118\n",
      "  0.37102895 0.51778678 0.60118119 0.34142788]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "10 loss -23646.708630970148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33682366 0.01425623 0.50435256 0.4711043  0.31030577 1.07961598\n",
      "  0.37058621 0.54668595 0.62667775 0.34891031]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "11 loss -24626.294582069993\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33800499 0.01216444 0.53168768 0.49604533 0.30737681 1.15699511\n",
      "  0.37011559 0.57561492 0.65218124 0.35637022]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "12 loss -25635.583678344366\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33914272 0.00993705 0.55908108 0.52102789 0.30423308 1.23439775\n",
      "  0.36962181 0.60457039 0.67769008 0.36380974]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "13 loss -26671.061594641385\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34024335 0.00759722 0.58652841 0.54604955 0.30091092 1.31181615\n",
      "  0.3691088  0.63354905 0.70320251 0.37123061]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "14 loss -27729.573303400488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "# LFS init random thetas\n",
    "\n",
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 7 4 1 0 6 3 2 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([ 0.4751682, 0.46430319 , 0.77729748 , 0.69961045 , 0.43660742,  4.98316919,\n",
    "   0.4786732 , -0.29070728, -0.31361022, -0.41560446])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00019475777320921748\n",
      "[0.2        0.2        0.20397272 0.20397272 0.2        0.2\n",
      " 0.2        0.19602728 0.19602728 0.19602728]\n",
      "[1.         1.         0.99818703 0.99876917 1.         1.\n",
      " 1.         1.00189171 1.00123147 1.00159078]\n",
      "2714 100\n",
      "0  dm  (0.6518128224023582, 0.583047619047619, 0.604245316341007, None)\n",
      "0  db  (0.36, 0.19047619047619047, 0.24913494809688577, None)\n",
      "\n",
      "-7.1199749476605e+32\n",
      "[ 2.39523991e-01  4.44712440e-01  1.00592853e+00  1.00306476e+00\n",
      "  7.93729267e-01 -2.47683784e+10  8.30695122e-01 -1.40683111e+17\n",
      " -1.37206290e+17 -1.36481844e+17]\n",
      "[9.68596811e-01 8.16929334e-01 8.01833673e-01 8.35694159e-01\n",
      " 6.32314677e-01 2.47683784e+10 9.20080431e-01 1.40683111e+17\n",
      " 1.37206290e+17 1.36481844e+17]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.6513554689606083e+67\n",
      "[ 2.87203003e-01  7.29222600e-01  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -4.86866279e+20  8.94995005e-01 -2.71479292e+34\n",
      " -2.64769995e+34 -2.63372016e+34]\n",
      "[9.31534051e-01 6.56469425e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 4.86866279e+20 9.15642897e-01 2.71479292e+34\n",
      " 2.64769995e+34 2.63372016e+34]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-9.873188985162394e+101\n",
      "[ 3.24040405e-01  8.92774981e-01  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -8.57801313e+30  8.94995005e-01 -5.23879560e+51\n",
      " -5.10932482e+51 -5.08234771e+51]\n",
      "[9.03578251e-01 6.06795047e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 8.57801313e+30 9.15642897e-01 5.23879560e+51\n",
      " 5.10932482e+51 5.08234771e+51]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-3.6766047358767233e+136\n",
      "[ 4.03004051e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -1.52645882e+41  9.22433705e-01 -1.01094191e+69\n",
      " -9.85957646e+68 -9.80751814e+68]\n",
      "[8.45939120e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 1.52645882e+41 9.12767335e-01 1.01094191e+69\n",
      " 9.85957646e+68 9.80751814e+68]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-1.3691039849622358e+171\n",
      "[ 4.52881733e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -8.87619724e+51  9.68003629e-01 -1.95083683e+86\n",
      " -1.90262415e+86 -1.89257834e+86]\n",
      "[8.11376459e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 8.87619724e+51 9.09801058e-01 1.95083683e+86\n",
      " 1.90262415e+86 1.89257834e+86]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "0 -2.516208931471449e+194\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "-1.2643949596623685e+189\n",
      "[ 4.92911523e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -4.18977398e+57  9.86196450e-01 -1.33557247e+96\n",
      " -1.30256533e+96 -1.29568782e+96]\n",
      "[7.84826492e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 4.18977398e+57 9.09252111e-01 1.33557247e+96\n",
      " 1.30256533e+96 1.29568782e+96]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-2.3895655466420104e+225\n",
      "[ 5.24003463e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -1.25083611e+068  1.00438011e+000 -2.57728356e+113\n",
      " -2.51358897e+113 -2.50031728e+113]\n",
      "[7.65011828e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 1.25083611e+068 9.09066965e-001 2.57728356e+113\n",
      " 2.51358897e+113 2.50031728e+113]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-8.898328613657434e+259\n",
      "[ 5.61785684e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -2.45873958e+078  1.00438011e+000 -4.97344076e+130\n",
      " -4.85052791e+130 -4.82491724e+130]\n",
      "[7.41972292e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 2.45873958e+078 9.09066965e-001 4.97344076e+130\n",
      " 4.85052791e+130 4.82491724e+130]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-3.313583602170098e+294\n",
      "[ 5.91204610e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -4.33201092e+088  1.00438011e+000 -9.59735801e+147\n",
      " -9.36017038e+147 -9.31074891e+147]\n",
      "[7.24887158e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 4.33201092e+088 9.09066965e-001 9.59735801e+147\n",
      " 9.36017038e+147 9.31074891e+147]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/ipykernel_launcher.py:60: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "1 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "2 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "3 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "4 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "                print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "                print(c,\" dm \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "                print(c,\" db \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "  \n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00024597518522208474\n",
      "[1.         1.         1.         1.         1.         1.00531229\n",
      " 1.         1.         1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "0 -0.9839007408883389\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002477972163026618\n",
      "[1.         1.         1.         1.         1.         1.01071759\n",
      " 1.         1.00531229 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "1 -0.9911888652106471\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.00024967684220414605\n",
      "[1.         1.         1.         1.         1.         1.01621772\n",
      " 1.         1.01071759 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "2 -0.9987073688165843\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002516160600819831\n",
      "[1.         1.         1.         1.         1.         1.0218145\n",
      " 1.         1.01621772 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "3 -1.0064642403279322\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002536169307750545\n",
      "[1.         1.         1.         1.         1.         1.02750979\n",
      " 1.         1.0218145  1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "4 -1.014467723100218\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network 2 to get thetas \n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
