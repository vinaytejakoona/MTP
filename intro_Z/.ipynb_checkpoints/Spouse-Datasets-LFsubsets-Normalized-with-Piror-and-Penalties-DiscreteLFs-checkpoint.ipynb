{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n",
    "print(len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 2625\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Discrete ##########\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,0)\n",
    "    \n",
    "def LF_familial_relationship(c):\n",
    "    return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x.decode('utf-8')).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,0)\n",
    "\n",
    "\n",
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Continuous ################\n",
    "\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "#     get_text_between, get_tagged_text,\n",
    "# )\n",
    "\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     global LF_Threshold\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for sw in spouses:\n",
    "#         sc=max(sc,get_similarity(word_vectors,sw))\n",
    "#     return (1,sc)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     global LF_Threshold\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for sw in spouses:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for sw in spouses:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "#     return(1,max(sc_1,sc_2))\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     global LF_Threshold\n",
    "#     word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "#     sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "#     if 'and' in get_between_tokens(c):\n",
    "#         return (1,sc)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_familial_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for fw in family:\n",
    "#         sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for fw in family:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for fw in family:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,max(sc_1,sc_2))\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for ow in other:\n",
    "#         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# # def LF_other_relationship_left_window(c):\n",
    "# #     sc = 0\n",
    "# #     word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "# #     for ow in other:\n",
    "# #         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "# #     return (-1,sc) \n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # # Read in known spouse pairs and save as set of tuples\n",
    "# # with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "# #     known_spouses = set(\n",
    "# #         tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "# #     )\n",
    "# # # Last name pairs for known spouses\n",
    "# # last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # def LF_Three_Lists_Left_Window(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife_left_window(c)\n",
    "# #     c2,s2 = LF_family_left_window(c)\n",
    "# #     c3,s3 = LF_other_relationship_left_window(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "\n",
    "# # def LF_Three_Lists_Between_Words(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife(c)\n",
    "# #     c2,s2 = LF_familial_relationship(c)\n",
    "# #     c3,s3 = LF_other_relationship(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "    \n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'BW:',s1,s2,s3,prob_sc\n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "    \n",
    "\n",
    "def drawLossVsF1(y_loss,x_f1s,text,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x_f1s, y_loss)\n",
    "\n",
    "    plt.xlabel('f1-score')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(title)\n",
    "    \n",
    "    for i, txt in enumerate(text):\n",
    "        ax.annotate(txt, (x_f1s[i],y_loss[i]))\n",
    "        \n",
    "    plt.savefig(title+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "(2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "LF_l = np.array([\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "])\n",
    "\n",
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))\n",
    "\n",
    "import numpy as np\n",
    "# dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def createSubsetOfDataAndLFs(train_L_S,dev_L_S,LF_l,data_subset_indices,LF_subset_indices):\n",
    "    global NoOfLFs\n",
    "    SizeOfDataSubSet = 10000\n",
    "    NoOfLFsInLFSubset = 6\n",
    "    NoOfLFs = NoOfLFsInLFSubset\n",
    "    train_L_S = train_L_S[data_subset_indices,:]\n",
    "    train_L_S = train_L_S[:,:,LF_subset_indices]\n",
    "    print(data_subset_indices.shape,LF_subset_indices.shape)\n",
    "    dev_L_S = dev_L_S[:,:,LF_subset_indices]\n",
    "    LF_l = LF_l[LF_subset_indices]\n",
    "    return train_L_S,dev_L_S,LF_l\n",
    "\n",
    "def train_nl_penalties(LF_subset_indices):\n",
    "    global train_L_S,dev_L_S,LF_l\n",
    "    print(LF_subset_indices)\n",
    "    \n",
    "    data_subset_indices = np.array(random.sample(range(train_L_S.shape[0]), SizeOfDataSubSet))\n",
    "\n",
    "    train_L_S,dev_L_S,LF_l = createSubsetOfDataAndLFs(train_L_S,dev_L_S,LF_l,data_subset_indices,LF_subset_indices)\n",
    "    print(train_L_S.shape,dev_L_S.shape,LF_l.shape)\n",
    "\n",
    "    train_nl(0.1/len(train_L_S),5,tf.truncated_normal_initializer(0,0.1,seed)) \n",
    "\n",
    "    train_nl_p(0.1/len(train_L_S),5,tf.truncated_normal_initializer(0,0.1,seed)) \n",
    "\n",
    "    train_nl_p2(0.1/len(train_L_S),5,tf.truncated_normal_initializer(0,0.1,seed))\n",
    "\n",
    "    train_nl_p3(0.1/len(train_L_S),5,tf.truncated_normal_initializer(0,0.1,seed)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "seed = 12 \n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 7 8 9]\n",
      "(10000,) (6,)\n",
      "(10000, 2, 6) (2814, 2, 6) (6,)\n",
      "0 loss 41641.76504616807\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.04529264 -0.09905075  0.03875814 -0.02447436  0.03313103 -0.01251145]]\n",
      "{0: 2390, 1: 424}\n",
      "(0.22641509433962265, 0.5079365079365079, 0.31321370309951063, None)\n",
      "\n",
      "1 loss 41595.04026404794\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-1.12811658e-04 -1.85190406e-02  1.06663502e-02 -5.73847509e-03\n",
      "  -2.45000975e-03  3.99037312e-05]]\n",
      "{0: 2217, 1: 597}\n",
      "(0.1574539363484087, 0.4973544973544973, 0.23918575063613226, None)\n",
      "\n",
      "2 loss 41588.887845803336\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-2.36561654e-17 -4.50008643e-17 -4.28936544e-17  5.90739548e-17\n",
      "   6.11807467e-17  2.60414069e-17]]\n",
      "{0: 2814}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loss 41588.830833630505\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 3.10903081e-17  4.82762452e-17 -2.59143103e-17  4.91687711e-18\n",
      "  -2.88735145e-18  4.44713718e-17]]\n",
      "{0: 2799, 1: 15}\n",
      "(0.26666666666666666, 0.021164021164021163, 0.0392156862745098, None)\n",
      "\n",
      "4 loss 41588.830833634966\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 6.66033802e-17  1.14761024e-16  6.06776279e-17 -4.18142729e-17\n",
      "  -8.41424476e-17  3.50142153e-17]]\n",
      "{0: 2202, 1: 612}\n",
      "(0.1650326797385621, 0.5343915343915344, 0.25218476903870163, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 6.66033802e-17  1.14761024e-16  6.06776279e-17 -4.18142729e-17\n",
      "  -8.41424476e-17  3.50142153e-17]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0952\n",
      "Neg. class accuracy: 0.909\n",
      "Precision            0.07\n",
      "Recall               0.0952\n",
      "F1                   0.0807\n",
      "----------------------------------------\n",
      "TP: 18 | FP: 239 | TN: 2386 | FN: 171\n",
      "========================================\n",
      "\n",
      "{0: 2202, 1: 612}\n",
      "acc 0.7871357498223169\n",
      "(array([0.96003633, 0.16503268]), array([0.80533333, 0.53439153]), array([0.87590636, 0.25218477]), array([2625,  189]))\n",
      "(0.5625345051735499, 0.6698624338624339, 0.5640455645483543, None)\n",
      "[[2114  511]\n",
      " [  88  101]]\n",
      "prec: tp/(tp+fp) 0.1650326797385621 recall: tp/(tp+fn) 0.5343915343915344\n",
      "(0.1650326797385621, 0.5343915343915344, 0.25218476903870163, None)\n",
      "0 loss 43002.655596927034\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 5.05514970e-02 -8.63328855e-02  4.87706366e-02  4.88520190e-06\n",
      "   3.37271773e-02  5.53260207e-05]]\n",
      "{0: 2556, 1: 258}\n",
      "(0.3488372093023256, 0.47619047619047616, 0.40268456375838924, None)\n",
      "\n",
      "1 loss 41967.30351234111\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[5.58885697e-05 8.22785263e-05 1.16045885e-02 3.47465367e-03\n",
      "  4.56075547e-03 2.31505709e-03]]\n",
      "{0: 2545, 1: 269}\n",
      "(0.35687732342007433, 0.5079365079365079, 0.4192139737991266, None)\n",
      "\n",
      "2 loss 41588.84704491809\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[2.74844718e-04 2.60321001e-04 3.50275769e-05 2.20554809e-04\n",
      "  1.94582836e-04 1.03894496e-04]]\n",
      "{0: 2593, 1: 221}\n",
      "(0.38461538461538464, 0.4497354497354497, 0.41463414634146345, None)\n",
      "\n",
      "3 loss 41588.83105067487\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.35692987e-04 1.35431146e-04 1.73598114e-04 2.84856300e-04\n",
      "  2.81456656e-04 1.07065547e-05]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "4 loss 41588.83100874175\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[2.76215500e-05 2.70155816e-05 3.52194591e-05 5.31472088e-05\n",
      "  4.13025071e-05 1.72126112e-05]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[2.76215500e-05 2.70155816e-05 3.52194591e-05 5.31472088e-05\n",
      "  4.13025071e-05 1.72126112e-05]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.455\n",
      "Neg. class accuracy: 0.947\n",
      "Precision            0.384\n",
      "Recall               0.455\n",
      "F1                   0.416\n",
      "----------------------------------------\n",
      "TP: 86 | FP: 138 | TN: 2487 | FN: 103\n",
      "========================================\n",
      "\n",
      "{0: 2590, 1: 224}\n",
      "acc 0.9143567874911158\n",
      "(array([0.96023166, 0.38392857]), array([0.94742857, 0.45502646]), array([0.95378715, 0.41646489]), array([2625,  189]))\n",
      "(0.6720801158301158, 0.7012275132275132, 0.6851260217430164, None)\n",
      "[[2487  138]\n",
      " [ 103   86]]\n",
      "prec: tp/(tp+fp) 0.38392857142857145 recall: tp/(tp+fn) 0.455026455026455\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5bffff60>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5bffff60>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d5bffff60>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 43000.63130243052\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.04446025 -0.08636132  0.03611076 -0.02164246  0.03420167 -0.0099945 ]]\n",
      "{0: 2390, 1: 424}\n",
      "(0.22641509433962265, 0.5079365079365079, 0.31321370309951063, None)\n",
      "\n",
      "1 loss 41966.733989915076\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[3.81553795e-05 7.87308348e-05 3.33471024e-03 2.95218112e-04\n",
      "  1.54727638e-03 8.43212793e-04]]\n",
      "{0: 2545, 1: 269}\n",
      "(0.35687732342007433, 0.5079365079365079, 0.4192139737991266, None)\n",
      "\n",
      "2 loss 41588.83167274608\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.48443530e-04 1.15109539e-04 1.82449415e-04 3.05086179e-04\n",
      "  1.82960285e-04 9.40974418e-05]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "3 loss 41588.83102246899\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[7.24080209e-05 6.72642220e-05 1.02099734e-04 5.46730147e-05\n",
      "  2.13805595e-04 3.47919333e-05]]\n",
      "{0: 2556, 1: 258}\n",
      "(0.37209302325581395, 0.5079365079365079, 0.42953020134228187, None)\n",
      "\n",
      "4 loss 41588.83102868187\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.00015718 0.00015185 0.0002266  0.0001243  0.00024096 0.00030744]]\n",
      "{0: 2560, 1: 254}\n",
      "(0.37401574803149606, 0.5026455026455027, 0.4288939051918736, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.00015718 0.00015185 0.0002266  0.0001243  0.00024096 0.00030744]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.503\n",
      "Neg. class accuracy: 0.939\n",
      "Precision            0.374\n",
      "Recall               0.503\n",
      "F1                   0.429\n",
      "----------------------------------------\n",
      "TP: 95 | FP: 159 | TN: 2466 | FN: 94\n",
      "========================================\n",
      "\n",
      "{0: 2560, 1: 254}\n",
      "acc 0.910092395167022\n",
      "(array([0.96328125, 0.37401575]), array([0.93942857, 0.5026455 ]), array([0.9512054 , 0.42889391]), array([2625,  189]))\n",
      "(0.668648499015748, 0.721037037037037, 0.6900496526923688, None)\n",
      "[[2466  159]\n",
      " [  94   95]]\n",
      "prec: tp/(tp+fp) 0.37401574803149606 recall: tp/(tp+fn) 0.5026455026455027\n",
      "(0.37401574803149606, 0.5026455026455027, 0.4288939051918736, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d610c0b00>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d610c0b00>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d610c0b00>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 81502.1506470213\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.21666989 -0.08647327  0.10881792  0.09944611  0.20484656  0.14030441]]\n",
      "{0: 2568, 1: 246}\n",
      "(0.37398373983739835, 0.48677248677248675, 0.4229885057471264, None)\n",
      "\n",
      "1 loss 78833.59265509271\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.31546659 0.01260159 0.20749485 0.19653671 0.30100983 0.23885532]]\n",
      "{0: 2562, 1: 252}\n",
      "(0.373015873015873, 0.4973544973544973, 0.4263038548752835, None)\n",
      "\n",
      "2 loss 76445.04474176883\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.41399234 0.11148472 0.30517841 0.29088138 0.39358324 0.33701439]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "3 loss 74339.8757578947\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.51211409 0.2100796  0.40116541 0.38154931 0.48141777 0.43453633]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "4 loss 72515.79407187033\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.6096446  0.30826555 0.49455496 0.4679286  0.56384456 0.53102825]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.6096446  0.30826555 0.49455496 0.4679286  0.56384456 0.53102825]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.503\n",
      "Neg. class accuracy: 0.94\n",
      "Precision            0.375\n",
      "Recall               0.503\n",
      "F1                   0.43\n",
      "----------------------------------------\n",
      "TP: 95 | FP: 158 | TN: 2467 | FN: 94\n",
      "========================================\n",
      "\n",
      "{0: 2561, 1: 253}\n",
      "acc 0.9104477611940298\n",
      "(array([0.96329559, 0.37549407]), array([0.93980952, 0.5026455 ]), array([0.95140764, 0.42986425]), array([2625,  189]))\n",
      "(0.6693948294036575, 0.7212275132275132, 0.6906359446682941, None)\n",
      "[[2467  158]\n",
      " [  94   95]]\n",
      "prec: tp/(tp+fp) 0.37549407114624506 recall: tp/(tp+fn) 0.5026455026455027\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n"
     ]
    }
   ],
   "source": [
    "# for picking lfs randomly\n",
    "# LF_subset_indices = np.array(random.sample(range(train_L_S.shape[2]), NoOfLFsInLFSubset))\n",
    "LF_subset_indices = np.array([0,1,2,7,8,9])\n",
    "train_nl_penalties(LF_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 7 8 9]\n",
      "0 loss 41641.78924151758\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.04604426 -0.09921623  0.03911107 -0.02382747  0.03316833 -0.01241809]]\n",
      "{0: 2390, 1: 424}\n",
      "(0.22641509433962265, 0.5079365079365079, 0.31321370309951063, None)\n",
      "\n",
      "1 loss 41595.08040766994\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-0.00030438 -0.01881479  0.01122429 -0.00587909 -0.00276936  0.00031191]]\n",
      "{0: 2217, 1: 597}\n",
      "(0.1574539363484087, 0.4973544973544973, 0.23918575063613226, None)\n",
      "\n",
      "2 loss 41588.891402601344\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-2.86628785e-17  2.71945027e-17  5.33240618e-17 -5.55645756e-17\n",
      "  -5.44732593e-17  4.77013805e-17]]\n",
      "{0: 2210, 1: 604}\n",
      "(0.16225165562913907, 0.5185185185185185, 0.24716267339218162, None)\n",
      "\n",
      "3 loss 41588.83083360085\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-8.61185714e-13  1.41031572e-13  1.23818976e-12  2.79418334e-12\n",
      "  -2.41384226e-12  8.43658991e-13]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.2945205479452055, 0.455026455026455, 0.3575883575883576, None)\n",
      "\n",
      "4 loss 41588.83083378579\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-4.55492066e-17 -3.02670420e-17 -6.27276481e-19 -3.77205403e-17\n",
      "  -4.41050292e-17 -2.08914018e-17]]\n",
      "{0: 2423, 1: 391}\n",
      "(0.04092071611253197, 0.08465608465608465, 0.05517241379310345, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[-4.55492066e-17 -3.02670420e-17 -6.27276481e-19 -3.77205403e-17\n",
      "  -4.41050292e-17 -2.08914018e-17]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.0\n",
      "Neg. class accuracy: 1.0\n",
      "Precision            0.0\n",
      "Recall               0.0\n",
      "F1                   0.0\n",
      "----------------------------------------\n",
      "TP: 0 | FP: 0 | TN: 2625 | FN: 189\n",
      "========================================\n",
      "\n",
      "{0: 2423, 1: 391}\n",
      "acc 0.8052594171997157\n",
      "(array([0.92860091, 0.04092072]), array([0.85714286, 0.08465608]), array([0.89144216, 0.05517241]), array([2625,  189]))\n",
      "(0.4847608120389321, 0.4708994708994709, 0.47330728455106835, None)\n",
      "[[2250  375]\n",
      " [ 173   16]]\n",
      "prec: tp/(tp+fp) 0.04092071611253197 recall: tp/(tp+fn) 0.08465608465608465\n",
      "(0.04092071611253197, 0.08465608465608465, 0.05517241379310345, None)\n",
      "0 loss 43002.74027885509\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 5.13172806e-02 -8.63388855e-02  4.89169307e-02  3.31134399e-05\n",
      "   3.40337048e-02  2.37390067e-07]]\n",
      "{0: 2556, 1: 258}\n",
      "(0.3488372093023256, 0.47619047619047616, 0.40268456375838924, None)\n",
      "\n",
      "1 loss 41967.37736458209\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[6.89220850e-05 8.79063551e-05 1.17674843e-02 3.50239503e-03\n",
      "  4.79471476e-03 2.36794157e-03]]\n",
      "{0: 2545, 1: 269}\n",
      "(0.35687732342007433, 0.5079365079365079, 0.4192139737991266, None)\n",
      "\n",
      "2 loss 41588.84766264465\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[4.74632069e-06 2.62463303e-04 7.47086701e-05 7.06722660e-06\n",
      "  6.69005667e-06 3.10969392e-04]]\n",
      "{0: 2543, 1: 271}\n",
      "(0.3505535055350554, 0.5026455026455027, 0.41304347826086957, None)\n",
      "\n",
      "3 loss 41588.83104337773\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.09182952e-04 1.09300656e-04 1.23981914e-04 8.71162767e-05\n",
      "  7.52379097e-05 2.96845583e-04]]\n",
      "{0: 2559, 1: 255}\n",
      "(0.3686274509803922, 0.4973544973544973, 0.42342342342342343, None)\n",
      "\n",
      "4 loss 41588.83104163467\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.00011822 0.00011636 0.00012426 0.00012897 0.00010899 0.00023745]]\n",
      "{0: 2592, 1: 222}\n",
      "(0.38288288288288286, 0.4497354497354497, 0.41362530413625304, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.00011822 0.00011636 0.00012426 0.00012897 0.00010899 0.00023745]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.45\n",
      "Neg. class accuracy: 0.948\n",
      "Precision            0.383\n",
      "Recall               0.45\n",
      "F1                   0.414\n",
      "----------------------------------------\n",
      "TP: 85 | FP: 137 | TN: 2488 | FN: 104\n",
      "========================================\n",
      "\n",
      "{0: 2592, 1: 222}\n",
      "acc 0.9143567874911158\n",
      "(array([0.95987654, 0.38288288]), array([0.94780952, 0.44973545]), array([0.95380487, 0.4136253 ]), array([2625,  189]))\n",
      "(0.6713797130463797, 0.6987724867724867, 0.6837150864173694, None)\n",
      "[[2488  137]\n",
      " [ 104   85]]\n",
      "prec: tp/(tp+fp) 0.38288288288288286 recall: tp/(tp+fn) 0.4497354497354497\n",
      "(0.38288288288288286, 0.4497354497354497, 0.41362530413625304, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59738d68>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59738d68>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59738d68>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 43000.73139555753\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.04506498 -0.08636658  0.03648744 -0.02100437  0.03424399 -0.00986764]]\n",
      "{0: 2390, 1: 424}\n",
      "(0.22641509433962265, 0.5079365079365079, 0.31321370309951063, None)\n",
      "\n",
      "1 loss 41966.79628219772\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[4.83503324e-05 7.50248610e-05 3.64056219e-03 3.55405101e-04\n",
      "  1.68317211e-03 9.01547943e-04]]\n",
      "{0: 2545, 1: 269}\n",
      "(0.35687732342007433, 0.5079365079365079, 0.4192139737991266, None)\n",
      "\n",
      "2 loss 41588.83182691384\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[8.14605552e-05 1.05415904e-04 7.19337715e-05 5.27726678e-05\n",
      "  1.85253423e-04 1.11183277e-04]]\n",
      "{0: 2560, 1: 254}\n",
      "(0.37401574803149606, 0.5026455026455027, 0.4288939051918736, None)\n",
      "\n",
      "3 loss 41588.83102293743\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.00013796 0.00013357 0.00012701 0.00019864 0.00022159 0.00011828]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "4 loss 41588.83102238382\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.42682748e-04 1.39087177e-04 1.23058379e-04 2.40044217e-04\n",
      "  2.40936752e-04 6.40449846e-05]]\n",
      "{0: 2590, 1: 224}\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[1.42682748e-04 1.39087177e-04 1.23058379e-04 2.40044217e-04\n",
      "  2.40936752e-04 6.40449846e-05]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.455\n",
      "Neg. class accuracy: 0.947\n",
      "Precision            0.384\n",
      "Recall               0.455\n",
      "F1                   0.416\n",
      "----------------------------------------\n",
      "TP: 86 | FP: 138 | TN: 2487 | FN: 103\n",
      "========================================\n",
      "\n",
      "{0: 2590, 1: 224}\n",
      "acc 0.9143567874911158\n",
      "(array([0.96023166, 0.38392857]), array([0.94742857, 0.45502646]), array([0.95378715, 0.41646489]), array([2625,  189]))\n",
      "(0.6720801158301158, 0.7012275132275132, 0.6851260217430164, None)\n",
      "[[2487  138]\n",
      " [ 103   86]]\n",
      "prec: tp/(tp+fp) 0.38392857142857145 recall: tp/(tp+fn) 0.455026455026455\n",
      "(0.38392857142857145, 0.455026455026455, 0.4164648910411622, None)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 6), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 6) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(6,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59587208>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59587208>\n",
      "<tf.Variable 'alphas:0' shape=(6,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 6), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 6), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f0d59587208>\n",
      "nls Tensor(\"mul:0\", shape=(?, 6), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 6), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(6,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 81501.76143776668\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[ 0.2166696  -0.08648185  0.10880209  0.09946645  0.20484577  0.14032152]]\n",
      "{0: 2568, 1: 246}\n",
      "(0.37398373983739835, 0.48677248677248675, 0.4229885057471264, None)\n",
      "\n",
      "1 loss 78832.58382041463\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.31546712 0.01259222 0.20745514 0.19654739 0.30100993 0.2388643 ]]\n",
      "{0: 2562, 1: 252}\n",
      "(0.373015873015873, 0.4973544973544973, 0.4263038548752835, None)\n",
      "\n",
      "2 loss 76443.5054400946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.41399362 0.11147213 0.30508981 0.29085318 0.39356162 0.33700021]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "3 loss 74337.9131892981\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.5121078  0.21005681 0.40098927 0.38145962 0.48135877 0.43448235]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "4 loss 72513.51480739823\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.60960522 0.30821777 0.49423617 0.46776718 0.56374246 0.53091902]]\n",
      "{0: 2561, 1: 253}\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855]\n",
      "[[0.60960522 0.30821777 0.49423617 0.46776718 0.56374246 0.53091902]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.503\n",
      "Neg. class accuracy: 0.94\n",
      "Precision            0.375\n",
      "Recall               0.503\n",
      "F1                   0.43\n",
      "----------------------------------------\n",
      "TP: 95 | FP: 158 | TN: 2467 | FN: 94\n",
      "========================================\n",
      "\n",
      "{0: 2561, 1: 253}\n",
      "acc 0.9104477611940298\n",
      "(array([0.96329559, 0.37549407]), array([0.93980952, 0.5026455 ]), array([0.95140764, 0.42986425]), array([2625,  189]))\n",
      "(0.6693948294036575, 0.7212275132275132, 0.6906359446682941, None)\n",
      "[[2467  158]\n",
      " [  94   95]]\n",
      "prec: tp/(tp+fp) 0.37549407114624506 recall: tp/(tp+fn) 0.5026455026455027\n",
      "(0.37549407114624506, 0.5026455026455027, 0.4298642533936652, None)\n"
     ]
    }
   ],
   "source": [
    "LF_subset_indices = np.array([0,1,2,7,8,9])\n",
    "train_nl_penalties(LF_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized training with different params\n",
    "\n",
    "def train_nl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty reduce_sum(max(0,-theta))\n",
    "\n",
    "def train_nl_p(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) +\\\n",
    "                        tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty2  -tf.minimum( tf.reduce_min(theta),0)\n",
    "\n",
    "def train_nl_p2(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     -tf.minimum( tf.reduce_min(thetas),0.0)\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty3 sum(log(1+e^(-x-pk)))\n",
    "\n",
    "def train_nl_p3(lr,ep,th,pk=0):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     +tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized loss with prior from other LFs\n",
    "\n",
    "def train_nlp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        \n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        \n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "               \n",
    "        \n",
    "        pout = tf.map_fn(lambda li: tf.map_fn(lambda lij:li*lij,li ),l)\n",
    "#         print(\"nls\",nls_)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        \n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        \n",
    "\n",
    "        sumy = tf.reduce_sum(t_pout-logz,axis=1)\n",
    "        print(\"sumy\",sumy)\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(t_pout-logz,axis=1) ))\n",
    "\n",
    "        \n",
    "        def index_along_every_row(array, index):\n",
    "            N, _ = array.shape\n",
    "            return array[np.arange(N), index]\n",
    "\n",
    "        #Best LF\n",
    "        blf = tf.argmax(t_pout,axis=1)\n",
    "        print(\"blf\",blf)\n",
    "        print(\"normloss\",normloss)\n",
    "        \n",
    "        \n",
    "        marginals = tf.py_func(index_along_every_row, [tf.squeeze(t_pout), tf.squeeze(blf)], [tf.float64])[0]\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict1 = tf.gather(k,tf.squeeze(blf))\n",
    "        \n",
    "        predict = tf.where(tf.equal(predict1,1),tf.ones_like(predict1),tf.zeros_like(predict1))\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "                print(b)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "            print(b)\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 10)\n",
      "(2814, 2)\n",
      "(22276, 10)\n",
      "(22276, 2)\n"
     ]
    }
   ],
   "source": [
    "#input L_S:train_L_S, K: no of classes\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def get_maj_prior(L_S,K):\n",
    "    maj_prior = []\n",
    "    \n",
    "    print(L_S[:,0,:].shape)\n",
    "    for row in np.nditer(L_S[:,0,:],flags=['external_loop'], order='C'):\n",
    "        p = np.ones(K)/K\n",
    "        unique, counts = np.unique(row, return_counts=True)\n",
    "        unique = [int(x) for x in unique]\n",
    "        rc = dict(zip(unique, counts))\n",
    "        tnz = np.count_nonzero(row)\n",
    "        if -1 in rc:\n",
    "            p[0] = rc[-1]\n",
    "        if 1 in rc:\n",
    "            p[1] = rc[1]\n",
    "        p = softmax(p)\n",
    "        maj_prior.append(p)\n",
    "    return np.array(maj_prior)\n",
    "\n",
    "dev_maj_pl=get_maj_prior(dev_L_S,2)\n",
    "print(dev_maj_pl.shape)\n",
    "\n",
    "\n",
    "train_maj_pl=get_maj_prior(train_L_S,2)\n",
    "print(train_maj_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized loss with majority prior\n",
    "\n",
    "\n",
    "\n",
    "def train_nlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout-logz),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Normalized loss with majority bias un-normalized\n",
    "\n",
    "def train_unlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
