{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "from __future__ import absolute_import, division, print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "(100,)\n",
      "test 1 normalized loss\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f489a6d6b00>\n",
      "l Tensor(\"IteratorGetNext_1:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 431.8368605568429\n",
      "[[ 1.18344996  0.89448721  1.08062081  1.07210997  1.17281864  1.11088738\n",
      "   1.23262683  1.10123923  1.21085699 -0.62844118]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "1 loss 224.67280498368527\n",
      "[[ 1.81996349  1.55238972  1.72444957  1.71655862  1.81007374  1.75252969\n",
      "   1.86575341  1.74357547  1.84547419 -1.30835671]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "2 loss 133.51244717737043\n",
      "[[ 2.30092311  2.0496746   2.21089559  2.20347427  2.29158481  2.23732509\n",
      "   2.34420826  2.22889375  2.3250286  -1.82332473]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "3 loss 88.24442478838142\n",
      "[[ 2.68162163  2.44179297  2.59541614  2.58832275  2.6726667   2.62069397\n",
      "   2.72316772  2.6126273   2.70475101 -2.22796207]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "4 loss 62.87661287384147\n",
      "[[ 2.99543436  2.76368155  2.91193113  2.90506964  2.98675044  2.9363946\n",
      "   3.03575131  2.92858579  3.0178738  -2.55870469]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "[[ 2.99543436  2.76368155  2.91193113  2.90506964  2.98675044  2.9363946\n",
      "   3.03575131  2.92858579  3.0178738  -2.55870469]]\n",
      "{1: 100}\n",
      "acc 1.0\n",
      "(array([1.]), array([1.]), array([1.]), array([100]))\n",
      "(1.0, 1.0, 1.0, None)\n",
      "[[100]]\n",
      "(1.0, 1.0, 1.0, None)\n",
      "test 1 un-normalized loss\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f489a387dd8>\n",
      "l Tensor(\"IteratorGetNext_1:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -694.0152479344697\n",
      "[[ 1.32084592  1.01751582  1.21286095  1.2039256   1.30967916  1.24463984\n",
      "   1.37250729  1.23450914  1.34963601 -0.738578  ]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "1 loss -1696.2225556733608\n",
      "[[ 2.32219881  2.01886872  2.21421384  2.20527849  2.31103206  2.24599273\n",
      "   2.37386018  2.23586203  2.3509889  -1.73993089]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "2 loss -2697.2228584853347\n",
      "[[ 3.32294958  3.01961948  3.2149646   3.20602925  3.31178282  3.2467435\n",
      "   3.37461095  3.2366128   3.35173966 -2.74068165]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "3 loss -3697.837758032137\n",
      "[[ 4.32345468  4.02012458  4.2154697   4.20653435  4.31228792  4.2472486\n",
      "   4.37511604  4.2371179   4.35224476 -3.74118675]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "4 loss -4698.270899124591\n",
      "[[ 5.32382599  5.02049589  5.21584101  5.20690567  5.31265923  5.24761991\n",
      "   5.37548736  5.23748921  5.35261607 -4.74155806]]\n",
      "{1: 100}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "[[ 5.32382599  5.02049589  5.21584101  5.20690567  5.31265923  5.24761991\n",
      "   5.37548736  5.23748921  5.35261607 -4.74155806]]\n",
      "{1: 100}\n",
      "acc 1.0\n",
      "(array([1.]), array([1.]), array([1.]), array([100]))\n",
      "(1.0, 1.0, 1.0, None)\n",
      "[[100]]\n",
      "(1.0, 1.0, 1.0, None)\n"
     ]
    }
   ],
   "source": [
    "# test one\n",
    "\n",
    "# 10 LFs, 9 give 1s and 1 give -1\n",
    "NoOfLFs = 10\n",
    "L_train = -np.ones([100,NoOfLFs])\n",
    "L_train[:,:-1] = np.ones([100,NoOfLFs-1])\n",
    "print(L_train.shape)\n",
    "# print(L_train)\n",
    "true_labels = np.ones([100])\n",
    "print(L_test.shape)\n",
    "LF_l = np.ones(10)\n",
    "LF_l[-1] = -1\n",
    "# print(LF_l)\n",
    "# print(L_test)\n",
    "\n",
    "print(\"test normalized loss\")\n",
    "train_nl(0.01,5,tf.truncated_normal_initializer(0.2,0.1,12))\n",
    "\n",
    "print(\"test un-normalized loss\")\n",
    "train_unl(0.01,5,tf.truncated_normal_initializer(0.2,0.1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 2)\n",
      "test 1 normalized loss\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 2) dtype=float64_ref>\n",
      "k Tensor(\"Const:0\", shape=(2,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f489aaeb9e8>\n",
      "l Tensor(\"IteratorGetNext_1:0\", shape=(?, 2), dtype=float64)\n",
      "pout Tensor(\"map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 2), dtype=float64)\n",
      "t_pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(2,), dtype=float64)\n",
      "zy Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 154.02419628482903\n",
      "[[-0.32674603 -0.78825775]]\n",
      "{0: 100}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "1 loss 140.30964808925694\n",
      "[[ 0.22599954 -0.64495052]]\n",
      "{0: 50, 1: 50}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "2 loss 130.2063000796349\n",
      "[[ 0.81771922 -0.85980187]]\n",
      "{0: 50, 1: 50}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "3 loss 117.30052350067575\n",
      "[[ 1.30773074 -1.27469706]]\n",
      "{0: 50, 1: 50}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "4 loss 105.25664065844025\n",
      "[[ 1.68021731 -1.6543208 ]]\n",
      "{0: 50, 1: 50}\n",
      "(1.0, 1.0, 1.0, None)\n",
      "\n",
      "[[ 1.68021731 -1.6543208 ]]\n",
      "{0: 50, 1: 50}\n",
      "acc 1.0\n",
      "(array([1., 1.]), array([1., 1.]), array([1., 1.]), array([50, 50]))\n",
      "[[50  0]\n",
      " [ 0 50]]\n",
      "(1.0, 1.0, 1.0, None)\n",
      "test 1 un-normalized loss\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thetas <tf.Variable 'thetas:0' shape=(1, 2) dtype=float64_ref>\n",
      "k Tensor(\"Const:0\", shape=(2,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f489a3877f0>\n",
      "l Tensor(\"IteratorGetNext_1:0\", shape=(?, 2), dtype=float64)\n",
      "pout Tensor(\"map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 2), dtype=float64)\n",
      "t_pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(2,), dtype=float64)\n",
      "zy Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -145.7902220807057\n",
      "[[-1.61745665 -1.91242043]]\n",
      "{0: 100}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "1 loss -215.49118183001372\n",
      "[[-2.38909221 -2.65270161]]\n",
      "{0: 100}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "2 loss -289.4953734361127\n",
      "[[-3.14800828 -3.38774624]]\n",
      "{0: 100}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "3 loss -363.3865409387889\n",
      "[[-3.89309528 -4.115853  ]]\n",
      "{0: 100}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "4 loss -436.59569550226297\n",
      "[[-4.62854042 -4.83883796]]\n",
      "{0: 100}\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "[[-4.62854042 -4.83883796]]\n",
      "{0: 100}\n",
      "acc 0.5\n",
      "(array([0.5, 0. ]), array([1., 0.]), array([0.66666667, 0.        ]), array([50, 50]))\n",
      "[[50  0]\n",
      " [50  0]]\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#test two\n",
    "NoOfLFs = 2\n",
    "\n",
    "true_labels = np.ones([100])\n",
    "true_labels[0:100:2] = 0\n",
    "print(true_labels.shape)\n",
    "LF1 = [1 if x==1 else 0 for x in true_labels]\n",
    "LF2 = [0 if x==1 else 1 for x in true_labels]\n",
    "L_train = np.array([LF1,LF2],dtype=np.float64).T\n",
    "print(L_train.shape)\n",
    "# print(L_train)\n",
    "# print(true_labels)\n",
    "LF_l = np.array([1,1])\n",
    "\n",
    "print(\"test normalized loss\")\n",
    "train_nl(0.01,5,tf.truncated_normal_initializer(0.2,0.1,12))\n",
    "\n",
    "print(\"test un-normalized loss\")\n",
    "train_unl(0.01,5,tf.truncated_normal_initializer(0.2,0.1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized training with different params\n",
    "\n",
    "def train_nl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(L_train).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(L_train).batch(L_train.shape[0])\n",
    "\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "#         alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "#                                  initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "#                                  dtype=tf.float64)\n",
    "\n",
    "#         thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                                  initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                                  dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "#         l,s =  tf.unstack(next_element,axis=1)\n",
    "        l = next_element\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "        print(\"l\",l)\n",
    "       \n",
    "\n",
    "#         s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                t,m,pl = sess.run([thetas,marginals,predict])\n",
    "\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            t,m,pl = sess.run([thetas,marginals,predict])\n",
    "\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(true_labels,np.array(pl)))\n",
    "#             print(precision_recall_fscore_support(true_labels,np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(true_labels,pl)\n",
    "            print(cf)\n",
    "#             print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(true_labels,np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(L_train).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(L_train).batch(L_train.shape[0])\n",
    "\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "#         alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "#                                  initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "#                                  dtype=tf.float64)\n",
    "\n",
    "#         thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                                  initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                                  dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "#         l,s =  tf.unstack(next_element,axis=1)\n",
    "        l = next_element\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "        print(\"l\",l)\n",
    "       \n",
    "\n",
    "#         s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                t,m,pl = sess.run([thetas,marginals,predict])\n",
    "\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            t,m,pl = sess.run([thetas,marginals,predict])\n",
    "\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(true_labels,np.array(pl)))\n",
    "#             print(precision_recall_fscore_support(true_labels,np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(true_labels,pl)\n",
    "            print(cf)\n",
    "#             print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(true_labels,np.array(pl),average=\"binary\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
