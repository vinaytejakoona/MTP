{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n",
    "print(len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 2625\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Discrete ##########\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,0)\n",
    "    \n",
    "def LF_familial_relationship(c):\n",
    "    return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,0)\n",
    "\n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x.decode('utf-8')).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,0)\n",
    "\n",
    "\n",
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Continuous ################\n",
    "\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "#     get_text_between, get_tagged_text,\n",
    "# )\n",
    "\n",
    "\n",
    "# spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "# family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "#               'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "# family = family | {f + '-in-law' for f in family}\n",
    "# other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# # Helper function to get last name\n",
    "# def last_name(s):\n",
    "#     name_parts = s.split(' ')\n",
    "#     return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "# def LF_husband_wife(c):\n",
    "#     global LF_Threshold\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for sw in spouses:\n",
    "#         sc=max(sc,get_similarity(word_vectors,sw))\n",
    "#     return (1,sc)\n",
    "\n",
    "# def LF_husband_wife_left_window(c):\n",
    "#     global LF_Threshold\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for sw in spouses:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for sw in spouses:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "#     return(1,max(sc_1,sc_2))\n",
    "    \n",
    "# def LF_same_last_name(c):\n",
    "#     p1_last_name = last_name(c.person1.get_span())\n",
    "#     p2_last_name = last_name(c.person2.get_span())\n",
    "#     if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "#         if c.person1.get_span() != c.person2.get_span():\n",
    "#             return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_no_spouse_in_sentence(c):\n",
    "#     return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "# def LF_and_married(c):\n",
    "#     global LF_Threshold\n",
    "#     word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "#     sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "#     if 'and' in get_between_tokens(c):\n",
    "#         return (1,sc)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_familial_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for fw in family:\n",
    "#         sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# def LF_family_left_window(c):\n",
    "#     sc_1 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "#     for fw in family:\n",
    "#         sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     sc_2 = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "#     for fw in family:\n",
    "#         sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "#     return (-1,max(sc_1,sc_2))\n",
    "\n",
    "# def LF_other_relationship(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "#     for ow in other:\n",
    "#         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "#     return (-1,sc) \n",
    "\n",
    "# # def LF_other_relationship_left_window(c):\n",
    "# #     sc = 0\n",
    "# #     word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "# #     for ow in other:\n",
    "# #         sc=max(sc,get_similarity(word_vectors,ow))\n",
    "# #     return (-1,sc) \n",
    "\n",
    "# import bz2\n",
    "\n",
    "# # Function to remove special characters from text\n",
    "# def strip_special(s):\n",
    "#     return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# # # Read in known spouse pairs and save as set of tuples\n",
    "# # with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "# #     known_spouses = set(\n",
    "# #         tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "# #     )\n",
    "# # # Last name pairs for known spouses\n",
    "# # last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "# def LF_distant_supervision(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "# def LF_distant_supervision_last_names(c):\n",
    "#     p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "#     p1n, p2n = last_name(p1), last_name(p2)\n",
    "#     return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # def LF_Three_Lists_Left_Window(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife_left_window(c)\n",
    "# #     c2,s2 = LF_family_left_window(c)\n",
    "# #     c3,s3 = LF_other_relationship_left_window(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "\n",
    "# # def LF_Three_Lists_Between_Words(c):\n",
    "# #     global softmax_Threshold\n",
    "# #     c1,s1 = LF_husband_wife(c)\n",
    "# #     c2,s2 = LF_familial_relationship(c)\n",
    "# #     c3,s3 = LF_other_relationship(c)\n",
    "# #     sc = np.array([s1,s2,s3])\n",
    "# #     c = [c1,c2,c3]\n",
    "# #     sharp_param = 1.5\n",
    "    \n",
    "# #     prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "# #     prob_sc = prob_sc / np.sum(prob_sc)\n",
    "# #     #print 'BW:',s1,s2,s3,prob_sc\n",
    "# #     if s1==s2 or s3==s1:\n",
    "# #         return (0,0)\n",
    "# #     return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "# LFs = [\n",
    "#     LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "#     LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "#     LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "#     LF_family_left_window, LF_other_relationship\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "    \n",
    "\n",
    "def drawLossVsF1(y_loss,x_f1s,text,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x_f1s, y_loss)\n",
    "\n",
    "    plt.xlabel('f1-score')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(title)\n",
    "    \n",
    "    for i, txt in enumerate(text):\n",
    "        ax.annotate(txt, (x_f1s[i],y_loss[i]))\n",
    "        \n",
    "    plt.savefig(title+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_l = [\n",
    "    1,1,1,1,1,-1,1,-1,-1,-1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 2, 10) (22276, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "# dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "test_L_S = dev_L_S\n",
    "true_labels = gold_labels_dev\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## smooth lfs training with discrete lf normalizer\n",
    "\n",
    "def train_dnl_s(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s,name = \"s_\")\n",
    "        print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "    \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),\\\n",
    "                           -tf.ones_like(v))\n",
    "            print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda c: iskequalsy(l,c)*s_ ,np.array([-1,1],dtype=np.float64),name=\"pout\")\n",
    "       \n",
    "#         print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,name=\"t_pout\")\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            print(\"intsy\",out1)\n",
    "            return out1\n",
    "        \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),np.arange(NoOfClasses,dtype=np.float64))\n",
    "\n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                    np.array([-1,1],dtype=np.float64),name=\"zy\")\n",
    "    \n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                       np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "       \n",
    "\n",
    "    \n",
    "        print(\"zy\",zy)\n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        \n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        \n",
    "        print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "\n",
    "\n",
    "        tf.summary.scalar('un-normloss', normloss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,normloss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(true_labels,pl))\n",
    "                print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "            predictAndPrint(pl)\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"macro\"))\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06bf3ba978>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06bf3ba978>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06bf3ba978>\n",
      "s_ Tensor(\"s_/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(10,), dtype=float64)\n",
      "out Tensor(\"map/while/Select:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 224352.1830065406\n",
      "acc 0.9250177683013504\n",
      "(0.358974358974359, 0.14814814814814814, 0.2097378277153558, None)\n",
      "\n",
      "1 loss 221164.44039844957\n",
      "acc 0.9292821606254442\n",
      "(0.4107142857142857, 0.12169312169312169, 0.18775510204081627, None)\n",
      "\n",
      "2 loss 217913.72686089252\n",
      "acc 0.9310589907604833\n",
      "(0.4444444444444444, 0.10582010582010581, 0.17094017094017094, None)\n",
      "\n",
      "3 loss 214610.1597126316\n",
      "acc 0.9314143567874911\n",
      "(0.4375, 0.07407407407407407, 0.12669683257918551, None)\n",
      "\n",
      "4 loss 211264.36852157532\n",
      "acc 0.9314143567874911\n",
      "(0.4166666666666667, 0.05291005291005291, 0.09389671361502347, None)\n",
      "\n",
      "[ 0.26742073 -0.03627596  0.15634345  0.14740788  0.2557389   0.1356449\n",
      "  0.31664329  0.08438536  0.19951407  0.11790122]\n",
      "[[1.06763842 0.76445285 0.96031139 0.95130697 1.05658912 1.069412\n",
      "  1.11940727 1.07829567 1.1932818  1.11222918]]\n",
      "{0: 2790, 1: 24}\n",
      "acc 0.9314143567874911\n",
      "acc 0.9314143567874911\n",
      "[[2611   14]\n",
      " [ 179   10]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x230.4 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4166666666666667, 0.05291005291005291, 0.09389671361502347, None)\n",
      "(0.6762544802867384, 0.5237883597883598, 0.5291274888481395, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = train_dnl_s(0.01/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sooth LFs normalized training with penalty reduce_sum(max(0,-theta))\n",
    "\n",
    "def train_unl_p_s(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: ls_*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)   )) +\\\n",
    "                        tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#         train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer(lr).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d122d5f8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d122d5f8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d122d5f8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -25494.51768544034\n",
      "[ 0.11754335 -0.18578984  0.01019951  0.00125849  0.1063722   0.04082102\n",
      "  0.16944019  0.030554    0.14560791  0.06404822]\n",
      "[[1.11754564 0.81422089 1.00912338 1.00019251 1.10638245 1.04175292\n",
      "  1.16912963 1.03164444 1.14668795 1.06510784]]\n",
      "{0: 2692, 1: 122}\n",
      "(0.3114754098360656, 0.20105820105820105, 0.24437299035369772, None)\n",
      "\n",
      "1 loss -25574.227638773078\n",
      "[ 0.11754208 -0.18579419  0.01084176  0.00189507  0.10636661  0.04030222\n",
      "  0.16967608  0.02989713  0.14487783  0.06337006]\n",
      "[[1.11754664 0.81422723 1.00868561 0.99975922 1.10638697 1.04216838\n",
      "  1.16905273 1.03208331 1.14704318 1.06549445]]\n",
      "{0: 2703, 1: 111}\n",
      "(0.32432432432432434, 0.19047619047619047, 0.24, None)\n",
      "\n",
      "2 loss -25654.600038473014\n",
      "[ 0.11754081 -0.18579852  0.01148642  0.00253402  0.10636108  0.03978217\n",
      "  0.16991365  0.02923724  0.14414447  0.06268883]\n",
      "[[1.11754765 0.81423354 1.00824634 0.99932446 1.10639143 1.04258495\n",
      "  1.1689753  1.03252447 1.14740039 1.06588313]]\n",
      "{0: 2707, 1: 107}\n",
      "(0.32710280373831774, 0.18518518518518517, 0.23648648648648646, None)\n",
      "\n",
      "[ 0.11754081 -0.18579852  0.01148642  0.00253402  0.10636108  0.03978217\n",
      "  0.16991365  0.02923724  0.14414447  0.06268883]\n",
      "[[1.11754765 0.81423354 1.00824634 0.99932446 1.10639143 1.04258495\n",
      "  1.1689753  1.03252447 1.14740039 1.06588313]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.185\n",
      "Neg. class accuracy: 0.973\n",
      "Precision            0.327\n",
      "Recall               0.185\n",
      "F1                   0.236\n",
      "----------------------------------------\n",
      "TP: 35 | FP: 72 | TN: 2553 | FN: 154\n",
      "========================================\n",
      "\n",
      "{0: 2707, 1: 107}\n",
      "acc 0.9196872778962332\n",
      "(array([0.94311045, 0.3271028 ]), array([0.97257143, 0.18518519]), array([0.9576144 , 0.23648649]), array([2625,  189]))\n",
      "(0.6351066290579287, 0.5788783068783069, 0.5970504450436933, None)\n",
      "[[2553   72]\n",
      " [ 154   35]]\n",
      "prec: tp/(tp+fp) 0.32710280373831774 recall: tp/(tp+fn) 0.18518518518518517\n",
      "(0.32710280373831774, 0.18518518518518517, 0.23648648648648646, None)\n"
     ]
    }
   ],
   "source": [
    "train_unl_p_s(0.001/len(train_L_S),3,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d122d978>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d122d978>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d122d978>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -25455.280569833903\n",
      "[ 0.11754462 -0.18578551  0.00956604  0.00063064  0.10637781  0.04133338\n",
      "  0.16920833  0.03120133  0.14632746  0.06471656]\n",
      "[[1.11754464 0.8142146  1.0095553  1.00062    1.10637792 1.04134269\n",
      "  1.16920524 1.0312122  1.14633824 1.06472713]]\n",
      "{0: 2685, 1: 129}\n",
      "(0.29457364341085274, 0.20105820105820105, 0.23899371069182387, None)\n",
      "\n",
      "1 loss -25456.0711237867\n",
      "[ 0.1175446  -0.18578556  0.00957243  0.00063697  0.10637776  0.04132821\n",
      "  0.16921066  0.0311948   0.14632021  0.06470983]\n",
      "[[1.11754465 0.81421466 1.00955094 1.00061569 1.10637796 1.04134683\n",
      "  1.16920448 1.03121656 1.14634176 1.06473097]]\n",
      "{0: 2685, 1: 129}\n",
      "(0.29457364341085274, 0.20105820105820105, 0.23899371069182387, None)\n",
      "\n",
      "2 loss -25456.861743618807\n",
      "[ 0.11754459 -0.1857856   0.00957881  0.0006433   0.1063777   0.04132304\n",
      "  0.169213    0.03118828  0.14631296  0.06470309]\n",
      "[[1.11754466 0.81421472 1.00954659 1.00061137 1.10637801 1.04135096\n",
      "  1.16920371 1.03122091 1.14634528 1.0647348 ]]\n",
      "{0: 2685, 1: 129}\n",
      "(0.29457364341085274, 0.20105820105820105, 0.23899371069182387, None)\n",
      "\n",
      "[ 0.11754459 -0.1857856   0.00957881  0.0006433   0.1063777   0.04132304\n",
      "  0.169213    0.03118828  0.14631296  0.06470309]\n",
      "[[1.11754466 0.81421472 1.00954659 1.00061137 1.10637801 1.04135096\n",
      "  1.16920371 1.03122091 1.14634528 1.0647348 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.201\n",
      "Neg. class accuracy: 0.965\n",
      "Precision            0.295\n",
      "Recall               0.201\n",
      "F1                   0.239\n",
      "----------------------------------------\n",
      "TP: 38 | FP: 91 | TN: 2534 | FN: 151\n",
      "========================================\n",
      "\n",
      "{0: 2685, 1: 129}\n",
      "acc 0.914001421464108\n",
      "(array([0.94376164, 0.29457364]), array([0.96533333, 0.2010582 ]), array([0.95442561, 0.23899371]), array([2625,  189]))\n",
      "(0.6191676410722793, 0.5831957671957673, 0.5967096613722773, None)\n",
      "[[2534   91]\n",
      " [ 151   38]]\n",
      "prec: tp/(tp+fp) 0.29457364341085274 recall: tp/(tp+fn) 0.20105820105820105\n",
      "(0.29457364341085274, 0.20105820105820105, 0.23899371069182387, None)\n"
     ]
    }
   ],
   "source": [
    "train_unl_p_s(0.00001/len(train_L_S),3,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty reduce_sum(max(0,-theta))\n",
    "\n",
    "def train_nl_p(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) +\\\n",
    "                        tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4c400>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4c400>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4c400>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "5 loss 158604.93738094255\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.54997522 0.38551056 0.71208165 0.68224501 0.60391497 1.3682661\n",
      "  0.58642215 0.89307    0.94179888 0.7020998 ]]\n",
      "{0: 2523, 1: 291}\n",
      "(0.35051546391752575, 0.5396825396825397, 0.42499999999999993, None)\n",
      "\n",
      "6 loss 157032.75110433032\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "7 loss 155668.8970006794\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.4314503  0.38839979 0.70424996 0.67037562 0.52367733 1.4280809\n",
      "  0.42606297 0.77043883 0.80142626 0.51281886]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "8 loss 154481.67664693782\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.39067389 0.37902675 0.69806429 0.66205327 0.48793706 1.45887735\n",
      "  0.37175861 0.71320089 0.73512861 0.4199123 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "9 loss 153446.87729269065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3546431  0.36275345 0.68942781 0.65117459 0.45284473 1.49068915\n",
      "  0.33247148 0.65941213 0.67218014 0.32889969]]\n",
      "{0: 2501, 1: 313}\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n",
      "\n",
      "10 loss 152548.81576950828\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3228263  0.34281758 0.67875204 0.63822859 0.41888411 1.52333766\n",
      "  0.3020918  0.60913597 0.61277898 0.24064353]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 151777.05782060773\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.29540707 0.32165781 0.66655386 0.62380273 0.38668169 1.55659623\n",
      "  0.27742105 0.56230756 0.55701578 0.15647785]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss 151121.9679241542\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.27235149 0.30085188 0.65330747 0.6084251  0.35673569 1.59025571\n",
      "  0.25725406 0.51880115 0.50491631 0.07840616]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss 150572.4023227708\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.25361337 0.28149093 0.63946432 0.5925888  0.32946785 1.62411754\n",
      "  0.24130367 0.47842506 0.45642398 0.00909516]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "14 loss 150138.42392283736\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.46084890e-01 2.67581703e-01 6.26581076e-01 5.77954388e-01\n",
      "  3.07501751e-01 1.65753169e+00 2.40881738e-01 4.40131950e-01\n",
      "  4.10577814e-01 3.68781955e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.46084890e-01 2.67581703e-01 6.26581076e-01 5.77954388e-01\n",
      "  3.07501751e-01 1.65753169e+00 2.40881738e-01 4.40131950e-01\n",
      "  4.10577814e-01 3.68781955e-06]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.917\n",
      "Precision            0.329\n",
      "Recall               0.566\n",
      "F1                   0.416\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 218 | TN: 2407 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2489, 1: 325}\n",
      "acc 0.8933901918976546\n",
      "(array([0.96705504, 0.32923077]), array([0.91695238, 0.56613757]), array([0.9413375 , 0.41634241]), array([2625,  189]))\n",
      "(0.6481429057081929, 0.7415449735449735, 0.6788399586699516, None)\n",
      "[[2407  218]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3292307692307692 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty init at 1\n",
    "\n",
    "train_nl_p(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4cc18>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4cc18>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4cc18>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 157574.7348655809\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 3.83943366e-02 -8.59176793e-02  4.12140554e-02  3.87969377e-02\n",
      "   4.94520347e-02  7.63448781e-02  7.43057986e-02  1.95670645e-02\n",
      "   9.40633695e-02  5.01784904e-06]]\n",
      "{0: 2492, 1: 322}\n",
      "(0.32919254658385094, 0.5608465608465608, 0.41487279843444225, None)\n",
      "\n",
      "1 loss 155215.51969439027\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.48005963e-02 2.43615010e-04 5.03492442e-02 4.70117231e-02\n",
      "  2.79224024e-02 1.26737596e-01 2.41703044e-02 1.56798558e-02\n",
      "  5.65577570e-02 8.61844142e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "2 loss 154322.58291672444\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.77687137e-02 2.25366351e-02 5.46633369e-02 5.03081359e-02\n",
      "  2.50382561e-02 1.80879939e-01 2.72792878e-02 1.69925746e-02\n",
      "  3.69843154e-02 2.72648967e-05]]\n",
      "{0: 2478, 1: 336}\n",
      "(0.31845238095238093, 0.5661375661375662, 0.4076190476190476, None)\n",
      "\n",
      "3 loss 154250.1440012092\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[3.43848963e-02 3.09483804e-02 6.19408185e-02 5.68594522e-02\n",
      "  2.84000132e-02 2.35087505e-01 3.40275329e-02 1.88589847e-02\n",
      "  2.68273317e-02 8.72434321e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "4 loss 154160.5610998261\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[4.12179267e-02 3.72599430e-02 7.14021436e-02 6.55816067e-02\n",
      "  3.32506935e-02 2.88978421e-01 4.08993327e-02 2.07114449e-02\n",
      "  2.11020578e-02 1.91588116e-05]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "5 loss 154053.4694733106\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[4.80415430e-02 4.33570741e-02 8.18242547e-02 7.51926350e-02\n",
      "  3.84333895e-02 3.42529050e-01 4.77808074e-02 2.25863020e-02\n",
      "  1.76787936e-02 6.76427632e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "6 loss 153929.90984735623\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[5.48397862e-02 4.94251383e-02 9.26811226e-02 8.51837503e-02\n",
      "  4.37284989e-02 3.95709962e-01 5.46449833e-02 2.44370372e-02\n",
      "  1.54873993e-02 1.55951167e-05]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "7 loss 153791.2735660695\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[6.15819442e-02 5.54714921e-02 1.03726593e-01 9.53277922e-02\n",
      "  4.90670243e-02 4.48488214e-01 6.14533949e-02 2.62108952e-02\n",
      "  1.39663665e-02 5.20903481e-06]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "8 loss 153639.10894393732\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[6.82410518e-02 6.14787514e-02 1.14830766e-01 1.05507612e-01\n",
      "  5.44143361e-02 5.00829892e-01 6.81769694e-02 2.78643791e-02\n",
      "  1.28032806e-02 1.83794073e-05]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "9 loss 153475.0381450644\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[7.47968704e-02 6.74293066e-02 1.25918191e-01 1.15655763e-01\n",
      "  5.97486933e-02 5.52701337e-01 7.47916700e-02 2.93636414e-02\n",
      "  1.18137930e-02 8.52827265e-06]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "10 loss 153300.70612045916\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[8.12330690e-02 7.33079095e-02 1.36940718e-01 1.25729164e-01\n",
      "  6.50541082e-02 6.04070055e-01 8.12821754e-02 3.06832781e-02\n",
      "  1.08840939e-02 1.07304503e-05]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "11 loss 153117.74120678956\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[8.75387193e-02 7.91029473e-02 1.47865921e-01 1.35698740e-01\n",
      "  7.03187041e-02 6.54905157e-01 8.76351841e-02 3.18044306e-02\n",
      "  9.94120752e-03 1.18910554e-05]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "12 loss 152927.72416793866\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[9.37047826e-02 8.48048197e-02 1.58670452e-01 1.45543460e-01\n",
      "  7.55326892e-02 7.05177821e-01 9.38417705e-02 3.27137902e-02\n",
      "  8.93754259e-03 6.40446939e-06]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "13 loss 152732.16378203008\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[9.97248729e-02 9.04061063e-02 1.69337039e-01 1.55247730e-01\n",
      "  8.06881524e-02 7.54861504e-01 9.98960813e-02 3.34025102e-02\n",
      "  7.84184459e-03 6.76373927e-06]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "14 loss 152532.47737766337\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.05594998e-01 9.59010774e-02 1.79852555e-01 1.64799734e-01\n",
      "  8.57785864e-02 8.03932113e-01 1.05792847e-01 3.38655112e-02\n",
      "  6.63396698e-03 7.87823267e-06]]\n",
      "{0: 2465, 1: 349}\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.05594998e-01 9.59010774e-02 1.79852555e-01 1.64799734e-01\n",
      "  8.57785864e-02 8.03932113e-01 1.05792847e-01 3.38655112e-02\n",
      "  6.63396698e-03 7.87823267e-06]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.908\n",
      "Precision            0.307\n",
      "Recall               0.566\n",
      "F1                   0.398\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 242 | TN: 2383 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2465, 1: 349}\n",
      "acc 0.8848614072494669\n",
      "(array([0.96673428, 0.30659026]), array([0.90780952, 0.56613757]), array([0.93634578, 0.39776952]), array([2625,  189]))\n",
      "(0.6366622688992601, 0.7369735449735451, 0.6670576463800293, None)\n",
      "[[2383  242]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.30659025787965616 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.30659025787965616, 0.5661375661375662, 0.3977695167286246, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty init at 0\n",
    "\n",
    "train_nl_p(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4c048>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4c048>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a4c048>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 155223.61682049933\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.2248407  0.00387133 0.16577774 0.15448145 0.22089214 0.30324402\n",
      "  0.27421098 0.23977818 0.33415093 0.20796125]]\n",
      "{0: 2528, 1: 286}\n",
      "(0.35664335664335667, 0.5396825396825397, 0.42947368421052634, None)\n",
      "\n",
      "1 loss 154570.1570840857\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.14950017 0.05469994 0.16727468 0.15639031 0.16863824 0.35229732\n",
      "  0.1822947  0.21921682 0.29583361 0.12217027]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "2 loss 154198.48120414754\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.1162838  0.08770334 0.1752097  0.16454925 0.14122249 0.40085759\n",
      "  0.11413716 0.19757307 0.25815161 0.04243535]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "3 loss 153943.26585100265\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00847828e-01 9.40567055e-02 1.78949962e-01 1.67772470e-01\n",
      "  1.20513366e-01 4.51197983e-01 9.73983617e-02 1.80055813e-01\n",
      "  2.25830651e-01 8.55240469e-06]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "4 loss 153745.60272039677\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01551336e-01 9.72546498e-02 1.83094882e-01 1.71258001e-01\n",
      "  1.09626077e-01 5.01817737e-01 1.00725773e-01 1.64491336e-01\n",
      "  1.97003442e-01 9.01725628e-06]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "5 loss 153554.6076933491\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03514379e-01 9.94714303e-02 1.87689512e-01 1.75085820e-01\n",
      "  1.04162565e-01 5.52445469e-01 1.02806874e-01 1.50694978e-01\n",
      "  1.71524877e-01 9.90828260e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "6 loss 153360.86856034605\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.05837400e-01 1.01469016e-01 1.92662154e-01 1.79219844e-01\n",
      "  1.01703595e-01 6.02909420e-01 1.05214620e-01 1.38519212e-01\n",
      "  1.49198808e-01 8.93711403e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "7 loss 153164.03981877724\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.08568080e-01 1.03626165e-01 1.98010167e-01 1.83688237e-01\n",
      "  1.01067393e-01 6.53075600e-01 1.08046530e-01 1.27764362e-01\n",
      "  1.29717980e-01 5.60335788e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "8 loss 152964.25000915473\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11680589e-01 1.06072053e-01 2.03724742e-01 1.88503112e-01\n",
      "  1.01620337e-01 7.02845014e-01 1.11259164e-01 1.18229691e-01\n",
      "  1.12736911e-01 7.68784974e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "9 loss 152761.90422660654\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.15110349e-01 1.08822560e-01 2.09782701e-01 1.93654174e-01\n",
      "  1.02994737e-01 7.52143899e-01 1.14780447e-01 1.09728869e-01\n",
      "  9.79097631e-02 2.05565572e-06]]\n",
      "{0: 2478, 1: 336}\n",
      "(0.31845238095238093, 0.5661375661375662, 0.4076190476190476, None)\n",
      "\n",
      "10 loss 152557.585413434\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.18786488e-01 1.11847534e-01 2.16149326e-01 1.99113982e-01\n",
      "  1.04960509e-01 8.00916248e-01 1.18535938e-01 1.02095962e-01\n",
      "  8.49130567e-02 1.17489292e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "11 loss 152351.9837168176\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.22643223e-01 1.15101038e-01 2.22782981e-01 2.04844575e-01\n",
      "  1.07363402e-01 8.49118744e-01 1.22459875e-01 9.51870393e-02\n",
      "  7.34574744e-02 8.35313661e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "12 loss 152145.83960653318\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.26624732e-01 1.18535479e-01 2.29639541e-01 2.10803365e-01\n",
      "  1.10094485e-01 8.96717334e-01 1.26496569e-01 8.88791990e-02\n",
      "  6.32916539e-02 7.01868845e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "13 loss 151939.90063925867\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.30684859e-01 1.22106872e-01 2.36675342e-01 2.16947053e-01\n",
      "  1.13073312e-01 9.43685001e-01 1.30600918e-01 8.30686990e-02\n",
      "  5.42015597e-02 8.06122721e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "14 loss 151734.88894362375\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.34786464e-01 1.25776877e-01 2.43849120e-01 2.23234178e-01\n",
      "  1.16238612e-01 9.90000283e-01 1.34737940e-01 7.76686543e-02\n",
      "  4.60073430e-02 6.61318449e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.34786464e-01 1.25776877e-01 2.43849120e-01 2.23234178e-01\n",
      "  1.16238612e-01 9.90000283e-01 1.34737940e-01 7.76686543e-02\n",
      "  4.60073430e-02 6.61318449e-06]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.908\n",
      "Precision            0.307\n",
      "Recall               0.566\n",
      "F1                   0.399\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 241 | TN: 2384 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2466, 1: 348}\n",
      "acc 0.8852167732764747\n",
      "(array([0.96674777, 0.30747126]), array([0.90819048, 0.56613757]), array([0.9365547 , 0.39851024]), array([2625,  189]))\n",
      "(0.6371095170176468, 0.7371640211640211, 0.66753247323297, None)\n",
      "[[2384  241]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3074712643678161 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty init at 0.2\n",
    "\n",
    "train_nl_p(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a846d8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a846d8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0a846d8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 177328.68299044456\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01874088 0.71637454 0.92309079 0.91201652 1.00877974 1.12112714\n",
      "  1.07023119 1.08008369 1.18919429 1.12356771]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 170637.539636164\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.92056097 0.61998249 0.84490993 0.83059571 0.91264682 1.19233795\n",
      "  0.9717674  1.09133108 1.18678918 1.07515922]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 166039.65982295244\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.82307524 0.5266278  0.7815422  0.76270372 0.81926292 1.25204292\n",
      "  0.87374085 1.06482643 1.14503249 0.9873967 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 162804.3234122617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72696341 0.44358681 0.73996419 0.71626628 0.73238314 1.29932764\n",
      "  0.77637412 1.01548014 1.08334649 0.89321343]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 160454.5286629692\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.63422629 0.39421639 0.71988625 0.69252585 0.65931495 1.33652514\n",
      "  0.68013933 0.95581433 1.01362187 0.79765871]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "5 loss 158604.93738094255\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.54997522 0.38551056 0.71208165 0.68224501 0.60391497 1.3682661\n",
      "  0.58642215 0.89307    0.94179888 0.7020998 ]]\n",
      "{0: 2523, 1: 291}\n",
      "(0.35051546391752575, 0.5396825396825397, 0.42499999999999993, None)\n",
      "\n",
      "6 loss 157032.75110433032\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "7 loss 155668.8970006794\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.4314503  0.38839979 0.70424996 0.67037562 0.52367733 1.4280809\n",
      "  0.42606297 0.77043883 0.80142626 0.51281886]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "8 loss 154481.67664693782\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.39067389 0.37902675 0.69806429 0.66205327 0.48793706 1.45887735\n",
      "  0.37175861 0.71320089 0.73512861 0.4199123 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "9 loss 153446.87729269065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3546431  0.36275345 0.68942781 0.65117459 0.45284473 1.49068915\n",
      "  0.33247148 0.65941213 0.67218014 0.32889969]]\n",
      "{0: 2501, 1: 313}\n",
      "(0.33865814696485624, 0.5608465608465608, 0.4223107569721115, None)\n",
      "\n",
      "10 loss 152548.81576950828\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.3228263  0.34281758 0.67875204 0.63822859 0.41888411 1.52333766\n",
      "  0.3020918  0.60913597 0.61277898 0.24064353]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 151777.05782060773\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.29540707 0.32165781 0.66655386 0.62380273 0.38668169 1.55659623\n",
      "  0.27742105 0.56230756 0.55701578 0.15647785]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss 151121.9679241542\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.27235149 0.30085188 0.65330747 0.6084251  0.35673569 1.59025571\n",
      "  0.25725406 0.51880115 0.50491631 0.07840616]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss 150572.4023227708\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.25361337 0.28149093 0.63946432 0.5925888  0.32946785 1.62411754\n",
      "  0.24130367 0.47842506 0.45642398 0.00909516]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "14 loss 150138.42392283736\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.46084890e-01 2.67581703e-01 6.26581076e-01 5.77954388e-01\n",
      "  3.07501751e-01 1.65753169e+00 2.40881738e-01 4.40131950e-01\n",
      "  4.10577814e-01 3.68781955e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[2.46084890e-01 2.67581703e-01 6.26581076e-01 5.77954388e-01\n",
      "  3.07501751e-01 1.65753169e+00 2.40881738e-01 4.40131950e-01\n",
      "  4.10577814e-01 3.68781955e-06]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.917\n",
      "Precision            0.329\n",
      "Recall               0.566\n",
      "F1                   0.416\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 218 | TN: 2407 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2489, 1: 325}\n",
      "acc 0.8933901918976546\n",
      "(array([0.96705504, 0.32923077]), array([0.91695238, 0.56613757]), array([0.9413375 , 0.41634241]), array([2625,  189]))\n",
      "(0.6481429057081929, 0.7415449735449735, 0.6788399586699516, None)\n",
      "[[2407  218]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3292307692307692 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty init at 0.2\n",
    "\n",
    "train_nl_p(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty2  -tf.minimum( tf.reduce_min(theta),0)\n",
    "\n",
    "def train_nl_p2(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     -tf.minimum( tf.reduce_min(thetas),0.0)\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06ccc98c50>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06ccc98c50>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06ccc98c50>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 180225.1071177936\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.48937504 -0.82917169  0.10462207  0.07296799  0.46144522  0.22720462\n",
      "   0.74651351  0.12799006  0.6630313   0.22980827]]\n",
      "{0: 2507, 1: 307}\n",
      "(0.3289902280130293, 0.5343915343915344, 0.40725806451612906, None)\n",
      "\n",
      "1 loss 176333.87695934216\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.3924136  -0.72936435  0.13720808  0.1161274   0.38788668  0.261267\n",
      "   0.64753064  0.11120358  0.5994196   0.1383464 ]]\n",
      "{0: 2477, 1: 337}\n",
      "(0.3086053412462908, 0.5502645502645502, 0.3954372623574144, None)\n",
      "\n",
      "2 loss 172741.64429152224\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.29829757 -0.62952899  0.15918832  0.14369496  0.3197603   0.30113541\n",
      "   0.54898404  0.09839164  0.53878679  0.05046   ]]\n",
      "{0: 2475, 1: 339}\n",
      "(0.30678466076696165, 0.5502645502645502, 0.3939393939393939, None)\n",
      "\n",
      "3 loss 169402.9183299918\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.21281638 -0.52968947  0.17704993  0.16417679  0.26210983  0.34400291\n",
      "   0.45082391  0.08698939  0.48052107 -0.02961473]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3159509202453988, 0.544973544973545, 0.39999999999999997, None)\n",
      "\n",
      "4 loss 166296.55408412803\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.15571084 -0.42985619  0.19145282  0.17942387  0.21677042  0.38914817\n",
      "   0.35318931  0.07709112  0.42514928 -0.08913701]]\n",
      "{0: 2492, 1: 322}\n",
      "(0.32298136645962733, 0.5502645502645502, 0.4070450097847358, None)\n",
      "\n",
      "5 loss 163380.26642195915\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.13522591 -0.33002316  0.20204796  0.18978836  0.18233551  0.43626656\n",
      "   0.25713694  0.06934668  0.37360189 -0.11451251]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.32515337423312884, 0.5608465608465608, 0.41165048543689325, None)\n",
      "\n",
      "6 loss 160604.12656384704\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.12821713 -0.23018922  0.21025702  0.19720508  0.15793784  0.48458045\n",
      "   0.16884284  0.06322381  0.32616864 -0.12011383]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.32515337423312884, 0.5608465608465608, 0.41165048543689325, None)\n",
      "\n",
      "7 loss 157943.1233507754\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.11728132 -0.13036786  0.21525824  0.20105461  0.13865805  0.53402611\n",
      "   0.11603659  0.05941655  0.283965   -0.11511196]]\n",
      "{0: 2487, 1: 327}\n",
      "(0.327217125382263, 0.5661375661375662, 0.41472868217054265, None)\n",
      "\n",
      "8 loss 155573.15807862\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.11387336 -0.05268158  0.2184925   0.20306257  0.1251259   0.58400426\n",
      "   0.11351503  0.05717372  0.24684043 -0.05267998]]\n",
      "{0: 2486, 1: 328}\n",
      "(0.32621951219512196, 0.5661375661375662, 0.413926499032882, None)\n",
      "\n",
      "9 loss 153760.70485517557\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.17255165e-01 5.18159688e-03 2.22959738e-01 2.06403074e-01\n",
      "  1.18985836e-01 6.33626331e-01 1.17002690e-01 5.45310530e-02\n",
      "  2.13318124e-01 5.51962577e-06]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3282208588957055, 0.5661375661375662, 0.4155339805825243, None)\n",
      "\n",
      "10 loss 153096.33089939132\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11484085e-01 4.98058946e-02 2.25569504e-01 2.07835104e-01\n",
      "  1.12656193e-01 6.83476644e-01 1.08873339e-01 5.34082751e-02\n",
      "  1.85004207e-01 6.49836578e-07]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3282208588957055, 0.5661375661375662, 0.4155339805825243, None)\n",
      "\n",
      "11 loss 152872.86564990375\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.10004497e-01 7.58176317e-02 2.27666628e-01 2.08843629e-01\n",
      "  1.08122528e-01 7.33158316e-01 1.08720546e-01 5.29827593e-02\n",
      "  1.60926172e-01 1.02647437e-05]]\n",
      "{0: 2478, 1: 336}\n",
      "(0.31845238095238093, 0.5661375661375662, 0.4076190476190476, None)\n",
      "\n",
      "12 loss 152657.67982412898\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.12470274e-01 9.10029912e-02 2.30392830e-01 2.10622650e-01\n",
      "  1.06205366e-01 7.82360799e-01 1.12056214e-01 5.25004042e-02\n",
      "  1.40000601e-01 8.19991284e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "13 loss 152445.19240849258\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.16538916e-01 1.00916363e-01 2.33951026e-01 2.13350179e-01\n",
      "  1.06299874e-01 8.30979235e-01 1.16494793e-01 5.17873334e-02\n",
      "  1.21651443e-01 9.25114589e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "14 loss 152234.09857827236\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.21127923e-01 1.08239329e-01 2.38287416e-01 2.16935458e-01\n",
      "  1.07754953e-01 8.78968041e-01 1.21219011e-01 5.08217245e-02\n",
      "  1.05475798e-01 4.55401280e-06]]\n",
      "{0: 2466, 1: 348}\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.21127923e-01 1.08239329e-01 2.38287416e-01 2.16935458e-01\n",
      "  1.07754953e-01 8.78968041e-01 1.21219011e-01 5.08217245e-02\n",
      "  1.05475798e-01 4.55401280e-06]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.908\n",
      "Precision            0.307\n",
      "Recall               0.566\n",
      "F1                   0.399\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 241 | TN: 2384 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2466, 1: 348}\n",
      "acc 0.8852167732764747\n",
      "(array([0.96674777, 0.30747126]), array([0.90819048, 0.56613757]), array([0.9365547 , 0.39851024]), array([2625,  189]))\n",
      "(0.6371095170176468, 0.7371640211640211, 0.66753247323297, None)\n",
      "[[2384  241]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3074712643678161 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3074712643678161, 0.5661375661375662, 0.39851024208566105, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 2 init at 0\n",
    "\n",
    "train_nl_p2(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06ccc98be0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06ccc98be0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06ccc98be0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 177492.7249819397\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.68958575 -0.62895691  0.22138901  0.18010784  0.64080276  0.46094227\n",
      "   0.94728643  0.34639597  0.87538308  0.4352255 ]]\n",
      "{0: 2505, 1: 309}\n",
      "(0.3365695792880259, 0.5502645502645502, 0.41767068273092367, None)\n",
      "\n",
      "1 loss 173202.83794113822\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.59188257 -0.52905091  0.21632204  0.18106158  0.55397453  0.51026328\n",
      "   0.84854098  0.32746318  0.81549547  0.3454317 ]]\n",
      "{0: 2505, 1: 309}\n",
      "(0.3365695792880259, 0.5502645502645502, 0.41767068273092367, None)\n",
      "\n",
      "2 loss 169194.57385213434\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.49504963 -0.42918369  0.22260929  0.19323097  0.47333532  0.55727983\n",
      "   0.74980963  0.30502245  0.75494679  0.25637081]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3493150684931507, 0.5396825396825397, 0.42411642411642414, None)\n",
      "\n",
      "3 loss 165446.9513101931\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40030753 -0.32933984  0.23369793  0.20890406  0.40164261  0.60338111\n",
      "   0.6511987   0.28176384  0.69508925  0.16935911]]\n",
      "{0: 2516, 1: 298}\n",
      "(0.348993288590604, 0.5502645502645502, 0.4271047227926078, None)\n",
      "\n",
      "4 loss 161957.47810858436\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.31108866 -0.22951385  0.2459019   0.22424696  0.34146799  0.64925918\n",
      "   0.55284042  0.25911841  0.63676224  0.08631993]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "5 loss 158719.10117301487\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.23705686 -0.12970578  0.25678163  0.23698017  0.29301528  0.69538079\n",
      "   0.45496163  0.23817556  0.58076928  0.01098949]]\n",
      "{0: 2503, 1: 311}\n",
      "(0.3408360128617363, 0.5608465608465608, 0.42399999999999993, None)\n",
      "\n",
      "6 loss 155716.15830235078\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.19049968 -0.03290737  0.26462208  0.24563387  0.25382467  0.74207588\n",
      "   0.35844857  0.21985953  0.52795969 -0.03290775]]\n",
      "{0: 2487, 1: 327}\n",
      "(0.327217125382263, 0.5661375661375662, 0.41472868217054265, None)\n",
      "\n",
      "7 loss 153576.5597961094\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.99356237e-01 2.67529721e-02 2.77332301e-01 2.58894093e-01\n",
      "  2.34236591e-01 7.87160959e-01 2.70492518e-01 1.99820406e-01\n",
      "  4.75507417e-01 4.39062096e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "8 loss 152996.43208649784\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.93864275e-01 8.47346831e-02 2.87748108e-01 2.69134445e-01\n",
      "  2.18532282e-01 8.32390816e-01 2.05035103e-01 1.81788980e-01\n",
      "  4.26255385e-01 4.62554724e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "9 loss 152657.56086168488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.82511219e-01 1.20901830e-01 2.94958929e-01 2.75642959e-01\n",
      "  2.03728205e-01 8.77980242e-01 1.78079097e-01 1.66446330e-01\n",
      "  3.80969189e-01 3.63365048e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "10 loss 152363.5015075809\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.73163647e-01 1.39640369e-01 3.00067071e-01 2.79773304e-01\n",
      "  1.90755379e-01 9.23642046e-01 1.68757575e-01 1.53242716e-01\n",
      "  3.39539806e-01 5.89594171e-06]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 152092.41808928293\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 1.68136562e-01  1.48986386e-01  3.04253765e-01  2.82874848e-01\n",
      "   1.80535684e-01  9.69061807e-01  1.65108096e-01  1.41470542e-01\n",
      "   3.01587517e-01 -6.08615604e-07]]\n",
      "{0: 2485, 1: 329}\n",
      "(0.3252279635258359, 0.5661375661375662, 0.4131274131274132, None)\n",
      "\n",
      "12 loss 151836.82722538378\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 1.65929394e-01  1.53789834e-01  3.08042349e-01  2.85545742e-01\n",
      "   1.72895151e-01  1.01408357e+00  1.63808611e-01  1.30769118e-01\n",
      "   2.66889031e-01 -6.32583084e-07]]\n",
      "{0: 2485, 1: 329}\n",
      "(0.3252279635258359, 0.5661375661375662, 0.4131274131274132, None)\n",
      "\n",
      "13 loss 151593.74903592735\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.65326983e-01 1.56482775e-01 3.11680149e-01 2.88069249e-01\n",
      "  1.67388019e-01 1.05861996e+00 1.63735410e-01 1.20931979e-01\n",
      "  2.35266317e-01 4.83937200e-06]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3282208588957055, 0.5661375661375662, 0.4155339805825243, None)\n",
      "\n",
      "14 loss 151361.54790524745\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.65682460e-01 1.58261937e-01 3.15298214e-01 2.90594945e-01\n",
      "  1.63578460e-01 1.10261477e+00 1.64416339e-01 1.11820420e-01\n",
      "  2.06537353e-01 6.74444004e-06]]\n",
      "{0: 2487, 1: 327}\n",
      "(0.327217125382263, 0.5661375661375662, 0.41472868217054265, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.65682460e-01 1.58261937e-01 3.15298214e-01 2.90594945e-01\n",
      "  1.63578460e-01 1.10261477e+00 1.64416339e-01 1.11820420e-01\n",
      "  2.06537353e-01 6.74444004e-06]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.916\n",
      "Precision            0.327\n",
      "Recall               0.566\n",
      "F1                   0.415\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 220 | TN: 2405 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2487, 1: 327}\n",
      "acc 0.892679459843639\n",
      "(array([0.96702855, 0.32721713]), array([0.91619048, 0.56613757]), array([0.94092332, 0.41472868]), array([2625,  189]))\n",
      "(0.6471228369171066, 0.7411640211640211, 0.6778259999272119, None)\n",
      "[[2405  220]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.327217125382263 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.327217125382263, 0.5661375661375662, 0.41472868217054265, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 2 init at 0.2\n",
    "\n",
    "train_nl_p2(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d1226240>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d1226240>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d1226240>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 188746.6595787068\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 1.48869746e+00 -9.47010361e-07  9.62616022e-01  9.15696318e-01\n",
      "   1.43399002e+00  1.28318365e+00  1.74684629e+00  1.19128331e+00\n",
      "   1.73377801e+00  1.32352285e+00]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "1 loss 182547.0356596896\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.38984647e+00 3.91438429e-06 8.84259985e-01 8.34329432e-01\n",
      "  1.33675971e+00 1.35226655e+00 1.64776663e+00 1.19434358e+00\n",
      "  1.70006364e+00 1.25201462e+00]]\n",
      "{0: 2518, 1: 296}\n",
      "(0.34797297297297297, 0.544973544973545, 0.42474226804123716, None)\n",
      "\n",
      "2 loss 177668.27067605642\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.29121466e+00 1.13692070e-06 8.16161576e-01 7.62638901e-01\n",
      "  1.24062056e+00 1.41254973e+00 1.54878535e+00 1.16968040e+00\n",
      "  1.64438688e+00 1.16346532e+00]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.34812286689419797, 0.5396825396825397, 0.4232365145228216, None)\n",
      "\n",
      "3 loss 173629.66505667067\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.19286455e+00 2.40684363e-05 7.62293106e-01 7.05268731e-01\n",
      "  1.14631393e+00 1.46345476e+00 1.44988265e+00 1.12638616e+00\n",
      "  1.57679911e+00 1.07014457e+00]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "4 loss 170211.82557715572\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.0948048  0.02351164 0.72431259 0.66488938 1.0548559  1.50595877\n",
      "  1.35095849 1.07271478 1.50326377 0.97527036]]\n",
      "{0: 2527, 1: 287}\n",
      "(0.3554006968641115, 0.5396825396825397, 0.42857142857142855, None)\n",
      "\n",
      "5 loss 167204.84027932014\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.99711511 0.07263095 0.69879171 0.63837992 0.96714013 1.54273417\n",
      "  1.25197824 1.01467865 1.42739888 0.88005382]]\n",
      "{0: 2527, 1: 287}\n",
      "(0.3554006968641115, 0.5396825396825397, 0.42857142857142855, None)\n",
      "\n",
      "6 loss 164495.81773126527\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.90013607 0.12657034 0.68206549 0.62176309 0.88424622 1.57570671\n",
      "  1.15306205 0.95498724 1.35071771 0.78485983]]\n",
      "{0: 2527, 1: 287}\n",
      "(0.3554006968641115, 0.5396825396825397, 0.42857142857142855, None)\n",
      "\n",
      "7 loss 162054.45849846237\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.804374   0.17873434 0.67144987 0.61193027 0.80734    1.60606608\n",
      "  1.0543113  0.89495425 1.27395846 0.68989549]]\n",
      "{0: 2526, 1: 288}\n",
      "(0.3541666666666667, 0.5396825396825397, 0.4276729559748428, None)\n",
      "\n",
      "8 loss 159869.8484011531\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.71076341 0.2259536  0.66502732 0.60661764 0.73755249 1.63459246\n",
      "  0.95585156 0.83535255 1.19757018 0.59533698]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "9 loss 157934.93753004723\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.62119822 0.2663731  0.66140011 0.60418278 0.67571671 1.6618335\n",
      "  0.85786778 0.77671305 1.12188031 0.5013841 ]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "10 loss 156241.60273977186\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.53947817 0.2988059  0.65943097 0.60331853 0.62197571 1.68823684\n",
      "  0.76065882 0.71948765 1.04718868 0.40831465]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "11 loss 154778.22845603948\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.47146874 0.32228557 0.65802075 0.60280119 0.57544152 1.71425247\n",
      "  0.66474304 0.66416846 0.97384158 0.31656725]]\n",
      "{0: 2520, 1: 294}\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "\n",
      "12 loss 153527.65884563094\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.42093924 0.33619101 0.65612259 0.60149998 0.5344185  1.7403262\n",
      "  0.57113918 0.61129817 0.90224513 0.22684802]]\n",
      "{0: 2502, 1: 312}\n",
      "(0.33974358974358976, 0.5608465608465608, 0.4231536926147705, None)\n",
      "\n",
      "13 loss 152469.23177191353\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.38520413 0.34127969 0.65310476 0.59876441 0.49742243 1.76673754\n",
      "  0.48215608 0.56128801 0.8327624  0.14023549]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "14 loss 151584.29886045877\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.35851916 0.33948085 0.64879097 0.59445201 0.46360466 1.79357614\n",
      "  0.4030887  0.51435094 0.76567594 0.05837478]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.35851916 0.33948085 0.64879097 0.59445201 0.46360466 1.79357614\n",
      "  0.4030887  0.51435094 0.76567594 0.05837478]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.917\n",
      "Precision            0.33\n",
      "Recall               0.566\n",
      "F1                   0.417\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 217 | TN: 2408 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2490, 1: 324}\n",
      "acc 0.8937455579246624\n",
      "(array([0.96706827, 0.33024691]), array([0.91733333, 0.56613757]), array([0.94154448, 0.417154  ]), array([2625,  189]))\n",
      "(0.6486575933363081, 0.7417354497354498, 0.6793492365648562, None)\n",
      "[[2408  217]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.33024691358024694 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 2\n",
    "\n",
    "train_nl_p2(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with penalty3 sum(log(1+e^(-x-pk)))\n",
    "\n",
    "def train_nl_p3(lr,ep,th,pk=0):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     +tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d118a550>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d118a550>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d118a550>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(1, 10), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss [[170903.44896258 188011.16398679 175157.40140373 175603.54333381\n",
      "  171121.21063972 173732.02768867 168974.37326679 174487.27751065\n",
      "  169662.17929191 173230.0527643 ]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.49463814 -0.83063797  0.12630918  0.08890911  0.49207027  0.25028323\n",
      "   0.74695876  0.15302329  0.67126951  0.23603718]]\n",
      "{0: 2507, 1: 307}\n",
      "(0.3289902280130293, 0.5343915343915344, 0.40725806451612906, None)\n",
      "\n",
      "1 loss [[170069.95273453 184876.80361435 172822.53393948 173165.60017995\n",
      "  169920.671179   171634.82476369 168071.48877905 172833.24856777\n",
      "  168471.37171746 172416.02503506]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40516474 -0.73243177  0.18647643  0.15755892  0.44009586  0.30461456\n",
      "   0.64857752  0.1618077   0.61965315  0.16278323]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3159509202453988, 0.544973544973545, 0.39999999999999997, None)\n",
      "\n",
      "2 loss [[169494.73102015 182094.60506318 170987.98650877 171247.14149494\n",
      "  169044.22494483 169798.82431675 167515.09284512 171422.13728473\n",
      "  167545.02959792 171736.40312103]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.33557765 -0.63435322  0.23236991  0.20941253  0.39528031  0.36188862\n",
      "   0.5511156   0.17258715  0.57350898  0.11387701]]\n",
      "{0: 2474, 1: 340}\n",
      "(0.3088235294117647, 0.5555555555555556, 0.3969754253308128, None)\n",
      "\n",
      "3 loss [[168909.67496666 179627.67540159 169544.10264754 169751.83297812\n",
      "  168355.70204645 168229.27058171 167263.94625027 170272.07364808\n",
      "  166855.49280978 171063.75121351]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.30609623 -0.53659453  0.26869928  0.24915872  0.36099211  0.42009828\n",
      "   0.45492997  0.18314552  0.53202216  0.09144571]]\n",
      "{0: 2487, 1: 327}\n",
      "(0.3241590214067278, 0.5608465608465608, 0.4108527131782945, None)\n",
      "\n",
      "4 loss [[168233.7896603  177418.73499182 168391.83301883 168571.78996216\n",
      "  167788.20455413 166875.9342701  167246.98402637 169328.90971323\n",
      "  166340.17684017 170369.12365827]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.29669341 -0.43927931  0.29800372  0.28013809  0.33589506  0.47861659\n",
      "   0.3632924   0.19335483  0.49521439  0.0848606 ]]\n",
      "{0: 2491, 1: 323}\n",
      "(0.3281733746130031, 0.5608465608465608, 0.4140625, None)\n",
      "\n",
      "5 loss [[167612.77409231 175417.56824242 167452.56215974 167619.64427077\n",
      "  167299.01563022 165690.47492264 167332.29204561 168541.23458833\n",
      "  165944.10731902 169712.55461361]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.28969777 -0.34269016  0.32187104  0.30454672  0.31735617  0.53711608\n",
      "   0.29473669  0.20327103  0.46308104  0.08565007]]\n",
      "{0: 2491, 1: 323}\n",
      "(0.3281733746130031, 0.5608465608465608, 0.4140625, None)\n",
      "\n",
      "6 loss [[167172.2685457  173613.48510388 166704.71129319 166868.83680941\n",
      "  166913.59748608 164655.73259788 167206.3551885  167883.7441024\n",
      "  165642.53059836 169107.33118077]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.27440593 -0.24737759  0.33960991  0.32201922  0.29955117  0.59593343\n",
      "   0.26837846  0.21433862  0.43679258  0.09449601]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "7 loss [[166857.85721548 171970.89724017 166102.38656277 166270.0827009\n",
      "  166615.98880432 163728.34272343 166917.07448912 167307.07111008\n",
      "  165386.21123076 168534.01624838]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.26020157 -0.15414277  0.35255177  0.33422028  0.28304341  0.65477289\n",
      "   0.25468824  0.22621882  0.41582862  0.10585107]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "8 loss [[166580.02692948 170474.86160175 165602.81264801 165777.55408944\n",
      "  166366.025585   162884.64043256 166628.90666966 166791.80636009\n",
      "  165155.18162786 168019.93209824]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.24965222 -0.06458285  0.36219033  0.34288509  0.26895369  0.71329771\n",
      "   0.24511906  0.23824039  0.39913955  0.11647891]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "9 loss [[166325.42709709 169137.3872617  165185.01509761 165368.42833034\n",
      "  166148.48444802 162116.77307618 166364.28173483 166333.7547986\n",
      "  164947.38603972 167575.47158793]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.2422736  0.01828553 0.36951731 0.349148   0.25749452 0.77130022\n",
      "  0.23862339 0.24999827 0.38583022 0.12541183]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "10 loss [[166087.09678334 167985.79151504 164831.72610611 165024.2972317\n",
      "  165952.75113251 161416.69682661 166115.72303373 165927.90808971\n",
      "  164761.22427    167199.17809739]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.23812254 0.08956802 0.3753869  0.35395681 0.24881059 0.82860266\n",
      "  0.2354901  0.2611231  0.37502938 0.13207361]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss [[165849.16364193 167050.95458156 164522.58830623 164724.01675311\n",
      "  165763.93678679 160772.18206463 165865.69252637 165565.30630911\n",
      "  164591.50809036 166884.06410119]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.23757832 0.14441407 0.380615   0.35819036 0.24310847 0.88503061\n",
      "  0.23616111 0.27122933 0.36588131 0.13594912]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss [[165595.16024504 166332.61817116 164235.18671309 164444.6624591\n",
      "  165564.86767538 160167.76152452 165599.39282791 165233.69407008\n",
      "  164429.67030446 166618.22891093]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.2403616  0.18244332 0.38585058 0.36252926 0.24035925 0.94044275\n",
      "  0.24004943 0.28001759 0.35767979 0.13691399]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss [[165321.83140141 165783.3816926  163953.33438042 164169.86541902\n",
      "  165346.04426664 159591.28276958 165317.04796127 164923.04907718\n",
      "  164268.97821814 166387.91268377]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.24527737 0.20781844 0.391422   0.36730109 0.24009245 0.99476684\n",
      "  0.24565834 0.28738987 0.35000076 0.13545153]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "14 loss [[165037.26132778 165343.29467117 163670.65065196 163893.33859718\n",
      "  165108.59976831 159037.12268566 165027.81995589 164628.48039243\n",
      "  164107.28671363 166181.48220836]]\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.25109563 0.225182   0.39739664 0.37255331 0.24162678 1.04798787\n",
      "  0.2517764  0.29341228 0.34265453 0.13232114]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.25109563 0.225182   0.39739664 0.37255331 0.24162678 1.04798787\n",
      "  0.2517764  0.29341228 0.34265453 0.13232114]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.917\n",
      "Precision            0.329\n",
      "Recall               0.566\n",
      "F1                   0.416\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 218 | TN: 2407 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2489, 1: 325}\n",
      "acc 0.8933901918976546\n",
      "(array([0.96705504, 0.32923077]), array([0.91695238, 0.56613757]), array([0.9413375 , 0.41634241]), array([2625,  189]))\n",
      "(0.6481429057081929, 0.7415449735449735, 0.6788399586699516, None)\n",
      "[[2407  218]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3292307692307692 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 3 init at 0.0\n",
    "\n",
    "train_nl_p3(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.0,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0e69630>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0e69630>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f06d0e69630>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 170178.0982794352\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.4924636  -0.83045975  0.12517425  0.08828112  0.48361199  0.24739416\n",
      "   0.74680647  0.14995751  0.66885822  0.23459565]]\n",
      "{0: 2507, 1: 307}\n",
      "(0.3289902280130293, 0.5343915343915344, 0.40725806451612906, None)\n",
      "\n",
      "1 loss 168393.44645338668\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40027126 -0.73208652  0.18303001  0.15514874  0.42656374  0.29926259\n",
      "   0.64821845  0.15591998  0.61417609  0.15741122]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3159509202453988, 0.544973544973545, 0.39999999999999997, None)\n",
      "\n",
      "2 loss 166962.8109195876\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.32314402 -0.63383511  0.22633326  0.20459667  0.37772282  0.35427427\n",
      "   0.55045669  0.16409363  0.56465044  0.10221902]]\n",
      "{0: 2474, 1: 340}\n",
      "(0.3088235294117647, 0.5555555555555556, 0.3969754253308128, None)\n",
      "\n",
      "3 loss 165802.94734474536\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.28662607 -0.53589074  0.26049802  0.24216025  0.34107647  0.4102321\n",
      "   0.45368315  0.17198762  0.51949394  0.07482023]]\n",
      "{0: 2488, 1: 326}\n",
      "(0.3220858895705521, 0.5555555555555556, 0.4077669902912622, None)\n",
      "\n",
      "4 loss 164838.66813647695\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.27581075 -0.43837309  0.28761144  0.27085043  0.3144716   0.46668535\n",
      "   0.36004344  0.17985752  0.47913452  0.06685567]]\n",
      "{0: 2491, 1: 323}\n",
      "(0.3281733746130031, 0.5608465608465608, 0.4140625, None)\n",
      "\n",
      "5 loss 164027.5202517761\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.26985387 -0.34151811  0.30956735  0.29324525  0.29563278  0.52319329\n",
      "   0.28316356  0.18753911  0.44354198  0.06679606]]\n",
      "{0: 2491, 1: 323}\n",
      "(0.3281733746130031, 0.5608465608465608, 0.4140625, None)\n",
      "\n",
      "6 loss 163343.05301179268\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.25600589 -0.24587182  0.32570578  0.30903628  0.27838854  0.58008853\n",
      "   0.25069792  0.19640877  0.41390151  0.0746988 ]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "7 loss 162748.88733683285\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.24130258 -0.1522582   0.33678291  0.31929743  0.26192286  0.63722878\n",
      "   0.23608867  0.20655397  0.39021536  0.08610741]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "8 loss 162229.10935856675\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.23043384 -0.06238045  0.34449924  0.3259934   0.2478028   0.69420082\n",
      "   0.22617376  0.21713618  0.37137117  0.09683626]]\n",
      "{0: 2490, 1: 324}\n",
      "(0.33024691358024694, 0.5661375661375662, 0.41715399610136455, None)\n",
      "\n",
      "9 loss 161777.5508854562\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.22296497 0.0203418  0.34995448 0.33037259 0.23639298 0.75075457\n",
      "  0.21957471 0.22764934 0.35638224 0.10573824]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "10 loss 161387.34629501915\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.21900127 0.0901527  0.3540996  0.33348035 0.22793032 0.80667664\n",
      "  0.21664019 0.23763694 0.34425582 0.11216092]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "11 loss 161046.5736825005\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.2189525  0.14174275 0.35785038 0.33629322 0.22268248 0.86175898\n",
      "  0.21780672 0.24663169 0.33400417 0.11554334]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "12 loss 160738.14033027567\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.22228466 0.17584414 0.36188918 0.33951707 0.22054544 0.91584467\n",
      "  0.22214877 0.25430314 0.32484335 0.1158773 ]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "13 loss 160447.589665776\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.22751047 0.19786366 0.36649536 0.34341826 0.22088416 0.96886639\n",
      "  0.22792069 0.26057798 0.31633848 0.1138529 ]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "14 loss 160167.6769160813\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.23336955 0.21274477 0.37167169 0.34796941 0.2229093  1.02081838\n",
      "  0.23396538 0.26555446 0.30830404 0.11034544]]\n",
      "{0: 2489, 1: 325}\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.23336955 0.21274477 0.37167169 0.34796941 0.2229093  1.02081838\n",
      "  0.23396538 0.26555446 0.30830404 0.11034544]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.566\n",
      "Neg. class accuracy: 0.917\n",
      "Precision            0.329\n",
      "Recall               0.566\n",
      "F1                   0.416\n",
      "----------------------------------------\n",
      "TP: 107 | FP: 218 | TN: 2407 | FN: 82\n",
      "========================================\n",
      "\n",
      "{0: 2489, 1: 325}\n",
      "acc 0.8933901918976546\n",
      "(array([0.96705504, 0.32923077]), array([0.91695238, 0.56613757]), array([0.9413375 , 0.41634241]), array([2625,  189]))\n",
      "(0.6481429057081929, 0.7415449735449735, 0.6788399586699516, None)\n",
      "[[2407  218]\n",
      " [  82  107]]\n",
      "prec: tp/(tp+fp) 0.3292307692307692 recall: tp/(tp+fn) 0.5661375661375662\n",
      "(0.3292307692307692, 0.5661375661375662, 0.41634241245136183, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 3 init at 0.0\n",
    "\n",
    "train_nl_p3(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.0,0.5,12),3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized loss with prior from other LFs\n",
    "\n",
    "def train_nlp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        \n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        \n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "               \n",
    "        \n",
    "        pout = tf.map_fn(lambda li: tf.map_fn(lambda lij:li*lij,li ),l)\n",
    "#         print(\"nls\",nls_)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        \n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        \n",
    "\n",
    "        sumy = tf.reduce_sum(t_pout-logz,axis=1)\n",
    "        print(\"sumy\",sumy)\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(t_pout-logz,axis=1) ))\n",
    "\n",
    "        \n",
    "        def index_along_every_row(array, index):\n",
    "            N, _ = array.shape\n",
    "            return array[np.arange(N), index]\n",
    "\n",
    "        #Best LF\n",
    "        blf = tf.argmax(t_pout,axis=1)\n",
    "        print(\"blf\",blf)\n",
    "        print(\"normloss\",normloss)\n",
    "        \n",
    "        \n",
    "        marginals = tf.py_func(index_along_every_row, [tf.squeeze(t_pout), tf.squeeze(blf)], [tf.float64])[0]\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict1 = tf.gather(k,tf.squeeze(blf))\n",
    "        \n",
    "        predict = tf.where(tf.equal(predict1,1),tf.ones_like(predict1),tf.zeros_like(predict1))\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "                print(b)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "            print(b)\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cb33fe278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cb33fe278>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cb33fe278>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"Sum_1:0\", shape=(?, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 2013128.8891574587\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[1.01869193 0.71557711 0.91125614 0.90227968 1.00758283 1.12364657\n",
      "  1.07030801 1.09970523 1.20222222 1.13217179]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "1 loss 1949188.521442267\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.91983684 0.61704297 0.81328829 0.804232   0.90882362 1.1353606\n",
      "  0.97138712 1.06032208 1.14009408 1.03651902]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "2 loss 1898049.864934474\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.82098162 0.51867251 0.71588567 0.70669635 0.81012091 1.07481351\n",
      "  0.87243628 0.97904733 1.05218136 0.93618352]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "3 loss 1853910.2943624642\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.72239588 0.42100099 0.61990766 0.61048509 0.71179248 0.99303804\n",
      "  0.77369255 0.88907487 0.95900911 0.83587022]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "4 loss 1816568.540433177\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.62434737 0.32532591 0.52747044 0.51758373 0.61424517 0.90400988\n",
      "  0.67533815 0.79571932 0.86367741 0.73563685]]\n",
      "{0: 628, 1: 10, 2: 221, 3: 41, 4: 15, 5: 1737, 6: 3, 7: 124, 8: 26, 9: 9}\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-406-cc0e441067d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-405-6f7f89e7e8c3>\u001b[0m in \u001b[0;36mtrain_nlp\u001b[0;34m(lr, ep, th)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Initialize an iterator over the validation dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_init_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarginals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7fcdc8d0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7fcdc8d0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7fcdc8d0>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 10, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"Sum_1:0\", shape=(?, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 2013128.8891574587\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[1.01869193 0.71557711 0.91125614 0.90227968 1.00758283 1.12364657\n",
      "  1.07030801 1.09970523 1.20222222 1.13217179]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "1 loss 1949188.521442267\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.91983684 0.61704297 0.81328829 0.804232   0.90882362 1.1353606\n",
      "  0.97138712 1.06032208 1.14009408 1.03651902]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "2 loss 1898049.864934474\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.82098162 0.51867251 0.71588567 0.70669635 0.81012091 1.07481351\n",
      "  0.87243628 0.97904733 1.05218136 0.93618352]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "3 loss 1853910.2943624642\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.72239588 0.42100099 0.61990766 0.61048509 0.71179248 0.99303804\n",
      "  0.77369255 0.88907487 0.95900911 0.83587022]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n",
      "4 loss 1816568.540433177\n",
      "[[5]\n",
      " [5]\n",
      " [0]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [2]]\n",
      "[[0.62434737 0.32532591 0.52747044 0.51758373 0.61424517 0.90400988\n",
      "  0.67533815 0.79571932 0.86367741 0.73563685]]\n",
      "{0.0: 1896, 1.0: 918}\n",
      "(0.14052287581699346, 0.6825396825396826, 0.23306233062330625, None)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-404-7fc2dfa8bfc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print blf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-403-e44df1238c94>\u001b[0m in \u001b[0;36mtrain_nlp\u001b[0;34m(lr, ep, th)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;31m# Initialize an iterator over the validation dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_init_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarginals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "#print blf\n",
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 10)\n",
      "(2814, 2)\n",
      "(22276, 10)\n",
      "(22276, 2)\n"
     ]
    }
   ],
   "source": [
    "#input L_S:train_L_S, K: no of classes\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def get_maj_prior(L_S,K):\n",
    "    maj_prior = []\n",
    "    \n",
    "    print(L_S[:,0,:].shape)\n",
    "    for row in np.nditer(L_S[:,0,:],flags=['external_loop'], order='C'):\n",
    "        p = np.ones(K)/K\n",
    "        unique, counts = np.unique(row, return_counts=True)\n",
    "        unique = [int(x) for x in unique]\n",
    "        rc = dict(zip(unique, counts))\n",
    "        tnz = np.count_nonzero(row)\n",
    "        if -1 in rc:\n",
    "            p[0] = rc[-1]\n",
    "        if 1 in rc:\n",
    "            p[1] = rc[1]\n",
    "        p = softmax(p)\n",
    "        maj_prior.append(p)\n",
    "    return np.array(maj_prior)\n",
    "\n",
    "dev_maj_pl=get_maj_prior(dev_L_S,2)\n",
    "print(dev_maj_pl.shape)\n",
    "\n",
    "\n",
    "train_maj_pl=get_maj_prior(train_L_S,2)\n",
    "print(train_maj_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Normalized loss with majority prior\n",
    "\n",
    "\n",
    "\n",
    "def train_nlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout-logz),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eb1be0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eb1be0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eb1be0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 408964.17801724526\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01864878 0.71548178 0.91071576 0.90178499 1.00748712 1.11280642\n",
      "  1.07028735 1.10642092 1.18568422 1.12757612]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "1 loss 395866.0696287779\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.91918559 0.61613422 0.81128926 0.80236173 0.90802752 1.00655596\n",
      "  0.97080806 1.00032437 1.08259992 1.02218264]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "2 loss 384524.60108956933\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.8198474  0.51695871 0.7120017  0.70307867 0.80869426 0.90341696\n",
      "  0.87144795 0.89703403 0.98053296 0.9193616 ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "3 loss 374466.47881153744\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.72068344 0.41804226 0.61291245 0.60399618 0.70953748 0.80139241\n",
      "  0.77225224 0.79495402 0.87897956 0.81746155]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "4 loss 365751.6824111813\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.62176833 0.31954721 0.51411781 0.50521268 0.61063372 0.69992109\n",
      "  0.67328743 0.69345504 0.7777644  0.71605413]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.62176833 0.31954721 0.51411781 0.50521268 0.61063372 0.69992109\n",
      "  0.67328743 0.69345504 0.7777644  0.71605413]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.545\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.353\n",
      "Recall               0.545\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 103 | FP: 189 | TN: 2436 | FN: 86\n",
      "========================================\n",
      "\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(array([0.96590008, 0.35273973]), array([0.928     , 0.54497354]), array([0.94657082, 0.42827443]), array([2625,  189]))\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "[[2436  189]\n",
      " [  86  103]]\n",
      "prec: tp/(tp+fp) 0.3527397260273973 recall: tp/(tp+fn) 0.544973544973545\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "train_nlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Normalized loss with majority bias un-normalized\n",
    "\n",
    "\n",
    "\n",
    "def train_unlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6ec940>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6ec940>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6ec940>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "1 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "2 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "3 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "4 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "{0: 2498, 1: 316}\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.561\n",
      "Neg. class accuracy: 0.92\n",
      "Precision            0.335\n",
      "Recall               0.561\n",
      "F1                   0.42\n",
      "----------------------------------------\n",
      "TP: 106 | FP: 210 | TN: 2415 | FN: 83\n",
      "========================================\n",
      "\n",
      "{0: 2498, 1: 316}\n",
      "acc 0.8958777540867093\n",
      "(array([0.96677342, 0.33544304]), array([0.92      , 0.56084656]), array([0.94280695, 0.41980198]), array([2625,  189]))\n",
      "(0.6511082283548357, 0.7404232804232804, 0.6813044646256545, None)\n",
      "[[2415  210]\n",
      " [  83  106]]\n",
      "prec: tp/(tp+fp) 0.33544303797468356 recall: tp/(tp+fn) 0.5608465608465608\n",
      "(0.33544303797468356, 0.5608465608465608, 0.41980198019801984, None)\n"
     ]
    }
   ],
   "source": [
    "train_unlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4278>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 153548.14977017298\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "dev loss 19382.46646189899\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "un-norma thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7d6d4c50>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 154340.00504922983\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "dev loss 19477.32881134378\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "[(153548.14977017298, 19382.46646189899, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)), (154340.00504922983, 19477.32881134378, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value Normalized\n",
    "\n",
    "def getNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getNLObjValue(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "                0.11186369,0.07306518,0.69216714,0.07467749,0.16012659,\\\n",
    "                    0.13682546,0.08183363]])))\n",
    " \n",
    "            \n",
    "print(\" un-norma thetas ep7 \")\n",
    "\n",
    "# l_f1s.append(getNLObjValue(np.array([[1.0,1.0,1.0,1.0,1.0,1.02750979,\\\n",
    "#                              1.0,1.0218145,1.0,1.0]])))\n",
    "\n",
    "l_f1s.append(getNLObjValue(np.array([[0.33293226,0.01940464,0.42274838,0.39655883,\\\n",
    "    0.31731244,0.84775084,0.37180681,0.46009105,0.5502137,0.32638473]])))\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Normalized loss plot loss vs f1s\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "y_loss=[153548.149,179552.66,157032.751,150114.45]\n",
    "x_f1s =[0.432,0.428,0.43,0.414]\n",
    "text=[\"snorkel-thetas\",\"old-unNormalized-thetas\",\"normalized-trained-thetas-ep7\",\\\n",
    "      \"normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"Spouse-Normalized-Loss\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0470>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -18693.37512233427\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "dev loss -2375.8227752836674\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "normalized thetas ep6 f10.43\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6c7ffa0e10>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -29153.740301843533\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.48198891 0.38912505 0.70829843 0.67643395 0.56100246 1.39815618\n",
      "  0.49929654 0.83071361 0.8706048  0.6070296 ]]\n",
      "dev loss -3721.7512709843986\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.354\n",
      "Recall               0.55\n",
      "F1                   0.431\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 190 | TN: 2435 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2520, 1: 294}\n",
      "acc 0.90227434257285\n",
      "(array([0.96626984, 0.3537415 ]), array([0.92761905, 0.55026455]), array([0.94655005, 0.43064182]), array([2625,  189]))\n",
      "(0.6600056689342404, 0.7389417989417989, 0.6885959352685174, None)\n",
      "[[2435  190]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35374149659863946 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None)\n",
      "[(-18693.37512233427, -2375.8227752836674, (0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)), (-29153.740301843533, -3721.7512709843986, (0.35374149659863946, 0.5502645502645502, 0.4306418219461698, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas Unnormalized # remove logz from obj\n",
    "\n",
    "def getUNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getUNLObjValue(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "                0.11186369,0.07306518,0.69216714,0.07467749,0.16012659,\\\n",
    "                    0.13682546,0.08183363]])))\n",
    " \n",
    "            \n",
    "print(\"normalized thetas ep6 f10.43\")\n",
    "\n",
    "l_f1s.append(getUNLObjValue(np.array([[0.48198891,0.38912505,0.70829843,0.67643395,0.56100246,\\\n",
    "        1.39815618,0.49929654,0.83071361,0.8706048,0.6070296 ]])))\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00273a90>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -27046.447774325923\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00304454 1.00650418 1.02847709 1.02590827 1.00626986 1.10374278\n",
      "  1.00047081 1.05014972 1.02497607 1.00775472]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "1 loss -28052.143411860863\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00609458 1.01281986 1.05679467 1.05166221 1.01210828 1.18026413\n",
      "  1.0008722  1.07854839 1.05000733 1.01580513]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "2 loss -29084.10636456966\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.00913348 1.01894236 1.08511902 1.07741817 1.01748748 1.25689478\n",
      "  1.00117106 1.10696165 1.07504049 1.02383491]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "3 loss -30139.25214077889\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01215851 1.02486468 1.11344882 1.10317555 1.02241298 1.33361627\n",
      "  1.00138832 1.13538697 1.10007485 1.03184426]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "4 loss -31214.8692847093\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01516631 1.03057898 1.14178284 1.12893381 1.02689378 1.41041477\n",
      "  1.0015406  1.16382207 1.1251098  1.03983336]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "5 loss -32308.533016622125\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.01815264 1.03607611 1.17012004 1.15469247 1.03094158 1.48727984\n",
      "  1.00164101 1.19226496 1.15014486 1.04780238]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "6 loss -33418.08609820088\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02111228 1.04134521 1.19845945 1.18045107 1.03456996 1.56420354\n",
      "  1.00169983 1.22071391 1.1751796  1.05575152]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "7 loss -34541.61763052727\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02403881 1.04637338 1.22680022 1.20620922 1.03779353 1.64117968\n",
      "  1.00172501 1.24916742 1.20021365 1.06368097]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "8 loss -35677.44092887072\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02692445 1.05114559 1.2551416  1.23196655 1.04062737 1.71820341\n",
      "  1.00172258 1.27762423 1.22524668 1.07159092]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "9 loss -36824.07134223953\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.02975997 1.05564493 1.28348296 1.25772274 1.04308654 1.79527078\n",
      "  1.00169666 1.30608329 1.2502784  1.07948156]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "10 loss -37980.20466766686\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03253478 1.05985339 1.3118237  1.28347749 1.04518599 1.87237845\n",
      "  1.00164896 1.33454373 1.27530856 1.0873531 ]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "11 loss -39144.69663656382\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03523736 1.06375291 1.34016334 1.30923055 1.0469405  1.94952346\n",
      "  1.00157637 1.36300482 1.30033694 1.09520574]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "12 loss -40316.54381636862\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.03785615 1.06732708 1.36850145 1.33498169 1.0483649  2.02670308\n",
      "  1.00146078 1.39146599 1.32536332 1.10303971]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "13 loss -41494.86615522479\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04038111 1.07056283 1.39683767 1.36073069 1.0494743  2.10391463\n",
      "  1.00124898 1.41992675 1.35038755 1.11085522]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.3511705685618729, 0.5555555555555556, 0.430327868852459, None)\n",
      "\n",
      "14 loss -42678.891105944065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04280545 1.07345216 1.42517169 1.38647739 1.05028432 2.18115548\n",
      "  1.0009374  1.44838672 1.37540947 1.11865252]]\n",
      "{0: 2519, 1: 295}\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.04280545 1.07345216 1.42517169 1.38647739 1.05028432 2.18115548\n",
      "  1.0009374  1.44838672 1.37540947 1.11865252]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.927\n",
      "Precision            0.353\n",
      "Recall               0.55\n",
      "F1                   0.43\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 191 | TN: 2434 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2519, 1: 295}\n",
      "acc 0.9019189765458422\n",
      "(array([0.96625645, 0.35254237]), array([0.9272381 , 0.55026455]), array([0.94634526, 0.42975207]), array([2625,  189]))\n",
      "(0.659399411926982, 0.7387513227513227, 0.6880486613626724, None)\n",
      "[[2434  191]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.3525423728813559 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.3525423728813559, 0.5502645502645502, 0.42975206611570244, None)\n"
     ]
    }
   ],
   "source": [
    "#init with old network thetas\n",
    "# train_unl(0.01,15,np.array([[1.0,1.0,1.0,\\\n",
    "#                 1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]]))   \n",
    "\n",
    "train_unl(0.1/len(train_L_S),15,tf.constant_initializer(np.array([[1.0,1.0,1.0,\\\n",
    "                1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## UN-Normalized loss plot loss vs f1s\n",
    "    \n",
    "y_loss=[-18693.37,-29153.74,-33418.08,-42678.89]\n",
    "x_f1s =[0.432,0.431,0.43,0.43]\n",
    "text=[\"snorkel-thetas\",\"normalized-thetas-ep7\",\"Un-normalized-trained-thetas-ep7\",\"Un-normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"Spouse-Un-Normalized-Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -16494.182287841937\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32011886 0.01742789 0.2357877  0.22487695 0.31165927 0.31422477\n",
      "  0.3704413  0.2596181  0.37190215 0.27239461]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "1 loss -16916.06023599083\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32240975 0.01967812 0.26207358 0.24913283 0.31554645 0.38878112\n",
      "  0.3713858  0.28807452 0.39742105 0.28029567]]\n",
      "{0: 2525, 1: 289}\n",
      "(0.35294117647058826, 0.5396825396825397, 0.42677824267782427, None)\n",
      "\n",
      "2 loss -17416.749670163787\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32452451 0.02109609 0.28853598 0.27350093 0.31813751 0.46426523\n",
      "  0.37184652 0.31660673 0.42290434 0.28811702]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3493150684931507, 0.5396825396825397, 0.42411642411642414, None)\n",
      "\n",
      "3 loss -17991.625112567406\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32647211 0.02179288 0.31514827 0.29796438 0.31959997 0.54035557\n",
      "  0.3721248  0.34520201 0.44836835 0.29587051]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "4 loss -18635.534044730877\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32826902 0.02187884 0.34189006 0.32251053 0.32010328 0.61685632\n",
      "  0.3722757  0.37385273 0.47382477 0.3035663 ]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "5 loss -19343.04096959059\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32993333 0.02145398 0.36874542 0.34712976 0.31980289 0.69364125\n",
      "  0.37231688 0.4025537  0.49928154 0.31121285]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "6 loss -20108.635126544148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33148239 0.02060487 0.39570164 0.37181452 0.31883431 0.77062559\n",
      "  0.37212214 0.43130096 0.52474361 0.31881709]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "7 loss -20926.887016926226\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "8 loss -21792.56392621291\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33429748 0.01791428 0.44987709 0.42135783 0.31533315 0.9249758\n",
      "  0.37143819 0.48892073 0.5756929  0.33392039]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "9 loss -22700.711796781943\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33559094 0.01618427 0.47708055 0.44620747 0.3129758  1.00227118\n",
      "  0.37102895 0.51778678 0.60118119 0.34142788]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "10 loss -23646.708630970148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33682366 0.01425623 0.50435256 0.4711043  0.31030577 1.07961598\n",
      "  0.37058621 0.54668595 0.62667775 0.34891031]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "11 loss -24626.294582069993\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33800499 0.01216444 0.53168768 0.49604533 0.30737681 1.15699511\n",
      "  0.37011559 0.57561492 0.65218124 0.35637022]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "12 loss -25635.583678344366\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33914272 0.00993705 0.55908108 0.52102789 0.30423308 1.23439775\n",
      "  0.36962181 0.60457039 0.67769008 0.36380974]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "13 loss -26671.061594641385\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34024335 0.00759722 0.58652841 0.54604955 0.30091092 1.31181615\n",
      "  0.3691088  0.63354905 0.70320251 0.37123061]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "14 loss -27729.573303400488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "# init random thetas\n",
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6cc841c3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -16494.182287841937\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32011886 0.01742789 0.2357877  0.22487695 0.31165927 0.31422477\n",
      "  0.3704413  0.2596181  0.37190215 0.27239461]]\n",
      "{0: 2515, 1: 299}\n",
      "(0.34782608695652173, 0.5502645502645502, 0.4262295081967213, None)\n",
      "\n",
      "1 loss -16916.06023599083\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32240975 0.01967812 0.26207358 0.24913283 0.31554645 0.38878112\n",
      "  0.3713858  0.28807452 0.39742105 0.28029567]]\n",
      "{0: 2525, 1: 289}\n",
      "(0.35294117647058826, 0.5396825396825397, 0.42677824267782427, None)\n",
      "\n",
      "2 loss -17416.749670163787\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32452451 0.02109609 0.28853598 0.27350093 0.31813751 0.46426523\n",
      "  0.37184652 0.31660673 0.42290434 0.28811702]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3493150684931507, 0.5396825396825397, 0.42411642411642414, None)\n",
      "\n",
      "3 loss -17991.625112567406\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32647211 0.02179288 0.31514827 0.29796438 0.31959997 0.54035557\n",
      "  0.3721248  0.34520201 0.44836835 0.29587051]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "4 loss -18635.534044730877\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32826902 0.02187884 0.34189006 0.32251053 0.32010328 0.61685632\n",
      "  0.3722757  0.37385273 0.47382477 0.3035663 ]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "5 loss -19343.04096959059\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.32993333 0.02145398 0.36874542 0.34712976 0.31980289 0.69364125\n",
      "  0.37231688 0.4025537  0.49928154 0.31121285]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "6 loss -20108.635126544148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33148239 0.02060487 0.39570164 0.37181452 0.31883431 0.77062559\n",
      "  0.37212214 0.43130096 0.52474361 0.31881709]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "7 loss -20926.887016926226\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33293226 0.01940464 0.42274838 0.39655883 0.31731244 0.84775084\n",
      "  0.37180681 0.46009105 0.5502137  0.32638473]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "8 loss -21792.56392621291\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33429748 0.01791428 0.44987709 0.42135783 0.31533315 0.9249758\n",
      "  0.37143819 0.48892073 0.5756929  0.33392039]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "9 loss -22700.711796781943\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33559094 0.01618427 0.47708055 0.44620747 0.3129758  1.00227118\n",
      "  0.37102895 0.51778678 0.60118119 0.34142788]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "10 loss -23646.708630970148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33682366 0.01425623 0.50435256 0.4711043  0.31030577 1.07961598\n",
      "  0.37058621 0.54668595 0.62667775 0.34891031]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "11 loss -24626.294582069993\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33800499 0.01216444 0.53168768 0.49604533 0.30737681 1.15699511\n",
      "  0.37011559 0.57561492 0.65218124 0.35637022]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "12 loss -25635.583678344366\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.33914272 0.00993705 0.55908108 0.52102789 0.30423308 1.23439775\n",
      "  0.36962181 0.60457039 0.67769008 0.36380974]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "13 loss -26671.061594641385\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34024335 0.00759722 0.58652841 0.54604955 0.30091092 1.31181615\n",
      "  0.3691088  0.63354905 0.70320251 0.37123061]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "14 loss -27729.573303400488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.34131234 0.00516399 0.61402568 0.57110806 0.29744021 1.38924481\n",
      "  0.36857982 0.66254768 0.72871677 0.37863427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "# LFS init random thetas\n",
    "\n",
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(0.2,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 7 4 1 0 6 3 2 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([ 0.4751682, 0.46430319 , 0.77729748 , 0.69961045 , 0.43660742,  4.98316919,\n",
    "   0.4786732 , -0.29070728, -0.31361022, -0.41560446])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00019475777320921748\n",
      "[0.2        0.2        0.20397272 0.20397272 0.2        0.2\n",
      " 0.2        0.19602728 0.19602728 0.19602728]\n",
      "[1.         1.         0.99818703 0.99876917 1.         1.\n",
      " 1.         1.00189171 1.00123147 1.00159078]\n",
      "2714 100\n",
      "0  dm  (0.6518128224023582, 0.583047619047619, 0.604245316341007, None)\n",
      "0  db  (0.36, 0.19047619047619047, 0.24913494809688577, None)\n",
      "\n",
      "-7.1199749476605e+32\n",
      "[ 2.39523991e-01  4.44712440e-01  1.00592853e+00  1.00306476e+00\n",
      "  7.93729267e-01 -2.47683784e+10  8.30695122e-01 -1.40683111e+17\n",
      " -1.37206290e+17 -1.36481844e+17]\n",
      "[9.68596811e-01 8.16929334e-01 8.01833673e-01 8.35694159e-01\n",
      " 6.32314677e-01 2.47683784e+10 9.20080431e-01 1.40683111e+17\n",
      " 1.37206290e+17 1.36481844e+17]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.6513554689606083e+67\n",
      "[ 2.87203003e-01  7.29222600e-01  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -4.86866279e+20  8.94995005e-01 -2.71479292e+34\n",
      " -2.64769995e+34 -2.63372016e+34]\n",
      "[9.31534051e-01 6.56469425e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 4.86866279e+20 9.15642897e-01 2.71479292e+34\n",
      " 2.64769995e+34 2.63372016e+34]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-9.873188985162394e+101\n",
      "[ 3.24040405e-01  8.92774981e-01  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -8.57801313e+30  8.94995005e-01 -5.23879560e+51\n",
      " -5.10932482e+51 -5.08234771e+51]\n",
      "[9.03578251e-01 6.06795047e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 8.57801313e+30 9.15642897e-01 5.23879560e+51\n",
      " 5.10932482e+51 5.08234771e+51]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-3.6766047358767233e+136\n",
      "[ 4.03004051e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -1.52645882e+41  9.22433705e-01 -1.01094191e+69\n",
      " -9.85957646e+68 -9.80751814e+68]\n",
      "[8.45939120e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 1.52645882e+41 9.12767335e-01 1.01094191e+69\n",
      " 9.85957646e+68 9.80751814e+68]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-1.3691039849622358e+171\n",
      "[ 4.52881733e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -8.87619724e+51  9.68003629e-01 -1.95083683e+86\n",
      " -1.90262415e+86 -1.89257834e+86]\n",
      "[8.11376459e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 8.87619724e+51 9.09801058e-01 1.95083683e+86\n",
      " 1.90262415e+86 1.89257834e+86]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "0 -2.516208931471449e+194\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "-1.2643949596623685e+189\n",
      "[ 4.92911523e-01  1.00084993e+00  1.00592853e+00  1.00306476e+00\n",
      "  1.00084993e+00 -4.18977398e+57  9.86196450e-01 -1.33557247e+96\n",
      " -1.30256533e+96 -1.29568782e+96]\n",
      "[7.84826492e-01 5.96709500e-01 8.01833673e-01 8.35694159e-01\n",
      " 5.96709500e-01 4.18977398e+57 9.09252111e-01 1.33557247e+96\n",
      " 1.30256533e+96 1.29568782e+96]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-2.3895655466420104e+225\n",
      "[ 5.24003463e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -1.25083611e+068  1.00438011e+000 -2.57728356e+113\n",
      " -2.51358897e+113 -2.50031728e+113]\n",
      "[7.65011828e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 1.25083611e+068 9.09066965e-001 2.57728356e+113\n",
      " 2.51358897e+113 2.50031728e+113]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-8.898328613657434e+259\n",
      "[ 5.61785684e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -2.45873958e+078  1.00438011e+000 -4.97344076e+130\n",
      " -4.85052791e+130 -4.82491724e+130]\n",
      "[7.41972292e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 2.45873958e+078 9.09066965e-001 4.97344076e+130\n",
      " 4.85052791e+130 4.82491724e+130]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "-3.313583602170098e+294\n",
      "[ 5.91204610e-001  1.00084993e+000  1.00592853e+000  1.00306476e+000\n",
      "  1.00084993e+000 -4.33201092e+088  1.00438011e+000 -9.59735801e+147\n",
      " -9.36017038e+147 -9.31074891e+147]\n",
      "[7.24887158e-001 5.96709500e-001 8.01833673e-001 8.35694159e-001\n",
      " 5.96709500e-001 4.33201092e+088 9.09066965e-001 9.59735801e+147\n",
      " 9.36017038e+147 9.31074891e+147]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/ipykernel_launcher.py:60: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "1 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "2 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "3 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "8000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "12000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "16000  db  (0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan 1.00084993 1.00592853 1.00306476 1.00084993        nan\n",
      " 1.00438011        nan        nan        nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  dm  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "20000  db  (0.0, 0.0, 0.0, None)\n",
      "4 nan\n",
      "0 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "                print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "                print(c,\" dm \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "                print(c,\" db \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "  \n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00024597518522208474\n",
      "[1.         1.         1.         1.         1.         1.00531229\n",
      " 1.         1.         1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "0 -0.9839007408883389\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002477972163026618\n",
      "[1.         1.         1.         1.         1.         1.01071759\n",
      " 1.         1.00531229 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "1 -0.9911888652106471\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.00024967684220414605\n",
      "[1.         1.         1.         1.         1.         1.01621772\n",
      " 1.         1.01071759 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "2 -0.9987073688165843\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002516160600819831\n",
      "[1.         1.         1.         1.         1.         1.0218145\n",
      " 1.         1.01621772 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "3 -1.0064642403279322\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002536169307750545\n",
      "[1.         1.         1.         1.         1.         1.02750979\n",
      " 1.         1.0218145  1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "4 -1.014467723100218\n",
      "2522 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network 2 to get thetas \n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
