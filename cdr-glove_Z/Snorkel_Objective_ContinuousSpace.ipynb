{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "from six.moves.cPickle import load\n",
    "\n",
    "with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "    ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)\n",
    "    \n",
    "    \n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist / 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist / 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist / 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist / 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<888x1 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 888 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_external_annotations import load_external_labels\n",
    "load_external_labels(session, ChemicalDisease, split=1, annotator='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888\n",
      "(296, 592)\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "gold_labels_dev = []\n",
    "for i,L in enumerate(L_gold_dev):\n",
    "    gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "    \n",
    "print(len(gold_labels_dev))\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "softmax_Threshold = 0.3\n",
    "LF_Threshold = 0.3\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "causal = ['induced', 'caused', 'due','associated with']\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_causal(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    for w in causal:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('{{A}}.{0,50}(not|no|none).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (1,sc)\n",
    "    \n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)   \n",
    "\n",
    "    \n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "    \n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "\n",
    "def LF_treat(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    for w in treat:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('{{A}}.{0,50}(not|no|none).{0,20}' + ltp(treat) + '.{0,50}{{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "    \n",
    "def LF_treat_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1]))\n",
    "    for w in treat:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .{0,50} {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "    \n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "    \n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "\n",
    "def LF_uncertain(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1]))\n",
    "    for w in uncertain:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .{0,50} {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "    \n",
    "def LF_far_c_d(c):\n",
    "    if(rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    if(rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "    \n",
    "def LF_develop_d_following_c(c):\n",
    "    sc1 = 0\n",
    "    sc2 = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1]))\n",
    "    sc1=max(sc1,get_similarity(word_vectors,'develop'))\n",
    "    \n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    sc2=max(sc2,get_similarity(word_vectors,'following'))\n",
    "    \n",
    "    sc = (sc1+sc2)/2\n",
    "    if(re.search('(not|no|none) .{0,50} {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (1,sc)\n",
    "    \n",
    "\n",
    "def LF_risk_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1]))\n",
    "    sc=max(sc,get_similarity(word_vectors,'risk'))\n",
    "    if(re.search(' (not|no|none) .{0,50}{{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (1,sc)\n",
    "    \n",
    "def LF_improve_before_disease(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1]))\n",
    "    sc=max(sc,get_similarity(word_vectors,'improve'))\n",
    "    if(re.search(' (not|no|none) .{0,50}{{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (1,sc)\n",
    "    \n",
    "procedure, following = ['inject', 'administrate'], ['following']\n",
    "def LF_d_following_c(c):\n",
    "    sc1 = 0\n",
    "    sc2 = 0\n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    for w in following:\n",
    "        sc1=max(sc1,get_similarity(word_vectors,w))\n",
    "    \n",
    "    word_vectors = get_word_vectors(get_right_tokens(c[1]))\n",
    "    for w in procedure:\n",
    "        sc2=max(sc2,get_similarity(word_vectors,w))\n",
    "    \n",
    "    sc = (sc1+sc2)/2\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_measure(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[0]))\n",
    "    sc=max(sc,get_similarity(word_vectors,'measure'))\n",
    "    return (-1,sc)\n",
    "    \n",
    "def LF_level(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_right_tokens(c[0]))\n",
    "    sc=max(sc,get_similarity(word_vectors,'level'))\n",
    "    return (-1,sc)\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "    \n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'aim', 'investigate','assess','study']\n",
    "\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1]))\n",
    "    for w in WEAK_PHRASES:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search(' (not|no|none) .{0,50}{{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (1,sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    return (l*cand_in_ctd_marker(c),s)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l,s = LF_c_induced_d(c)\n",
    "    return (l*cand_in_ctd_marker(c),s)\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_treat(c)\n",
    "    return (l* cand_in_ctd_therapy(c),s)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_treat(c)\n",
    "    return (l * cand_in_ctd_unspecified(c),s)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l,s = LF_c_induced_d(c)\n",
    "    return (l*cand_in_ctd_unspecified(c),s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "LFs = [LF_in_ctd_unspecified,LF_in_ctd_marker,LF_in_ctd_therapy,LF_closer_chem, \n",
    "       LF_closer_dis,LF_causal,LF_c_induced_d,LF_c_d,LF_in_patient_with,LF_uncertain,\n",
    "       LF_far_c_d,LF_far_d_c,LF_develop_d_following_c,LF_d_following_c,LF_measure,\n",
    "      LF_level,LF_neg_d,LF_weak_assertions,LF_ctd_marker_c_d,LF_ctd_therapy_treat,\n",
    "      LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_improve_before_disease,\n",
    "       LF_risk_d,LF_treat,LF_treat_d,LF_induce,LF_induce_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "    return L_S\n",
    "\n",
    "def get_L_S(cands):  # sign gives label abs value gives score\n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        l_s=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            s= (s+1)/2  #to scale scores in [0,1] \n",
    "            l_s.append(l*s)\n",
    "        L_S.append(l_s)\n",
    "    return L_S\n",
    "\n",
    "def get_Initial_P_cap_L_S(L_S):\n",
    "    P_cap = []\n",
    "    for L,S in L_S:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "   \n",
    "    \n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "\n",
    "\n",
    "# train_P_cap= get_Initial_P_cap_L_S(train_L_S) \n",
    "\n",
    "# dev_P_cap = get_Initial_P_cap_L_S(dev_L_S)\n",
    "\n",
    "# test_P_cap = get_Initial_P_cap_L_S(test_L_S)\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "# pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare batch data\n",
    "# train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "# train_P_cap_batch,dev_P_cap_batch = get_P_cap_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "\n",
    "# pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "# pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "#pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "#pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))\n",
    "\n",
    "dev_L_S = pkl.load( open( \"dev_L_S.p\", \"rb\" ) )\n",
    "train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )\n",
    "\n",
    "# train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "# dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "# test_P_cap = pkl.load( open( \"test_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "def get_L_S_batch():\n",
    "    dev_L_batch = []\n",
    "    dev_S_batch = []\n",
    "    dev_L_S_batch = []\n",
    "    train_L_batch = []\n",
    "    train_S_batch = []\n",
    "    train_L_S_batch = []\n",
    "    for l,s in train_L_S:\n",
    "        train_L_batch.append(l)\n",
    "        train_S_batch.append(s)\n",
    "    train_L_S_batch = [train_L_batch, train_S_batch]\n",
    "    for l,s in dev_L_S:\n",
    "        dev_L_batch.append(l)\n",
    "        dev_S_batch.append(s)\n",
    "    dev_L_S_batch = [dev_L_batch, dev_S_batch]\n",
    "    return train_L_S_batch,dev_L_S_batch\n",
    "\n",
    "\n",
    "def get_P_cap_batch():\n",
    "    kp1_train= []\n",
    "    kn1_train = []\n",
    "    kp1_dev= []\n",
    "    kn1_dev = []\n",
    "    for pci in train_P_cap:\n",
    "        kp1_train.append(pci[0])\n",
    "        kn1_train.append(pci[1])\n",
    "    for pci in dev_P_cap:\n",
    "        kp1_dev.append(pci[0])\n",
    "        kn1_dev.append(pci[1])\n",
    "    return [kp1_train,kn1_train],[kp1_dev,kn1_dev]\n",
    "        \n",
    "def get_mini_batches(X,P_cap,bsize): #X : (train/dev/)_L_S_batch\n",
    "    for i in range(0, len(X[0]) - bsize + 1, bsize):\n",
    "        indices = slice(i, i + bsize)\n",
    "        #print(indices)\n",
    "        yield [X[0][indices],X[1][indices]],P_cap[indices]\n",
    "\n",
    "# train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "\n",
    "#for x in get_mini_batches(train_L_S_batch,200):\n",
    "#    print(len(x),len(x[0]),len(x[0][0]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.808128308193\n",
      "-0.00151581608805\n",
      "[ 0.2         0.20000697  0.19999303  0.2         0.2         0.20000697\n",
      "  0.2         0.2         0.2         0.19999303  0.2         0.2\n",
      "  0.20000697  0.20000697  0.19999303  0.19999303  0.19999303  0.19999303\n",
      "  0.2         0.19999303  0.2         0.2         0.20000697  0.20000697\n",
      "  0.19999303  0.19999303]\n",
      "[ 0.2         0.19997212  0.20002788  0.2         0.2         0.19998157\n",
      "  0.2         0.2         0.2         0.20001714  0.2         0.2\n",
      "  0.19998261  0.19998356  0.20001656  0.20001677  0.20002029  0.20002135\n",
      "  0.2         0.20001834  0.2         0.2         0.19998443  0.19998413\n",
      "  0.20001834  0.20001834]\n",
      "\n",
      "805 83\n",
      "0  d  (0.67499064581306589, 0.56672297297297292, 0.54548665345831537, None)\n",
      "dev err: -0.835173447317\n",
      "-0.80440284948\n",
      "[ 0.19755467  0.20137045  0.19856294  0.19922373  0.19993977  0.20328807\n",
      "  0.1997506   0.19974915  0.2         0.19683756  0.1995668   0.19952999\n",
      "  0.20311313  0.20320042  0.19686939  0.19668855  0.19675667  0.19680593\n",
      "  0.19975518  0.1985751   0.19746796  0.19981272  0.20302159  0.20300145\n",
      "  0.19656001  0.19680774]\n",
      "[ 0.20956845  0.19444638  0.20567359  0.20308316  0.20024083  0.19060791\n",
      "  0.20099536  0.20100113  0.2         0.20792523  0.201726    0.201872\n",
      "  0.19183349  0.19239151  0.20715856  0.2079097   0.20892034  0.20876472\n",
      "  0.20097712  0.20400519  0.20687241  0.20074785  0.19284214  0.19273249\n",
      "  0.20927526  0.2079327 ]\n",
      "\n",
      "820 68\n",
      "500  d  (0.68579626972740315, 0.5591216216216216, 0.53005634592036854, None)\n",
      "dev err: -0.873446738295\n",
      "-0.83935817176\n",
      "[ 0.19429384  0.20353099  0.19661393  0.19825771  0.19982139  0.20698284\n",
      "  0.19954987  0.19956661  0.2         0.19277854  0.19911715  0.19920853\n",
      "  0.20672555  0.20689192  0.19294618  0.19250577  0.19259319  0.19272003\n",
      "  0.19957783  0.19668815  0.19419194  0.19964425  0.20655982  0.20656192\n",
      "  0.19227475  0.19274727]\n",
      "[ 0.22172825  0.18537401  0.21314273  0.2068599   0.20071335  0.17958155\n",
      "  0.20179311  0.20172667  0.2         0.21770591  0.20350308  0.2031431\n",
      "  0.18190187  0.18336965  0.21577205  0.21761705  0.21983127  0.21966211\n",
      "  0.20168216  0.20904283  0.21531477  0.20141837  0.18415724  0.18368551\n",
      "  0.2201342   0.21782401]\n",
      "\n",
      "827 61\n",
      "1000  d  (0.6906951850456915, 0.55489864864864868, 0.5214525556522821, None)\n",
      "dev err: -0.927830359745\n",
      "-0.903229658181\n",
      "[ 0.1903335   0.20584976  0.19410387  0.19683937  0.19971746  0.2111696\n",
      "  0.19945234  0.19950087  0.2         0.18744066  0.19844283  0.1985434\n",
      "  0.21096867  0.21129792  0.18781194  0.18695927  0.18700332  0.18732604\n",
      "  0.19949843  0.1943187   0.19046238  0.19957057  0.21080239  0.21080469\n",
      "  0.18677852  0.18740547]\n",
      "[ 0.23571494  0.17514476  0.22241785  0.21229133  0.20112733  0.16611146\n",
      "  0.20217965  0.2019874   0.2         0.23028645  0.20614118  0.20574972\n",
      "  0.16964728  0.1721344   0.22656762  0.2299349   0.23394237  0.23311045\n",
      "  0.20199709  0.21537986  0.22473971  0.20171096  0.17299485  0.17244001\n",
      "  0.23378181  0.23050221]\n",
      "\n",
      "845 43\n",
      "1500  d  (0.67922113664510797, 0.53716216216216217, 0.48777258423091374, None)\n",
      "dev err: -0.998717194474\n",
      "-0.96552508127\n",
      "[ 0.18505836  0.20866601  0.19192875  0.19490023  0.19950239  0.21551256\n",
      "  0.19940157  0.19948388  0.2         0.18114512  0.19754165  0.19773697\n",
      "  0.21546262  0.21600399  0.18148036  0.18023086  0.18037104  0.18093893\n",
      "  0.1994768   0.19226313  0.18568102  0.19954099  0.21522381  0.21520909\n",
      "  0.18015387  0.18110674]\n",
      "[ 0.25324391  0.16189878  0.23017542  0.21951455  0.20198153  0.1504903\n",
      "  0.20238058  0.20205477  0.2         0.2444001   0.20961845  0.20886936\n",
      "  0.15592223  0.15942158  0.23950868  0.24407857  0.24979813  0.24850354\n",
      "  0.20208278  0.22064191  0.23626096  0.20182831  0.160782    0.16012246\n",
      "  0.24916895  0.2444419 ]\n",
      "\n",
      "855 33\n",
      "2000  d  (0.67309941520467831, 0.5278716216216216, 0.46838968791945607, None)\n",
      "dev err: -1.08311776445\n",
      "-1.01703411963\n",
      "[ 0.17878699  0.21154299  0.18943232  0.19367806  0.19941375  0.2195652\n",
      "  0.19940732  0.19957988  0.1999843   0.17389017  0.19692848  0.19711088\n",
      "  0.21980275  0.22063197  0.17434462  0.17262335  0.17272813  0.17356734\n",
      "  0.19952846  0.19005712  0.18029591  0.19956957  0.21958515  0.21957482\n",
      "  0.1728477   0.17385004]\n",
      "[ 0.27275352  0.14719699  0.2387929   0.22395607  0.20233259  0.13509837\n",
      "  0.20235787  0.20167414  0.20006282  0.25961447  0.211954    0.21126169\n",
      "  0.14159876  0.14617894  0.2535362   0.25936978  0.26693192  0.2650333\n",
      "  0.20187807  0.2260246   0.24833892  0.20171494  0.14810745  0.14724183\n",
      "  0.26492813  0.2596072 ]\n",
      "\n",
      "882 6\n",
      "2500  d  (0.5, 0.5, 0.40553703487379467, None)\n",
      "dev err: -1.19051510151\n",
      "-1.14450787884\n",
      "[ 0.17129631  0.21431988  0.18641486  0.19090067  0.19933107  0.2236527\n",
      "  0.19944649  0.19968633  0.19996696  0.16528314  0.19617362  0.19630597\n",
      "  0.22422674  0.2254356   0.16589108  0.16333341  0.16345045  0.16476388\n",
      "  0.19961414  0.18736851  0.17402024  0.19960857  0.22405737  0.22401671\n",
      "  0.16396052  0.1652447 ]\n",
      "[ 0.29454164  0.13151671  0.24884432  0.23375966  0.20265952  0.11680335\n",
      "  0.20220288  0.20125125  0.20013216  0.27688619  0.21479674  0.21430086\n",
      "  0.12567215  0.13138417  0.26938928  0.27739198  0.2865091   0.28366986\n",
      "  0.20153811  0.23260028  0.2620617   0.20156017  0.13379561  0.13279395\n",
      "  0.28334107  0.27680156]\n",
      "\n",
      "882 6\n",
      "3000  d  (0.5, 0.5, 0.40553703487379467, None)\n",
      "dev err: -1.31026013016\n",
      "-1.19963037795\n",
      "[ 0.16373567  0.21662283  0.18385657  0.18897182  0.19926691  0.22716332\n",
      "  0.19963734  0.19999752  0.19991511  0.15561601  0.19536788  0.19571253\n",
      "  0.22825883  0.22990475  0.15659113  0.1529922   0.15294339  0.15484484\n",
      "  0.19986287  0.18517138  0.1681843   0.19976498  0.22828644  0.22822007\n",
      "  0.1544931   0.15560261]\n",
      "[ 0.31518919  0.11697886  0.25708627  0.24035177  0.2029129   0.09934241\n",
      "  0.20144599  0.2000103   0.20033934  0.29567794  0.21779311  0.21651615\n",
      "  0.10926766  0.11609532  0.28620053  0.29642336  0.30772543  0.30380297\n",
      "  0.20054811  0.23781916  0.27418457  0.20093822  0.11883814  0.11747455\n",
      "  0.30200032  0.29540645]\n",
      "\n",
      "882 6\n",
      "3500  d  (0.5, 0.5, 0.40553703487379467, None)\n",
      "dev err: -1.45391399078\n",
      "-1.35087947165\n",
      "[ 0.15516234  0.21875373  0.18072018  0.18704973  0.19895403  0.23037607\n",
      "  0.19986964  0.20048939  0.19991511  0.14407086  0.19438425  0.19488195\n",
      "  0.23212263  0.23411373  0.14593984  0.14106195  0.14077663  0.14298798\n",
      "  0.20023949  0.18250779  0.16140825  0.19995475  0.23243585  0.23231765\n",
      "  0.14326932  0.14406796]\n",
      "[ 0.33728069  0.10173961  0.26687698  0.2467607   0.20414432  0.07993959\n",
      "  0.20052115  0.19803396  0.20033934  0.3166782   0.22140004  0.21958185\n",
      "  0.09105437  0.09977024  0.30467029  0.31733533  0.33068653  0.32677349\n",
      "  0.19904038  0.24408761  0.2879761   0.20018119  0.10231488  0.1008148\n",
      "  0.32302774  0.31631123]\n",
      "\n",
      "883 5\n",
      "4000  d  (0.43295583238958102, 0.4983108108108108, 0.4019663269328228, None)\n",
      "dev err: -1.62113747016\n",
      "-1.52711627047\n",
      "[ 0.14571945  0.22075556  0.17725109  0.18436678  0.19855656  0.23302003\n",
      "  0.20020547  0.20090721  0.19985865  0.13117075  0.19277279  0.19393856\n",
      "  0.23539535  0.23793447  0.13341014  0.12725237  0.12663452  0.12966786\n",
      "  0.20064626  0.17959733  0.15420404  0.20020218  0.23610953  0.2359359\n",
      "  0.13061126  0.13118605]\n",
      "[ 0.36028368  0.08501903  0.27734525  0.25546202  0.20569868  0.0594871\n",
      "  0.19917706  0.19634052  0.20056474  0.33934845  0.22719463  0.22301654\n",
      "  0.07183855  0.08193308  0.32563629  0.34053404  0.35648012  0.35120764\n",
      "  0.19739977  0.25073973  0.30207663  0.19919013  0.08510507  0.08335663\n",
      "  0.34575975  0.33873492]\n",
      "\n",
      "884 4\n",
      "4500  d  (0.45814479638009048, 0.49915540540540543, 0.40238482384823854, None)\n",
      "dev err: -1.81133850249\n",
      "-1.67931977638\n",
      "[ 0.13497781  0.22275229  0.1737882   0.1821952   0.19829852  0.23496631\n",
      "  0.2006259   0.20143828  0.19983969  0.11664084  0.19118319  0.19332555\n",
      "  0.23804028  0.24114682  0.11936563  0.11177153  0.11081469  0.11457827\n",
      "  0.20111021  0.17688494  0.14629573  0.20055769  0.2392333   0.23900203\n",
      "  0.11634245  0.11665965]\n",
      "[ 0.38506372  0.06420121  0.28745652  0.26231366  0.20670194  0.03801537\n",
      "  0.19748227  0.19416812  0.2006404   0.36377605  0.23278027  0.22522246\n",
      "  0.05192101  0.06355634  0.34791981  0.36539396  0.38370606  0.37742905\n",
      "  0.19551285  0.25677741  0.31699795  0.19775803  0.06744221  0.06531873\n",
      "  0.37020459  0.36268148]\n",
      "\n",
      "884 4\n",
      "5000  d  (0.45814479638009048, 0.49915540540540543, 0.40238482384823854, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev err: -2.02011446481\n",
      "-1.87574590604\n",
      "[ 0.12247436  0.22423201  0.16960538  0.18014492  0.19806343  0.23604349\n",
      "  0.20110288  0.20206738  0.19981979  0.10127702  0.19000269  0.19275152\n",
      "  0.23981858  0.24360987  0.10379307  0.09467813  0.09333744  0.09861487\n",
      "  0.20167722  0.17357607  0.13759376  0.20097895  0.2416233   0.24130609\n",
      "  0.10143693  0.10134621]\n",
      "[ 0.41238783  0.04272337  0.29926804  0.26863834  0.20761203  0.01718905\n",
      "  0.19554287  0.19156475  0.20071974  0.38860911  0.23685005  0.22727022\n",
      "  0.03247346  0.04519465  0.37196367  0.39195348  0.41278226  0.4042859\n",
      "  0.19318325  0.26402082  0.33275174  0.19604839  0.04987857  0.04733578\n",
      "  0.39449853  0.3871097 ]\n",
      "\n",
      "884 4\n",
      "5500  d  (0.45814479638009048, 0.49915540540540543, 0.40238482384823854, None)\n",
      "dev err: -2.25717250296\n",
      "-2.14537073727\n",
      "[ 0.10854275  0.22517874  0.16509213  0.1775073   0.19796135  0.2362943\n",
      "  0.20151645  0.20260914  0.19981979  0.08384136  0.18882292  0.1921948\n",
      "  0.24080837  0.24526978  0.08690064  0.07580321  0.0742392   0.08046095\n",
      "  0.20215756  0.169908    0.12762585  0.20135552  0.24337115  0.242942\n",
      "  0.08417552  0.08402164]\n",
      "[ 0.44126002  0.01894905  0.31157367  0.27658454  0.20800606 -0.00561179\n",
      "  0.19384659  0.1892959   0.20071974  0.41574864  0.2408544   0.22924009\n",
      "  0.01142341  0.02626051  0.39669227  0.4201179   0.44315777  0.43329381\n",
      "  0.19118892  0.27194524  0.35037472  0.19450822  0.0312002   0.02841876\n",
      "  0.42175163  0.41393765]\n",
      "\n",
      "887 1\n",
      "6000  d  (0.33314543404735064, 0.49915540540540543, 0.39959432048681542, None)\n",
      "dev err: -2.51307307369\n",
      "-2.40425647079\n",
      "[ 0.09376953  0.22540722  0.159018    0.17549576  0.1978295   0.23559359\n",
      "  0.20185963  0.20313844  0.19981979  0.06517059  0.18741531  0.19130118\n",
      "  0.24086133  0.24606825  0.06834353  0.05505367  0.05300028  0.06097709\n",
      "  0.20267521  0.16515434  0.11747688  0.20163991  0.24432339  0.2437692\n",
      "  0.06590432  0.06543313]\n",
      "[ 0.4703934  -0.00258357  0.32750396  0.28251094  0.20851396 -0.02793525\n",
      "  0.19242835  0.18705414  0.20071974  0.44341122  0.24555407  0.23236994\n",
      " -0.0092722   0.00700977  0.42291252  0.44966033  0.47533648  0.46328982\n",
      "  0.1890175   0.28178506  0.36768424  0.19333742  0.01287189  0.00978774\n",
      "  0.44935145  0.44134169]\n",
      "\n",
      "887 1\n",
      "6500  d  (0.33314543404735064, 0.49915540540540543, 0.39959432048681542, None)\n",
      "dev err: -2.7960255809\n",
      "-2.70801709909\n",
      "[ 0.0766744   0.22505232  0.15342383  0.17314136  0.19768478  0.23387414\n",
      "  0.20241851  0.20380664  0.19978018  0.04451316  0.18612396  0.19037948\n",
      "  0.2399248   0.24593756  0.04893763  0.03267016  0.03039373  0.03937194\n",
      "  0.2033157   0.16067567  0.10591003  0.20214562  0.24446935  0.24376988\n",
      "  0.04590779  0.0448657 ]\n",
      "[ 0.50254205 -0.02362621  0.34161431  0.28931107  0.20907011 -0.05175078\n",
      "  0.1900974   0.18418735  0.20087763  0.47308242  0.2497948   0.23555813\n",
      " -0.03078129 -0.01244512  0.44964473  0.48080194  0.50838607  0.49517606\n",
      "  0.18629781  0.29086971  0.38698102  0.1912388  -0.00645983 -0.00977735\n",
      "  0.47881532  0.47119451]\n",
      "\n",
      "888 0\n",
      "7000  d  (0.33333333333333331, 0.5, 0.40000000000000002, None)\n",
      "dev err: -3.09812626184\n",
      "-2.94681867835\n",
      "[ 0.06015866  0.22394033  0.1484872   0.16987657  0.19762238  0.23111466\n",
      "  0.20285254  0.20442178  0.19976017  0.02278988  0.18452001  0.18938535\n",
      "  0.23798128  0.24484274  0.02742429  0.00838675  0.00636168  0.01666326\n",
      "  0.20388469  0.15697595  0.09491383  0.20249811  0.24378008  0.24292594\n",
      "  0.02450583  0.02327476]\n",
      "[ 0.53227999 -0.04779908  0.35367219  0.2985151   0.20930951 -0.07543448\n",
      "  0.18826842  0.18151035  0.20095734  0.50327289  0.25497301  0.23895313\n",
      " -0.05244013 -0.03233063  0.47849498  0.5132584   0.54272568  0.52778391\n",
      "  0.18384982  0.29832784  0.40480449  0.18976309 -0.02516875 -0.02885979\n",
      "  0.50944726  0.50116435]\n",
      "\n",
      "888 0\n",
      "7500  d  (0.33333333333333331, 0.5, 0.40000000000000002, None)\n",
      "dev err: -3.42707183678\n",
      "-3.26167101135\n",
      "[ 0.04252185  0.22229741  0.14345044  0.16664653  0.19743399  0.22739755\n",
      "  0.20343749  0.20529844  0.19976017 -0.00100976  0.18303808  0.18790245\n",
      "  0.23499363  0.24277313  0.00406348 -0.01766568 -0.02082118 -0.00848219\n",
      "  0.20466841  0.15333426  0.08347869  0.20296208  0.24219351  0.24116231\n",
      "  0.00219413 -0.00032392]\n",
      "[ 0.56283796 -0.06956372  0.36563362  0.3073841   0.21003064 -0.09920199\n",
      "  0.18577656  0.17762925  0.20095734  0.53519536  0.25967428  0.24393705\n",
      " -0.07412516 -0.05196046  0.50902288  0.54727388  0.57884338  0.56214602\n",
      "  0.18042657  0.3053167   0.42250202  0.18780405 -0.04455972 -0.04879392\n",
      "  0.53988582  0.53273601]\n",
      "\n",
      "888 0\n",
      "8000  d  (0.33333333333333331, 0.5, 0.40000000000000002, None)\n",
      "0 -1002.9052049\n",
      "888 0\n",
      "(0.33333333333333331, 0.5, 0.40000000000000002, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%500==0):\n",
    "                pl = []\n",
    "                t_de=0\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                    t_de+=de_curr\n",
    "                predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "                print(\"dev err:\",t_de/888)\n",
    "                print(total_te/500)\n",
    "                total_te=0\n",
    "                print(a)\n",
    "                print(t)\n",
    "                print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
