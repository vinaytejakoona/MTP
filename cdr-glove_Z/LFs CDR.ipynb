{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distanceCD(c):\n",
    "    dist = 0\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    return dist\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# def LF_induce(c):\n",
    "#     return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,distanceCD(c)) if re.search(r'{{A}}.*induc.*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "# def LF_d_induced_by_c(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1),1)\n",
    "\n",
    "def LF_d_induced_by_c(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    for w in causal_past:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "\n",
    "# def LF_d_induced_by_c_tight(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1),1)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    return (rule_regex_search_btw_BA(c, '.*' + ltp(causal_past) + ' (by|to) ', 1),distanceCD(c))\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "# def LF_c_cause_d(c):\n",
    "#     return (1,1) if (\n",
    "#         re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#     ) else (0,0)\n",
    "\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,1) if (\n",
    "        re.search(r'{{A}}.* ' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.*(not|no).*' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "\n",
    "# def LF_d_treat_c(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1),1)\n",
    "\n",
    "def LF_d_treat_c(c):\n",
    "    return (rule_regex_search_btw_BA(c, '.*' + ltp(treat) + '.*', -1),1)\n",
    "\n",
    "# def LF_c_treat_d(c):\n",
    "#     return (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1),1)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    return (rule_regex_search_btw_AB(c, '.*' + ltp(treat) + '.*', -1),1)\n",
    "\n",
    "# def LF_treat_d(c):\n",
    "#     return (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1),1)\n",
    "\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],5))\n",
    "    for w in treat:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "    \n",
    "def LF_c_treat_d_wide(c):\n",
    "    return (rule_regex_search_btw_AB(c, '.*' + ltp(treat) + '.*', -1),1)\n",
    "\n",
    "# def LF_c_treat_d_wide(c):\n",
    "#     return (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1),1)\n",
    "\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    return (rule_regex_search_before_B(c, 'improv.*', -1),1)\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "# def LF_uncertain(c):\n",
    "#     return (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1),1)\n",
    "\n",
    "def LF_uncertain(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],5))\n",
    "    for w in uncertain:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "    \n",
    "# def LF_induced_other(c):\n",
    "#     return (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1),1)\n",
    "\n",
    "def LF_induced_other(c):\n",
    "    return (rule_regex_search_tagged_text(c, '{{A}}.*-induced {{B}}', -1),1)\n",
    "\n",
    "# def LF_far_c_d(c):\n",
    "#     return (rule_regex_search_btw_AB(c, '.{100,5000}', -1),1)\n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    return (rule_regex_search_btw_AB(c, '.*', -1),distanceCD(c))\n",
    "\n",
    "# def LF_far_d_c(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{100,5000}', -1),1)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    return (rule_regex_search_btw_BA(c, '.*', -1),distanceCD(c))\n",
    "\n",
    "# def LF_risk_d(c):\n",
    "#     return (rule_regex_search_before_B(c, 'risk of ', 1),1)\n",
    "\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],5))\n",
    "    sc=max(sc,get_similarity(word_vectors,'risk'))\n",
    "    return (1,sc)\n",
    "\n",
    "# def LF_develop_d_following_c(c):\n",
    "#     return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1) if re.search(r'develop.*{{B}}.*following.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "# def LF_d_following_c(c):\n",
    "#     return (1,1) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1) if re.search('{{B}}.*' + ltp(following) + '.*{{A}}.*' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_measure(c):\n",
    "#     return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,1) if re.search('measur.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_level(c):\n",
    "#     return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,1) if re.search('{{A}}.* level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_neg_d(c):\n",
    "#     return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,1) if re.search('(none|not|no) .*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
