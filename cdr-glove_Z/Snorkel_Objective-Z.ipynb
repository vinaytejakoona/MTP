{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "8272 888\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()\n",
    "print(len(train_cands),len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 592\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from six.moves.cPickle import load\n",
    "\n",
    "with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "    ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "##### Discrete #########\n",
    "\n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "def LF_d_induced_by_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,1) if (\n",
    "        re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "def LF_d_treat_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d_wide(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "def LF_uncertain(c):\n",
    "    if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induced_other(c):\n",
    "    if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)  \n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_therapy(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "    LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "    LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "    LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "    LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "    LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "    LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "]\n",
    "\n",
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 7-5-2018, 13:38:29\n",
      "500data points labelled in 0.024617326259613038 mins\n",
      "500data points labelled in 0.06944836378097534 mins\n",
      "1000data points labelled in 0.10239864190419515 mins\n",
      "1500data points labelled in 0.12567600806554158 mins\n",
      "2000data points labelled in 0.1551079233487447 mins\n",
      "2500data points labelled in 0.1808343489964803 mins\n",
      "3000data points labelled in 0.2076396902402242 mins\n",
      "3500data points labelled in 0.23080986340840656 mins\n",
      "4000data points labelled in 0.2608593622843424 mins\n",
      "4500data points labelled in 0.2797205646832784 mins\n",
      "5000data points labelled in 0.30112220843633014 mins\n",
      "5500data points labelled in 0.32616100311279295 mins\n",
      "6000data points labelled in 0.3475878000259399 mins\n",
      "6500data points labelled in 0.3676290432612101 mins\n",
      "7000data points labelled in 0.39147427479426067 mins\n",
      "7500data points labelled in 0.4106962045033773 mins\n",
      "8000data points labelled in 0.4307020902633667 mins\n",
      "--- 26.474006175994873 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "np.save(\"dev_L_S_discrete\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_discrete\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 2, 33) (8272, 2, 33)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 184222.4562227816\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.27082211 -0.01928787 -0.14063245  0.37856253  0.43681819 -0.15844807\n",
      "   0.13280198 -0.01935702 -0.10775934  0.34390113  0.39762823 -0.14286955\n",
      "  -0.39588527 -0.33699178 -0.37821404  0.38378715 -0.39537146  0.11504936\n",
      "   0.21906794  0.39699417 -0.27113816  8.13838832  0.39548336 -0.31328908\n",
      "   0.25503373  0.28019293  0.39734506  0.39700564  0.37866251 -0.39156514\n",
      "   0.38002959  0.21917987  0.13605525]]\n",
      "{0: 253, 1: 635}\n",
      "(0.462992125984252, 0.9932432432432432, 0.6315789473684211, None)\n",
      "\n",
      "1 loss 182926.8350570609\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.32930229 -0.072816   -0.19433839  0.43560275  0.51398692 -0.0365946\n",
      "   0.19811178 -0.0729731  -0.16134781  0.40620045  0.46910414 -0.1965869\n",
      "  -0.45399143 -0.39169289 -0.43424009  0.43969335 -0.45314662  0.20099496\n",
      "   0.27412612  0.45449307 -0.13677994 10.2419757   0.45333961 -0.37154561\n",
      "   0.1955587   0.33629475  0.4545094   0.45440313  0.43567656 -0.4480074\n",
      "   0.43491147  0.28277911  0.20811263]]\n",
      "{0: 290, 1: 598}\n",
      "(0.4916387959866221, 0.9932432432432432, 0.6577181208053691, None)\n",
      "\n",
      "2 loss 182800.53538895075\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.50033750e-01 -9.20612978e-02 -2.13641010e-01  4.56135636e-01\n",
      "   5.39595897e-01  1.96766814e-03  2.20784600e-01 -9.22805491e-02\n",
      "  -1.80658821e-01  4.27914802e-01  4.93027965e-01 -2.15923034e-01\n",
      "  -4.74703809e-01 -4.11355405e-01 -4.54355973e-01  4.59505852e-01\n",
      "  -4.73775237e-01  2.29612258e-01  2.93641733e-01  4.75037473e-01\n",
      "  -9.49920742e-02  1.09749630e+01  4.73964547e-01 -3.92341571e-01\n",
      "   1.74156761e-01  3.56763707e-01  4.74947673e-01  4.74935290e-01\n",
      "   4.56160538e-01 -4.68232180e-01  4.54509106e-01  3.04807646e-01\n",
      "   2.33081953e-01]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "3 loss 182779.13781656895\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35716556 -0.09869963 -0.22030067  0.46322675  0.54827254  0.01475931\n",
      "   0.2285349  -0.09894358 -0.18732523  0.43535596  0.50115104 -0.22259733\n",
      "  -0.48184533 -0.41814551 -0.46130134  0.46632557 -0.48089044  0.2392675\n",
      "   0.30036809  0.48212319 -0.08115096 11.22596956  0.48107634 -0.39950962\n",
      "   0.16677415  0.36384845  0.48199793  0.48202006  0.46323675 -0.47521353\n",
      "   0.46127745  0.31235554  0.24160462]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "4 loss 182774.49158656417\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.512\n",
      "Precision            0.504\n",
      "Recall               0.993\n",
      "F1                   0.669\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 289 | TN: 303 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 305, 1: 583}\n",
      "acc 0.6722972972972973\n",
      "(array([0.99344262, 0.50428816]), array([0.51182432, 0.99324324]), array([0.67558528, 0.66894198]), array([592, 296]))\n",
      "(0.7488653938081714, 0.7525337837837838, 0.6722636319015605, None)\n",
      "[[303 289]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5042881646655232 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n"
     ]
    }
   ],
   "source": [
    "## class wise reproduced using map over y\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 184222.4562227816\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.27082211 -0.01928787 -0.14063245  0.37856253  0.43681819 -0.15844807\n",
      "   0.13280198 -0.01935702 -0.10775934  0.34390113  0.39762823 -0.14286955\n",
      "  -0.39588527 -0.33699178 -0.37821404  0.38378715 -0.39537146  0.11504936\n",
      "   0.21906794  0.39699417 -0.27113816  8.13838832  0.39548336 -0.31328908\n",
      "   0.25503373  0.28019293  0.39734506  0.39700564  0.37866251 -0.39156514\n",
      "   0.38002959  0.21917987  0.13605525]]\n",
      "{0: 253, 1: 635}\n",
      "(0.462992125984252, 0.9932432432432432, 0.6315789473684211, None)\n",
      "\n",
      "1 loss 182926.8350570609\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.32930229 -0.072816   -0.19433839  0.43560275  0.51398692 -0.0365946\n",
      "   0.19811178 -0.0729731  -0.16134781  0.40620045  0.46910414 -0.1965869\n",
      "  -0.45399143 -0.39169289 -0.43424009  0.43969335 -0.45314662  0.20099496\n",
      "   0.27412612  0.45449307 -0.13677994 10.2419757   0.45333961 -0.37154561\n",
      "   0.1955587   0.33629475  0.4545094   0.45440313  0.43567656 -0.4480074\n",
      "   0.43491147  0.28277911  0.20811263]]\n",
      "{0: 290, 1: 598}\n",
      "(0.4916387959866221, 0.9932432432432432, 0.6577181208053691, None)\n",
      "\n",
      "2 loss 182800.53538895075\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.50033750e-01 -9.20612978e-02 -2.13641010e-01  4.56135636e-01\n",
      "   5.39595897e-01  1.96766814e-03  2.20784600e-01 -9.22805491e-02\n",
      "  -1.80658821e-01  4.27914802e-01  4.93027965e-01 -2.15923034e-01\n",
      "  -4.74703809e-01 -4.11355405e-01 -4.54355973e-01  4.59505852e-01\n",
      "  -4.73775237e-01  2.29612258e-01  2.93641733e-01  4.75037473e-01\n",
      "  -9.49920742e-02  1.09749630e+01  4.73964547e-01 -3.92341571e-01\n",
      "   1.74156761e-01  3.56763707e-01  4.74947673e-01  4.74935290e-01\n",
      "   4.56160538e-01 -4.68232180e-01  4.54509106e-01  3.04807646e-01\n",
      "   2.33081953e-01]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "3 loss 182779.13781656895\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35716556 -0.09869963 -0.22030067  0.46322675  0.54827254  0.01475931\n",
      "   0.2285349  -0.09894358 -0.18732523  0.43535596  0.50115104 -0.22259733\n",
      "  -0.48184533 -0.41814551 -0.46130134  0.46632557 -0.48089044  0.2392675\n",
      "   0.30036809  0.48212319 -0.08115096 11.22596956  0.48107634 -0.39950962\n",
      "   0.16677415  0.36384845  0.48199793  0.48202006  0.46323675 -0.47521353\n",
      "   0.46127745  0.31235554  0.24160462]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "4 loss 182774.49158656417\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.512\n",
      "Precision            0.504\n",
      "Recall               0.993\n",
      "F1                   0.669\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 289 | TN: 303 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 305, 1: 583}\n",
      "acc 0.6722972972972973\n",
      "(array([0.99344262, 0.50428816]), array([0.51182432, 0.99324324]), array([0.67558528, 0.66894198]), array([592, 296]))\n",
      "(0.7488653938081714, 0.7525337837837838, 0.6722636319015605, None)\n",
      "[[303 289]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5042881646655232 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n"
     ]
    }
   ],
   "source": [
    "## class wise reproduced using map over y\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,l = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "z-weight: 0.0\n",
      "0 loss -227408.91347570377\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 14.67744856  25.90583748  22.69587475  12.78045065  14.39716461\n",
      "    0.44035997  -3.2445657   25.08846134  24.96396903  13.96286426\n",
      "   11.82652493  22.46522151   1.79888645  16.7243985   11.77938823\n",
      "   -3.43412061   2.63100655   1.29551022  -6.77434535  -0.59566388\n",
      "  -10.15505727  61.98589432   1.58045045  12.01429343  28.43790004\n",
      "    3.19948727   1.48865339  -1.91094183  -4.65161603   5.80171757\n",
      "   -2.764758     1.47742811 -15.12888705]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 1.0\n",
      "Neg. class accuracy: 0.314\n",
      "Precision            0.422\n",
      "Recall               1.0\n",
      "F1                   0.593\n",
      "----------------------------------------\n",
      "TP: 296 | FP: 406 | TN: 186 | FN: 0\n",
      "========================================\n",
      "\n",
      "{0: 186, 1: 702}\n",
      "acc 0.5427927927927928\n",
      "(array([1.        , 0.42165242]), array([0.31418919, 1.        ]), array([0.4781491 , 0.59318637]), array([592, 296]))\n",
      "(0.7108262108262109, 0.6570945945945946, 0.5356677365012802, None)\n",
      "[[186 406]\n",
      " [  0 296]]\n",
      "prec: tp/(tp+fp) 0.42165242165242167 recall: tp/(tp+fn) 1.0\n",
      "(0.42165242165242167, 1.0, 0.5931863727454909, None)\n",
      "\n",
      "z-weight: 0.1111111111111111\n",
      "0 loss -119393.34333310764\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ -1.55495507   1.81238967   0.4215572    4.86093815   5.33593905\n",
      "   -0.46940592   2.37181565   1.35127544   1.1895527    4.98889206\n",
      "    4.10979856   0.44686917  -5.92391325  -0.82443449  -1.64468996\n",
      "    1.72211265  -4.3741264    1.19145527   0.69531406   3.82908758\n",
      "  -13.59053119  59.58797677   4.37463643  -1.68991432   5.96233837\n",
      "    2.07511514   3.84541269   3.50532919   2.59915108  -2.75507519\n",
      "    2.16501225   3.62028343   0.37233849]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.402\n",
      "Precision            0.454\n",
      "Recall               0.993\n",
      "F1                   0.623\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 354 | TN: 238 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 240, 1: 648}\n",
      "acc 0.5990990990990991\n",
      "(array([0.99166667, 0.4537037 ]), array([0.40202703, 0.99324324]), array([0.57211538, 0.62288136]), array([592, 296]))\n",
      "(0.7226851851851852, 0.6976351351351351, 0.597498370273794, None)\n",
      "[[238 354]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.4537037037037037 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.4537037037037037, 0.9932432432432432, 0.6228813559322034, None)\n",
      "\n",
      "z-weight: 0.2222222222222222\n",
      "0 loss -67297.10695909889\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-2.12712516e+00 -3.32614018e-01 -8.27306783e-01  2.23820234e+00\n",
      "   1.60257984e+00  2.53483915e-01  2.88554944e+00 -2.73214892e-01\n",
      "  -4.66232366e-01  1.34056882e+00  1.64109754e+00 -7.43948404e-01\n",
      "  -4.40931548e+00 -1.59032385e+00 -2.19509216e+00  2.26128927e+00\n",
      "  -3.79266438e+00  1.13626702e+00  1.42608194e+00  3.30397500e+00\n",
      "  -1.13641614e+01  5.41774757e+01  3.84822520e+00 -2.14689008e+00\n",
      "   2.15987198e-02  2.12651687e+00  3.32665252e+00  3.32779256e+00\n",
      "   2.81169826e+00 -2.70706992e+00  2.42245597e+00  2.14348796e+00\n",
      "   1.45342684e+00]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.427\n",
      "Precision            0.464\n",
      "Recall               0.993\n",
      "F1                   0.633\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 339 | TN: 253 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 255, 1: 633}\n",
      "acc 0.615990990990991\n",
      "(array([0.99215686, 0.46445498]), array([0.42736486, 0.99324324]), array([0.5974026 , 0.63293864]), array([592, 296]))\n",
      "(0.7283059195242079, 0.7103040540540541, 0.6151706205527518, None)\n",
      "[[253 339]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.46445497630331756 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.46445497630331756, 0.9932432432432432, 0.6329386437029064, None)\n",
      "\n",
      "z-weight: 0.3333333333333333\n",
      "0 loss -20898.4966657875\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.39654331 -0.32982647 -0.68065499  2.02672178  2.49695948  1.5465588\n",
      "   1.55228355 -0.30114009 -0.47509532  2.27421879  2.27522007 -0.64276916\n",
      "  -1.9909666  -1.23034186 -1.4960202   1.63290691 -1.91311068  1.90215617\n",
      "   1.29233145  1.79939409  0.37015213 49.01407705  1.88915495 -1.46360602\n",
      "  -0.11304114  1.89103796  1.80147403  1.82279554  1.78941021 -1.55785983\n",
      "   1.64268534  1.82300258  1.44669285]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.4444444444444444\n",
      "0 loss 21366.36183458681\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.21039898 -0.43583024 -0.71446238  1.62628005  2.01277839  1.30526789\n",
      "   1.26968363 -0.42178058 -0.57532032  1.7759869   1.85932575 -0.69572398\n",
      "  -1.65267706 -1.1604753  -1.34837724  1.41200589 -1.62326002  1.54509986\n",
      "   1.12102022  1.54245021  0.86974974 42.66083782  1.61142022 -1.29425214\n",
      "  -0.23707812  1.51451552  1.52229232  1.55548205  1.53035949 -1.39273037\n",
      "   1.40881072  1.45936542  1.25183945]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.5555555555555556\n",
      "0 loss 60755.62596127175\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.02085992 -0.43466938 -0.66139027  1.31188971  1.62509742  1.01080258\n",
      "   1.01674327 -0.4293375  -0.56422908  1.39456541  1.50020989 -0.65374794\n",
      "  -1.35917832 -1.03111118 -1.16824761  1.19302404 -1.34496595  1.21490643\n",
      "   0.92983185  1.29825564  0.75445451 35.57630169  1.34091977 -1.10682104\n",
      "  -0.21598728  1.19624718  1.27694178  1.30661787  1.27389902 -1.20651613\n",
      "   1.19174522  1.16412989  1.0257656 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 96689.88058612194\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.82561982 -0.36723542 -0.55485411  1.03417808  1.27583029  0.71717905\n",
      "   0.77682786 -0.36610815 -0.4850003   1.0696494   1.17258026 -0.55315662\n",
      "  -1.07879907 -0.86485836 -0.96553447  0.97462023 -1.0725238   0.90759641\n",
      "   0.73696937  1.05145869  0.54532653 28.29318095  1.07189892 -0.90287243\n",
      "  -0.12768098  0.91690854  1.0383057   1.05647057  1.02109501 -0.99756409\n",
      "   0.97302553  0.89269963  0.79117205]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.7777777777777777\n",
      "0 loss 129091.99006880184\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-6.31714466e-01 -2.62813907e-01 -4.21292218e-01  7.87840280e-01\n",
      "   9.65961780e-01  4.31942819e-01  5.49566565e-01 -2.63051686e-01\n",
      "  -3.69013368e-01  7.91216636e-01  8.83652633e-01 -4.22286759e-01\n",
      "  -8.22743005e-01 -6.83976872e-01 -7.58142021e-01  7.63739239e-01\n",
      "  -8.19994964e-01  6.25106963e-01  5.50305355e-01  8.13307816e-01\n",
      "   2.98833345e-01  2.11678770e+01  8.20120283e-01 -6.96067139e-01\n",
      "  -6.29539821e-03  6.73365598e-01  8.07851513e-01  8.15588317e-01\n",
      "   7.83632999e-01 -7.83295199e-01  7.59563950e-01  6.44231646e-01\n",
      "   5.61002120e-01]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.8888888888888888\n",
      "0 loss 158160.98403968994\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.44480025 -0.14112093 -0.27827197  0.56940597  0.69191377  0.14765073\n",
      "   0.33466023 -0.14136785 -0.23734191  0.55131697  0.63060267 -0.28015566\n",
      "  -0.59473098 -0.50377587 -0.55889202  0.56530252 -0.59334806  0.36438264\n",
      "   0.37588674  0.59301109  0.02948187 14.40619354  0.59354635 -0.49639742\n",
      "   0.12729776  0.46023162  0.59149847  0.59337293  0.56775534 -0.57774544\n",
      "   0.55959104  0.4201602   0.34073775]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.59\n",
      "Precision            0.547\n",
      "Recall               0.993\n",
      "F1                   0.706\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 243 | TN: 349 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 351, 1: 537}\n",
      "acc 0.7240990990990991\n",
      "(array([0.99430199, 0.54748603]), array([0.58952703, 0.99324324]), array([0.74019088, 0.70588235]), array([592, 296]))\n",
      "(0.7708940139107736, 0.7913851351351351, 0.7230366165554238, None)\n",
      "[[349 243]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.547486033519553 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.547486033519553, 0.9932432432432432, 0.7058823529411765, None)\n",
      "\n",
      "z-weight: 1.0\n",
      "0 loss 184222.4562227816\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.27082211 -0.01928787 -0.14063245  0.37856253  0.43681819 -0.15844807\n",
      "   0.13280198 -0.01935702 -0.10775934  0.34390113  0.39762823 -0.14286955\n",
      "  -0.39588527 -0.33699178 -0.37821404  0.38378715 -0.39537146  0.11504936\n",
      "   0.21906794  0.39699417 -0.27113816  8.13838832  0.39548336 -0.31328908\n",
      "   0.25503373  0.28019293  0.39734506  0.39700564  0.37866251 -0.39156514\n",
      "   0.38002959  0.21917987  0.13605525]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.424\n",
      "Precision            0.463\n",
      "Recall               0.993\n",
      "F1                   0.632\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 341 | TN: 251 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 253, 1: 635}\n",
      "acc 0.6137387387387387\n",
      "(array([0.99209486, 0.46299213]), array([0.42398649, 0.99324324]), array([0.59408284, 0.63157895]), array([592, 296]))\n",
      "(0.7275434938221655, 0.7086148648648649, 0.6128308938025537, None)\n",
      "[[251 341]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.462992125984252 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.462992125984252, 0.9932432432432432, 0.6315789473684211, None)\n"
     ]
    }
   ],
   "source": [
    "## varying z weight \n",
    "\n",
    "def train(z_weight):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        \n",
    "        zw = tf.convert_to_tensor(z_weight,dtype=tf.float64)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "#         print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - zw*logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "\n",
    "for w in np.linspace(0,1,10):\n",
    "    print()\n",
    "    print(\"z-weight:\",w)\n",
    "    train(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00347860>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00347860>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00347860>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 144059.2118620104\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.4018328   0.39681139  0.69874603  0.66187815  0.37243063  4.19841682\n",
      "   0.40312402 -0.18403703 -0.29869982 -0.39304135]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "1 loss 143856.85057093657\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "2 loss 143856.85044932846\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "3 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "4 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with snorkel thetas and train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[0.07472098,\\\n",
    "                            0.07514459,  0.11910277,0.11186369,0.07306518,0.69216714,\\\n",
    "                            0.07467749,0.16012659, 0.13682546,0.08183363]])),\\\n",
    "                            dtype=tf.float64)\n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 144095.64128461378\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.4018328   0.3968114   0.69874604  0.66187815  0.37243063  4.19841685\n",
      "   0.40312403 -0.18403703 -0.29869983 -0.39304135]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "1 loss 143856.85056997798\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "2 loss 143856.85044932846\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "3 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "4 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with old network thetas and train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[1.0,1.0,1.0,\\\n",
    "                             1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]])),\\\n",
    "                    dtype=tf.float64)\n",
    "                             \n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 153548.14977017298\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.convert_to_tensor(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "            0.11186369,0.07306518,0.69216714,0.07467749,0.16012659, 0.13682546,0.08183363]]))\n",
    "\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(1):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "#                     _,ls = sess.run([train_step,normloss])\n",
    "                    ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 179552.66617224828\n",
      "[[1.         1.         1.         1.         1.         1.02750979\n",
      "  1.         1.0218145  1.         1.        ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.         1.         1.         1.         1.         1.02750979\n",
      "  1.         1.0218145  1.         1.        ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.545\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.353\n",
      "Recall               0.545\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 103 | FP: 189 | TN: 2436 | FN: 86\n",
      "========================================\n",
      "\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(array([0.96590008, 0.35273973]), array([0.928     , 0.54497354]), array([0.94657082, 0.42827443]), array([2625,  189]))\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "[[2436  189]\n",
      " [  86  103]]\n",
      "prec: tp/(tp+fp) 0.3527397260273973 recall: tp/(tp+fn) 0.544973544973545\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "## Objective value on thetas from old network\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.convert_to_tensor(np.array([[1.0,1.0,1.0,1.0,1.0,1.02750979,\\\n",
    "                             1.0,1.0218145,1.0,1.0]]))\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(1):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "#                     _,ls = sess.run([train_step,normloss])\n",
    "                    ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 145767.7856663791\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "1 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "2 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "3 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "4 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "5 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "6 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "7 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "8 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "9 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "10 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "11 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "12 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "13 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "14 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "15 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "16 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "17 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "18 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "19 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "21 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "22 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "23 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "24 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "25 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "26 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "27 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "28 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "29 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "acc 0.7949538024164889\n",
      "(array([0.93353091, 0.07079646]), array([0.84      , 0.16931217]), array([0.88429918, 0.09984399]), array([2625,  189]))\n",
      "(0.5021636830944227, 0.5046560846560846, 0.49207158581109633, None)\n",
      "[[2205  420]\n",
      " [ 157   32]]\n",
      "prec: tp/(tp+fp) 0.07079646017699115 recall: tp/(tp+fn) 0.1693121693121693\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n"
     ]
    }
   ],
   "source": [
    "## same network that didn't train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(30):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 16 29 14 13 23 20  0  5 11  2  8  7  1 17  6 32 18 31 24 25  9  3 28\n",
      " 30 15 22 19 27 26 10  4 21]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([-0.27082211, -0.01928787, -0.14063245,  0.37856253,  0.43681819, -0.15844807,\n",
    "   0.13280198, -0.01935702, -0.10775934,  0.34390113,  0.39762823, -0.14286955,\n",
    "  -0.39588527, -0.33699178, -0.37821404,  0.38378715, -0.39537146,  0.11504936,\n",
    "   0.21906794,  0.39699417, -0.27113816,  8.13838832,  0.39548336, -0.31328908,\n",
    "   0.25503373,  0.28019293,  0.39734506,  0.39700564,  0.37866251, -0.39156514,\n",
    "   0.38002959,  0.21917987,  0.13605525])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00024597518522208474\n",
      "[1.         1.         1.         1.         1.         1.00531229\n",
      " 1.         1.         1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "0 -0.9839007408883389\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002477972163026618\n",
      "[1.         1.         1.         1.         1.         1.01071759\n",
      " 1.         1.00531229 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "1 -0.9911888652106471\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.00024967684220414605\n",
      "[1.         1.         1.         1.         1.         1.01621772\n",
      " 1.         1.01071759 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "2 -0.9987073688165843\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002516160600819831\n",
      "[1.         1.         1.         1.         1.         1.0218145\n",
      " 1.         1.01621772 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "3 -1.0064642403279322\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002536169307750545\n",
      "[1.         1.         1.         1.         1.         1.02750979\n",
      " 1.         1.0218145  1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "4 -1.014467723100218\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
