{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# import gensim.matutils as gm\n",
    "\n",
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# # Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "# model = KeyedVectors.load_word2vec_format('../glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "# wordvec_unavailable= set()\n",
    "# def write_to_file(wordvec_unavailable):\n",
    "#     with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "#         for word in wordvec_unavailable:\n",
    "#             f.write(word+\"\\n\")\n",
    "\n",
    "# def preprocess(tokens):\n",
    "#     btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "#     btw_words = [word for word in btw_words if word.isalpha()]\n",
    "#     return btw_words\n",
    "\n",
    "# def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "#     word_vectors= []\n",
    "#     for word in btw_words:\n",
    "#         try:\n",
    "#             word_v = np.array(model[word])\n",
    "#             word_v = word_v.reshape(len(word_v),1)\n",
    "#             #print(word_v.shape)\n",
    "#             word_vectors.append(model[word])\n",
    "#         except:\n",
    "#             wordvec_unavailable.add(word)\n",
    "#     return word_vectors\n",
    "\n",
    "# def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "#     similarity = 0\n",
    "#     target_word_vector = 0\n",
    "#     try:\n",
    "#         target_word_vector = model[target_word]\n",
    "#     except:\n",
    "#         wordvec_unavailable.add(target_word+\" t\")\n",
    "#         return similarity\n",
    "#     target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "#     for wv in word_vectors:\n",
    "#         wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "#         similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "#     return similarity\n",
    "\n",
    "# import bz2\n",
    "# from six.moves.cPickle import load\n",
    "\n",
    "# with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "#     ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distanceCD(c):\n",
    "    dist = 0\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    return (dist/5000)\n",
    "\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import bz2\n",
    "# from six.moves.cPickle import load\n",
    "\n",
    "# with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "#     ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)\n",
    "    \n",
    "    \n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    #print(\"LF_CHEM\",len(sent.words),dis_end + dist // 2)\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<888x1 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 888 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_external_annotations import load_external_labels\n",
    "load_external_labels(session, ChemicalDisease, split=1, annotator='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888\n",
      "296 592\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "gold_labels_dev = []\n",
    "for i,L in enumerate(L_gold_dev):\n",
    "    gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "    \n",
    "print(len(gold_labels_dev))\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# def LF_induce(c):\n",
    "#     return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,distanceCD(c)) if re.search(r'{{A}}.*induc.*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "# def LF_d_induced_by_c(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1),1)\n",
    "\n",
    "def LF_d_induced_by_c(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    for w in causal_past:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "\n",
    "# def LF_d_induced_by_c_tight(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1),1)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    return (rule_regex_search_btw_BA(c, '.*' + ltp(causal_past) + ' (by|to) ', 1),distanceCD(c))\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "# def LF_c_cause_d(c):\n",
    "#     return (1,1) if (\n",
    "#         re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#     ) else (0,0)\n",
    "\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,1) if (\n",
    "        re.search(r'{{A}}.* ' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.*(not|no).*' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "\n",
    "# def LF_d_treat_c(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1),1)\n",
    "\n",
    "def LF_d_treat_c(c):\n",
    "    return (rule_regex_search_btw_BA(c, '.*' + ltp(treat) + '.*', -1),1)\n",
    "\n",
    "# def LF_c_treat_d(c):\n",
    "#     return (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1),1)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    return (rule_regex_search_btw_AB(c, '.*' + ltp(treat) + '.*', -1),1)\n",
    "\n",
    "# def LF_treat_d(c):\n",
    "#     return (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1),1)\n",
    "\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],5))\n",
    "    for w in treat:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "    \n",
    "def LF_c_treat_d_wide(c):\n",
    "    return (rule_regex_search_btw_AB(c, '.*' + ltp(treat) + '.*', -1),1)\n",
    "\n",
    "# def LF_c_treat_d_wide(c):\n",
    "#     return (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1),1)\n",
    "\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    return (rule_regex_search_before_B(c, 'improv.*', -1),1)\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "# def LF_uncertain(c):\n",
    "#     return (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1),1)\n",
    "\n",
    "def LF_uncertain(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],5))\n",
    "    for w in uncertain:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "    \n",
    "# def LF_induced_other(c):\n",
    "#     return (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1),1)\n",
    "\n",
    "def LF_induced_other(c):\n",
    "    return (rule_regex_search_tagged_text(c, '{{A}}.*-induced {{B}}', -1),1)\n",
    "\n",
    "# def LF_far_c_d(c):\n",
    "#     return (rule_regex_search_btw_AB(c, '.{100,5000}', -1),1)\n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    return (rule_regex_search_btw_AB(c, '.*', -1),distanceCD(c))\n",
    "\n",
    "# def LF_far_d_c(c):\n",
    "#     return (rule_regex_search_btw_BA(c, '.{100,5000}', -1),1)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    return (rule_regex_search_btw_BA(c, '.*', -1),distanceCD(c))\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    return (rule_regex_search_before_B(c, 'risk of ', 1),1)\n",
    "\n",
    "\n",
    "# def LF_risk_d(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_left_tokens(c[1],5))\n",
    "#     sc=max(sc,get_similarity(word_vectors,'risk'))\n",
    "#     return (1,sc)\n",
    "\n",
    "# def LF_develop_d_following_c(c):\n",
    "#     return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1) if re.search(r'develop.*{{B}}.*following.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "# def LF_d_following_c(c):\n",
    "#     return (1,1) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1) if re.search('{{B}}.*' + ltp(following) + '.*{{A}}.*' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_measure(c):\n",
    "#     return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,1) if re.search('measur.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_level(c):\n",
    "#     return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,1) if re.search('{{A}}.* level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_neg_d(c):\n",
    "#     return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,1) if re.search('(none|not|no) .*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    return (l*cand_in_ctd_marker(c),s)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    return ((l1 or l2) * cand_in_ctd_marker(c),(s1*s2))\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    return (l* cand_in_ctd_therapy(c),s)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    return (l* cand_in_ctd_unspecified(c),s)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    return ((l1 or l2) * cand_in_ctd_unspecified(c),(s1*s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LFs = [\n",
    "    LF_c_cause_d,\n",
    "    LF_c_d,\n",
    "    LF_c_induced_d,\n",
    "    LF_c_treat_d,\n",
    "    LF_c_treat_d_wide,\n",
    "    LF_closer_chem,\n",
    "    LF_closer_dis,\n",
    "    LF_ctd_marker_c_d,\n",
    "    LF_ctd_marker_induce,\n",
    "    LF_ctd_therapy_treat,\n",
    "    LF_ctd_unspecified_treat,\n",
    "    LF_ctd_unspecified_induce,\n",
    "    LF_d_following_c,\n",
    "    LF_d_induced_by_c,\n",
    "    LF_d_induced_by_c_tight,\n",
    "    LF_d_treat_c,\n",
    "    LF_develop_d_following_c,\n",
    "    LF_far_c_d,\n",
    "    LF_far_d_c,\n",
    "    LF_improve_before_disease,\n",
    "    LF_in_ctd_therapy,\n",
    "    LF_in_ctd_marker,\n",
    "    LF_in_patient_with,\n",
    "    LF_induce,\n",
    "    LF_induce_name,\n",
    "    LF_induced_other,\n",
    "    LF_level,\n",
    "    LF_measure,\n",
    "    LF_neg_d,\n",
    "    LF_risk_d,\n",
    "    LF_treat_d,\n",
    "    LF_uncertain,\n",
    "    LF_weak_assertions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "    return L_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "   \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "# dev_L_S = pkl.load( open( \"dev_L_S.p\", \"rb\" ) )\n",
    "# train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.709864071201\n",
      "-0.00699214594392\n",
      "\n",
      "352 536\n",
      "0  d  (0.76435295115332424, 0.7846283783783784, 0.71734436114732725, None)\n",
      "too low\n",
      "0 -6.53379640393\n",
      "[ 1.11754463  0.81412327  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98358388  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94117112  0.99621468  1.16301113\n",
      "  0.87352575  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82204968  0.83837747  0.96215782]\n",
      "[ 2.23508926  1.62843948  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714118  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287898  1.99242937  2.32602226\n",
      "  1.74748188  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398285  1.67675493  1.92430093]\n",
      "352 536\n",
      "(array([ 0.98579545,  0.54291045]), array([ 0.58614865,  0.98310811]), array([ 0.73516949,  0.69951923]), array([592, 296]))\n",
      "\n",
      "(0.76435295115332424, 0.7846283783783784, 0.71734436114732725, None)\n"
     ]
    }
   ],
   "source": [
    "# class wise\n",
    "# new baseline + LF_induce + LF_d_induced_by_c_tight + distance/5000 + replaced {0,5000} with *\n",
    "\n",
    "# LF_c_cause_d  + LF_d_treat_c + LF_c_treat_d\n",
    "\n",
    "# LF_c_treat_d_wide + all LFs has *\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.709864071201\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "352 536\n",
      "0  d  (0.76435295115332424, 0.7846283783783784, 0.71734436114732725, None)\n",
      "too low\n",
      "0 -6.53379640393\n",
      "352 536\n",
      "(0.76435295115332424, 0.7846283783783784, 0.71734436114732725, None)\n"
     ]
    }
   ],
   "source": [
    "# new baseline + LF_induce + LF_d_induced_by_c_tight + distance/5000 + replaced {0,5000} with *\n",
    "\n",
    "# LF_c_cause_d  + LF_d_treat_c + LF_c_treat_d\n",
    "\n",
    "# LF_c_treat_d_wide + all LFs has *\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.710642966426\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "426 462\n",
      "0  d  (0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n",
      "too low\n",
      "0 -6.54621731303\n",
      "426 462\n",
      "(0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n"
     ]
    }
   ],
   "source": [
    "# new baseline + LF_induce + LF_d_induced_by_c_tight + distance/5000 + replaced {0,5000} with *\n",
    "\n",
    "# LF_c_cause_d  + LF_d_treat_c + LF_c_treat_d\n",
    "\n",
    "# LF_c_treat_d_wide\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.71063170149\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "426 462\n",
      "0  d  (0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n",
      "too low\n",
      "0 -6.54621731303\n",
      "426 462\n",
      "(0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n"
     ]
    }
   ],
   "source": [
    "# new baseline + LF_induce + LF_d_induced_by_c_tight + distance/5000 + replaced {0,5000} with *\n",
    "\n",
    "# LF_c_cause_d  + LF_d_treat_c + LF_c_treat_d\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.71063170149\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "426 462\n",
      "0  d  (0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n",
      "too low\n",
      "0 -6.54621731303\n",
      "426 462\n",
      "(0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n"
     ]
    }
   ],
   "source": [
    "# new baseline + LF_induce + LF_d_induced_by_c_tight + distance/5000 + replaced {0,5000} with *\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.71063170149\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "426 462\n",
      "0  d  (0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n",
      "too low\n",
      "0 -6.54621731303\n",
      "426 462\n",
      "(0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n"
     ]
    }
   ],
   "source": [
    "# new baseline + LF_induce + LF_d_induced_by_c_tight + distance/5000\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#new \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.truncated_normal_initializer(1,0.1,12),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.truncated_normal_initializer(2,0.2,12),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                print(\"too low\")\n",
    "                break\n",
    "\n",
    "            if(c%100==0):\n",
    "                pl = []\n",
    "                t_de=0\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                    t_de+=de_curr\n",
    "                predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "                print(\"dev err:\",t_de/888)\n",
    "                print(total_te/100)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "                print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        \n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "#         a,t = sess.run([alphas,thetas],feed_dict={_x:dev_L_S[0]})\n",
    "        a,t = sess.run([alphas,thetas])\n",
    "        print(a)\n",
    "        print(t)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        print()\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -2.49070364329\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "360 528\n",
      "0  d  (0.71489898989898992, 0.73310810810810811, 0.67398221424492122, None)\n",
      "too low\n",
      "0 -6.54621731303\n",
      "360 528\n",
      "(0.71489898989898992, 0.73310810810810811, 0.67398221424492122, None)\n"
     ]
    }
   ],
   "source": [
    "# new baseline + LF_induce\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.71063170149\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "426 462\n",
      "0  d  (0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n",
      "too low\n",
      "0 -6.54621731303\n",
      "426 462\n",
      "(0.74589964026583744, 0.77618243243243246, 0.73301677975854151, None)\n"
     ]
    }
   ],
   "source": [
    "# new baseline\n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev err: -0.712200279855\n",
      "-0.00699214594392\n",
      "[ 1.11754463  0.81421453  1.00955965  1.00062431  1.10637787  1.04133855\n",
      "  1.169206    1.03120785  1.14633471  1.0647233   0.98357064  1.06661405\n",
      "  0.98238184  1.07858159  1.04081247  1.04223612  0.85334134  0.91993454\n",
      "  1.05962482  0.83641602  1.03537624  0.94141469  0.99621468  1.16301113\n",
      "  0.87373343  1.17747756  1.06808572  1.06850778  1.05747627  0.99034372\n",
      "  0.82199156  0.83837747  0.96215054]\n",
      "[ 2.23508926  1.62842906  2.01911931  2.00124861  2.21275574  2.0826771\n",
      "  2.33841199  2.0624157   2.29266943  2.12944659  1.96714129  2.13322809\n",
      "  1.96476369  2.15716318  2.08162495  2.08447223  1.70668268  1.83986908\n",
      "  2.11924965  1.67283205  2.07075248  1.88287139  1.99242937  2.32602226\n",
      "  1.74746687  2.35495511  2.13617144  2.13701556  2.11495255  1.98068744\n",
      "  1.64398312  1.67675493  1.92430107]\n",
      "\n",
      "433 455\n",
      "0  d  (0.74865111793518258, 0.77956081081081074, 0.73813646844857261, None)\n",
      "too low\n",
      "0 -5.85618950661\n",
      "433 455\n",
      "(0.74865111793518258, 0.77956081081081074, 0.73813646844857261, None)\n"
     ]
    }
   ],
   "source": [
    "# for reference\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.truncated_normal_initializer(1,0.1,12),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.truncated_normal_initializer(2,0.2,12),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                print(\"too low\")\n",
    "                break\n",
    "\n",
    "            if(c%100==0):\n",
    "                pl = []\n",
    "                t_de=0\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                    t_de+=de_curr\n",
    "                predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "                print(\"dev err:\",t_de/888)\n",
    "                print(total_te/100)\n",
    "                total_te=0\n",
    "                print(a)\n",
    "                print(t)\n",
    "                print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
