{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "8272 888\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()\n",
    "print(len(train_cands),len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 592\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from six.moves.cPickle import load\n",
    "\n",
    "with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "    ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "# ##### Discrete #########\n",
    "\n",
    "# def cand_in_ctd_unspecified(c):\n",
    "#     return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "# def cand_in_ctd_therapy(c):\n",
    "#     return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "# def cand_in_ctd_marker(c):\n",
    "#     return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "# def LF_in_ctd_unspecified(c):\n",
    "#     if(cand_in_ctd_unspecified(c)==1):\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_in_ctd_therapy(c):\n",
    "#     if(cand_in_ctd_therapy(c)==1):\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_in_ctd_marker(c):\n",
    "#     if(cand_in_ctd_marker(c)==1):\n",
    "#         return (1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_tagged_text,\n",
    "#     rule_regex_search_tagged_text,\n",
    "#     rule_regex_search_btw_AB,\n",
    "#     rule_regex_search_btw_BA,\n",
    "#     rule_regex_search_before_A,\n",
    "#     rule_regex_search_before_B,\n",
    "# )\n",
    "\n",
    "# # List to parenthetical\n",
    "# def ltp(x):\n",
    "#     return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# def LF_induce(c):\n",
    "#     return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# causal_past = ['induced', 'caused', 'due']\n",
    "# def LF_d_induced_by_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_d_induced_by_c_tight(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_induce_name(c):\n",
    "#     return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "# causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "# def LF_c_cause_d(c):\n",
    "#     return (1,1) if (\n",
    "#         re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#     ) else (0,0)\n",
    "\n",
    "# treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "# def LF_d_treat_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_c_treat_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_treat_d(c):\n",
    "#     if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_c_treat_d_wide(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_c_d(c):\n",
    "#     return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "# def LF_c_induced_d(c):\n",
    "#     return (1,1) if (\n",
    "#         ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "#         (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "#         ) else (0,0)\n",
    "\n",
    "# def LF_improve_before_disease(c):\n",
    "#     if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# pat_terms = ['in a patient with ', 'in patients with']\n",
    "# def LF_in_patient_with(c):\n",
    "#     return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# uncertain = ['combin', 'possible', 'unlikely']\n",
    "# def LF_uncertain(c):\n",
    "#     if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_induced_other(c):\n",
    "#     if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)  \n",
    "\n",
    "# def LF_far_c_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_far_d_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_risk_d(c):\n",
    "#     if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_develop_d_following_c(c):\n",
    "#     return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# procedure, following = ['inject', 'administrat'], ['following']\n",
    "# def LF_d_following_c(c):\n",
    "#     return (1,1) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_measure(c):\n",
    "#     return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_level(c):\n",
    "#     return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_neg_d(c):\n",
    "#     return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "#                 'seems', 'suggests', 'risk', 'implicated',\n",
    "#                'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "# WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "# def LF_weak_assertions(c):\n",
    "#     return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     l,s = LF_c_d(c)\n",
    "#     cl = cand_in_ctd_marker(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     l1,s1 = LF_c_induced_d(c)\n",
    "#     l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "#     cl = cand_in_ctd_marker(c)\n",
    "#     return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     l,s = LF_c_treat_d_wide(c)\n",
    "#     cl = cand_in_ctd_therapy(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     l,s = LF_c_treat_d_wide(c)\n",
    "#     cl = cand_in_ctd_unspecified(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     l1,s1 = LF_c_induced_d(c)\n",
    "#     l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "#     cl = cand_in_ctd_unspecified(c)\n",
    "#     return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# # def LF_ctd_marker_c_d(c):\n",
    "# #     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# # def LF_ctd_marker_induce(c):\n",
    "# #     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# # def LF_ctd_therapy_treat(c):\n",
    "# #     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# # def LF_ctd_unspecified_treat(c):\n",
    "# #     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# # def LF_ctd_unspecified_induce(c):\n",
    "# #     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_closer_chem(c):\n",
    "#     # Get distance between chemical and disease\n",
    "#     chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "#     dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "#     if dis_start < chem_start:\n",
    "#         dist = chem_start - dis_end\n",
    "#     else:\n",
    "#         dist = dis_start - chem_end\n",
    "#     # Try to find chemical closer than @dist/2 in either direction\n",
    "#     sent = c.get_parent()\n",
    "#     closest_other_chem = float('inf')\n",
    "#     for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "#             return (-1,1)\n",
    "#     for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "#             return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_closer_dis(c):\n",
    "#     # Get distance between chemical and disease\n",
    "#     chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "#     dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "#     if dis_start < chem_start:\n",
    "#         dist = chem_start - dis_end\n",
    "#     else:\n",
    "#         dist = dis_start - chem_end\n",
    "#     # Try to find chemical disease than @dist/8 in either direction\n",
    "#     sent = c.get_parent()\n",
    "#     for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "#             return (-1,1)\n",
    "#     for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "#             return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "# LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "#     LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "#     LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "#     LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "#     LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "#     LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "#     LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "# ]\n",
    "\n",
    "# LF_l = [\n",
    "#     1,1,1,-1,-1,-1,\n",
    "#     -1,1,1,-1,\n",
    "#     -1,1,1,\n",
    "#     1,1,-1,1,\n",
    "#     -1,-1,-1,-1,\n",
    "#     1,-1,1,1,-1,\n",
    "#     -1,-1,-1,1,-1,-1,-1\n",
    "# ]\n",
    "# print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distanceCD(c):\n",
    "    dist = 0\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    return dist/5000\n",
    "\n",
    "\n",
    "def distanceCD_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{A}})(.*)('+w+r')(.*)({{B}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "def distanceDC_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{B}})(.*)('+w+r')(.*)({{A}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "   \n",
    "\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "##### Smooth LFs #########\n",
    "\n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# def LF_induce(c):\n",
    "#     return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,distanceCD_(c,['induc'])) if re.search(r'{{A}}.*induc.*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "# def LF_d_induced_by_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    for w in causal_past:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "\n",
    "# def LF_d_induced_by_c_tight(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.*' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "        return (1,(1-distanceDC_(c,causal_past)))\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "# def LF_c_cause_d(c):\n",
    "#     return (1,1) if (\n",
    "#         re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#     ) else (0,0)\n",
    "\n",
    "\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,(1-distanceCD_(c,causal))) if (\n",
    "        re.search(r'{{A}}.* ' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "# def LF_d_treat_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_d_treat_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1-distanceDC_(c,treat))\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "# def LF_c_treat_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1-distanceCD_(c,treat))\n",
    "    return (0,0)\n",
    "\n",
    "# def LF_treat_d(c):\n",
    "#     if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "    for w in treat:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "\n",
    "# def LF_c_treat_d_wide(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_c_treat_d_wide(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "        return (-1,1-distanceCD_(c,treat))\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "# def LF_improve_before_disease(c):\n",
    "#     if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "def distanceImproveBeforeDisease(c):\n",
    "    m=re.search(r'(improv)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "    if(m):\n",
    "        return len(m.group(2))/5000\n",
    "    return 0\n",
    "\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    if(rule_regex_search_before_B(c, 'improv.*', -1) == -1):\n",
    "        return (-1,1-distanceImproveBeforeDisease(c))\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "# def LF_uncertain(c):\n",
    "#     if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_uncertain(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "    for w in uncertain:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "\n",
    "# def LF_induced_other(c):\n",
    "#     if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)  \n",
    "\n",
    "def LF_induced_other(c):\n",
    "    if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "        return (-1,distanceCD(c))\n",
    "    return (0,0)  \n",
    "\n",
    "# def LF_far_c_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,distanceCD(c))\n",
    "    return (0,0)\n",
    "\n",
    "# def LF_far_d_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,distanceCD(c))\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "# def LF_risk_d(c):\n",
    "#     if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "    sc=max(sc,get_similarity(word_vectors,'risk'))\n",
    "    return (1,sc)\n",
    "\n",
    "\n",
    "# def LF_develop_d_following_c(c):\n",
    "#     return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceDevFol(c):\n",
    "    dist = 0\n",
    "    matchObj = re.search(r'(develop)(.*)({{B}})(.*)(following)(.*)({{A}})', get_tagged_text(c), flags=re.I)\n",
    "    if(matchObj):\n",
    "        match_groups = matchObj.group(2,4,6)\n",
    "        dist = sum([len(mg) for mg in match_groups])\n",
    "    return dist/5000\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1-distanceDevFol(c)) if re.search(r'develop.*{{B}}.*following.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "# def LF_d_following_c(c):\n",
    "#     return (1,distanceDFollC(c)) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1-distanceDC_(c,following)) if re.search('{{B}}.*' + ltp(following) + '.*{{A}}.*' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_measure(c):\n",
    "#     return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceMeasureA(c):\n",
    "    m = re.search('(measur)(.*)({{A}})', get_tagged_text(c), flags=re.I) \n",
    "    if(m):\n",
    "        return (5000-len(m.group(2)))/5000\n",
    "    return 0\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,distanceMeasureA(c)) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_level(c):\n",
    "#     return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceLevel(c):\n",
    "    m = re.search('({{A}})(.*)(level)', get_tagged_text(c), flags=re.I)\n",
    "    if(m):\n",
    "        return (5000-len(m.group(2)))/5000\n",
    "    return 0\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,distanceLevel(c)) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_neg_d(c):\n",
    "#     return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceNeg(c):\n",
    "    m = re.search('(none|not|no)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "    if(m):\n",
    "        return (5000-len(m.group(2)))/5000\n",
    "    return 0\n",
    "\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,distanceNeg(c)) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_therapy(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "    LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "    LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "    LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "    LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "    LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "    LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "]\n",
    "\n",
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 10-5-2018, 15:49:50\n",
      "500data points labelled in 0.1927181283632914 mins\n",
      "500data points labelled in 0.5129977146784465 mins\n",
      "1000data points labelled in 0.6846127947171529 mins\n",
      "1500data points labelled in 0.8712111790974935 mins\n",
      "2000data points labelled in 1.058900495370229 mins\n",
      "2500data points labelled in 1.2317643483479819 mins\n",
      "3000data points labelled in 1.414737331867218 mins\n",
      "3500data points labelled in 1.5898069739341736 mins\n",
      "4000data points labelled in 1.7648872137069702 mins\n",
      "4500data points labelled in 1.980595056215922 mins\n",
      "5000data points labelled in 2.167068350315094 mins\n",
      "5500data points labelled in 2.3528613487879437 mins\n",
      "6000data points labelled in 2.522417092323303 mins\n",
      "6500data points labelled in 2.712505849202474 mins\n",
      "7000data points labelled in 2.896349223454793 mins\n",
      "7500data points labelled in 3.0802456935246787 mins\n",
      "8000data points labelled in 3.269578385353088 mins\n",
      "time taken:  0:03:22.427927\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "# np.save(\"dev_L_S_discrete\",np.array(dev_L_S))\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "# np.save(\"train_L_S_discrete\",np.array(train_L_S))\n",
    "\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "print(\"time taken: \",str(datetime.timedelta(seconds=(time.time() - start_time))))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "#     print(\"thresholds\",thresholds,len(thresholds))\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 2, 33) (8272, 2, 33)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7c06a0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7c06a0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7c06a0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "ints <function ints at 0x7fb31ef9fd08>\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 157663.4239712699\n",
      "[0.30942666 0.00824841 0.20416342 0.2088081  0.31456134 0.24688082\n",
      " 0.37693811 0.22589902 0.3413499  0.27290705 0.19175437 0.26133249\n",
      " 0.17425764 0.275199   0.23269167 0.25041993 0.04532646 0.12682676\n",
      " 0.26670032 0.04457243 0.24017989 0.13576066 0.20437571 0.35800374\n",
      " 0.06803031 0.3845174  0.27611913 0.27659584 0.26541891 0.1867712\n",
      " 0.02721688 0.04357527 0.16890286]\n",
      "[[1.12558178 0.81835146 1.01314035 0.99243529 1.09819158 1.0376181\n",
      "  1.16219466 1.03485924 1.14974841 1.05653575 0.97538132 1.0700974\n",
      "  0.99042387 1.08110362 1.04885026 1.03404799 0.86063543 0.91252557\n",
      "  1.05179797 0.82828909 1.03263866 0.94619926 0.98809149 1.16494719\n",
      "  0.87768373 1.16946665 1.06030222 1.06057561 1.0499189  0.99310821\n",
      "  0.81733318 0.83367959 0.95694783]]\n",
      "{0: 462, 1: 426}\n",
      "(0.49765258215962443, 0.7162162162162162, 0.5872576177285319, None)\n",
      "\n",
      "1 loss 156894.22254245274\n",
      "[0.30142829 0.00272017 0.19923649 0.21695128 0.32270435 0.25204155\n",
      " 0.38453637 0.22105633 0.33679828 0.28105035 0.19989741 0.25653566\n",
      " 0.16624505 0.27190321 0.22468664 0.25856327 0.0374995  0.13342627\n",
      " 0.27356678 0.05268134 0.24461363 0.13023032 0.21248953 0.35352204\n",
      " 0.06272329 0.39123681 0.2840807  0.28465027 0.27326811 0.18331479\n",
      " 0.0323024  0.04863578 0.1754156 ]\n",
      "[[1.13336926 0.82207163 1.01632627 0.98428246 1.0900422  1.03419961\n",
      "  1.15539569 1.03813993 1.15282391 1.04838475 0.96722813 1.07316553\n",
      "  0.99823099 1.0835813  1.05664299 1.02589626 0.86711304 0.90529444\n",
      "  1.04405444 0.82021926 1.03013934 0.95092561 0.98002114 1.16674845\n",
      "  0.88129196 1.16154334 1.05264395 1.0526695  1.04251695 0.99578787\n",
      "  0.8127622  0.82906882 0.95201931]]\n",
      "{0: 447, 1: 441}\n",
      "(0.4943310657596372, 0.7364864864864865, 0.5915875169606513, None)\n",
      "\n",
      "2 loss 156227.80695221535\n",
      "[ 0.29359138 -0.00222996  0.19495301  0.22507134  0.33082457  0.256838\n",
      "  0.39198696  0.21680693  0.33280838  0.28917079  0.20801723  0.25239008\n",
      "  0.15837545  0.26871537  0.21683392  0.26668367  0.02994928  0.13973202\n",
      "  0.2801559   0.06075738  0.24866725  0.12487634  0.22057245  0.34954152\n",
      "  0.05798053  0.39768786  0.29197421  0.29265806  0.28101204  0.17999872\n",
      "  0.03723673  0.05354806  0.18161552]\n",
      "[[1.14042618 0.82529634 1.01902339 0.97614822 1.08191142 1.03104504\n",
      "  1.14883722 1.04098739 1.15548351 1.04025229 0.95909357 1.07572054\n",
      "  1.00541651 1.08600959 1.06374283 1.01776307 0.87199484 0.89823267\n",
      "  1.03639745 0.81218776 1.02786436 0.9555997  0.97198797 1.16854522\n",
      "  0.88445655 1.15366732 1.04510287 1.0448372  1.03529655 0.9983666\n",
      "  0.80827714 0.82453969 0.94742203]]\n",
      "{0: 416, 1: 472}\n",
      "(0.4936440677966102, 0.7871621621621622, 0.6067708333333334, None)\n",
      "\n",
      "3 loss 155665.54379628124\n",
      "[ 0.28611206 -0.00644186  0.1914655   0.23317367  0.33892739  0.2612119\n",
      "  0.39924034  0.2132769   0.32948183  0.29727377  0.21611921  0.24904073\n",
      "  0.15079277  0.26565734  0.2093018   0.27478654  0.02303516  0.14566957\n",
      "  0.28638236  0.06880146  0.25228959  0.11973157  0.2286256   0.34614746\n",
      "  0.05394084  0.40381143  0.29978279  0.30061217  0.28861651  0.17684744\n",
      "  0.04200277  0.05829544  0.18741978]\n",
      "[[1.13535216 0.82796603 1.02118182 0.96802696 1.07379367 1.02817393\n",
      "  1.14258004 1.04335991 1.15768879 1.03213278 0.95097202 1.07771034\n",
      "  1.00175662 1.08837081 1.05910929 1.00964282 0.87326623 0.89138442\n",
      "  1.02884971 0.80419703 1.02582915 0.9602046  0.96399497 1.17033724\n",
      "  0.88712843 1.14584468 1.03771105 1.03709668 1.02830797 1.00082398\n",
      "  0.80388954 0.82010342 0.9432008 ]]\n",
      "{0: 387, 1: 501}\n",
      "(0.48303393213572854, 0.8175675675675675, 0.6072772898368883, None)\n",
      "\n",
      "4 loss 155195.1995755015\n",
      "[ 0.28260082 -0.00980907  0.18885381  0.24126365  0.3470182   0.26514542\n",
      "  0.40624176  0.21052614  0.3268563   0.30536467  0.22420875  0.24655619\n",
      "  0.14474431  0.26272146  0.2042218   0.28287727  0.01802306  0.15118396\n",
      "  0.2921721   0.07681289  0.25547689  0.11480185  0.23664871  0.34336958\n",
      "  0.05068467  0.4095658   0.3074844   0.30850153  0.29603882  0.17385299\n",
      "  0.04660927  0.06288693  0.19276713]\n",
      "[[1.12616699 0.8300766  1.02280658 0.95991315 1.06568348 1.02557449\n",
      "  1.13667117 1.04526282 1.15944657 1.02402074 0.94285795 1.07914153\n",
      "  0.99247317 1.09067253 1.04989314 1.00153002 0.86998082 0.88478664\n",
      "  1.02143401 0.79625117 1.02401747 0.96474178 0.95604658 1.17212473\n",
      "  0.88930871 1.13808063 1.03050077 1.02946933 1.02159848 1.00316963\n",
      "  0.79958985 0.81575061 0.93936341]]\n",
      "{0: 361, 1: 527}\n",
      "(0.4686907020872865, 0.8344594594594594, 0.600243013365735, None)\n",
      "\n",
      "[ 0.28260082 -0.00980907  0.18885381  0.24126365  0.3470182   0.26514542\n",
      "  0.40624176  0.21052614  0.3268563   0.30536467  0.22420875  0.24655619\n",
      "  0.14474431  0.26272146  0.2042218   0.28287727  0.01802306  0.15118396\n",
      "  0.2921721   0.07681289  0.25547689  0.11480185  0.23664871  0.34336958\n",
      "  0.05068467  0.4095658   0.3074844   0.30850153  0.29603882  0.17385299\n",
      "  0.04660927  0.06288693  0.19276713]\n",
      "[[1.12616699 0.8300766  1.02280658 0.95991315 1.06568348 1.02557449\n",
      "  1.13667117 1.04526282 1.15944657 1.02402074 0.94285795 1.07914153\n",
      "  0.99247317 1.09067253 1.04989314 1.00153002 0.86998082 0.88478664\n",
      "  1.02143401 0.79625117 1.02401747 0.96474178 0.95604658 1.17212473\n",
      "  0.88930871 1.13808063 1.03050077 1.02946933 1.02159848 1.00316963\n",
      "  0.79958985 0.81575061 0.93936341]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.834\n",
      "Neg. class accuracy: 0.527\n",
      "Precision            0.469\n",
      "Recall               0.834\n",
      "F1                   0.6\n",
      "----------------------------------------\n",
      "TP: 247 | FP: 280 | TN: 312 | FN: 49\n",
      "========================================\n",
      "\n",
      "{0: 361, 1: 527}\n",
      "acc 0.6295045045045045\n",
      "(array([0.86426593, 0.4686907 ]), array([0.52702703, 0.83445946]), array([0.6547744 , 0.60024301]), array([592, 296]))\n",
      "(0.6664783150325629, 0.6807432432432432, 0.6275087050039587, None)\n",
      "[[312 280]\n",
      " [ 49 247]]\n",
      "prec: tp/(tp+fp) 0.4686907020872865 recall: tp/(tp+fn) 0.8344594594594594\n",
      "(0.4686907020872865, 0.8344594594594594, 0.600243013365735, None)\n"
     ]
    }
   ],
   "source": [
    "## smooth LFs \n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: ls_*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    def ints(y):\n",
    "        return alphas+((tf.exp((t_k*y)*(1-alphas))-1)/(t_k*y))\n",
    "    \n",
    "    print(\"ints\",ints)\n",
    "    \n",
    "#     zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                   np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.000001).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ea1d080>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ea1d080>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ea1d080>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "ints <function ints at 0x7fb31e303048>\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 157663.4239712699\n",
      "[0.30942666 0.00824841 0.20416342 0.2088081  0.31456134 0.24688082\n",
      " 0.37693811 0.22589902 0.3413499  0.27290705 0.19175437 0.26133249\n",
      " 0.17425764 0.275199   0.23269167 0.25041993 0.04532646 0.12682676\n",
      " 0.26670032 0.04457243 0.24017989 0.13576066 0.20437571 0.35800374\n",
      " 0.06803031 0.3845174  0.27611913 0.27659584 0.26541891 0.1867712\n",
      " 0.02721688 0.04357527 0.16890286]\n",
      "[[1.12558178 0.81835146 1.01314035 0.99243529 1.09819158 1.0376181\n",
      "  1.16219466 1.03485924 1.14974841 1.05653575 0.97538132 1.0700974\n",
      "  0.99042387 1.08110362 1.04885026 1.03404799 0.86063543 0.91252557\n",
      "  1.05179797 0.82828909 1.03263866 0.94619926 0.98809149 1.16494719\n",
      "  0.87768373 1.16946665 1.06030222 1.06057561 1.0499189  0.99310821\n",
      "  0.81733318 0.83367959 0.95694783]]\n",
      "{0: 462, 1: 426}\n",
      "(0.49765258215962443, 0.7162162162162162, 0.5872576177285319, None)\n",
      "\n",
      "1 loss 156894.22254245274\n",
      "[0.30142829 0.00272017 0.19923649 0.21695128 0.32270435 0.25204155\n",
      " 0.38453637 0.22105633 0.33679828 0.28105035 0.19989741 0.25653566\n",
      " 0.16624505 0.27190321 0.22468664 0.25856327 0.0374995  0.13342627\n",
      " 0.27356678 0.05268134 0.24461363 0.13023032 0.21248953 0.35352204\n",
      " 0.06272329 0.39123681 0.2840807  0.28465027 0.27326811 0.18331479\n",
      " 0.0323024  0.04863578 0.1754156 ]\n",
      "[[1.13336926 0.82207163 1.01632627 0.98428246 1.0900422  1.03419961\n",
      "  1.15539569 1.03813993 1.15282391 1.04838475 0.96722813 1.07316553\n",
      "  0.99823099 1.0835813  1.05664299 1.02589626 0.86711304 0.90529444\n",
      "  1.04405444 0.82021926 1.03013934 0.95092561 0.98002114 1.16674845\n",
      "  0.88129196 1.16154334 1.05264395 1.0526695  1.04251695 0.99578787\n",
      "  0.8127622  0.82906882 0.95201931]]\n",
      "{0: 447, 1: 441}\n",
      "(0.4943310657596372, 0.7364864864864865, 0.5915875169606513, None)\n",
      "\n",
      "2 loss 156227.80695221535\n",
      "[ 0.29359138 -0.00222996  0.19495301  0.22507134  0.33082457  0.256838\n",
      "  0.39198696  0.21680693  0.33280838  0.28917079  0.20801723  0.25239008\n",
      "  0.15837545  0.26871537  0.21683392  0.26668367  0.02994928  0.13973202\n",
      "  0.2801559   0.06075738  0.24866725  0.12487634  0.22057245  0.34954152\n",
      "  0.05798053  0.39768786  0.29197421  0.29265806  0.28101204  0.17999872\n",
      "  0.03723673  0.05354806  0.18161552]\n",
      "[[1.14042618 0.82529634 1.01902339 0.97614822 1.08191142 1.03104504\n",
      "  1.14883722 1.04098739 1.15548351 1.04025229 0.95909357 1.07572054\n",
      "  1.00541651 1.08600959 1.06374283 1.01776307 0.87199484 0.89823267\n",
      "  1.03639745 0.81218776 1.02786436 0.9555997  0.97198797 1.16854522\n",
      "  0.88445655 1.15366732 1.04510287 1.0448372  1.03529655 0.9983666\n",
      "  0.80827714 0.82453969 0.94742203]]\n",
      "{0: 416, 1: 472}\n",
      "(0.4936440677966102, 0.7871621621621622, 0.6067708333333334, None)\n",
      "\n",
      "3 loss 155665.54379628124\n",
      "[ 0.28611206 -0.00644186  0.1914655   0.23317367  0.33892739  0.2612119\n",
      "  0.39924034  0.2132769   0.32948183  0.29727377  0.21611921  0.24904073\n",
      "  0.15079277  0.26565734  0.2093018   0.27478654  0.02303516  0.14566957\n",
      "  0.28638236  0.06880146  0.25228959  0.11973157  0.2286256   0.34614746\n",
      "  0.05394084  0.40381143  0.29978279  0.30061217  0.28861651  0.17684744\n",
      "  0.04200277  0.05829544  0.18741978]\n",
      "[[1.13535216 0.82796603 1.02118182 0.96802696 1.07379367 1.02817393\n",
      "  1.14258004 1.04335991 1.15768879 1.03213278 0.95097202 1.07771034\n",
      "  1.00175662 1.08837081 1.05910929 1.00964282 0.87326623 0.89138442\n",
      "  1.02884971 0.80419703 1.02582915 0.9602046  0.96399497 1.17033724\n",
      "  0.88712843 1.14584468 1.03771105 1.03709668 1.02830797 1.00082398\n",
      "  0.80388954 0.82010342 0.9432008 ]]\n",
      "{0: 387, 1: 501}\n",
      "(0.48303393213572854, 0.8175675675675675, 0.6072772898368883, None)\n",
      "\n",
      "4 loss 155195.1995755015\n",
      "[ 0.28260082 -0.00980907  0.18885381  0.24126365  0.3470182   0.26514542\n",
      "  0.40624176  0.21052614  0.3268563   0.30536467  0.22420875  0.24655619\n",
      "  0.14474431  0.26272146  0.2042218   0.28287727  0.01802306  0.15118396\n",
      "  0.2921721   0.07681289  0.25547689  0.11480185  0.23664871  0.34336958\n",
      "  0.05068467  0.4095658   0.3074844   0.30850153  0.29603882  0.17385299\n",
      "  0.04660927  0.06288693  0.19276713]\n",
      "[[1.12616699 0.8300766  1.02280658 0.95991315 1.06568348 1.02557449\n",
      "  1.13667117 1.04526282 1.15944657 1.02402074 0.94285795 1.07914153\n",
      "  0.99247317 1.09067253 1.04989314 1.00153002 0.86998082 0.88478664\n",
      "  1.02143401 0.79625117 1.02401747 0.96474178 0.95604658 1.17212473\n",
      "  0.88930871 1.13808063 1.03050077 1.02946933 1.02159848 1.00316963\n",
      "  0.79958985 0.81575061 0.93936341]]\n",
      "{0: 361, 1: 527}\n",
      "(0.4686907020872865, 0.8344594594594594, 0.600243013365735, None)\n",
      "\n",
      "5 loss 154795.48735649546\n",
      "[ 0.29246049 -0.01237731  0.18704122  0.24937577  0.35513086  0.2686956\n",
      "  0.41297993  0.20848039  0.32485409  0.31347755  0.23232045  0.24485167\n",
      "  0.15345443  0.25985841  0.21396721  0.29099003  0.01782775  0.15629372\n",
      "  0.29752738  0.08482308  0.25830302  0.11004289  0.24467068  0.34114078\n",
      "  0.04815166  0.41496729  0.31508968  0.31634312  0.30326942  0.17096097\n",
      "  0.05110547  0.06737136  0.19767836]\n",
      "[[1.11757951 0.83170728 1.02397938 0.95177882 1.05755303 1.023192\n",
      "  1.13110107 1.04676775 1.16082607 1.01588833 0.93472331 1.08010159\n",
      "  0.98385059 1.09295577 1.04129179 0.99339679 0.86441503 0.87842838\n",
      "  1.01413926 0.78832736 1.02237168 0.9692401  0.94812283 1.17390841\n",
      "  0.89107234 1.13035347 1.02346596 1.02194945 1.01517117 1.00545403\n",
      "  0.79533822 0.81144228 0.9358535 ]]\n",
      "{0: 330, 1: 558}\n",
      "(0.4444444444444444, 0.8378378378378378, 0.5807962529274004, None)\n",
      "\n",
      "6 loss 154438.34153010175\n",
      "[ 0.3012858  -0.0142241   0.18592444  0.25748248  0.3632383   0.27192056\n",
      "  0.41941143  0.20704884  0.32338771  0.3215852   0.24042666  0.24381909\n",
      "  0.1624107   0.25702554  0.22284843  0.29909752  0.02256662  0.16101654\n",
      "  0.30245023  0.09279672  0.26083861  0.10541964  0.25265827  0.3393858\n",
      "  0.0462609   0.42002925  0.32255326  0.32409601  0.31025536  0.16812401\n",
      "  0.05553162  0.07178868  0.20217625]\n",
      "[[1.1091041  0.83293522 1.02477776 0.94364618 1.04942444 1.02097734\n",
      "  1.12586665 1.04794134 1.16189146 1.00775767 0.92659036 1.0806732\n",
      "  0.97536001 1.09525638 1.03280978 0.98526528 0.85792951 0.87231616\n",
      "  1.00698842 0.78045433 1.02084143 0.97372136 0.94025242 1.17568901\n",
      "  0.89248918 1.1226802  1.01663467 1.01457037 1.00904513 1.00772109\n",
      "  0.79110246 0.80714702 0.93261811]]\n",
      "{0: 314, 1: 574}\n",
      "(0.43902439024390244, 0.8513513513513513, 0.5793103448275863, None)\n",
      "\n",
      "7 loss 154113.5728357942\n",
      "[ 0.30987358 -0.0153621   0.18547188  0.26557829  0.37133517  0.27483215\n",
      "  0.4254728   0.20620655  0.32243225  0.32968226  0.24852183  0.2434233\n",
      "  0.17104407  0.25421233  0.23145681  0.30719435  0.02928827  0.16533628\n",
      "  0.30690747  0.10071525  0.26310309  0.10092697  0.26059384  0.33808638\n",
      "  0.0449957   0.42473981  0.32982732  0.33172459  0.31693293  0.16533015\n",
      "  0.05990045  0.07615153  0.2062452 ]\n",
      "[[1.10068651 0.83378507 1.02522834 0.93551871 1.04130121 1.01891524\n",
      "  1.12099057 1.04880547 1.16266361 0.99963224 0.91846262 1.08088537\n",
      "  0.96693127 1.09758281 1.02438703 0.97713895 0.85095501 0.8664747\n",
      "  1.00001101 0.77264861 1.01941063 0.97818869 0.93245373 1.17746675\n",
      "  0.89358121 1.11507236 1.01004354 1.00736677 1.00325294 1.00998202\n",
      "  0.78687284 0.80285541 0.92964381]]\n",
      "{0: 293, 1: 595}\n",
      "(0.4319327731092437, 0.8682432432432432, 0.5768799102132436, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss 153815.5251421055\n",
      "[ 0.31835586 -0.01582679  0.18563496  0.27366109  0.3794195   0.27744689\n",
      "  0.4311001   0.20591543  0.32195355  0.33776674  0.25660381  0.24361447\n",
      "  0.17954996  0.2514077   0.2399501   0.31527849  0.03670726  0.1692465\n",
      "  0.31087821  0.10855665  0.26511887  0.09655755  0.26845681  0.33721666\n",
      "  0.04432247  0.42909454  0.33685382  0.3391837   0.32323345  0.16256673\n",
      "  0.06422554  0.08047334  0.20988056]\n",
      "[[1.09230708 0.83428544 1.02536049 0.92739683 1.03318379 1.01698996\n",
      "  1.11648732 1.04938442 1.16316546 0.99151244 0.91034048 1.08076977\n",
      "  0.95854277 1.09994377 1.01600314 0.96901821 0.84369622 0.86092448\n",
      "  0.99323782 0.76492767 1.01806321 0.9826458  0.9247465  1.17924186\n",
      "  0.89437332 1.10754085 1.0037289  1.00037647 0.99782008 1.01224837\n",
      "  0.78263939 0.79855778 0.92691383]]\n",
      "{0: 272, 1: 616}\n",
      "(0.42045454545454547, 0.875, 0.5679824561403509, None)\n",
      "\n",
      "9 loss 153539.41592051525\n",
      "[ 0.32677757 -0.01567138  0.18635365  0.28172847  0.38748903  0.27978531\n",
      "  0.43623222  0.20612774  0.32191072  0.34583637  0.2646701   0.24433327\n",
      "  0.18798641  0.24859994  0.24837888  0.32334766  0.04442048  0.17275051\n",
      "  0.3143556   0.11628708  0.26691095  0.09230214  0.27621641  0.33674478\n",
      "  0.04419459  0.43309629  0.34355909  0.34641106  0.32908535  0.15982027\n",
      "  0.068521    0.0847681   0.21308873]\n",
      "[[1.08395506 0.83446781 1.02520537 0.91928069 1.02507238 1.01518531\n",
      "  1.11236247 1.04970415 1.16342146 0.98339845 0.90222411 1.08035966\n",
      "  0.95018285 1.10234834 1.00764708 0.96090323 0.8362608  0.85568126\n",
      "  0.98670203 0.75731358 1.01678295 0.98709694 0.91715549 1.18101456\n",
      "  0.89489265 1.10009843 0.99772696 0.99364166 0.99276363 1.0145322\n",
      "  0.77839198 0.79424433 0.9244087 ]]\n",
      "{0: 259, 1: 629}\n",
      "(0.4133545310015898, 0.8783783783783784, 0.5621621621621622, None)\n",
      "\n",
      "[ 0.32677757 -0.01567138  0.18635365  0.28172847  0.38748903  0.27978531\n",
      "  0.43623222  0.20612774  0.32191072  0.34583637  0.2646701   0.24433327\n",
      "  0.18798641  0.24859994  0.24837888  0.32334766  0.04442048  0.17275051\n",
      "  0.3143556   0.11628708  0.26691095  0.09230214  0.27621641  0.33674478\n",
      "  0.04419459  0.43309629  0.34355909  0.34641106  0.32908535  0.15982027\n",
      "  0.068521    0.0847681   0.21308873]\n",
      "[[1.08395506 0.83446781 1.02520537 0.91928069 1.02507238 1.01518531\n",
      "  1.11236247 1.04970415 1.16342146 0.98339845 0.90222411 1.08035966\n",
      "  0.95018285 1.10234834 1.00764708 0.96090323 0.8362608  0.85568126\n",
      "  0.98670203 0.75731358 1.01678295 0.98709694 0.91715549 1.18101456\n",
      "  0.89489265 1.10009843 0.99772696 0.99364166 0.99276363 1.0145322\n",
      "  0.77839198 0.79424433 0.9244087 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.878\n",
      "Neg. class accuracy: 0.377\n",
      "Precision            0.413\n",
      "Recall               0.878\n",
      "F1                   0.562\n",
      "----------------------------------------\n",
      "TP: 260 | FP: 369 | TN: 223 | FN: 36\n",
      "========================================\n",
      "\n",
      "{0: 259, 1: 629}\n",
      "acc 0.543918918918919\n",
      "(array([0.86100386, 0.41335453]), array([0.37668919, 0.87837838]), array([0.52408931, 0.56216216]), array([592, 296]))\n",
      "(0.6371791960027253, 0.6275337837837838, 0.5431257344300823, None)\n",
      "[[223 369]\n",
      " [ 36 260]]\n",
      "prec: tp/(tp+fp) 0.4133545310015898 recall: tp/(tp+fn) 0.8783783783783784\n",
      "(0.4133545310015898, 0.8783783783783784, 0.5621621621621622, None)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## smooth LFs # other configs\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: ls_*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    def ints(y):\n",
    "        return alphas+((tf.exp((t_k*y)*(1-alphas))-1)/(t_k*y))\n",
    "    \n",
    "    print(\"ints\",ints)\n",
    "    \n",
    "#     zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                   np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.000001).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(10):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            drawPRcurve(np.array(gold_labels_dev),np.array(m[1::].flatten()),it)\n",
    "            print()\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "        \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "        \n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb382a5efd0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 184222.4562227816\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.27082211 -0.01928787 -0.14063245  0.37856253  0.43681819 -0.15844807\n",
      "   0.13280198 -0.01935702 -0.10775934  0.34390113  0.39762823 -0.14286955\n",
      "  -0.39588527 -0.33699178 -0.37821404  0.38378715 -0.39537146  0.11504936\n",
      "   0.21906794  0.39699417 -0.27113816  8.13838832  0.39548336 -0.31328908\n",
      "   0.25503373  0.28019293  0.39734506  0.39700564  0.37866251 -0.39156514\n",
      "   0.38002959  0.21917987  0.13605525]]\n",
      "{0: 253, 1: 635}\n",
      "(0.462992125984252, 0.9932432432432432, 0.6315789473684211, None)\n",
      "\n",
      "1 loss 182926.8350570609\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.32930229 -0.072816   -0.19433839  0.43560275  0.51398692 -0.0365946\n",
      "   0.19811178 -0.0729731  -0.16134781  0.40620045  0.46910414 -0.1965869\n",
      "  -0.45399143 -0.39169289 -0.43424009  0.43969335 -0.45314662  0.20099496\n",
      "   0.27412612  0.45449307 -0.13677994 10.2419757   0.45333961 -0.37154561\n",
      "   0.1955587   0.33629475  0.4545094   0.45440313  0.43567656 -0.4480074\n",
      "   0.43491147  0.28277911  0.20811263]]\n",
      "{0: 290, 1: 598}\n",
      "(0.4916387959866221, 0.9932432432432432, 0.6577181208053691, None)\n",
      "\n",
      "2 loss 182800.53538895075\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.50033750e-01 -9.20612978e-02 -2.13641010e-01  4.56135636e-01\n",
      "   5.39595897e-01  1.96766814e-03  2.20784600e-01 -9.22805491e-02\n",
      "  -1.80658821e-01  4.27914802e-01  4.93027965e-01 -2.15923034e-01\n",
      "  -4.74703809e-01 -4.11355405e-01 -4.54355973e-01  4.59505852e-01\n",
      "  -4.73775237e-01  2.29612258e-01  2.93641733e-01  4.75037473e-01\n",
      "  -9.49920742e-02  1.09749630e+01  4.73964547e-01 -3.92341571e-01\n",
      "   1.74156761e-01  3.56763707e-01  4.74947673e-01  4.74935290e-01\n",
      "   4.56160538e-01 -4.68232180e-01  4.54509106e-01  3.04807646e-01\n",
      "   2.33081953e-01]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "3 loss 182779.13781656895\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35716556 -0.09869963 -0.22030067  0.46322675  0.54827254  0.01475931\n",
      "   0.2285349  -0.09894358 -0.18732523  0.43535596  0.50115104 -0.22259733\n",
      "  -0.48184533 -0.41814551 -0.46130134  0.46632557 -0.48089044  0.2392675\n",
      "   0.30036809  0.48212319 -0.08115096 11.22596956  0.48107634 -0.39950962\n",
      "   0.16677415  0.36384845  0.48199793  0.48202006  0.46323675 -0.47521353\n",
      "   0.46127745  0.31235554  0.24160462]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "4 loss 182774.49158656417\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.512\n",
      "Precision            0.504\n",
      "Recall               0.993\n",
      "F1                   0.669\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 289 | TN: 303 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 305, 1: 583}\n",
      "acc 0.6722972972972973\n",
      "(array([0.99344262, 0.50428816]), array([0.51182432, 0.99324324]), array([0.67558528, 0.66894198]), array([592, 296]))\n",
      "(0.7488653938081714, 0.7525337837837838, 0.6722636319015605, None)\n",
      "[[303 289]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5042881646655232 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n"
     ]
    }
   ],
   "source": [
    "## class wise reproduced using map over y\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,l = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "z-weight: 0.0\n",
      "0 loss -227408.91347570377\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 14.67744856  25.90583748  22.69587475  12.78045065  14.39716461\n",
      "    0.44035997  -3.2445657   25.08846134  24.96396903  13.96286426\n",
      "   11.82652493  22.46522151   1.79888645  16.7243985   11.77938823\n",
      "   -3.43412061   2.63100655   1.29551022  -6.77434535  -0.59566388\n",
      "  -10.15505727  61.98589432   1.58045045  12.01429343  28.43790004\n",
      "    3.19948727   1.48865339  -1.91094183  -4.65161603   5.80171757\n",
      "   -2.764758     1.47742811 -15.12888705]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 1.0\n",
      "Neg. class accuracy: 0.314\n",
      "Precision            0.422\n",
      "Recall               1.0\n",
      "F1                   0.593\n",
      "----------------------------------------\n",
      "TP: 296 | FP: 406 | TN: 186 | FN: 0\n",
      "========================================\n",
      "\n",
      "{0: 186, 1: 702}\n",
      "acc 0.5427927927927928\n",
      "(array([1.        , 0.42165242]), array([0.31418919, 1.        ]), array([0.4781491 , 0.59318637]), array([592, 296]))\n",
      "(0.7108262108262109, 0.6570945945945946, 0.5356677365012802, None)\n",
      "[[186 406]\n",
      " [  0 296]]\n",
      "prec: tp/(tp+fp) 0.42165242165242167 recall: tp/(tp+fn) 1.0\n",
      "(0.42165242165242167, 1.0, 0.5931863727454909, None)\n",
      "\n",
      "z-weight: 0.1111111111111111\n",
      "0 loss -119393.34333310764\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ -1.55495507   1.81238967   0.4215572    4.86093815   5.33593905\n",
      "   -0.46940592   2.37181565   1.35127544   1.1895527    4.98889206\n",
      "    4.10979856   0.44686917  -5.92391325  -0.82443449  -1.64468996\n",
      "    1.72211265  -4.3741264    1.19145527   0.69531406   3.82908758\n",
      "  -13.59053119  59.58797677   4.37463643  -1.68991432   5.96233837\n",
      "    2.07511514   3.84541269   3.50532919   2.59915108  -2.75507519\n",
      "    2.16501225   3.62028343   0.37233849]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.402\n",
      "Precision            0.454\n",
      "Recall               0.993\n",
      "F1                   0.623\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 354 | TN: 238 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 240, 1: 648}\n",
      "acc 0.5990990990990991\n",
      "(array([0.99166667, 0.4537037 ]), array([0.40202703, 0.99324324]), array([0.57211538, 0.62288136]), array([592, 296]))\n",
      "(0.7226851851851852, 0.6976351351351351, 0.597498370273794, None)\n",
      "[[238 354]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.4537037037037037 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.4537037037037037, 0.9932432432432432, 0.6228813559322034, None)\n",
      "\n",
      "z-weight: 0.2222222222222222\n",
      "0 loss -67297.10695909889\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-2.12712516e+00 -3.32614018e-01 -8.27306783e-01  2.23820234e+00\n",
      "   1.60257984e+00  2.53483915e-01  2.88554944e+00 -2.73214892e-01\n",
      "  -4.66232366e-01  1.34056882e+00  1.64109754e+00 -7.43948404e-01\n",
      "  -4.40931548e+00 -1.59032385e+00 -2.19509216e+00  2.26128927e+00\n",
      "  -3.79266438e+00  1.13626702e+00  1.42608194e+00  3.30397500e+00\n",
      "  -1.13641614e+01  5.41774757e+01  3.84822520e+00 -2.14689008e+00\n",
      "   2.15987198e-02  2.12651687e+00  3.32665252e+00  3.32779256e+00\n",
      "   2.81169826e+00 -2.70706992e+00  2.42245597e+00  2.14348796e+00\n",
      "   1.45342684e+00]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.427\n",
      "Precision            0.464\n",
      "Recall               0.993\n",
      "F1                   0.633\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 339 | TN: 253 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 255, 1: 633}\n",
      "acc 0.615990990990991\n",
      "(array([0.99215686, 0.46445498]), array([0.42736486, 0.99324324]), array([0.5974026 , 0.63293864]), array([592, 296]))\n",
      "(0.7283059195242079, 0.7103040540540541, 0.6151706205527518, None)\n",
      "[[253 339]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.46445497630331756 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.46445497630331756, 0.9932432432432432, 0.6329386437029064, None)\n",
      "\n",
      "z-weight: 0.3333333333333333\n",
      "0 loss -20898.4966657875\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.39654331 -0.32982647 -0.68065499  2.02672178  2.49695948  1.5465588\n",
      "   1.55228355 -0.30114009 -0.47509532  2.27421879  2.27522007 -0.64276916\n",
      "  -1.9909666  -1.23034186 -1.4960202   1.63290691 -1.91311068  1.90215617\n",
      "   1.29233145  1.79939409  0.37015213 49.01407705  1.88915495 -1.46360602\n",
      "  -0.11304114  1.89103796  1.80147403  1.82279554  1.78941021 -1.55785983\n",
      "   1.64268534  1.82300258  1.44669285]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.4444444444444444\n",
      "0 loss 21366.36183458681\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.21039898 -0.43583024 -0.71446238  1.62628005  2.01277839  1.30526789\n",
      "   1.26968363 -0.42178058 -0.57532032  1.7759869   1.85932575 -0.69572398\n",
      "  -1.65267706 -1.1604753  -1.34837724  1.41200589 -1.62326002  1.54509986\n",
      "   1.12102022  1.54245021  0.86974974 42.66083782  1.61142022 -1.29425214\n",
      "  -0.23707812  1.51451552  1.52229232  1.55548205  1.53035949 -1.39273037\n",
      "   1.40881072  1.45936542  1.25183945]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.5555555555555556\n",
      "0 loss 60755.62596127175\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.02085992 -0.43466938 -0.66139027  1.31188971  1.62509742  1.01080258\n",
      "   1.01674327 -0.4293375  -0.56422908  1.39456541  1.50020989 -0.65374794\n",
      "  -1.35917832 -1.03111118 -1.16824761  1.19302404 -1.34496595  1.21490643\n",
      "   0.92983185  1.29825564  0.75445451 35.57630169  1.34091977 -1.10682104\n",
      "  -0.21598728  1.19624718  1.27694178  1.30661787  1.27389902 -1.20651613\n",
      "   1.19174522  1.16412989  1.0257656 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 96689.88058612194\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.82561982 -0.36723542 -0.55485411  1.03417808  1.27583029  0.71717905\n",
      "   0.77682786 -0.36610815 -0.4850003   1.0696494   1.17258026 -0.55315662\n",
      "  -1.07879907 -0.86485836 -0.96553447  0.97462023 -1.0725238   0.90759641\n",
      "   0.73696937  1.05145869  0.54532653 28.29318095  1.07189892 -0.90287243\n",
      "  -0.12768098  0.91690854  1.0383057   1.05647057  1.02109501 -0.99756409\n",
      "   0.97302553  0.89269963  0.79117205]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.7777777777777777\n",
      "0 loss 129091.99006880184\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-6.31714466e-01 -2.62813907e-01 -4.21292218e-01  7.87840280e-01\n",
      "   9.65961780e-01  4.31942819e-01  5.49566565e-01 -2.63051686e-01\n",
      "  -3.69013368e-01  7.91216636e-01  8.83652633e-01 -4.22286759e-01\n",
      "  -8.22743005e-01 -6.83976872e-01 -7.58142021e-01  7.63739239e-01\n",
      "  -8.19994964e-01  6.25106963e-01  5.50305355e-01  8.13307816e-01\n",
      "   2.98833345e-01  2.11678770e+01  8.20120283e-01 -6.96067139e-01\n",
      "  -6.29539821e-03  6.73365598e-01  8.07851513e-01  8.15588317e-01\n",
      "   7.83632999e-01 -7.83295199e-01  7.59563950e-01  6.44231646e-01\n",
      "   5.61002120e-01]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.598\n",
      "Precision            0.553\n",
      "Recall               0.993\n",
      "F1                   0.71\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 238 | TN: 354 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 356, 1: 532}\n",
      "acc 0.7297297297297297\n",
      "(array([0.99438202, 0.55263158]), array([0.59797297, 0.99324324]), array([0.74683544, 0.71014493]), array([592, 296]))\n",
      "(0.7735068007096393, 0.7956081081081081, 0.7284901852871033, None)\n",
      "[[354 238]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5526315789473685 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5526315789473685, 0.9932432432432432, 0.7101449275362319, None)\n",
      "\n",
      "z-weight: 0.8888888888888888\n",
      "0 loss 158160.98403968994\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.44480025 -0.14112093 -0.27827197  0.56940597  0.69191377  0.14765073\n",
      "   0.33466023 -0.14136785 -0.23734191  0.55131697  0.63060267 -0.28015566\n",
      "  -0.59473098 -0.50377587 -0.55889202  0.56530252 -0.59334806  0.36438264\n",
      "   0.37588674  0.59301109  0.02948187 14.40619354  0.59354635 -0.49639742\n",
      "   0.12729776  0.46023162  0.59149847  0.59337293  0.56775534 -0.57774544\n",
      "   0.55959104  0.4201602   0.34073775]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.59\n",
      "Precision            0.547\n",
      "Recall               0.993\n",
      "F1                   0.706\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 243 | TN: 349 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 351, 1: 537}\n",
      "acc 0.7240990990990991\n",
      "(array([0.99430199, 0.54748603]), array([0.58952703, 0.99324324]), array([0.74019088, 0.70588235]), array([592, 296]))\n",
      "(0.7708940139107736, 0.7913851351351351, 0.7230366165554238, None)\n",
      "[[349 243]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.547486033519553 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.547486033519553, 0.9932432432432432, 0.7058823529411765, None)\n",
      "\n",
      "z-weight: 1.0\n",
      "0 loss 184222.4562227816\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.27082211 -0.01928787 -0.14063245  0.37856253  0.43681819 -0.15844807\n",
      "   0.13280198 -0.01935702 -0.10775934  0.34390113  0.39762823 -0.14286955\n",
      "  -0.39588527 -0.33699178 -0.37821404  0.38378715 -0.39537146  0.11504936\n",
      "   0.21906794  0.39699417 -0.27113816  8.13838832  0.39548336 -0.31328908\n",
      "   0.25503373  0.28019293  0.39734506  0.39700564  0.37866251 -0.39156514\n",
      "   0.38002959  0.21917987  0.13605525]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.424\n",
      "Precision            0.463\n",
      "Recall               0.993\n",
      "F1                   0.632\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 341 | TN: 251 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 253, 1: 635}\n",
      "acc 0.6137387387387387\n",
      "(array([0.99209486, 0.46299213]), array([0.42398649, 0.99324324]), array([0.59408284, 0.63157895]), array([592, 296]))\n",
      "(0.7275434938221655, 0.7086148648648649, 0.6128308938025537, None)\n",
      "[[251 341]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.462992125984252 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.462992125984252, 0.9932432432432432, 0.6315789473684211, None)\n"
     ]
    }
   ],
   "source": [
    "## varying z weight \n",
    "\n",
    "def train(z_weight):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        \n",
    "        zw = tf.convert_to_tensor(z_weight,dtype=tf.float64)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "#         print(alphas.graph)\n",
    "#         print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "#         print(s)\n",
    "#         print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "#         print(\"nls\",nls_)\n",
    "#         print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "#         print(\"pout\",pout)\n",
    "\n",
    "#         print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "#         print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "#         print(\"zy\",zy)\n",
    "#         print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "#         print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - zw*logz))\n",
    "\n",
    "\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "#         print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "#         print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "\n",
    "for w in np.linspace(0,1,10):\n",
    "    print()\n",
    "    print(\"z-weight:\",w)\n",
    "    train(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00347860>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00347860>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00347860>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 144059.2118620104\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.4018328   0.39681139  0.69874603  0.66187815  0.37243063  4.19841682\n",
      "   0.40312402 -0.18403703 -0.29869982 -0.39304135]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "1 loss 143856.85057093657\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "2 loss 143856.85044932846\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "3 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "4 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with snorkel thetas and train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[0.07472098,\\\n",
    "                            0.07514459,  0.11910277,0.11186369,0.07306518,0.69216714,\\\n",
    "                            0.07467749,0.16012659, 0.13682546,0.08183363]])),\\\n",
    "                            dtype=tf.float64)\n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6ce865fa20>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 144095.64128461378\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.4018328   0.3968114   0.69874604  0.66187815  0.37243063  4.19841685\n",
      "   0.40312403 -0.18403703 -0.29869983 -0.39304135]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "1 loss 143856.85056997798\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "2 loss 143856.85044932846\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "3 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "4 loss 143856.850449328\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "{0: 2367, 1: 447}\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.40183318  0.39681177  0.69874666  0.66187873  0.37243099  4.19842114\n",
      "   0.4031244  -0.18403749 -0.2987002  -0.39304171]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.577\n",
      "Neg. class accuracy: 0.871\n",
      "Precision            0.244\n",
      "Recall               0.577\n",
      "F1                   0.343\n",
      "----------------------------------------\n",
      "TP: 109 | FP: 338 | TN: 2287 | FN: 80\n",
      "========================================\n",
      "\n",
      "{0: 2367, 1: 447}\n",
      "acc 0.851457000710732\n",
      "(array([0.96620194, 0.24384787]), array([0.8712381 , 0.57671958]), array([0.91626603, 0.3427673 ]), array([2625,  189]))\n",
      "(0.6050249090543065, 0.7239788359788359, 0.6295166606192549, None)\n",
      "[[2287  338]\n",
      " [  80  109]]\n",
      "prec: tp/(tp+fp) 0.24384787472035793 recall: tp/(tp+fn) 0.5767195767195767\n",
      "(0.24384787472035793, 0.5767195767195767, 0.34276729559748426, None)\n"
     ]
    }
   ],
   "source": [
    "## init thetas with old network thetas and train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                            initializer=tf.constant_initializer(np.array([[1.0,1.0,1.0,\\\n",
    "                             1.0,1.0,1.02750979,1.0,1.0218145,1.0,1.0]])),\\\n",
    "                    dtype=tf.float64)\n",
    "                             \n",
    "    print(\"thetas\",thetas)\n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    \n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(5):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00eda3c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 153548.14977017298\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "{0: 2521, 1: 293}\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[0.07472098 0.07514459 0.11910277 0.11186369 0.07306518 0.69216714\n",
      "  0.07467749 0.16012659 0.13682546 0.08183363]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.55\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.355\n",
      "Recall               0.55\n",
      "F1                   0.432\n",
      "----------------------------------------\n",
      "TP: 104 | FP: 189 | TN: 2436 | FN: 85\n",
      "========================================\n",
      "\n",
      "{0: 2521, 1: 293}\n",
      "acc 0.9026297085998578\n",
      "(array([0.96628322, 0.35494881]), array([0.928     , 0.55026455]), array([0.94675476, 0.43153527]), array([2625,  189]))\n",
      "(0.6606160132024104, 0.7391322751322751, 0.6891450153444726, None)\n",
      "[[2436  189]\n",
      " [  85  104]]\n",
      "prec: tp/(tp+fp) 0.35494880546075086 recall: tp/(tp+fn) 0.5502645502645502\n",
      "(0.35494880546075086, 0.5502645502645502, 0.4315352697095436, None)\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.convert_to_tensor(np.array([[ 0.07472098,  0.07514459,  0.11910277,\\\n",
    "            0.11186369,0.07306518,0.69216714,0.07467749,0.16012659, 0.13682546,0.08183363]]))\n",
    "\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(1):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "#                     _,ls = sess.run([train_step,normloss])\n",
    "                    ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f6d00452da0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 10), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 179552.66617224828\n",
      "[[1.         1.         1.         1.         1.         1.02750979\n",
      "  1.         1.0218145  1.         1.        ]]\n",
      "{0: 2522, 1: 292}\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[1.         1.         1.         1.         1.         1.02750979\n",
      "  1.         1.0218145  1.         1.        ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.545\n",
      "Neg. class accuracy: 0.928\n",
      "Precision            0.353\n",
      "Recall               0.545\n",
      "F1                   0.428\n",
      "----------------------------------------\n",
      "TP: 103 | FP: 189 | TN: 2436 | FN: 86\n",
      "========================================\n",
      "\n",
      "{0: 2522, 1: 292}\n",
      "acc 0.90227434257285\n",
      "(array([0.96590008, 0.35273973]), array([0.928     , 0.54497354]), array([0.94657082, 0.42827443]), array([2625,  189]))\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "[[2436  189]\n",
      " [  86  103]]\n",
      "prec: tp/(tp+fp) 0.3527397260273973 recall: tp/(tp+fn) 0.544973544973545\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "## Objective value on thetas from old network\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "#                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "#                              dtype=tf.float64)\n",
    "    \n",
    "    thetas = tf.convert_to_tensor(np.array([[1.0,1.0,1.0,1.0,1.0,1.02750979,\\\n",
    "                             1.0,1.0218145,1.0,1.0]]))\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "#     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(1):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "#                     _,ls = sess.run([train_step,normloss])\n",
    "                    ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "        MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 145767.7856663791\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "1 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "2 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "3 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "4 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "5 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "6 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "7 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "8 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "9 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "10 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "11 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "12 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "13 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "14 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "15 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "16 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "17 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "18 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "19 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "21 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "22 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "23 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "24 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "25 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "26 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "27 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "28 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "29 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "acc 0.7949538024164889\n",
      "(array([0.93353091, 0.07079646]), array([0.84      , 0.16931217]), array([0.88429918, 0.09984399]), array([2625,  189]))\n",
      "(0.5021636830944227, 0.5046560846560846, 0.49207158581109633, None)\n",
      "[[2205  420]\n",
      " [ 157   32]]\n",
      "prec: tp/(tp+fp) 0.07079646017699115 recall: tp/(tp+fn) 0.1693121693121693\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n"
     ]
    }
   ],
   "source": [
    "## same network that didn't train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(30):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 16 29 14 13 23 20  0  5 11  2  8  7  1 17  6 32 18 31 24 25  9  3 28\n",
      " 30 15 22 19 27 26 10  4 21]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([-0.27082211, -0.01928787, -0.14063245,  0.37856253,  0.43681819, -0.15844807,\n",
    "   0.13280198, -0.01935702, -0.10775934,  0.34390113,  0.39762823, -0.14286955,\n",
    "  -0.39588527, -0.33699178, -0.37821404,  0.38378715, -0.39537146,  0.11504936,\n",
    "   0.21906794,  0.39699417, -0.27113816,  8.13838832,  0.39548336, -0.31328908,\n",
    "   0.25503373,  0.28019293,  0.39734506,  0.39700564,  0.37866251, -0.39156514,\n",
    "   0.38002959,  0.21917987,  0.13605525])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00024597518522208474\n",
      "[1.         1.         1.         1.         1.         1.00531229\n",
      " 1.         1.         1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "0 -0.9839007408883389\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002477972163026618\n",
      "[1.         1.         1.         1.         1.         1.01071759\n",
      " 1.         1.00531229 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "1 -0.9911888652106471\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.00024967684220414605\n",
      "[1.         1.         1.         1.         1.         1.01621772\n",
      " 1.         1.01071759 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "2 -0.9987073688165843\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002516160600819831\n",
      "[1.         1.         1.         1.         1.         1.0218145\n",
      " 1.         1.01621772 1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "3 -1.0064642403279322\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n",
      "\n",
      "-0.0002536169307750545\n",
      "[1.         1.         1.         1.         1.         1.02750979\n",
      " 1.         1.0218145  1.         1.        ]\n",
      "0 292\n",
      "0  d  (0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "4 -1.014467723100218\n",
      "0 292\n",
      "(0.6593199026647693, 0.7364867724867725, 0.6874226231133167, None)\n",
      "(0.3527397260273973, 0.544973544973545, 0.4282744282744283, None)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
