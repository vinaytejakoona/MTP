{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8272 888\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()\n",
    "print(len(train_cands),len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "296 592\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "# from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from six.moves.cPickle import load\n",
    "\n",
    "with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "    ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Discrete #########\n",
    "\n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "def LF_d_induced_by_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,1) if (\n",
    "        re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "def LF_d_treat_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d_wide(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "def LF_uncertain(c):\n",
    "    if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induced_other(c):\n",
    "    if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)  \n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_therapy(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "    LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "    LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "    LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "    LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "    LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "    LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "]\n",
    "\n",
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distanceCD(c):\n",
    "    dist = 0\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    return dist/5000\n",
    "\n",
    "\n",
    "def distanceCD_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{A}})(.*)('+w+r')(.*)({{B}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "def distanceDC_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{B}})(.*)('+w+r')(.*)({{A}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "   \n",
    "\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Smooth LFs #########\n",
    "\n",
    "# def cand_in_ctd_unspecified(c):\n",
    "#     return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "# def cand_in_ctd_therapy(c):\n",
    "#     return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "# def cand_in_ctd_marker(c):\n",
    "#     return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "# def LF_in_ctd_unspecified(c):\n",
    "#     if(cand_in_ctd_unspecified(c)==1):\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_in_ctd_therapy(c):\n",
    "#     if(cand_in_ctd_therapy(c)==1):\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_in_ctd_marker(c):\n",
    "#     if(cand_in_ctd_marker(c)==1):\n",
    "#         return (1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_tagged_text,\n",
    "#     rule_regex_search_tagged_text,\n",
    "#     rule_regex_search_btw_AB,\n",
    "#     rule_regex_search_btw_BA,\n",
    "#     rule_regex_search_before_A,\n",
    "#     rule_regex_search_before_B,\n",
    "# )\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "#     get_text_between, get_tagged_text,\n",
    "# )\n",
    "\n",
    "# # List to parenthetical\n",
    "# def ltp(x):\n",
    "#     return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# # def LF_induce(c):\n",
    "# #     return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_induce(c):\n",
    "#     return (1,distanceCD_(c,['induc'])) if re.search(r'{{A}}.*induc.*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# causal_past = ['induced', 'caused', 'due']\n",
    "# # def LF_d_induced_by_c(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "# #         return (1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_d_induced_by_c(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "#     for w in causal_past:\n",
    "#         sc=max(sc,get_similarity(word_vectors,w))\n",
    "#     return (1,sc)\n",
    "\n",
    "# # def LF_d_induced_by_c_tight(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "# #         return (1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_d_induced_by_c_tight(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.*' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "#         return (1,(1-distanceDC_(c,causal_past)))\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_induce_name(c):\n",
    "#     return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "# causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "# # def LF_c_cause_d(c):\n",
    "# #     return (1,1) if (\n",
    "# #         re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "# #         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "# #     ) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_c_cause_d(c):\n",
    "#     return (1,(1-distanceCD_(c,causal))) if (\n",
    "#         re.search(r'{{A}}.* ' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "#         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#     ) else (0,0)\n",
    "\n",
    "\n",
    "# treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "# # def LF_d_treat_c(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_d_treat_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1-distanceDC_(c,treat))\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "# # def LF_c_treat_d(c):\n",
    "# #     if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_c_treat_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1-distanceCD_(c,treat))\n",
    "#     return (0,0)\n",
    "\n",
    "# # def LF_treat_d(c):\n",
    "# #     if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_treat_d(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "#     for w in treat:\n",
    "#         sc=max(sc,get_similarity(word_vectors,w))\n",
    "#     if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "#         return (0,0)\n",
    "#     else:\n",
    "#         return (-1,sc)\n",
    "\n",
    "# # def LF_c_treat_d_wide(c):\n",
    "# #     if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_c_treat_d_wide(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "#         return (-1,1-distanceCD_(c,treat))\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_c_d(c):\n",
    "#     return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "# def LF_c_induced_d(c):\n",
    "#     return (1,1) if (\n",
    "#         ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "#         (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "#         ) else (0,0)\n",
    "\n",
    "# # def LF_improve_before_disease(c):\n",
    "# #     if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "\n",
    "# def distanceImproveBeforeDisease(c):\n",
    "#     m=re.search(r'(improv)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "#     if(m):\n",
    "#         return len(m.group(2))/5000\n",
    "#     return 0\n",
    "\n",
    "\n",
    "# def LF_improve_before_disease(c):\n",
    "#     if(rule_regex_search_before_B(c, 'improv.*', -1) == -1):\n",
    "#         return (-1,1-distanceImproveBeforeDisease(c))\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "\n",
    "# pat_terms = ['in a patient with ', 'in patients with']\n",
    "# def LF_in_patient_with(c):\n",
    "#     return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# uncertain = ['combin', 'possible', 'unlikely']\n",
    "# # def LF_uncertain(c):\n",
    "# #     if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_uncertain(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "#     for w in uncertain:\n",
    "#         sc=max(sc,get_similarity(word_vectors,w))\n",
    "#     if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "#         return (0,0)\n",
    "#     else:\n",
    "#         return (-1,sc)\n",
    "\n",
    "# # def LF_induced_other(c):\n",
    "# #     if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)  \n",
    "\n",
    "# def LF_induced_other(c):\n",
    "#     if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "#         return (-1,distanceCD(c))\n",
    "#     return (0,0)  \n",
    "\n",
    "# # def LF_far_c_d(c):\n",
    "# #     if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_far_c_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,distanceCD(c))\n",
    "#     return (0,0)\n",
    "\n",
    "# # def LF_far_d_c(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_far_d_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,distanceCD(c))\n",
    "#     return (0,0)\n",
    "\n",
    "# #without deps\n",
    "# gen_model.weights.lf_accuracy\n",
    "# # def LF_risk_d(c):\n",
    "# #     if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "# #         return (1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "\n",
    "# def LF_risk_d(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "#     sc=max(sc,get_similarity(word_vectors,'risk'))\n",
    "#     return (1,sc)\n",
    "\n",
    "\n",
    "# # def LF_develop_d_following_c(c):\n",
    "# #     return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceDevFol(c):\n",
    "#     dist = 0\n",
    "#     matchObj = re.search(r'(develop)(.*)({{B}})(.*)(following)(.*)({{A}})', get_tagged_text(c), flags=re.I)\n",
    "#     if(matchObj):\n",
    "#         match_groups = matchObj.group(2,4,6)\n",
    "#         dist = sum([len(mg) for mg in match_groups])\n",
    "#     return dist/5000\n",
    "\n",
    "# def LF_develop_d_following_c(c):\n",
    "#     return (1,1-distanceDevFol(c)) if re.search(r'develop.*{{B}}.*following.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# procedure, following = ['inject', 'administrat'], ['following']\n",
    "# # def LF_d_following_c(c):\n",
    "# #     return (1,distanceDFollC(c)) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_d_following_c(c):\n",
    "#     return (1,1-distanceDC_(c,following)) if re.search('{{B}}.*' + ltp(following) + '.*{{A}}.*' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# # def LF_measure(c):\n",
    "# #     return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceMeasureA(c):\n",
    "#     m = re.search('(measur)(.*)({{A}})', get_tagged_text(c), flags=re.I) \n",
    "#     if(m):\n",
    "#         return (5000-len(m.group(2)))/5000\n",
    "#     return 0\n",
    "\n",
    "# def LF_measure(c):\n",
    "#     return (-1,distanceMeasureA(c)) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# # def LF_level(c):\n",
    "# #     return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceLevel(c):\n",
    "#     m = re.search('({{A}})(.*)(level)', get_tagged_text(c), flags=re.I)\n",
    "#     if(m):\n",
    "#         return (5000-len(m.group(2)))/5000\n",
    "#     return 0\n",
    "\n",
    "# def LF_level(c):\n",
    "#     return (-1,distanceLevel(c)) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# # def LF_neg_d(c):\n",
    "# #     return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceNeg(c):\n",
    "#     m = re.search('(none|not|no)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "#     if(m):\n",
    "#         return (5000-len(m.group(2)))/5000\n",
    "#     return 0\n",
    "\n",
    "\n",
    "# def LF_neg_d(c):\n",
    "#     return (-1,distanceNeg(c)) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "#                 'seems', 'suggests', 'risk', 'implicated',\n",
    "#                'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "# WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "# def LF_weak_assertions(c):\n",
    "#     return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     l,s = LF_c_d(c)\n",
    "#     cl = cand_in_ctd_marker(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     l1,s1 = LF_c_induced_d(c)\n",
    "#     l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "#     cl = cand_in_ctd_marker(c)\n",
    "#     return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     l,s = LF_c_treat_d_wide(c)\n",
    "#     cl = cand_in_ctd_therapy(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     l,s = LF_c_treat_d_wide(c)\n",
    "#     cl = cand_in_ctd_unspecified(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     l1,s1 = LF_c_induced_d(c)\n",
    "#     l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "#     cl = cand_in_ctd_unspecified(c)\n",
    "#     return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# # def LF_ctd_marker_c_d(c):\n",
    "# #     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# # def LF_ctd_marker_induce(c):\n",
    "# #     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# # def LF_ctd_therapy_treat(c):\n",
    "# #     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# # def LF_ctd_unspecified_treat(c):\n",
    "# #     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# # def LF_ctd_unspecified_induce(c):\n",
    "# #     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_closer_chem(c):\n",
    "#     # Get distance between chemical and disease\n",
    "#     chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "#     dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "#     if dis_start < chem_start:\n",
    "#         dist = chem_start - dis_end\n",
    "#     else:\n",
    "#         dist = dis_start - chem_end\n",
    "#     # Try to find chemical closer than @dist/2 in either direction\n",
    "#     sent = c.get_parent()\n",
    "#     closest_other_chem = float('inf')\n",
    "#     for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "#             return (-1,1)\n",
    "#     for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "#             return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_closer_dis(c):\n",
    "#     # Get distance between chemical and disease\n",
    "#     chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "#     dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "#     if dis_start < chem_start:\n",
    "#         dist = chem_start - dis_end\n",
    "#     else:\n",
    "#         dist = dis_start - chem_end\n",
    "#     # Try to find chemical disease than @dist/8 in either direction\n",
    "#     sent = c.get_parent()\n",
    "#     for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "#             return (-1,1)\n",
    "#     for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "#             return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "# LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "#     LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "#     LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "#     LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "#     LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "#     LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "#     LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "# ]\n",
    "\n",
    "# LF_l = [\n",
    "#     1,1,1,-1,-1,-1,\n",
    "#     -1,1,1,-1,\n",
    "#     -1,1,1,\n",
    "#     1,1,-1,1,\n",
    "#     -1,-1,-1,-1,\n",
    "#     1,-1,1,1,-1,\n",
    "#     -1,-1,-1,1,-1,-1,-1\n",
    "# ]\n",
    "# print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "\n",
    "np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "#     draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "\n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 2, 33) (8272, 2, 33)\n"
     ]
    }
   ],
   "source": [
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "# dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "test_L_S = dev_L_S\n",
    "true_labels = gold_labels_dev\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "\n",
    "\n",
    "def merge(a,b):\n",
    "    c = []\n",
    "    for i in range(len(a)):\n",
    "        ci = []\n",
    "        ci_l = a[i,0,:].tolist()+b[i,0,:].tolist()\n",
    "        ci_s = a[i,1,:].tolist()+b[i,1,:].tolist()\n",
    "        ci.append(ci_l)\n",
    "        ci.append(ci_s)\n",
    "        c.append(ci)\n",
    "    return c\n",
    "import numpy as np\n",
    "dev_L_S_s = np.load(\"dev_L_S_smooth.npy\")\n",
    "train_L_S_s = np.load(\"train_L_S_smooth.npy\")\n",
    "\n",
    "dev_L_S_d = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S_d = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "dev_L_S = np.array(merge(dev_L_S_d,dev_L_S_s))\n",
    "train_L_S = np.array(merge(train_L_S_d,train_L_S_s))\n",
    "\n",
    "LF_l = LF_l + LF_l\n",
    "print(len(LF_l))\n",
    "test_L_S = dev_L_S\n",
    "true_labels = gold_labels_dev\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32\n",
    "seed = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LF_l)\n",
    "NoOfClasses = 2\n",
    "print(len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized model with smooth LFs + penalties\n",
    "\n",
    "def train(lr,ep,th,af,pcl=np.array([-1,1],dtype=np.float64),norm=True,smooth=True,penalty=0,p3k=3,alp=0.99):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (don't let alpha to be greater than this value)\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=af,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "#         print(\"k\",k)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "        \n",
    "        s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "        print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "        if(smooth):\n",
    "            pout = tf.map_fn(lambda c: l*c*s_ ,pcl,name=\"pout\")\n",
    "        else:\n",
    "            pout = tf.map_fn(lambda c: l*c ,pcl,name=\"pout\")\n",
    "\n",
    "        print(\"pout\",pout)    \n",
    "\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                           name=\"t_pout\")\n",
    "    \n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            print(\"intsy\",out1)\n",
    "            return out1\n",
    "                \n",
    "\n",
    "        if(smooth):\n",
    "            #smooth normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "        else:\n",
    "            #discrete normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "\n",
    "    \n",
    "# \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        \n",
    "        print(\"zy\",zy)\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        \n",
    "        print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -tf.minimum( tf.reduce_min(thetas),0.0)\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk)))\n",
    "        else:\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "            \n",
    "        print(\"loss\",loss)\n",
    "        tf.summary.scalar('un-normloss', loss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,loss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "                print(a)\n",
    "                print(t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(true_labels,pl))\n",
    "                print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "            predictAndPrint(pl)\n",
    "            print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha-mean 0.0\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 159150.60326214734\n",
      "[-0.00447464 -0.04252255 -0.03231582  0.07985649  0.08190977  0.06301405\n",
      "  0.08009967 -0.03403547 -0.03038038  0.08111706  0.07951211 -0.03041316\n",
      " -0.00292876 -0.06221915  0.00034632  0.08068069 -0.01503576  0.06650608\n",
      "  0.07175197  0.07580917  0.06114897 -0.07043002  0.07946602 -0.03382529\n",
      " -0.04185595  0.07650909  0.07978575  0.08031117  0.07827929 -0.06386051\n",
      "  0.0708784   0.07113775  0.06685725]\n",
      "[[1.08278782 0.84411411 1.02965798 0.9169339  1.021924   0.98928665\n",
      "  1.09128774 1.05406911 1.16462231 0.98055598 0.90001513 1.08346316\n",
      "  0.95519205 1.1356093  1.01030778 0.95823092 0.8448266  0.84590752\n",
      "  0.98127479 0.75536372 0.98541985 1.00847048 0.91338353 1.18634061\n",
      "  0.90396377 1.09699329 0.98713911 0.98640539 0.97993099 1.0486249\n",
      "  0.75104922 0.76689364 0.90283562]]\n",
      "{0: 192, 1: 696}\n",
      "acc 0.5022522522522522\n",
      "(0.39511494252873564, 0.9290540540540541, 0.5544354838709677, None)\n",
      "\n",
      "1 loss 152689.3188220243\n",
      "[ 0.09935719 -0.0088636   0.0167684   0.08327441  0.12567541  0.09262381\n",
      "  0.11510939  0.00894472  0.01521747  0.10936765  0.07744197  0.02044013\n",
      "  0.10226242 -0.12980687  0.10537732  0.09961859  0.08711622  0.09268011\n",
      "  0.0968018   0.05780379  0.09884058 -0.12526659  0.09822102  0.0087276\n",
      " -0.01404723  0.11112387  0.10898757  0.11216695  0.10782453 -0.13237398\n",
      "  0.14863413  0.14898827  0.09255887]\n",
      "[[0.98075588 0.82906116 0.99930067 0.83605306 0.93927338 0.95442174\n",
      "  1.04096557 1.02962967 1.1320064  0.89852497 0.81953596 1.04620512\n",
      "  0.85284406 1.20285779 0.9081499  0.87655517 0.74833722 0.8010639\n",
      "  0.92871438 0.69841274 0.94349471 1.07096251 0.84670083 1.2071078\n",
      "  0.89425798 1.0386251  0.92973799 0.92199774 0.93427903 1.11646633\n",
      "  0.6725225  0.68804925 0.86910423]]\n",
      "{0: 46, 1: 842}\n",
      "acc 0.38063063063063063\n",
      "(0.3491686460807601, 0.9932432432432432, 0.5166959578207381, None)\n",
      "\n",
      "2 loss 147175.18683690298\n",
      "[ 0.19612885  0.02917514  0.06799206 -0.02503193  0.03923451  0.12832921\n",
      "  0.13343806  0.05483831  0.06291046  0.00181366 -0.03028016  0.07274114\n",
      "  0.19879015 -0.21891407  0.20200527 -0.00948263  0.18065644  0.11655887\n",
      "  0.1193978  -0.01530615  0.14639662 -0.18051385  0.05827431  0.05517179\n",
      "  0.0171894   0.13505998  0.11066893  0.11018931  0.11975389 -0.22192819\n",
      "  0.2385854   0.23900348  0.1176087 ]\n",
      "[[0.88460235 0.81247725 0.9690671  0.73724363 0.84045782 0.9116883\n",
      "  1.00029884 1.00475469 1.09940695 0.79907248 0.7210378  1.00946764\n",
      "  0.75674553 1.29111222 0.81204601 0.77721003 0.65881279 0.75976375\n",
      "  0.8808234  0.6515831  0.89092568 1.13455045 0.7820651  1.22785997\n",
      "  0.88396789 0.98668623 0.88091828 0.86940602 0.89884319 1.20570716\n",
      "  0.58299195 0.59843785 0.8334599 ]]\n",
      "{0: 11, 1: 877}\n",
      "acc 0.34572072072072074\n",
      "(0.33751425313568983, 1.0, 0.5046888320545609, None)\n",
      "\n",
      "3 loss 140988.2853170526\n",
      "[ 0.29322865  0.06741402  0.1181374  -0.13421096 -0.07543156  0.16760125\n",
      "  0.14128416  0.10007929  0.1095131  -0.11014723 -0.13890016  0.12357633\n",
      "  0.29566829 -0.31738893  0.29897569 -0.12031579  0.27383194  0.13963416\n",
      "  0.14041929 -0.10239404  0.19725861 -0.23301768 -0.01153929  0.10194571\n",
      "  0.04843321  0.15193126  0.09364831  0.08209375  0.12142378 -0.32053968\n",
      "  0.33252987  0.33300558  0.14254562]\n",
      "[[0.78828029 0.79685533 0.94131801 0.6525831  0.74656427 0.86554934\n",
      "  0.96751274 0.98182126 1.06905671 0.70758433 0.63974097 0.97540004\n",
      "  0.66040311 1.38840383 0.71573432 0.68748451 0.57132002 0.72235116\n",
      "  0.83965872 0.62859258 0.83538356 1.19625848 0.73650008 1.24866475\n",
      "  0.87468329 0.94224826 0.84584244 0.83191474 0.87061306 1.30397981\n",
      "  0.49072798 0.50589689 0.79767177]]\n",
      "{0: 1, 1: 887}\n",
      "acc 0.3344594594594595\n",
      "(0.3337091319052988, 1.0, 0.5004226542688082, None)\n",
      "\n",
      "4 loss 134763.85930048535\n",
      "[ 0.38937916  0.10584395  0.1670187  -0.23746801 -0.1798946   0.20533191\n",
      "  0.13890807  0.14454158  0.15491853 -0.21406174 -0.24203817  0.17282977\n",
      "  0.39151352 -0.41771406  0.39495302 -0.22396616  0.36505446  0.15954243\n",
      "  0.15733416 -0.19386181  0.24682271 -0.28174821 -0.09429606  0.14866627\n",
      "  0.08024784  0.16159126  0.06137384  0.03382005  0.11294374 -0.42092063\n",
      "  0.42669826  0.42723753  0.16442289]\n",
      "[[0.69283026 0.78208368 0.91593631 0.63853258 0.6634384  0.82054678\n",
      "  0.94074599 0.96073342 1.04083493 0.64167111 0.63826343 0.94385046\n",
      "  0.56493523 1.487414   0.62030087 0.63928878 0.48750908 0.69041439\n",
      "  0.80550794 0.62948005 0.78087985 1.25540975 0.70965594 1.26951904\n",
      "  0.8658757  0.90581358 0.82166845 0.80559479 0.84821823 1.40386217\n",
      "  0.3999349  0.4144149  0.76400525]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.38937916  0.10584395  0.1670187  -0.23746801 -0.1798946   0.20533191\n",
      "  0.13890807  0.14454158  0.15491853 -0.21406174 -0.24203817  0.17282977\n",
      "  0.39151352 -0.41771406  0.39495302 -0.22396616  0.36505446  0.15954243\n",
      "  0.15733416 -0.19386181  0.24682271 -0.28174821 -0.09429606  0.14866627\n",
      "  0.08024784  0.16159126  0.06137384  0.03382005  0.11294374 -0.42092063\n",
      "  0.42669826  0.42723753  0.16442289]\n",
      "[[0.69283026 0.78208368 0.91593631 0.63853258 0.6634384  0.82054678\n",
      "  0.94074599 0.96073342 1.04083493 0.64167111 0.63826343 0.94385046\n",
      "  0.56493523 1.487414   0.62030087 0.63928878 0.48750908 0.69041439\n",
      "  0.80550794 0.62948005 0.78087985 1.25540975 0.70965594 1.26951904\n",
      "  0.8658757  0.90581358 0.82166845 0.80559479 0.84821823 1.40386217\n",
      "  0.3999349  0.4144149  0.76400525]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.1\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 156594.4009004276\n",
      "[0.11718078 0.06191364 0.07264874 0.1800596  0.18214066 0.15906918\n",
      " 0.17919161 0.0706353  0.07439458 0.18133814 0.17970976 0.0747029\n",
      " 0.10937297 0.03917977 0.11281169 0.18089588 0.09472379 0.16360028\n",
      " 0.16953787 0.17575302 0.15711621 0.03085618 0.17951692 0.07077451\n",
      " 0.06246231 0.17404717 0.17949001 0.18020461 0.17742545 0.03753085\n",
      " 0.16948785 0.16974677 0.16366484]\n",
      "[[1.07044235 0.84175642 1.02741058 0.91700248 1.0219802  0.99408723\n",
      "  1.09366142 1.05207329 1.1624004  0.98061655 0.90008615 1.08088654\n",
      "  0.94327273 1.13479004 0.99817995 0.9582941  0.83820118 0.84662721\n",
      "  0.98140238 0.75588241 0.99016321 1.0080302  0.91381354 1.18624094\n",
      "  0.90188904 1.09770881 0.98837471 0.98722209 0.98203012 1.04786002\n",
      "  0.75245667 0.76818624 0.90754515]]\n",
      "{0: 191, 1: 697}\n",
      "acc 0.5011261261261262\n",
      "(0.3945480631276901, 0.9290540540540541, 0.5538771399798591, None)\n",
      "\n",
      "1 loss 151104.71804238812\n",
      "[ 0.22134394  0.08976323  0.11557901  0.18062749  0.22153513  0.18926758\n",
      "  0.21264633  0.10747865  0.11381334  0.2049035   0.17517228  0.11945276\n",
      "  0.21392316 -0.03119107  0.21718854  0.19574702  0.19543911  0.19020979\n",
      "  0.19506841  0.15847022  0.19527335 -0.02636015  0.19758031  0.10698285\n",
      "  0.08465644  0.20704554  0.20771158  0.21038479  0.20579461 -0.03367636\n",
      "  0.24825283  0.24859381  0.1895483 ]\n",
      "[[0.96910216 0.83234792 1.00382949 0.8369946  0.94018439 0.95981705\n",
      "  1.04783572 1.03417233 1.13641845 0.89941781 0.82050782 1.05033083\n",
      "  0.84149937 1.20442356 0.89660484 0.87745297 0.74416721 0.80198502\n",
      "  0.9284804  0.70351322 0.94871401 1.07253959 0.85112876 1.20719305\n",
      "  0.89760963 1.04039502 0.93526232 0.92759789 0.94032924 1.11825086\n",
      "  0.67373154 0.68910113 0.87519825]]\n",
      "{0: 39, 1: 849}\n",
      "acc 0.37725225225225223\n",
      "(0.3486454652532391, 1.0, 0.5170305676855895, None)\n",
      "\n",
      "2 loss 146101.8036713557\n",
      "[ 0.31783772  0.12210608  0.16096265  0.07221402  0.12177342  0.22570068\n",
      "  0.22900776  0.14746126  0.15552667  0.09578382  0.06730813  0.16588696\n",
      "  0.31022997 -0.1219712   0.31358334  0.0862623   0.2879223   0.21471383\n",
      "  0.21788145  0.08852977  0.24331594 -0.08381277  0.15871157  0.14717002\n",
      "  0.11021917  0.22960748  0.20777265  0.20645925  0.21663016 -0.12484925\n",
      "  0.33888476  0.33928117  0.21505989]\n",
      "[[0.87344591 0.82119844 0.97998965 0.73836648 0.84082826 0.91751781\n",
      "  1.01236926 1.01551045 1.11005237 0.7998638  0.72228021 1.01986777\n",
      "  0.74581637 1.29409924 0.80093347 0.77811016 0.65723993 0.76198245\n",
      "  0.8820485  0.66482297 0.89648113 1.13800492 0.79424637 1.22812881\n",
      "  0.89258996 0.99032052 0.89301669 0.88272452 0.90918854 1.20914809\n",
      "  0.58456176 0.59979711 0.84069332]]\n",
      "{0: 5, 1: 883}\n",
      "acc 0.33896396396396394\n",
      "(0.3352208380520951, 1.0, 0.5021204410517388, None)\n",
      "\n",
      "3 loss 140437.99347378718\n",
      "[ 0.41457111  0.15514021  0.20558972 -0.03656726  0.00860153  0.26490473\n",
      "  0.2351947   0.18718341  0.19647075 -0.01534229 -0.04097278  0.21111678\n",
      "  0.40679671 -0.22078111  0.41023143 -0.02394112  0.37962741  0.23804712\n",
      "  0.238543    0.00442175  0.29412326 -0.1381587   0.09204731  0.18788926\n",
      "  0.13629204  0.24517305  0.19044817  0.17857403  0.21746024 -0.22378996\n",
      "  0.43308949  0.43353421  0.24003429]\n",
      "[[0.7777521  0.81046513 0.95806145 0.6588616  0.74790681 0.87235102\n",
      "  0.98424824 0.99821232 1.0853654  0.71001212 0.64865746 0.99151815\n",
      "  0.65001072 1.39161965 0.7051777  0.69070993 0.57329229 0.72699942\n",
      "  0.84369492 0.64874613 0.84160973 1.20147121 0.75679088 1.24909966\n",
      "  0.88803979 0.94880907 0.86445632 0.85267034 0.88495558 1.30783498\n",
      "  0.49389941 0.50863474 0.80623761]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 134727.96263250773\n",
      "[ 0.51023352  0.18836023  0.24888093 -0.13958563 -0.09556591  0.30271758\n",
      "  0.23291622  0.22609622  0.23616989 -0.11899854 -0.14388094  0.2546794\n",
      "  0.50221445 -0.32107379  0.50576979 -0.12733706  0.46879477  0.25871148\n",
      "  0.25570748 -0.08401937  0.34378929 -0.18869561  0.01368793  0.22838511\n",
      "  0.16294236  0.25453494  0.16106885  0.134311    0.209869   -0.32414305\n",
      "  0.52628221  0.52674696  0.26239641]\n",
      "[[0.68315391 0.80037299 0.93823426 0.65547527 0.67031166 0.82821911\n",
      "  0.96120584 0.98249451 1.06252936 0.65834208 0.65490195 0.96541883\n",
      "  0.55528194 1.49055569 0.6105068  0.65718797 0.49427036 0.69788424\n",
      "  0.81261436 0.6503787  0.78767569 1.26245533 0.73514496 1.2701081\n",
      "  0.88376942 0.91573457 0.84490811 0.83179543 0.86569766 1.40777747\n",
      "  0.40670241 0.42010517 0.77361355]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.51023352  0.18836023  0.24888093 -0.13958563 -0.09556591  0.30271758\n",
      "  0.23291622  0.22609622  0.23616989 -0.11899854 -0.14388094  0.2546794\n",
      "  0.50221445 -0.32107379  0.50576979 -0.12733706  0.46879477  0.25871148\n",
      "  0.25570748 -0.08401937  0.34378929 -0.18869561  0.01368793  0.22838511\n",
      "  0.16294236  0.25453494  0.16106885  0.134311    0.209869   -0.32414305\n",
      "  0.52628221  0.52674696  0.26239641]\n",
      "[[0.68315391 0.80037299 0.93823426 0.65547527 0.67031166 0.82821911\n",
      "  0.96120584 0.98249451 1.06252936 0.65834208 0.65490195 0.96541883\n",
      "  0.55528194 1.49055569 0.6105068  0.65718797 0.49427036 0.69788424\n",
      "  0.81261436 0.6503787  0.78767569 1.26245533 0.73514496 1.2701081\n",
      "  0.88376942 0.91573457 0.84490811 0.83179543 0.86569766 1.40777747\n",
      "  0.40670241 0.42010517 0.77361355]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.2\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 154514.21704116586\n",
      "[0.23087354 0.16615656 0.17717389 0.28026418 0.28238017 0.25419366\n",
      " 0.27767511 0.17483293 0.17858022 0.28156533 0.27990747 0.1793282\n",
      " 0.2224756  0.14032872 0.22617559 0.28111568 0.20416318 0.25984761\n",
      " 0.26654822 0.27557426 0.25209229 0.1320137  0.27947024 0.17481491\n",
      " 0.16648502 0.27051137 0.27889624 0.27989509 0.27607258 0.13869782\n",
      " 0.26795008 0.26820534 0.25939844]\n",
      "[[1.05689369 0.83986433 1.02589012 0.91704796 1.02201043 0.99928424\n",
      "  1.09701785 1.050751   1.16088023 0.98065208 0.9001347  1.07902952\n",
      "  0.93030825 1.13396958 0.98496175 0.95833286 0.83199069 0.84680177\n",
      "  0.9809657  0.75660728 0.99537348 1.00770307 0.91444335 1.18617443\n",
      "  0.90030004 1.09811631 0.99019352 0.98847245 0.9849273  1.04714781\n",
      "  0.75407427 0.76963084 0.91309392]]\n",
      "{0: 189, 1: 699}\n",
      "acc 0.4988738738738739\n",
      "(0.39341917024320455, 0.9290540540540541, 0.5527638190954773, None)\n",
      "\n",
      "1 loss 149881.82038288173\n",
      "[0.33422246 0.18721888 0.21259242 0.27522253 0.31386568 0.28480477\n",
      " 0.30854014 0.20428545 0.21049043 0.29734035 0.27020491 0.21660017\n",
      " 0.32620692 0.06698109 0.32973586 0.28895649 0.30281731 0.28661726\n",
      " 0.29212963 0.25734706 0.29054768 0.0721652  0.29545118 0.20330741\n",
      " 0.18212955 0.30135827 0.30502317 0.30694968 0.30239851 0.06462467\n",
      " 0.34774831 0.34807179 0.28516712]\n",
      "[[0.95633897 0.83630934 1.00955568 0.83789202 0.94107861 0.96551844\n",
      "  1.0563255  1.03976298 1.14206184 0.90026072 0.82144377 1.05577818\n",
      "  0.82930362 1.20585871 0.8841622  0.87829758 0.74148962 0.80192121\n",
      "  0.92685149 0.70973436 0.95435992 1.07424225 0.8567572  1.20728868\n",
      "  0.90163486 1.04094901 0.94208192 0.93490987 0.94757269 1.12009934\n",
      "  0.67543737 0.69055056 0.88211494]]\n",
      "{0: 36, 1: 852}\n",
      "acc 0.3738738738738739\n",
      "(0.3474178403755869, 1.0, 0.5156794425087108, None)\n",
      "\n",
      "2 loss 145337.1754724418\n",
      "[ 0.43045763  0.21288529  0.25089035  0.16634609  0.20540619  0.32182316\n",
      "  0.32246851  0.23718513  0.24499906  0.18671168  0.16186393  0.25590277\n",
      "  0.42229355 -0.02543006  0.42589376  0.17876526  0.39371247  0.3116184\n",
      "  0.31487547  0.19013367  0.33899481  0.01217468  0.25619932  0.23595362\n",
      "  0.20110805  0.32229874  0.30253722  0.30015582  0.31167055 -0.02812099\n",
      "  0.43905522  0.43942424  0.31098555]\n",
      "[[0.86124227 0.8308204  0.99251029 0.73988498 0.84134942 0.92361502\n",
      "  1.02606994 1.02764317 1.12237021 0.80081257 0.72401406 1.03208838\n",
      "  0.73412669 1.29673717 0.78902052 0.77923408 0.65837487 0.76330882\n",
      "  0.88160831 0.68037041 0.90242761 1.14163063 0.80927836 1.22838382\n",
      "  0.90206366 0.99186086 0.90706091 0.89846561 0.92071017 1.21254041\n",
      "  0.58740275 0.60223856 0.8486778 ]]\n",
      "{0: 3, 1: 885}\n",
      "acc 0.3367117117117117\n",
      "(0.3344632768361582, 1.0, 0.5012701100762066, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loss 140141.90481797565\n",
      "[ 0.52673974  0.23990906  0.28903584  0.05832913  0.09402863  0.36090272\n",
      "  0.32676526  0.27047305  0.27934142  0.07682882  0.05428584  0.29455223\n",
      "  0.51844173 -0.12455807  0.52210938  0.06958962  0.48311668  0.33518403\n",
      "  0.33509012  0.10914047  0.38973407 -0.04448638  0.19208055  0.26960907\n",
      "  0.22125483  0.33644545  0.2843544   0.27187966  0.31137668 -0.12737405\n",
      "  0.53213645  0.53254082  0.33594628]\n",
      "[[0.76636842 0.82514387 0.97663233 0.67168777 0.74986707 0.8793652\n",
      "  1.0024562  1.01617565 1.10359306 0.71375899 0.66727589 1.00974006\n",
      "  0.63908247 1.39438325 0.69405186 0.69612301 0.57960419 0.73172226\n",
      "  0.84663261 0.67197748 0.84818679 1.20695434 0.78092796 1.24949681\n",
      "  0.90238191 0.95321841 0.88499797 0.87593609 0.90032598 1.31159122\n",
      "  0.50022339 0.51399175 0.8154414 ]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 134917.47797162665\n",
      "[ 0.62180101  0.26722601  0.32595149 -0.04449392 -0.00985066  0.39882679\n",
      "  0.32436704  0.30308805  0.31258124 -0.02654225 -0.04846646  0.33163846\n",
      "  0.61330095 -0.22482104  0.61707116 -0.0335458   0.56904616  0.35665264\n",
      "  0.35250626  0.02423364  0.4395523  -0.09739657  0.11828051  0.30306859\n",
      "  0.24208835  0.34545199  0.25722641  0.23097275  0.30440931 -0.22770273\n",
      "  0.62353255  0.62369661  0.35879634]\n",
      "[[0.67291703 0.81984528 0.962465   0.67636248 0.68509905 0.83600259\n",
      "  0.98293089 1.00591939 1.08625225 0.68045003 0.67547953 0.98922682\n",
      "  0.54541827 1.49321117 0.60048168 0.67882126 0.50743025 0.70725063\n",
      "  0.81993219 0.67494482 0.79475002 1.26989253 0.76483465 1.270635\n",
      "  0.90273903 0.92417186 0.86988307 0.86039202 0.88400186 1.41158614\n",
      "  0.41984201 0.43032254 0.78372265]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.62180101  0.26722601  0.32595149 -0.04449392 -0.00985066  0.39882679\n",
      "  0.32436704  0.30308805  0.31258124 -0.02654225 -0.04846646  0.33163846\n",
      "  0.61330095 -0.22482104  0.61707116 -0.0335458   0.56904616  0.35665264\n",
      "  0.35250626  0.02423364  0.4395523  -0.09739657  0.11828051  0.30306859\n",
      "  0.24208835  0.34545199  0.25722641  0.23097275  0.30440931 -0.22770273\n",
      "  0.62353255  0.62369661  0.35879634]\n",
      "[[0.67291703 0.81984528 0.962465   0.67636248 0.68509905 0.83600259\n",
      "  0.98293089 1.00591939 1.08625225 0.68045003 0.67547953 0.98922682\n",
      "  0.54541827 1.49321117 0.60048168 0.67882126 0.50743025 0.70725063\n",
      "  0.81993219 0.67494482 0.79475002 1.26989253 0.76483465 1.270635\n",
      "  0.90273903 0.92417186 0.86988307 0.86039202 0.88400186 1.41158614\n",
      "  0.41984201 0.43032254 0.78372265]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.30000000000000004\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 152929.69648361602\n",
      "[0.34575332 0.26982512 0.28076967 0.38056177 0.38271039 0.34848991\n",
      " 0.37523786 0.27811541 0.28173941 0.38188432 0.38019838 0.2829926\n",
      " 0.33669091 0.24111681 0.34068412 0.38142778 0.31237052 0.35516085\n",
      " 0.36267559 0.37531022 0.34613929 0.23299208 0.37932756 0.27785258\n",
      " 0.26983576 0.36560141 0.37787778 0.37927258 0.37398996 0.23953309\n",
      " 0.36633358 0.36658077 0.35389949]\n",
      "[[1.04167205 0.8386411  1.0253237  0.91703365 1.02198585 1.00453611\n",
      "  1.10173465 1.05028927 1.16024451 0.9806308  0.9001227  1.07812181\n",
      "  0.9158368  1.13302546 0.97017352 0.95831374 0.82562812 0.84551592\n",
      "  0.97936118 0.75761676 1.00076672 1.00749083 0.91540728 1.18613792\n",
      "  0.8993653  1.09747574 0.99287582 0.99049393 0.98896156 1.04644703\n",
      "  0.75601226 0.77128675 0.91932069]]\n",
      "{0: 185, 1: 703}\n",
      "acc 0.4988738738738739\n",
      "(0.3940256045519203, 0.9358108108108109, 0.5545545545545546, None)\n",
      "\n",
      "1 loss 149023.18541985136\n",
      "[0.44823379 0.28322919 0.30736763 0.36793042 0.40229557 0.37956968\n",
      " 0.40285429 0.2990248  0.30491734 0.38734063 0.36342101 0.31146619\n",
      " 0.43954774 0.16468003 0.44337323 0.38003627 0.40813361 0.38205671\n",
      " 0.38819385 0.3555558  0.38492148 0.17035448 0.39241904 0.29739286\n",
      " 0.27811306 0.39404684 0.40118162 0.40216998 0.39777259 0.16249227\n",
      " 0.4472339  0.44753362 0.37952348]\n",
      "[[0.94207883 0.84101265 1.01648977 0.83885834 0.94197426 0.97105291\n",
      "  1.06654707 1.04638197 1.14887738 0.90111625 0.82246821 1.06254953\n",
      "  0.81576847 1.20687052 0.87032068 0.87917223 0.74019709 0.79895321\n",
      "  0.92159169 0.7173278  0.96003535 1.07604568 0.86391577 1.207389\n",
      "  0.90637507 1.03752376 0.95041632 0.94434329 0.95614964 1.12190551\n",
      "  0.67815157 0.69279546 0.88954968]]\n",
      "{0: 34, 1: 854}\n",
      "acc 0.3716216216216216\n",
      "(0.34660421545667447, 1.0, 0.5147826086956522, None)\n",
      "\n",
      "2 loss 144886.57946275844\n",
      "[0.54416267 0.30148165 0.33752299 0.25844845 0.29000653 0.41705951\n",
      " 0.41411792 0.32389752 0.33122398 0.2758051  0.25443911 0.34256383\n",
      " 0.53538037 0.07076916 0.5392544  0.26922211 0.49650059 0.40745707\n",
      " 0.41064853 0.29059537 0.43371404 0.10766311 0.35172757 0.3214837\n",
      " 0.28986667 0.41318342 0.39553708 0.39199823 0.40515824 0.06830171\n",
      " 0.53764871 0.53798286 0.40554514]\n",
      "[[0.84774263 0.84120416 1.00635746 0.7423834  0.84226947 0.92951118\n",
      "  1.04133404 1.04086969 1.13599947 0.80224693 0.72695359 1.04583218\n",
      "  0.72127532 1.29860939 0.7758966  0.78097206 0.66271348 0.76221654\n",
      "  0.87643746 0.69846538 0.90837102 1.14534975 0.82756394 1.22861679\n",
      "  0.91223611 0.98640412 0.92309684 0.91674895 0.93346176 1.21570585\n",
      "  0.59309071 0.60702191 0.85709067]]\n",
      "{0: 2, 1: 886}\n",
      "acc 0.3355855855855856\n",
      "(0.3340857787810384, 1.0, 0.5008460236886634, None)\n",
      "\n",
      "3 loss 140148.78200477955\n",
      "[ 0.63977408  0.32177641  0.36831258  0.15153644  0.18054727  0.45605833\n",
      "  0.416706    0.34993636  0.35813422  0.16739475  0.14791392  0.37375467\n",
      "  0.63090412 -0.02863759  0.63482589  0.16134164  0.58217608  0.43139484\n",
      "  0.43050638  0.21305218  0.48443771  0.04833649  0.29010399  0.34723486\n",
      "  0.3034452   0.42596958  0.37654626  0.36338101  0.40384605 -0.03122534\n",
      "  0.62918838  0.62925558  0.43058538]\n",
      "[[0.75413729 0.84065649 0.99660299 0.69753918 0.75312023 0.88608547\n",
      "  1.02184873 1.03528955 1.12320644 0.72067866 0.69656054 1.02959396\n",
      "  0.62741129 1.39623086 0.68214667 0.70733968 0.59141495 0.73822744\n",
      "  0.84721181 0.69776954 0.85470075 1.21259336 0.80900877 1.249848\n",
      "  0.91746394 0.95027427 0.90713419 0.90135896 0.91658692 1.31503922\n",
      "  0.51273906 0.52371282 0.82489728]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 135328.12876856743\n",
      "[ 0.73391822  0.34266323  0.39825723  0.04872178  0.07693503  0.494167\n",
      "  0.41410736  0.37569209  0.38435895  0.06423842  0.04510879  0.40377364\n",
      "  0.72492174 -0.1288762   0.72891381  0.05836644  0.66290184  0.45387949\n",
      "  0.44831522  0.13213428  0.53448632 -0.00733772  0.22084552  0.37310554\n",
      "  0.31798869  0.43460454  0.35116329  0.32518454  0.39736446 -0.13153513\n",
      "  0.71083973  0.71443444  0.45397546]\n",
      "[[0.66257641 0.84014264 0.98802632 0.70130803 0.71070321 0.84337671\n",
      "  1.0055757  1.03043169 1.11128232 0.70641443 0.70014725 1.01460557\n",
      "  0.5354893  1.49490081 0.59038344 0.70447015 0.52865814 0.72914275\n",
      "  0.83243511 0.70304552 0.80168667 1.27754847 0.79898075 1.2710925\n",
      "  0.92242442 0.9288616  0.89626243 0.89104662 0.90294855 1.41507298\n",
      "  0.45648523 0.45722418 0.79393004]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.73391822  0.34266323  0.39825723  0.04872178  0.07693503  0.494167\n",
      "  0.41410736  0.37569209  0.38435895  0.06423842  0.04510879  0.40377364\n",
      "  0.72492174 -0.1288762   0.72891381  0.05836644  0.66290184  0.45387949\n",
      "  0.44831522  0.13213428  0.53448632 -0.00733772  0.22084552  0.37310554\n",
      "  0.31798869  0.43460454  0.35116329  0.32518454  0.39736446 -0.13153513\n",
      "  0.71083973  0.71443444  0.45397546]\n",
      "[[0.66257641 0.84014264 0.98802632 0.70130803 0.71070321 0.84337671\n",
      "  1.0055757  1.03043169 1.11128232 0.70641443 0.70014725 1.01460557\n",
      "  0.5354893  1.49490081 0.59038344 0.70447015 0.52865814 0.72914275\n",
      "  0.83243511 0.70304552 0.80168667 1.27754847 0.79898075 1.2710925\n",
      "  0.92242442 0.9288616  0.89626243 0.89104662 0.90294855 1.41507298\n",
      "  0.45648523 0.45722418 0.79393004]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.4\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 151834.2101644053\n",
      "[0.46242239 0.3725323  0.3829524  0.48086962 0.48304969 0.44226111\n",
      " 0.47121915 0.38008751 0.38352621 0.48221314 0.48049949 0.38528004\n",
      " 0.45253516 0.34152249 0.45689148 0.48174997 0.41961411 0.44950852\n",
      " 0.45778035 0.47474369 0.43950809 0.33383118 0.47881869 0.37947007\n",
      " 0.3721814  0.4590158  0.47598735 0.47782996 0.47058224 0.34001436\n",
      " 0.46469161 0.4649248  0.44706556]\n",
      "[[1.02361519 0.83817315 1.02576889 0.9170593  1.0220072  1.00940085\n",
      "  1.10831058 1.05071983 1.16050469 0.98065305 0.90014976 1.0782208\n",
      "  0.89896718 1.13148266 0.95279252 0.95833687 0.81993362 0.84067361\n",
      "  0.97601496 0.75922469 1.00599148 1.00731713 0.91711334 1.18612346\n",
      "  0.89913673 1.09419513 0.99692084 0.99407904 0.99472159 1.04546475\n",
      "  0.75869852 0.77345075 0.92592166]]\n",
      "{0: 172, 1: 716}\n",
      "acc 0.49774774774774777\n",
      "(0.3952513966480447, 0.956081081081081, 0.5592885375494071, None)\n",
      "\n",
      "1 loss 148540.36490673537\n",
      "[0.56394473 0.37779059 0.39982455 0.45880142 0.48777726 0.47389805\n",
      " 0.49519915 0.39172754 0.39719058 0.47534885 0.45483182 0.40404481\n",
      " 0.55443517 0.26213095 0.5586242  0.46921787 0.5110776  0.47649991\n",
      " 0.48316238 0.45339062 0.47865235 0.2684315  0.48845796 0.38930378\n",
      " 0.37264841 0.48493255 0.4958884  0.49582255 0.49150222 0.26013611\n",
      " 0.5447224  0.54500391 0.47257641]\n",
      "[[0.92526991 0.84631674 1.02434809 0.84011569 0.94297892 0.9759852\n",
      "  1.0786848  1.05375149 1.15647176 0.90215346 0.82382036 1.07029799\n",
      "  0.80010117 1.2064718  0.85416114 0.88026508 0.74182968 0.78633973\n",
      "  0.90505461 0.72674742 0.96541427 1.07779375 0.87312594 1.20748368\n",
      "  0.91166841 1.01950187 0.96074944 0.95647648 0.9664969  1.12305274\n",
      "  0.68333022 0.696934   0.89717947]]\n",
      "{0: 34, 1: 854}\n",
      "acc 0.3716216216216216\n",
      "(0.34660421545667447, 1.0, 0.5147826086956522, None)\n",
      "\n",
      "2 loss 144810.15056910855\n",
      "[0.6594196  0.38817586 0.42106958 0.34875003 0.37465603 0.5117076\n",
      " 0.50392271 0.4079354  0.41462119 0.36346557 0.34521724 0.42617073\n",
      " 0.64989027 0.16703073 0.65409465 0.35800777 0.59523577 0.50208663\n",
      " 0.5027488  0.39035249 0.5276903  0.20302956 0.44601799 0.4041976\n",
      " 0.37683686 0.50133293 0.48706558 0.48233418 0.49695434 0.16480011\n",
      " 0.63406435 0.63406933 0.49874955]\n",
      "[[0.83212375 0.85204274 1.02100308 0.74743325 0.84404119 0.93484888\n",
      "  1.05807468 1.05469323 1.15024911 0.80487415 0.7331902  1.06045141\n",
      "  0.70667524 1.29842203 0.76085805 0.78422294 0.67255557 0.77106018\n",
      "  0.85242716 0.71892552 0.91405828 1.14895949 0.84923287 1.22881745\n",
      "  0.92278817 0.94255128 0.94111128 0.937555   0.94770344 1.21778531\n",
      "  0.60455481 0.6157577  0.86560158]]\n",
      "{0: 2, 1: 886}\n",
      "acc 0.3355855855855856\n",
      "(0.3340857787810384, 1.0, 0.5008460236886634, None)\n",
      "\n",
      "3 loss 140477.074046309\n",
      "[0.75368098 0.40070815 0.44332564 0.24370833 0.26785522 0.55100069\n",
      " 0.50629393 0.42562219 0.43300073 0.25731188 0.24046082 0.44873112\n",
      " 0.74414125 0.06734339 0.74836057 0.25222241 0.67433319 0.5030337\n",
      " 0.50280263 0.31872453 0.57867431 0.14059603 0.38984702 0.42100964\n",
      " 0.3829303  0.5014517  0.46952776 0.45606274 0.49603747 0.06499691\n",
      " 0.71439683 0.7185788  0.52444008]\n",
      "[[0.74094756 0.85683804 1.01755597 0.70883064 0.75389851 0.8920057\n",
      "  1.04166675 1.05517343 1.14359972 0.72450053 0.70704882 1.05052729\n",
      "  0.61507878 1.39584403 0.66944546 0.71483253 0.61206446 0.86394279\n",
      "  0.92199108 0.72214468 0.86080918 1.21825011 0.83842078 1.25014652\n",
      "  0.93309829 0.9331326  0.92963632 0.92729473 0.93333347 1.31732391\n",
      "  0.54872588 0.55028631 0.83405353]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 136237.21701042802\n",
      "[ 0.84589949  0.41399624  0.46510414  0.14086843  0.16435101  0.58979604\n",
      "  0.50528655  0.443361    0.45108004  0.15424535  0.1376221   0.47050253\n",
      "  0.8364154  -0.03288179  0.84062707  0.14930383  0.74611258  0.5031308\n",
      "  0.50278342  0.24464819  0.62930383  0.08148908  0.32834664  0.43823722\n",
      "  0.39011843  0.50143008  0.44842553  0.42362786  0.49186125 -0.03530444\n",
      "  0.76004479  0.76616099  0.54909516]\n",
      "[[0.6534205  0.86141919 1.01480263 0.70449533 0.71476075 0.84965324\n",
      "  1.02747488 1.05595223 1.1373137  0.71028947 0.70312107 1.0413122\n",
      "  0.52682001 1.49428087 0.5815051  0.70813113 0.56200063 0.95388083\n",
      "  0.99389429 0.72625253 0.80804646 1.28541961 0.83102648 1.27147999\n",
      "  0.94293226 0.9994189  0.92127635 0.92008056 0.92120203 1.41737284\n",
      "  0.51988973 0.53187814 0.80337931]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.84589949  0.41399624  0.46510414  0.14086843  0.16435101  0.58979604\n",
      "  0.50528655  0.443361    0.45108004  0.15424535  0.1376221   0.47050253\n",
      "  0.8364154  -0.03288179  0.84062707  0.14930383  0.74611258  0.5031308\n",
      "  0.50278342  0.24464819  0.62930383  0.08148908  0.32834664  0.43823722\n",
      "  0.39011843  0.50143008  0.44842553  0.42362786  0.49186125 -0.03530444\n",
      "  0.76004479  0.76616099  0.54909516]\n",
      "[[0.6534205  0.86141919 1.01480263 0.70449533 0.71476075 0.84965324\n",
      "  1.02747488 1.05595223 1.1373137  0.71028947 0.70312107 1.0413122\n",
      "  0.52682001 1.49428087 0.5815051  0.70813113 0.56200063 0.95388083\n",
      "  0.99389429 0.72625253 0.80804646 1.28541961 0.83102648 1.27147999\n",
      "  0.94293226 0.9994189  0.92127635 0.92008056 0.92120203 1.41737284\n",
      "  0.51988973 0.53187814 0.80337931]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 151228.20711905838\n",
      "[0.58282698 0.47443896 0.48391449 0.58011028 0.58243425 0.53520155\n",
      " 0.56373474 0.48100303 0.48430314 0.58154668 0.57971203 0.48648391\n",
      " 0.57194014 0.44270689 0.57671201 0.58105307 0.52913563 0.57145899\n",
      " 0.57760402 0.57212448 0.5316755  0.43530687 0.57637126 0.47984938\n",
      " 0.47382052 0.58387754 0.57126425 0.57348273 0.56391305 0.44139751\n",
      " 0.55944883 0.55965261 0.53820357]\n",
      "[[1.00293308 0.83817432 1.02685175 0.91787828 1.02276438 1.01418146\n",
      "  1.11778494 1.05167506 1.16126346 0.98143179 0.90098114 1.07893484\n",
      "  0.87821286 1.12666009 0.93187085 0.95912867 0.81671336 0.83749116\n",
      "  0.976134   0.76292085 1.01158894 1.00644756 0.92097616 1.18610748\n",
      "  0.89925244 1.09336525 1.00386471 1.00130198 1.0035808  1.04179669\n",
      "  0.76360774 0.77716067 0.93313344]]\n",
      "{0: 164, 1: 724}\n",
      "acc 0.49774774774774777\n",
      "(0.39640883977900554, 0.9695945945945946, 0.5627450980392157, None)\n",
      "\n",
      "1 loss 148569.6664569317\n",
      "[0.68230609 0.47157609 0.49076918 0.54731478 0.57757868 0.5661589\n",
      " 0.58377172 0.4832405  0.48835938 0.56446186 0.5432603  0.49533446\n",
      " 0.67176658 0.36233886 0.67638735 0.55806265 0.61327942 0.52444048\n",
      " 0.56085876 0.54929252 0.56983301 0.36777128 0.58299262 0.47988157\n",
      " 0.46663568 0.60309773 0.58750849 0.58691584 0.5816534  0.36062282\n",
      " 0.63810382 0.63796744 0.56288664]\n",
      "[[0.90710688 0.85170852 1.03244031 0.83845553 0.94243183 0.98192217\n",
      "  1.09350427 1.06122127 1.16401933 0.90113897 0.82198783 1.07819993\n",
      "  0.78181143 1.19922893 0.83572478 0.87901872 0.74935729 0.76061892\n",
      "  0.89580921 0.73782339 0.97238718 1.0782959  0.88463748 1.20754831\n",
      "  0.91684165 1.01354684 0.97441588 0.97218359 0.97986146 1.11862227\n",
      "  0.69467851 0.70507343 0.90595421]]\n",
      "{0: 35, 1: 853}\n",
      "acc 0.37274774774774777\n",
      "(0.347010550996483, 1.0, 0.515230635335074, None)\n",
      "\n",
      "2 loss 145273.1478540768\n",
      "[0.77606271 0.47164973 0.50028173 0.43799843 0.46594021 0.60510193\n",
      " 0.59552038 0.48820429 0.49436381 0.45362536 0.4343255  0.50951287\n",
      " 0.76561396 0.26670143 0.77020338 0.44776054 0.687485   0.50345807\n",
      " 0.50328889 0.50476504 0.61966654 0.29895248 0.55856642 0.48317445\n",
      " 0.46110625 0.50205655 0.58679847 0.58302046 0.59085123 0.26477617\n",
      " 0.71570933 0.72008185 0.59104122]\n",
      "[[0.81698524 0.86383231 1.036808   0.73824546 0.83900932 0.94100747\n",
      "  1.0743176  1.0694594  1.16529986 0.79867198 0.72262803 1.0763095\n",
      "  0.6911907  1.29003592 0.74532019 0.77721531 0.6943311  0.69923088\n",
      "  0.79953575 0.72586709 0.92128299 1.1515999  0.86329707 1.22897262\n",
      "  0.93395192 0.90968236 0.95652288 0.95517652 0.96183225 1.21329629\n",
      "  0.63899018 0.64052424 0.8743698 ]]\n",
      "{0: 2, 1: 886}\n",
      "acc 0.3355855855855856\n",
      "(0.3340857787810384, 1.0, 0.5008460236886634, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loss 141565.87893554522\n",
      "[0.86865119 0.47519576 0.51262634 0.32903582 0.35385528 0.64511787\n",
      " 0.60014354 0.49626159 0.50629916 0.34276948 0.32584108 0.52888431\n",
      " 0.85847381 0.1672301  0.86295846 0.33756198 0.75552028 0.50311416\n",
      " 0.50281331 0.44308504 0.67116163 0.23309087 0.51250711 0.48988638\n",
      " 0.45868136 0.50146914 0.57466015 0.56334251 0.59293551 0.16518564\n",
      " 0.76120133 0.76668789 0.61820841]\n",
      "[[0.73021032 0.87423404 1.03988769 0.70388925 0.75122053 0.89856825\n",
      "  1.05950121 1.07620643 1.16500909 0.72130928 0.70196653 1.07363855\n",
      "  0.60344026 1.38648435 0.65798952 0.71095512 0.64831419 0.80253448\n",
      "  0.83501726 0.73023711 0.86841286 1.22303197 0.85751174 1.250382\n",
      "  0.94953944 0.85904072 0.94739705 0.948062   0.94847328 1.3124269\n",
      "  0.6088739  0.62014644 0.84300336]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 137786.92984722756\n",
      "[0.95480372 0.48100367 0.52634059 0.22547007 0.2495362  0.68465078\n",
      " 0.60039087 0.50608595 0.52402561 0.23892482 0.22227855 0.54875606\n",
      " 0.94565006 0.06704234 0.94972566 0.23388758 0.81458843 0.50313384\n",
      " 0.50277148 0.37533635 0.72243579 0.17057692 0.45720824 0.49866838\n",
      " 0.45866629 0.50145511 0.55667164 0.53514979 0.59085734 0.06491635\n",
      " 0.79007393 0.78378663 0.64412384]\n",
      "[[0.6562205  0.88353803 1.0424664  0.71045176 0.72371079 0.85648126\n",
      "  1.04728028 1.08217526 1.16450005 0.71781054 0.70874579 1.07108381\n",
      "  0.526678   1.48448028 0.582393   0.71503048 0.61188547 0.89625584\n",
      "  0.92504139 0.7397864  0.81587669 1.29236252 0.85851836 1.27178322\n",
      "  0.96390088 0.94349319 0.94220683 0.94537842 0.93773719 1.4124077\n",
      "  0.58750848 0.61768019 0.81249237]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[0.95480372 0.48100367 0.52634059 0.22547007 0.2495362  0.68465078\n",
      " 0.60039087 0.50608595 0.52402561 0.23892482 0.22227855 0.54875606\n",
      " 0.94565006 0.06704234 0.94972566 0.23388758 0.81458843 0.50313384\n",
      " 0.50277148 0.37533635 0.72243579 0.17057692 0.45720824 0.49866838\n",
      " 0.45866629 0.50145511 0.55667164 0.53514979 0.59085734 0.06491635\n",
      " 0.79007393 0.78378663 0.64412384]\n",
      "[[0.6562205  0.88353803 1.0424664  0.71045176 0.72371079 0.85648126\n",
      "  1.04728028 1.08217526 1.16450005 0.71781054 0.70874579 1.07108381\n",
      "  0.526678   1.48448028 0.582393   0.71503048 0.61188547 0.89625584\n",
      "  0.92504139 0.7397864  0.81587669 1.29236252 0.85851836 1.27178322\n",
      "  0.96390088 0.94349319 0.94220683 0.94537842 0.93773719 1.4124077\n",
      "  0.58750848 0.61768019 0.81249237]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.6000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 151070.04968550202\n",
      "[0.7100312  0.57418233 0.5824313  0.68044697 0.6827378  0.62771296\n",
      " 0.6535066  0.57969128 0.58607797 0.68186428 0.68005313 0.58783509\n",
      " 0.69632263 0.55669071 0.70261765 0.68137777 0.63293874 0.67851152\n",
      " 0.68175493 0.66942482 0.62303135 0.53824994 0.67353768 0.57802203\n",
      " 0.57378252 0.68415518 0.66441633 0.66645546 0.65417283 0.54929844\n",
      " 0.65369588 0.65333552 0.62843143]\n",
      "[[1.01091522 0.83921447 1.02895548 0.91831226 1.02325235 1.01939494\n",
      "  1.12932677 1.05355753 1.16394247 0.98189872 0.90140628 1.08156152\n",
      "  0.87044758 1.11958062 0.93159832 0.95958411 0.80732299 0.83836977\n",
      "  0.97683873 0.76822063 1.01802121 1.00383745 0.92632396 1.18525391\n",
      "  0.89995565 1.0938882  1.0131493  1.01244895 1.01540976 1.03276681\n",
      "  0.77442174 0.78369828 0.94072303]]\n",
      "{0: 179, 1: 709}\n",
      "acc 0.49436936936936937\n",
      "(0.3921015514809591, 0.9391891891891891, 0.5532338308457712, None)\n",
      "\n",
      "1 loss 149091.77978572235\n",
      "[0.8074287  0.56311844 0.57901695 0.64637851 0.67307277 0.6566593\n",
      " 0.67141411 0.5724181  0.58510895 0.6619117  0.64256344 0.59052983\n",
      " 0.79427137 0.48443879 0.80032377 0.65623656 0.70598668 0.62930793\n",
      " 0.66060303 0.65381472 0.65783508 0.46992837 0.68167709 0.56817983\n",
      " 0.55912255 0.69353694 0.67974907 0.67896528 0.67046747 0.47355746\n",
      " 0.71656153 0.72170647 0.65170996]\n",
      "[[0.91890065 0.85781128 1.04115604 0.83891409 0.94366025 0.98984931\n",
      "  1.10929578 1.06919914 1.17426094 0.90209758 0.82229632 1.08870357\n",
      "  0.77744045 1.18206971 0.83903678 0.87981331 0.75406133 0.76122704\n",
      "  0.8970349  0.74855237 0.98276713 1.07578132 0.8974089  1.20675118\n",
      "  0.9220714  1.01520205 0.98957026 0.99020277 0.9954171  1.1020431\n",
      "  0.73076184 0.72824837 0.91603447]]\n",
      "{0: 36, 1: 852}\n",
      "acc 0.3738738738738739\n",
      "(0.3474178403755869, 1.0, 0.5156794425087108, None)\n",
      "\n",
      "2 loss 146595.25466818732\n",
      "[0.89748848 0.55462129 0.57829004 0.53945972 0.56709004 0.694257\n",
      " 0.68335259 0.56772886 0.58663832 0.55517134 0.53570417 0.59553369\n",
      " 0.88493052 0.39085536 0.8907287  0.54933651 0.76640082 0.52284809\n",
      " 0.55381425 0.62081593 0.70585977 0.39886542 0.66436367 0.5614399\n",
      " 0.54575712 0.59116808 0.68110362 0.67826349 0.6805959  0.38003795\n",
      " 0.76053897 0.76691922 0.67971314]\n",
      "[[0.83625846 0.87512705 1.05197217 0.73342271 0.83622884 0.95111147\n",
      "  1.09242747 1.08349639 1.1833173  0.79525588 0.71727782 1.09473949\n",
      "  0.69325569 1.26566187 0.75553168 0.77338047 0.71423985 0.65876076\n",
      "  0.79028035 0.73988937 0.93412596 1.15052786 0.88243226 1.22823456\n",
      "  0.943904   0.90736984 0.97502829 0.97682928 0.97912183 1.19214898\n",
      "  0.7002251  0.70404992 0.88580609]]\n",
      "{0: 2, 1: 886}\n",
      "acc 0.3355855855855856\n",
      "(0.3340857787810384, 1.0, 0.5008460236886634, None)\n",
      "\n",
      "3 loss 143536.04489254346\n",
      "[0.97946488 0.54889718 0.58016671 0.42712376 0.45021355 0.7346816\n",
      " 0.69101437 0.56573025 0.59054991 0.4402568  0.42400031 0.60269746\n",
      " 0.96962381 0.29222709 0.97428896 0.435376   0.81776832 0.50318844\n",
      " 0.50283824 0.57562292 0.75750676 0.32954586 0.63344516 0.55773934\n",
      " 0.53468133 0.50187724 0.67548365 0.66748333 0.68659948 0.28129651\n",
      " 0.79057995 0.78604842 0.70832061]\n",
      "[[0.77264193 0.89100773 1.0614632  0.67038522 0.74255808 0.90939376\n",
      "  1.07824783 1.09638123 1.19118978 0.7075524  0.66599298 1.09976136\n",
      "  0.62374947 1.35901557 0.6885558  0.69111029 0.68362489 0.67863677\n",
      "  0.7290775  0.74072649 0.88204061 1.22417354 0.87704124 1.24970523\n",
      "  0.96461095 0.81177534 0.96629845 0.97012703 0.9657764  1.28962655\n",
      "  0.67683733 0.69666402 0.85459782]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 140036.6899532312\n",
      "[1.         0.54687109 0.58536291 0.32202053 0.34365603 0.77510298\n",
      " 0.69340533 0.56722604 0.59751584 0.33441675 0.31899271 0.61270833\n",
      " 1.         0.19216449 1.         0.32985459 0.86212482 0.5030759\n",
      " 0.50282311 0.51625694 0.80951436 0.26347634 0.58570489 0.55779109\n",
      " 0.52716384 0.50148724 0.66168157 0.64504031 0.68744264 0.18115024\n",
      " 0.81945766 0.79692266 0.73574796]\n",
      "[[0.76643536 0.90500619 1.06931499 0.70506603 0.72006357 0.86750335\n",
      "  1.06703974 1.1074915  1.19758608 0.71361728 0.70300877 1.10346104\n",
      "  0.61163193 1.45581391 0.67939492 0.71045482 0.65903884 0.77967825\n",
      "  0.82662249 0.7535201  0.82980343 1.2957595  0.88314446 1.27116015\n",
      "  0.98348059 0.85271444 0.96265989 0.96984542 0.95551338 1.38922343\n",
      "  0.6597741  0.6957072  0.82400555]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[1.         0.54687109 0.58536291 0.32202053 0.34365603 0.77510298\n",
      " 0.69340533 0.56722604 0.59751584 0.33441675 0.31899271 0.61270833\n",
      " 1.         0.19216449 1.         0.32985459 0.86212482 0.5030759\n",
      " 0.50282311 0.51625694 0.80951436 0.26347634 0.58570489 0.55779109\n",
      " 0.52716384 0.50148724 0.66168157 0.64504031 0.68744264 0.18115024\n",
      " 0.81945766 0.79692266 0.73574796]\n",
      "[[0.76643536 0.90500619 1.06931499 0.70506603 0.72006357 0.86750335\n",
      "  1.06703974 1.1074915  1.19758608 0.71361728 0.70300877 1.10346104\n",
      "  0.61163193 1.45581391 0.67939492 0.71045482 0.65903884 0.77967825\n",
      "  0.82662249 0.7535201  0.82980343 1.2957595  0.88314446 1.27116015\n",
      "  0.98348059 0.85271444 0.96265989 0.96984542 0.95551338 1.38922343\n",
      "  0.6597741  0.6957072  0.82400555]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.7000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 151207.95102208134\n",
      "[0.80989634 0.67315788 0.68017891 0.78008226 0.78229886 0.7209384\n",
      " 0.73905133 0.6777094  0.68313512 0.78145473 0.77970013 0.68498014\n",
      " 0.81359542 0.6657093  0.8115127  0.78098404 0.74263401 0.77820079\n",
      " 0.78134898 0.76310889 0.71415314 0.64373272 0.76641697 0.67630578\n",
      " 0.67339027 0.78366627 0.75220351 0.75261417 0.73701073 0.67243256\n",
      " 0.72924456 0.73752678 0.71826069]\n",
      "[[1.019613   0.84043425 1.03121076 0.91986869 1.02490664 1.02395617\n",
      "  1.14220693 1.05554699 1.16671749 0.98351553 0.90294608 1.08432899\n",
      "  0.88295845 1.11028577 0.94208126 0.96118012 0.81351358 0.83984537\n",
      "  0.97845085 0.77795277 1.02472561 0.99854039 0.93630415 1.18525401\n",
      "  0.90046597 1.09560349 1.0256237  1.02858404 1.03157643 0.99170345\n",
      "  0.80039274 0.79720836 0.94816149]]\n",
      "{0: 217, 1: 671}\n",
      "acc 0.5168918918918919\n",
      "(0.4008941877794337, 0.9087837837837838, 0.5563598759048605, None)\n",
      "\n",
      "1 loss 149935.3607799438\n",
      "[0.90312327 0.65495563 0.66771003 0.75200622 0.77560989 0.74365941\n",
      " 0.75377123 0.66211804 0.67236441 0.76624852 0.74830646 0.67738822\n",
      " 0.90712696 0.61033613 0.90492723 0.76120706 0.80001801 0.73496257\n",
      " 0.76510302 0.75839459 0.73784346 0.57970352 0.77741609 0.6578106\n",
      " 0.65306996 0.79118822 0.76615915 0.76535412 0.75061421 0.61447491\n",
      " 0.75766262 0.76664419 0.73614902]\n",
      "[[0.9352901  0.86331554 1.04906189 0.84014895 0.94675612 1.00120897\n",
      "  1.12740785 1.07635469 1.18323105 0.90457573 0.82313718 1.09793637\n",
      "  0.79841885 1.15370673 0.85759749 0.88189627 0.77576856 0.76028799\n",
      "  0.89942728 0.76235229 1.00104599 1.06575024 0.91467365 1.20678165\n",
      "  0.92556894 1.01906119 1.00872329 1.01191279 1.01678605 1.02036521\n",
      "  0.78420604 0.774154   0.92985358]]\n",
      "{0: 114, 1: 774}\n",
      "acc 0.4617117117117117\n",
      "(0.38242894056847543, 1.0, 0.5532710280373832, None)\n",
      "\n",
      "2 loss 148359.30984137114\n",
      "[0.98198386 0.63896751 0.65768672 0.64695937 0.6777669  0.77539121\n",
      " 0.76513808 0.64882451 0.66400274 0.66470255 0.64269894 0.67211334\n",
      " 0.98500551 0.53403193 0.98338951 0.65814963 0.84280963 0.62818419\n",
      " 0.663185   0.73548465 0.77731924 0.50907464 0.76690807 0.64201623\n",
      " 0.63345565 0.70193213 0.76925111 0.76784181 0.76109419 0.53480538\n",
      " 0.78246081 0.78572581 0.7607043 ]\n",
      "[[0.87616534 0.88502818 1.06555353 0.72973571 0.83540497 0.96901062\n",
      "  1.11328082 1.09590159 1.19840561 0.7933952  0.71302735 1.11029392\n",
      "  0.74111266 1.21733873 0.79922507 0.77091703 0.75060292 0.65172953\n",
      "  0.78828577 0.75549016 0.96138264 1.13965599 0.90412504 1.22830107\n",
      "  0.9508771  0.90799413 0.99680788 1.00105722 1.00255896 1.08972317\n",
      "  0.76454146 0.76105467 0.90406182]]\n",
      "{0: 21, 1: 867}\n",
      "acc 0.356981981981982\n",
      "(0.3414071510957324, 1.0, 0.5090283748925193, None)\n",
      "\n",
      "3 loss 146163.9236450166\n",
      "[1.         0.62506072 0.64987757 0.53331341 0.55911585 0.81375832\n",
      " 0.77469006 0.6376841  0.65779355 0.54812453 0.52973111 0.66884367\n",
      " 1.         0.44141386 1.         0.54266264 0.87460684 0.51741003\n",
      " 0.54685873 0.7054032  0.82623251 0.43711751 0.7479184  0.62878057\n",
      " 0.61495215 0.58075862 0.76803526 0.76414151 0.76979525 0.44152815\n",
      " 0.8111072  0.79971356 0.78905393]\n",
      "[[0.87141684 0.9056346  1.08081944 0.62907489 0.729309   0.93024903\n",
      "  1.09991771 1.11425164 1.21238929 0.68905895 0.61374113 1.12157744\n",
      "  0.73765931 1.29806423 0.79509189 0.66768882 0.73357813 0.56203542\n",
      "  0.68418881 0.75447004 0.91274218 1.21506976 0.89953036 1.24981215\n",
      "  0.97586762 0.7995112  0.98838775 0.99436398 0.98935591 1.17801881\n",
      "  0.74452309 0.75363397 0.87432198]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 143204.1860546232\n",
      "[1.         0.6141656  0.64519309 0.42316092 0.44473015 0.8547065\n",
      " 0.78121308 0.62964982 0.65462676 0.43550865 0.42012572 0.66849217\n",
      " 1.         0.34259559 1.         0.43097104 0.90098276 0.5031852\n",
      " 0.50287494 0.6631614  0.87840714 0.36706247 0.71646511 0.61897872\n",
      " 0.59905508 0.50181676 0.76177213 0.7524     0.77543323 0.34250719\n",
      " 0.83790322 0.80662882 0.81815699]\n",
      "[[0.87141684 0.92468423 1.09443357 0.62655408 0.6506798  0.8891065\n",
      "  1.08822226 1.13093454 1.22476692 0.63505599 0.62447476 1.13135958\n",
      "  0.73765931 1.38990427 0.79509189 0.63169924 0.72030977 0.64014726\n",
      "  0.67007572 0.76219333 0.86156335 1.28924639 0.90258417 1.27131128\n",
      "  0.99955236 0.71363503 0.9834891  0.9924263  0.97804982 1.27481696\n",
      "  0.73179934 0.7507365  0.84363032]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[1.         0.6141656  0.64519309 0.42316092 0.44473015 0.8547065\n",
      " 0.78121308 0.62964982 0.65462676 0.43550865 0.42012572 0.66849217\n",
      " 1.         0.34259559 1.         0.43097104 0.90098276 0.5031852\n",
      " 0.50287494 0.6631614  0.87840714 0.36706247 0.71646511 0.61897872\n",
      " 0.59905508 0.50181676 0.76177213 0.7524     0.77543323 0.34250719\n",
      " 0.83790322 0.80662882 0.81815699]\n",
      "[[0.87141684 0.92468423 1.09443357 0.62655408 0.6506798  0.8891065\n",
      "  1.08822226 1.13093454 1.22476692 0.63505599 0.62447476 1.13135958\n",
      "  0.73765931 1.38990427 0.79509189 0.63169924 0.72030977 0.64014726\n",
      "  0.67007572 0.76219333 0.86156335 1.28924639 0.90258417 1.27131128\n",
      "  0.99955236 0.71363503 0.9834891  0.9924263  0.97804982 1.27481696\n",
      "  0.73179934 0.7507365  0.84363032]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.8\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 151519.0951873517\n",
      "[0.89860178 0.77197473 0.77783958 0.87683398 0.87902713 0.81793164\n",
      " 0.82273579 0.77568639 0.78023768 0.87819595 0.87645222 0.78208853\n",
      " 0.89872044 0.77431343 0.89861127 0.87773041 0.84204752 0.87494234\n",
      " 0.87809149 0.85045084 0.81162536 0.7487729  0.85426961 0.77373461\n",
      " 0.77262434 0.88036442 0.83678754 0.83224125 0.81865131 0.8217925\n",
      " 0.80584993 0.81735828 0.81118132]\n",
      "[[1.02952527 0.84158693 1.03325309 0.92493444 1.03012347 1.02520639\n",
      "  1.15415958 1.05729872 1.16916424 0.98867365 0.90798687 1.08678319\n",
      "  0.8934441  1.10259916 0.95229827 0.9663061  0.82046908 0.8447918\n",
      "  0.98360171 0.79289384 1.02552223 0.99359927 0.94869669 1.18524352\n",
      "  0.90112687 1.10091807 1.03814748 1.04573848 1.04519434 0.97593694\n",
      "  0.81098224 0.80875071 0.9527008 ]]\n",
      "{0: 322, 1: 566}\n",
      "acc 0.6283783783783784\n",
      "(0.46996466431095407, 0.8986486486486487, 0.617169373549884, None)\n",
      "\n",
      "1 loss 150558.13224078083\n",
      "[0.9790989  0.74847343 0.75857279 0.85457464 0.8716835  0.83669123\n",
      " 0.83244114 0.75386117 0.7620678  0.86536288 0.85148261 0.76671208\n",
      " 0.9797005  0.7437686  0.97939104 0.86172787 0.8810081  0.8391747\n",
      " 0.8645527  0.85365697 0.82734186 0.69256892 0.86352867 0.74878302\n",
      " 0.74827641 0.88132835 0.84608245 0.84311391 0.82691617 0.84685108\n",
      " 0.81274041 0.82539799 0.8228148 ]\n",
      "[[0.96898453 0.86768497 1.05541018 0.84390965 0.952983   1.00670461\n",
      "  1.14540459 1.08201171 1.19040812 0.91000128 0.82634786 1.10531013\n",
      "  0.8324287  1.12770063 0.89147148 0.88680451 0.79701916 0.76097393\n",
      "  0.90474128 0.78232881 1.01003462 1.05241738 0.93646148 1.20677724\n",
      "  0.92817092 1.02633862 1.02918285 1.0346507  1.03737455 0.96377803\n",
      "  0.80472593 0.79497016 0.94120092]]\n",
      "{0: 260, 1: 628}\n",
      "acc 0.5923423423423423\n",
      "(0.44745222929936307, 0.9493243243243243, 0.6082251082251083, None)\n",
      "\n",
      "2 loss 149549.9157467605\n",
      "[1.         0.72646208 0.74099729 0.77681044 0.80891626 0.85978008\n",
      " 0.8409003  0.73361526 0.74561983 0.79737013 0.77073819 0.75299207\n",
      " 1.         0.69694164 1.         0.79056374 0.90729419 0.74660229\n",
      " 0.79586391 0.84334137 0.85110417 0.62989633 0.86013492 0.72571251\n",
      " 0.72453687 0.8258913  0.84906548 0.84785087 0.8340558  0.86187261\n",
      " 0.82359329 0.83120611 0.83833687]\n",
      "[[0.96308993 0.89292926 1.07653807 0.73006641 0.84006091 0.9836735\n",
      "  1.13623737 1.10579836 1.21059904 0.79675388 0.71233193 1.12285007\n",
      "  0.82675167 1.1586952  0.88567747 0.77336108 0.78279584 0.6464176\n",
      "  0.79145075 0.77785909 0.98633718 1.11809355 0.93074025 1.22831009\n",
      "  0.95525522 0.91388517 1.02269937 1.02687684 1.02896456 0.95899895\n",
      "  0.79776294 0.78626296 0.92526398]]\n",
      "{0: 163, 1: 725}\n",
      "acc 0.5123873873873874\n",
      "(0.40551724137931033, 0.9932432432432432, 0.5759059745347699, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loss 148357.2438651451\n",
      "[1.         0.70590839 0.72502243 0.67603069 0.73309359 0.88916426\n",
      " 0.84958157 0.71489991 0.730792   0.71291082 0.66552024 0.74079165\n",
      " 1.         0.63047504 1.         0.70075413 0.92434827 0.63207741\n",
      " 0.71023493 0.82542147 0.88571012 0.56130657 0.85091772 0.7045156\n",
      " 0.70119319 0.76162555 0.84960332 0.84925384 0.8416776  0.86842292\n",
      " 0.83819047 0.83639085 0.85954721]\n",
      "[[0.96308993 0.9173235  1.09669013 0.61911851 0.73005234 0.95452374\n",
      "  1.12591498 1.12867177 1.22979697 0.68638785 0.6013022  1.13948109\n",
      "  0.82675167 1.20492835 0.88567747 0.66278312 0.7744246  0.53627405\n",
      "  0.68103757 0.77599188 0.95219654 1.18986357 0.92779899 1.24984087\n",
      "  0.98252717 0.8043764  1.01675959 1.02082804 1.0191513  0.96000744\n",
      "  0.79012535 0.7802457  0.90348374]]\n",
      "{0: 67, 1: 821}\n",
      "acc 0.40878378378378377\n",
      "(0.36053593179049936, 1.0, 0.5299910474485228, None)\n",
      "\n",
      "4 loss 146912.06720867575\n",
      "[1.         0.6867659  0.71053652 0.55125644 0.63400456 0.92421299\n",
      " 0.85881032 0.69765199 0.71746163 0.59971787 0.5419107  0.72995918\n",
      " 1.         0.55266483 1.         0.5808105  0.9356397  0.5168928\n",
      " 0.59536881 0.8060492  0.92898484 0.49002004 0.83982    0.6851252\n",
      " 0.67840056 0.68208322 0.84989125 0.84919566 0.8500411  0.87003515\n",
      " 0.84527349 0.83972643 0.88526427]\n",
      "[[0.96308993 0.94087302 1.11592746 0.51077555 0.6211221  0.92040056\n",
      "  1.11474425 1.150648   1.24807008 0.57708796 0.49376621 1.15528662\n",
      "  0.82675167 1.26798222 0.88567747 0.55347397 0.76944095 0.43268736\n",
      "  0.5717151  0.77418678 0.91062683 1.26440856 0.92550672 1.27136853\n",
      "  1.00975359 0.69614302 1.01082184 1.01568936 1.00836797 0.96420883\n",
      "  0.78203136 0.77532293 0.87740079]]\n",
      "{0: 9, 1: 879}\n",
      "acc 0.34346846846846846\n",
      "(0.33674630261660976, 1.0, 0.5038297872340425, None)\n",
      "\n",
      "[1.         0.6867659  0.71053652 0.55125644 0.63400456 0.92421299\n",
      " 0.85881032 0.69765199 0.71746163 0.59971787 0.5419107  0.72995918\n",
      " 1.         0.55266483 1.         0.5808105  0.9356397  0.5168928\n",
      " 0.59536881 0.8060492  0.92898484 0.49002004 0.83982    0.6851252\n",
      " 0.67840056 0.68208322 0.84989125 0.84919566 0.8500411  0.87003515\n",
      " 0.84527349 0.83972643 0.88526427]\n",
      "[[0.96308993 0.94087302 1.11592746 0.51077555 0.6211221  0.92040056\n",
      "  1.11474425 1.150648   1.24807008 0.57708796 0.49376621 1.15528662\n",
      "  0.82675167 1.26798222 0.88567747 0.55347397 0.76944095 0.43268736\n",
      "  0.5717151  0.77418678 0.91062683 1.26440856 0.92550672 1.27136853\n",
      "  1.00975359 0.69614302 1.01082184 1.01568936 1.00836797 0.96420883\n",
      "  0.78203136 0.77532293 0.87740079]]\n",
      "{0: 9, 1: 879}\n",
      "acc 0.34346846846846846\n",
      "acc 0.34346846846846846\n",
      "[[  9 583]\n",
      " [  0 296]]\n",
      "(0.33674630261660976, 1.0, 0.5038297872340425, None)\n",
      "\n",
      "alpha-mean 0.9\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 151878.4451930209\n",
      "[0.98143189 0.87048724 0.87547943 0.95736725 0.96006688 0.92049363\n",
      " 0.91134971 0.87361129 0.87748489 0.95906963 0.9568743  0.87930833\n",
      " 0.98119868 0.87922882 0.98128172 0.95849746 0.92699809 0.95485433\n",
      " 0.9589421  0.92865549 0.91622594 0.85183945 0.93582332 0.87149888\n",
      " 0.87121691 0.96160889 0.9230289  0.91356621 0.90769222 0.9095905\n",
      " 0.90729866 0.91719953 0.90982374]\n",
      "[[1.0555966  0.84298243 1.03531526 0.94460283 1.05035843 1.02189767\n",
      "  1.16147835 1.05912165 1.17150831 1.00869034 0.92755956 1.08909424\n",
      "  0.91945621 1.10108257 0.97831781 0.98620248 0.832654   0.86399335\n",
      "  1.00359129 0.81151755 1.02005317 0.99084238 0.96435688 1.18522444\n",
      "  0.90237032 1.12151398 1.04836467 1.05879104 1.05211369 0.98686294\n",
      "  0.81485175 0.82418017 0.95251222]]\n",
      "{0: 342, 1: 546}\n",
      "acc 0.6554054054054054\n",
      "(0.4908424908424908, 0.9054054054054054, 0.6365795724465558, None)\n",
      "\n",
      "1 loss 150987.5492981786\n",
      "[1.         0.84356933 0.8518615  0.91416011 0.92443483 0.94334735\n",
      " 0.917288   0.84774337 0.85454741 0.92069731 0.91223487 0.85898637\n",
      " 1.         0.85710246 1.         0.91852083 0.94438582 0.90422154\n",
      " 0.92021415 0.92874487 0.93904654 0.79671014 0.93689213 0.84256102\n",
      " 0.84343819 0.93005649 0.92652171 0.91889514 0.91314152 0.91463086\n",
      " 0.9112765  0.92156013 0.92361059]\n",
      "[[1.0511611  0.87149079 1.06056315 0.84327672 0.95341891 0.99984434\n",
      "  1.15583614 1.08670062 1.19614108 0.91007596 0.82548305 1.11105073\n",
      "  0.91486341 1.12437303 0.97378731 0.88665586 0.82323309 0.75900356\n",
      "  0.9047673  0.80714171 0.99796722 1.04845255 0.96127163 1.2067455\n",
      "  0.93178799 1.02728681 1.04467034 1.05331634 1.04678326 0.98787304\n",
      "  0.81069297 0.81853516 0.93891815]]\n",
      "{0: 284, 1: 604}\n",
      "acc 0.6486486486486487\n",
      "(0.4867549668874172, 0.9932432432432432, 0.6533333333333333, None)\n",
      "\n",
      "2 loss 150036.45021007184\n",
      "[1.         0.8176137  0.82942681 0.85790538 0.87798975 0.97002544\n",
      " 0.92376871 0.82292665 0.8328323  0.87081079 0.85401542 0.83982157\n",
      " 1.         0.83362802 1.         0.86656468 0.95560257 0.83735893\n",
      " 0.86987242 0.91915833 0.96920457 0.73498227 0.93296642 0.81484262\n",
      " 0.81584276 0.8885331  0.92723129 0.92130329 0.91895912 0.9154729\n",
      " 0.91510496 0.92215737 0.94142354]\n",
      "[[1.0511611  0.89934981 1.08496221 0.7275928  0.8383118  0.97488821\n",
      "  1.14890997 1.11356016 1.21989592 0.79476286 0.70968713 1.13220346\n",
      "  0.91486341 1.14893442 0.97378731 0.77122086 0.81785819 0.64272643\n",
      "  0.78942728 0.80740614 0.96987441 1.112877   0.96074451 1.22827136\n",
      "  0.96136831 0.91248243 1.04172007 1.04939797 1.04028743 0.99194017\n",
      "  0.80591905 0.81500132 0.92133018]]\n",
      "{0: 276, 1: 612}\n",
      "acc 0.6396396396396397\n",
      "(0.4803921568627451, 0.9932432432432432, 0.6475770925110133, None)\n",
      "\n",
      "3 loss 149010.02480131487\n",
      "[1.         0.792684   0.80820733 0.78660618 0.82294256 0.99013171\n",
      " 0.93106199 0.79923076 0.81237611 0.81026235 0.77924447 0.82183631\n",
      " 1.         0.80680324 1.         0.8026049  0.96274233 0.74642346\n",
      " 0.80858039 0.90507471 0.99001861 0.66860149 0.9264215  0.78850566\n",
      " 0.78870233 0.84099101 0.92685079 0.92220638 0.92535518 0.91391987\n",
      " 0.91918768 0.92149681 0.96287913]\n",
      "[[1.0511611  0.9265468  1.10852909 0.61548694 0.7270071  0.94897359\n",
      "  1.14075355 1.13968122 1.24278461 0.68317761 0.59742004 1.15256646\n",
      "  0.91486341 1.17472939 0.97378731 0.6594667  0.8149301  0.5297491\n",
      "  0.67780489 0.80968875 0.93909462 1.18212924 0.96145418 1.24980054\n",
      "  0.99088202 0.80157906 1.03904417 1.04625804 1.03282286 0.99763532\n",
      "  0.8004108  0.81219129 0.90059635]]\n",
      "{0: 267, 1: 621}\n",
      "acc 0.6295045045045045\n",
      "(0.47342995169082125, 0.9932432432432432, 0.6412213740458015, None)\n",
      "\n",
      "4 loss 147936.28535208703\n",
      "[1.         0.76882334 0.78820136 0.69176339 0.75732834 0.99005149\n",
      " 0.93902439 0.77670147 0.79317972 0.73529504 0.67759038 0.80501408\n",
      " 1.         0.77416869 1.         0.72155766 0.96712271 0.61618478\n",
      " 0.7323062  0.89013944 0.99032347 0.59983882 0.91831495 0.76362388\n",
      " 0.76230342 0.78726723 0.9262215  0.92241599 0.93226026 0.91103192\n",
      " 0.92352599 0.92061944 0.98692358]\n",
      "[[1.0511611  0.953058   1.13128393 0.50512454 0.61790169 0.91641956\n",
      "  1.13183311 1.16503754 1.2648249  0.57364633 0.48679523 1.17216423\n",
      "  0.91486341 1.20204473 0.97378731 0.54967102 0.81360181 0.41843732\n",
      "  0.56821599 0.8120205  0.8998126  1.25393133 0.96278622 1.27133156\n",
      "  1.02005788 0.69305451 1.03647766 1.0434793  1.02476961 1.00418767\n",
      "  0.79446027 0.80954257 0.8789564 ]]\n",
      "{0: 231, 1: 657}\n",
      "acc 0.588963963963964\n",
      "(0.4474885844748858, 0.9932432432432432, 0.6169989506820566, None)\n",
      "\n",
      "[1.         0.76882334 0.78820136 0.69176339 0.75732834 0.99005149\n",
      " 0.93902439 0.77670147 0.79317972 0.73529504 0.67759038 0.80501408\n",
      " 1.         0.77416869 1.         0.72155766 0.96712271 0.61618478\n",
      " 0.7323062  0.89013944 0.99032347 0.59983882 0.91831495 0.76362388\n",
      " 0.76230342 0.78726723 0.9262215  0.92241599 0.93226026 0.91103192\n",
      " 0.92352599 0.92061944 0.98692358]\n",
      "[[1.0511611  0.953058   1.13128393 0.50512454 0.61790169 0.91641956\n",
      "  1.13183311 1.16503754 1.2648249  0.57364633 0.48679523 1.17216423\n",
      "  0.91486341 1.20204473 0.97378731 0.54967102 0.81360181 0.41843732\n",
      "  0.56821599 0.8120205  0.8998126  1.25393133 0.96278622 1.27133156\n",
      "  1.02005788 0.69305451 1.03647766 1.0434793  1.02476961 1.00418767\n",
      "  0.79446027 0.80954257 0.8789564 ]]\n",
      "{0: 231, 1: 657}\n",
      "acc 0.588963963963964\n",
      "acc 0.588963963963964\n",
      "[[229 363]\n",
      " [  2 294]]\n",
      "(0.4474885844748858, 0.9932432432432432, 0.6169989506820566, None)\n",
      "\n",
      "alpha-mean 1.0\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 152121.7798116882\n",
      "[1.14995953 0.96736098 1.14913754 1.         1.14765066 1.14494593\n",
      " 1.14594278 1.14926481 1.14915676 1.14765606 1.         1.14925046\n",
      " 0.96106408 1.14859467 1.15009615 1.14765269 0.98439188 1.\n",
      " 1.14765607 1.         1.14505682 0.97009559 1.         1.14843891\n",
      " 0.97703046 1.14764036 1.14697615 1.14722185 1.14618334 0.98110906\n",
      " 1.         1.         1.        ]\n",
      "[[1.27389343 0.84751612 1.08314033 1.00062431 1.26093067 1.0238133\n",
      "  1.19996393 1.10605588 1.22249539 1.21908834 0.98357063 1.14176572\n",
      "  0.88389638 1.10762291 1.19690682 1.19647866 0.85720937 0.91993383\n",
      "  1.21396454 0.83673073 1.02509125 0.97272732 0.99745288 1.18563744\n",
      "  0.89019195 1.33232444 1.18198975 1.1738481  1.11003135 1.00062552\n",
      "  0.82098903 0.83698581 0.95192142]]\n",
      "{0: 285, 1: 603}\n",
      "acc 0.6497747747747747\n",
      "(0.48756218905472637, 0.9932432432432432, 0.6540600667408232, None)\n",
      "\n",
      "1 loss 151220.67363027754\n",
      "[1.2671341  0.93499126 1.26666738 1.         1.26269034 1.25765956\n",
      " 1.26085948 1.26673235 1.26641132 1.26270825 1.         1.26666025\n",
      " 0.84235239 1.26552219 1.2674032  1.26271271 0.96779495 1.\n",
      " 1.26270993 1.         1.25819354 0.93328744 1.         1.2642593\n",
      " 0.95445665 1.2626618  1.26207773 1.26228226 1.261292   0.9641581\n",
      " 1.         1.         1.        ]\n",
      "[[1.39588048 0.88148348 1.20144714 1.00062431 1.38129727 1.0529202\n",
      "  1.31251862 1.22419164 1.34127061 1.33944973 0.98357063 1.26077485\n",
      "  0.74326575 1.19553786 1.31896012 1.31683812 0.86881831 0.91993383\n",
      "  1.33432539 0.83597734 1.07026995 1.0127474  0.99676878 1.20713685\n",
      "  0.91417639 1.45270116 1.30178288 1.29365589 1.22755459 1.01574041\n",
      "  0.81695729 0.83287403 0.93925244]]\n",
      "{0: 283, 1: 605}\n",
      "acc 0.6475225225225225\n",
      "(0.4859504132231405, 0.9932432432432432, 0.6526082130965594, None)\n",
      "\n",
      "2 loss 149817.23868757227\n",
      "[1.37874918 0.90125013 1.37852145 1.         1.3722461  1.36652602\n",
      " 1.37044283 1.37855167 1.37809453 1.37225777 0.99999958 1.37843463\n",
      " 0.67780839 1.37735163 1.37909416 1.37225929 0.93459666 1.\n",
      " 1.37225875 1.00000004 1.36735017 0.88533215 1.         1.37466503\n",
      " 0.92496626 1.3722298  1.3716392  1.37183696 1.37086381 0.94194296\n",
      " 1.         1.         1.        ]\n",
      "[[1.51030496 0.91496154 1.31551513 1.0006243  1.49406561 1.14462307\n",
      "  1.42493274 1.33819898 1.45534291 1.45219957 0.98357063 1.37491144\n",
      "  0.63342707 1.30668534 1.4334055  1.42957836 0.88933549 0.91993383\n",
      "  1.44707301 0.83481187 1.17009509 1.06306426 0.99563723 1.22867949\n",
      "  0.94422756 1.56550202 1.41451069 1.4063817  1.34015511 1.0332037\n",
      "  0.81215574 0.82826757 0.92371197]]\n",
      "{0: 283, 1: 605}\n",
      "acc 0.6475225225225225\n",
      "(0.4859504132231405, 0.9932432432432432, 0.6526082130965594, None)\n",
      "\n",
      "3 loss 147382.81224084098\n",
      "[1.48829961 0.86490641 1.48816598 1.         1.47931282 1.47312181\n",
      " 1.47746126 1.48818039 1.48766856 1.47928461 1.         1.48804323\n",
      " 0.56059413 1.48700945 1.48866807 1.47926428 0.87830634 1.00000007\n",
      " 1.47928065 1.         1.47403592 0.825607   1.         1.4829657\n",
      " 0.88991798 1.47936339 1.47863599 1.47882365 1.4777929  0.91184106\n",
      " 1.         1.         1.        ]\n",
      "[[1.6218184  0.94919966 1.42687415 1.00062428 1.60346502 1.25067792\n",
      "  1.53424153 1.44954184 1.56674361 1.56155619 0.98357061 1.48630393\n",
      "  0.73153888 1.41771121 1.54489352 1.53891195 0.92323044 0.91993381\n",
      "  1.55642441 0.83345029 1.27745525 1.12426987 0.9942437  1.25032617\n",
      "  0.9783869  1.6749747  1.52383564 1.51569639 1.4493988  1.05454641\n",
      "  0.80648503 0.82349413 0.90494247]]\n",
      "{0: 283, 1: 605}\n",
      "acc 0.6475225225225225\n",
      "(0.4859504132231405, 0.9932432432432432, 0.6526082130965594, None)\n",
      "\n",
      "4 loss 143032.41003677688\n",
      "[1.59704143 0.82489414 1.59681278 1.         1.58504881 1.57794236\n",
      " 1.58305683 1.59683322 1.59637557 1.58493488 1.         1.59670964\n",
      " 0.44825816 1.59565783 1.59736023 1.58486737 0.80098108 0.99999973\n",
      " 1.58492029 0.99999996 1.57896249 0.75570971 1.         1.59016478\n",
      " 0.85000199 1.58524256 1.58420806 1.58437811 1.58320166 0.87152704\n",
      " 0.99999997 1.00000081 1.        ]\n",
      "[[1.73209813 0.98497461 1.53689519 1.00062423 1.71102736 1.35697694\n",
      "  1.64167816 1.55957581 1.67691832 1.6690411  0.98357056 1.59639346\n",
      "  0.85024173 1.52766512 1.65507863 1.6463551  0.97304828 0.91993376\n",
      "  1.66389985 0.83199044 1.38403646 1.19452465 0.99269843 1.27218806\n",
      "  1.0154991  1.7826703  1.63125445 1.62309916 1.55668064 1.08134694\n",
      "  0.79996987 0.81862841 0.88313801]]\n",
      "{0: 283, 1: 605}\n",
      "acc 0.6475225225225225\n",
      "(0.4859504132231405, 0.9932432432432432, 0.6526082130965594, None)\n",
      "\n",
      "[1.59704143 0.82489414 1.59681278 1.         1.58504881 1.57794236\n",
      " 1.58305683 1.59683322 1.59637557 1.58493488 1.         1.59670964\n",
      " 0.44825816 1.59565783 1.59736023 1.58486737 0.80098108 0.99999973\n",
      " 1.58492029 0.99999996 1.57896249 0.75570971 1.         1.59016478\n",
      " 0.85000199 1.58524256 1.58420806 1.58437811 1.58320166 0.87152704\n",
      " 0.99999997 1.00000081 1.        ]\n",
      "[[1.73209813 0.98497461 1.53689519 1.00062423 1.71102736 1.35697694\n",
      "  1.64167816 1.55957581 1.67691832 1.6690411  0.98357056 1.59639346\n",
      "  0.85024173 1.52766512 1.65507863 1.6463551  0.97304828 0.91993376\n",
      "  1.66389985 0.83199044 1.38403646 1.19452465 0.99269843 1.27218806\n",
      "  1.0154991  1.7826703  1.63125445 1.62309916 1.55668064 1.08134694\n",
      "  0.79996987 0.81862841 0.88313801]]\n",
      "{0: 283, 1: 605}\n",
      "acc 0.6475225225225225\n",
      "acc 0.6475225225225225\n",
      "[[281 311]\n",
      " [  2 294]]\n",
      "(0.4859504132231405, 0.9932432432432432, 0.6526082130965594, None)\n"
     ]
    }
   ],
   "source": [
    "#66\n",
    "for i in np.linspace(0,1,11):\n",
    "    print()\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.1/len(train_L_S),5,th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                            af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                          pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha-mean 0.0\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 158843.28008797162\n",
      "[ 0.02561872  0.05365972 -0.03058625  0.06756809  0.08039238  0.06706934\n",
      "  0.08194717 -0.02961741 -0.01135461  0.07676846  0.06372107 -0.02079654\n",
      " -0.00373641 -0.09307571  0.0160582   0.07421146  0.0498814   0.04891352\n",
      "  0.06717309 -0.02649248  0.06975022 -0.07984346  0.06581545 -0.01339946\n",
      "  0.04451805  0.07525528  0.07550219  0.07554218  0.07300099 -0.09548072\n",
      " -0.09125804 -0.09054115  0.05520118]\n",
      "[[ 1.01662466 -0.33310265  0.57085354  0.42753732  0.94774119  0.64410021\n",
      "   1.26450132  0.67886915  1.23459614  0.74170764  0.34521455  0.84474912\n",
      "   0.39356842  0.98646002  0.66030521  0.63094035 -0.13515935  0.04514003\n",
      "   0.72427518 -0.21609341  0.61056419  0.2841854   0.40957465  1.33801623\n",
      "  -0.03600152  1.30635648  0.76238077  0.7642576   0.71366453  0.54677276\n",
      "  -0.30349815 -0.222357    0.26009119]]\n",
      "{0: 1, 1: 887}\n",
      "acc 0.3344594594594595\n",
      "(0.3337091319052988, 1.0, 0.5004226542688082, None)\n",
      "\n",
      "1 loss 149696.78395155893\n",
      "[ 0.1272152   0.06926991  0.00081331 -0.02149632  0.14539946  0.10797678\n",
      "  0.13914898 -0.00072797  0.03409742  0.04895215 -0.03393937  0.02041504\n",
      "  0.09831381 -0.19126751  0.11819172  0.0157123  -0.02799584  0.05683668\n",
      "  0.09519638  0.06673329  0.12115083 -0.14983411  0.00279636  0.03125717\n",
      "  0.04500294  0.11992874  0.09202062  0.08892305  0.08754569 -0.19493531\n",
      " -0.1816711  -0.17954492  0.07148727]\n",
      "[[ 0.9168567  -0.23651777  0.55838029  0.44887064  0.86159995  0.59778079\n",
      "   1.19961795  0.66832273  1.20165015  0.65868833  0.40661369  0.8176876\n",
      "   0.29164087  1.08370447  0.55968778  0.5542593  -0.03585408  0.03718492\n",
      "   0.67829568 -0.11592838  0.55506879  0.35709942  0.40523486  1.35874188\n",
      "   0.00915972  1.23823558  0.71581658  0.71309595  0.67720814  0.64553034\n",
      "  -0.22034044 -0.1400015   0.23271483]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "2 loss 142304.15608651997\n",
      "[ 0.223959    0.07645485  0.03540583 -0.12422104  0.09897722  0.14605357\n",
      "  0.18047018  0.03112171  0.0797573  -0.0600835  -0.13646969  0.06288634\n",
      "  0.19162157 -0.29170952  0.21396768 -0.0893832  -0.04534689  0.06107604\n",
      "  0.11691275  0.14074217  0.17063255 -0.21762422 -0.08027226  0.07751721\n",
      "  0.03820003  0.15646525  0.08211437  0.06936431  0.08635354 -0.29614049\n",
      " -0.27275199 -0.26748437  0.08322248]\n",
      "[[ 8.20742220e-01 -1.39653294e-01  5.44804273e-01  4.70714437e-01\n",
      "   7.65526251e-01  5.53244345e-01  1.14725953e+00  6.56855717e-01\n",
      "   1.17000187e+00  5.64361401e-01  4.68636383e-01  7.90964087e-01\n",
      "   1.95604026e-01  1.18284922e+00  4.63634379e-01  4.79502665e-01\n",
      "   1.08355017e-05  3.36769702e-02  6.40314115e-01 -1.61357012e-02\n",
      "   5.00876994e-01  4.28966370e-01  4.23846582e-01  1.37949240e+00\n",
      "   2.96806275e-02  1.17425410e+00  6.83323737e-01  6.76121885e-01\n",
      "   6.51663621e-01  7.45809856e-01 -1.48933085e-01 -7.00911753e-02\n",
      "   2.07766964e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "3 loss 136064.27799557612\n",
      "[ 0.32027166  0.07872561  0.0691292  -0.22728836 -0.01555164  0.18291344\n",
      "  0.21118141  0.06217338  0.12291686 -0.16482927 -0.23952859  0.10334567\n",
      "  0.28175711 -0.39235403  0.30904277 -0.19254164 -0.0334733   0.06487829\n",
      "  0.13627417  0.08492403  0.21910891 -0.28318375 -0.16616061  0.12252613\n",
      "  0.03383414  0.18791102  0.06349352  0.03704154  0.08092882 -0.39731715\n",
      " -0.36325653 -0.34932337  0.09362204]\n",
      "[[ 7.25064447e-01 -4.28384263e-02  5.32516168e-01  4.93743680e-01\n",
      "   6.67887521e-01  5.09863112e-01  1.10419392e+00  6.46608173e-01\n",
      "   1.14149043e+00  4.98355086e-01  4.93077050e-01  7.66732710e-01\n",
      "   9.97168232e-02  1.28225993e+00  3.67914612e-01  4.96068302e-01\n",
      "   1.59818011e-07  3.09370462e-02  6.06922033e-01  2.05106992e-02\n",
      "   4.47628859e-01  4.99437141e-01  4.41375636e-01  1.40031273e+00\n",
      "   4.92391193e-02  1.11459515e+00  6.57794265e-01  6.47171602e-01\n",
      "   6.30423465e-01  8.46144805e-01 -9.68838475e-02 -2.00661514e-02\n",
      "   1.83788372e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 130677.8707901976\n",
      "[ 0.41601019  0.07933646  0.10224292 -0.33016132 -0.12148533  0.21828351\n",
      "  0.23360785  0.09272084  0.16377968 -0.26767247 -0.34241671  0.14207282\n",
      "  0.3538065  -0.49302651  0.40301754 -0.29537241 -0.00108141  0.06744102\n",
      "  0.1530555  -0.02713903  0.26646101 -0.34626976 -0.25526508  0.16622791\n",
      "  0.03194028  0.21420609  0.03590111 -0.00751061  0.07035771 -0.49837178\n",
      " -0.45451126 -0.38366494  0.10210872]\n",
      "[[ 6.29964658e-01  4.70089228e-03  5.21289886e-01  5.15790231e-01\n",
      "   5.77081879e-01  4.67683331e-01  1.06775615e+00  6.37339483e-01\n",
      "   1.11580022e+00  5.16891942e-01  5.15834572e-01  7.44655069e-01\n",
      "   3.68932606e-03  1.38169831e+00  2.72650319e-01  5.16138084e-01\n",
      "   4.86237475e-05  2.97363371e-02  5.77906075e-01  9.92611840e-02\n",
      "   3.95297209e-01  5.68286534e-01  4.61026963e-01  1.42119722e+00\n",
      "   6.74936862e-02  1.05984863e+00  6.37831428e-01  6.25082112e-01\n",
      "   6.12676603e-01  9.46383988e-01 -6.99987107e-02 -2.03766070e-05\n",
      "   1.61011774e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.41601019  0.07933646  0.10224292 -0.33016132 -0.12148533  0.21828351\n",
      "  0.23360785  0.09272084  0.16377968 -0.26767247 -0.34241671  0.14207282\n",
      "  0.3538065  -0.49302651  0.40301754 -0.29537241 -0.00108141  0.06744102\n",
      "  0.1530555  -0.02713903  0.26646101 -0.34626976 -0.25526508  0.16622791\n",
      "  0.03194028  0.21420609  0.03590111 -0.00751061  0.07035771 -0.49837178\n",
      " -0.45451126 -0.38366494  0.10210872]\n",
      "[[ 6.29964658e-01  4.70089228e-03  5.21289886e-01  5.15790231e-01\n",
      "   5.77081879e-01  4.67683331e-01  1.06775615e+00  6.37339483e-01\n",
      "   1.11580022e+00  5.16891942e-01  5.15834572e-01  7.44655069e-01\n",
      "   3.68932606e-03  1.38169831e+00  2.72650319e-01  5.16138084e-01\n",
      "   4.86237475e-05  2.97363371e-02  5.77906075e-01  9.92611840e-02\n",
      "   3.95297209e-01  5.68286534e-01  4.61026963e-01  1.42119722e+00\n",
      "   6.74936862e-02  1.05984863e+00  6.37831428e-01  6.25082112e-01\n",
      "   6.12676603e-01  9.46383988e-01 -6.99987107e-02 -2.03766070e-05\n",
      "   1.61011774e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.1\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 158419.26558881055\n",
      "[0.1499137  0.15082562 0.07270765 0.16857515 0.18145911 0.16501479\n",
      " 0.18204951 0.07347361 0.09159005 0.17788657 0.16454107 0.08251467\n",
      " 0.1046959  0.00691102 0.12662741 0.17533871 0.14931197 0.14670861\n",
      " 0.16547624 0.07781194 0.16825034 0.02066909 0.16634309 0.08947537\n",
      " 0.14194594 0.17312801 0.17605769 0.17608128 0.17293748 0.00447228\n",
      " 0.00884568 0.00963754 0.1528604 ]\n",
      "[[ 1.003418   -0.33227888  0.56935385  0.42696152  0.94726028  0.64687218\n",
      "   1.26572992  0.67774878  1.2347549   0.74117716  0.34467785  0.84379761\n",
      "   0.38483983  0.9859142   0.64942893  0.63038319 -0.13462698  0.04683403\n",
      "   0.7242518  -0.21668312  0.61246263  0.28385588  0.41008266  1.33801026\n",
      "  -0.03505911  1.3065888   0.76291804  0.76492721  0.71525835  0.54662728\n",
      "  -0.29728758 -0.21570516  0.26285448]]\n",
      "{0: 3, 1: 885}\n",
      "acc 0.3367117117117117\n",
      "(0.3344632768361582, 1.0, 0.5012701100762066, None)\n",
      "\n",
      "1 loss 149789.01964146135\n",
      "[ 0.25117239  0.16843505  0.09786173  0.07981161  0.24610307  0.20620177\n",
      "  0.23779503  0.0962558   0.13031864  0.15430572  0.0663849   0.11713172\n",
      "  0.20647881 -0.09119868  0.22839998  0.11950559  0.07442079  0.15750813\n",
      "  0.19392019  0.16971244  0.21993993 -0.05040837  0.10808323  0.12715109\n",
      "  0.14119756  0.21579026  0.19419169  0.1915457   0.18892916 -0.09494309\n",
      " -0.08133405 -0.07880811  0.1711873 ]\n",
      "[[ 0.90419187 -0.23502964  0.56211272  0.45273476  0.86199189  0.60127797\n",
      "   1.20442402  0.67245701  1.20876559  0.65897314  0.40807456  0.82300647\n",
      "   0.28318713  1.08277842  0.54919772  0.55474241 -0.03515824  0.03929761\n",
      "   0.67841672 -0.11656617  0.55738947  0.35747416  0.40623706  1.35893582\n",
      "   0.0096686   1.2388777   0.71967906  0.71797307  0.68125961  0.64534114\n",
      "  -0.20777845 -0.12651822  0.23499946]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "2 loss 142683.17929820425\n",
      "[ 0.34752023  0.17841417  0.12659401 -0.0228843   0.18973973  0.24482867\n",
      "  0.27635911  0.12226101  0.16956006  0.04513375 -0.03602304  0.15342557\n",
      "  0.29945898 -0.19153287  0.32387819  0.01425194  0.05800386  0.16523001\n",
      "  0.21627247  0.24277363  0.26988619 -0.11943061  0.02859263  0.16661801\n",
      "  0.13051628  0.25018982  0.18670635  0.17594469  0.18964498 -0.19609152\n",
      " -0.17139175 -0.16346875  0.18543878]\n",
      "[[ 8.08691910e-01 -1.37543571e-01  5.53423449e-01  4.73929829e-01\n",
      "   7.65627895e-01  5.57196604e-01  1.15677561e+00  6.65959396e-01\n",
      "   1.18357758e+00  5.64432114e-01  4.70952547e-01  8.02026440e-01\n",
      "   1.87475491e-01  1.18169384e+00  4.53592795e-01  4.83139748e-01\n",
      "   3.80777182e-06  3.65092474e-02  6.41439112e-01 -1.67289915e-02\n",
      "   5.03448149e-01  4.30168880e-01  4.25037358e-01  1.37987499e+00\n",
      "   3.17942597e-02  1.17543313e+00  6.91150737e-01  6.86045566e-01\n",
      "   6.58034056e-01  7.45641279e-01 -1.25284585e-01 -4.44744280e-02\n",
      "   2.09306738e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "3 loss 136680.32414557898\n",
      "[ 0.44344393  0.18376341  0.15490764 -0.12615331  0.07549641  0.28235673\n",
      "  0.30425214  0.14786494  0.20652184 -0.06001302 -0.13926964  0.18806026\n",
      "  0.38907329 -0.29210478  0.41868252 -0.08909969  0.07293499  0.17273913\n",
      "  0.23637828  0.19281894  0.3189407  -0.18623365 -0.0539104   0.20490084\n",
      "  0.12265215  0.27952058  0.17131793  0.14889276  0.18645178 -0.29723863\n",
      " -0.25803399 -0.19806245  0.1985351 ]\n",
      "[[ 7.13709803e-01 -4.00779016e-02  5.45529465e-01  4.98244805e-01\n",
      "   6.67949743e-01  5.14125502e-01  1.11839820e+00  6.60228854e-01\n",
      "   1.16099941e+00  5.05740998e-01  4.96755516e-01  7.82965395e-01\n",
      "   9.18506061e-02  1.28098080e+00  3.58341713e-01  5.03009263e-01\n",
      "   1.98398189e-06  3.46048313e-02  6.09569031e-01  1.64236565e-02\n",
      "   4.50336793e-01  5.01530998e-01  4.42687015e-01  1.40086438e+00\n",
      "   5.29683688e-02  1.11636467e+00  6.69196309e-01  6.61619617e-01\n",
      "   6.38867626e-01  8.46035633e-01 -5.49716227e-02 -9.82499067e-06\n",
      "   1.84456969e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 131595.45890715093\n",
      "[ 0.53876926  0.18429045  0.18338007 -0.22930754 -0.03076508  0.31837309\n",
      "  0.32350241  0.17368885  0.24176759 -0.16312625 -0.24243781  0.22165725\n",
      "  0.45458898 -0.39268476  0.51237706 -0.19221478  0.12465487  0.17898334\n",
      "  0.253786    0.08033502  0.36689426 -0.250364   -0.14053345  0.24227988\n",
      "  0.11775286  0.30345822  0.1467515   0.10931274  0.1777586  -0.39823725\n",
      " -0.32789895 -0.20165197  0.2096455 ]\n",
      "[[ 6.19453605e-01  5.60715293e-03  5.38067773e-01  5.22221623e-01\n",
      "   5.79482922e-01  4.72206484e-01  1.08672456e+00  6.54879297e-01\n",
      "   1.14051783e+00  5.27480247e-01  5.21494160e-01  7.65319595e-01\n",
      "  -2.73435997e-06  1.38032207e+00  2.63604535e-01  5.24842387e-01\n",
      "   8.86739291e-06  3.48244856e-02  5.83092194e-01  9.25515593e-02\n",
      "   3.98088640e-01  5.71253031e-01  4.63859044e-01  1.42189709e+00\n",
      "   7.26777505e-02  1.06285546e+00  6.53023689e-01  6.44247066e-01\n",
      "   6.23345919e-01  9.46321486e-01 -2.63760823e-03 -1.02083447e-05\n",
      "   1.60808632e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.53876926  0.18429045  0.18338007 -0.22930754 -0.03076508  0.31837309\n",
      "  0.32350241  0.17368885  0.24176759 -0.16312625 -0.24243781  0.22165725\n",
      "  0.45458898 -0.39268476  0.51237706 -0.19221478  0.12465487  0.17898334\n",
      "  0.253786    0.08033502  0.36689426 -0.250364   -0.14053345  0.24227988\n",
      "  0.11775286  0.30345822  0.1467515   0.10931274  0.1777586  -0.39823725\n",
      " -0.32789895 -0.20165197  0.2096455 ]\n",
      "[[ 6.19453605e-01  5.60715293e-03  5.38067773e-01  5.22221623e-01\n",
      "   5.79482922e-01  4.72206484e-01  1.08672456e+00  6.54879297e-01\n",
      "   1.14051783e+00  5.27480247e-01  5.21494160e-01  7.65319595e-01\n",
      "  -2.73435997e-06  1.38032207e+00  2.63604535e-01  5.24842387e-01\n",
      "   8.86739291e-06  3.48244856e-02  5.83092194e-01  9.25515593e-02\n",
      "   3.98088640e-01  5.71253031e-01  4.63859044e-01  1.42189709e+00\n",
      "   7.26777505e-02  1.06285546e+00  6.53023689e-01  6.44247066e-01\n",
      "   6.23345919e-01  9.46321486e-01 -2.63760823e-03 -1.02083447e-05\n",
      "   1.60808632e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.2\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 158210.03859209007\n",
      "[0.26414037 0.24729911 0.17590831 0.26891772 0.28224116 0.26204197\n",
      " 0.28138032 0.17635652 0.19344356 0.278643   0.26451814 0.18529094\n",
      " 0.21435052 0.10727066 0.23845713 0.27603656 0.2476518  0.24378774\n",
      " 0.26292258 0.18338063 0.26617378 0.12135799 0.26606016 0.19115269\n",
      " 0.23893171 0.26969202 0.27604398 0.27596671 0.27199705 0.10478307\n",
      " 0.10915799 0.10997002 0.24985167]\n",
      "[[ 0.98794283 -0.331511    0.568259    0.42674932  0.94693807  0.65033768\n",
      "   1.26801337  0.67702065  1.23573805  0.7408421   0.34462191  0.84342687\n",
      "   0.37491951  0.98475062  0.63717883  0.63005603 -0.13419527  0.04871718\n",
      "   0.7238268  -0.21712391  0.61480209  0.28340523  0.41126077  1.33802187\n",
      "  -0.0341842   1.30659455  0.76405076  0.76636632  0.71791894  0.54610914\n",
      "  -0.29438251 -0.21263162  0.26589806]]\n",
      "{0: 3, 1: 885}\n",
      "acc 0.3367117117117117\n",
      "(0.3344632768361582, 1.0, 0.5012701100762066, None)\n",
      "\n",
      "1 loss 150072.91886504318\n",
      "[0.36479837 0.26662949 0.19427838 0.17879372 0.3447344  0.30316315\n",
      " 0.33439055 0.19250401 0.22430803 0.25499253 0.16457172 0.21246297\n",
      " 0.31554538 0.00931454 0.33965274 0.2201753  0.17623541 0.25730748\n",
      " 0.29131128 0.27325201 0.31792172 0.04923463 0.21130549 0.22053832\n",
      " 0.23681124 0.3095447  0.29419678 0.29149666 0.28826924 0.00547977\n",
      " 0.0191221  0.02185697 0.2699212 ]\n",
      "[[ 0.88941589 -0.23364833  0.56624141  0.45830805  0.86266286  0.60563886\n",
      "   1.2112248   0.67696236  1.21710144  0.65941011  0.41159026  0.82908711\n",
      "   0.27380737  1.08096646  0.53755914  0.55565175 -0.03458118  0.04317627\n",
      "   0.67765814 -0.11705271  0.56026896  0.3576942   0.40860592  1.35911968\n",
      "   0.00999339  1.23816975  0.7247103   0.72435493  0.68672029  0.64467244\n",
      "  -0.20108415 -0.11945344  0.23766959]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 loss 143223.99827703982\n",
      "[ 0.46072089  0.27929093  0.21645613  0.07603492  0.2712612   0.34220655\n",
      "  0.36920988  0.21199373  0.25606549  0.14598428  0.06217736  0.2416887\n",
      "  0.408167   -0.09090398  0.43479648  0.11484171  0.16076337  0.26858532\n",
      "  0.31404406  0.34488241  0.36825565 -0.02107536  0.13560278  0.2520049\n",
      "  0.2222409   0.34130882  0.28789081  0.27822236  0.29017206 -0.09560347\n",
      " -0.0702428  -0.06000989  0.28655828]\n",
      "[[ 7.94645557e-01 -1.35601424e-01  5.62542831e-01  4.80491935e-01\n",
      "   7.65394940e-01  5.62027858e-01  1.16881530e+00  6.75516944e-01\n",
      "   1.19869893e+00  5.64957510e-01  4.76273314e-01  8.14068656e-01\n",
      "   1.78494957e-01  1.17953551e+00  4.42523757e-01  4.90917532e-01\n",
      "   2.04256244e-05  4.29522692e-02  6.41682958e-01 -1.71745228e-02\n",
      "   5.06585978e-01  4.31235609e-01  4.27613894e-01  1.38022021e+00\n",
      "   3.37106242e-02  1.17356316e+00  7.00338370e-01  6.97680397e-01\n",
      "   6.65806649e-01  7.44967655e-01 -1.11833668e-01 -3.02196990e-02\n",
      "   2.11241311e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "3 loss 137431.718993949\n",
      "[ 0.55621758  0.28749045  0.2389709  -0.0275945   0.15775189  0.38024052\n",
      "  0.39334701  0.23175938  0.28619507  0.04051669 -0.04141167  0.26998225\n",
      "  0.49717414 -0.19142834  0.5292954   0.01117412  0.17708831  0.27979651\n",
      "  0.33452953  0.30136596  0.41779997 -0.08917611  0.05625818  0.28281153\n",
      "  0.21100461  0.36793697  0.27387226  0.25388979  0.28808046 -0.19674603\n",
      " -0.15230672 -0.07653342  0.30209948]\n",
      "[[ 7.00528159e-01 -3.75576551e-02  5.59029050e-01  5.07657744e-01\n",
      "   6.68075427e-01  5.19281079e-01  1.13546721e+00  6.74279640e-01\n",
      "   1.18209305e+00  5.20610652e-01  5.05197916e-01  8.00188288e-01\n",
      "   8.32039585e-02  1.27865898e+00  3.47896141e-01  5.15288465e-01\n",
      "   5.67665287e-06  4.42567422e-02  6.12214794e-01  1.26288493e-02\n",
      "   4.53618051e-01  5.03502809e-01  4.46515584e-01  1.40135079e+00\n",
      "   5.64465660e-02  1.11326513e+00  6.82384523e-01  6.78394996e-01\n",
      "   6.48867388e-01  8.45436341e-01 -2.94275744e-02  1.99544065e-06\n",
      "   1.85582408e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 132602.93312903098\n",
      "[ 0.65103578  0.28746997  0.26239328 -0.13119995  0.05123386  0.41685576\n",
      "  0.40903131  0.25244713  0.31533598 -0.06302635 -0.14502981  0.29799464\n",
      "  0.5552215  -0.29193474  0.62262762 -0.09239544  0.15318877  0.28991952\n",
      "  0.35240618  0.18857664  0.46635333 -0.15451457 -0.02796295  0.31332008\n",
      "  0.20305792  0.38917771  0.25090006  0.21736861  0.2804925  -0.29771028\n",
      " -0.17106941 -0.08220706  0.31573997]\n",
      "[[ 6.07422269e-01  6.34478660e-03  5.55381965e-01  5.39644012e-01\n",
      "   5.86247412e-01  4.77582716e-01  1.10851403e+00  6.72886982e-01\n",
      "   1.16679009e+00  5.49662632e-01  5.38004651e-01  7.86988106e-01\n",
      "   7.28416835e-05  1.37788506e+00  2.53948592e-01  5.44943999e-01\n",
      "  -2.81136312e-04  4.86773496e-02  5.90076214e-01  8.53693542e-02\n",
      "   4.01426419e-01  5.74142086e-01  4.69844275e-01  1.42250640e+00\n",
      "   7.76324016e-02  1.05920529e+00  6.70127556e-01  6.66030913e-01\n",
      "   6.35532289e-01  9.45784604e-01 -9.61689205e-07 -2.86979441e-04\n",
      "   1.61050798e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.65103578  0.28746997  0.26239328 -0.13119995  0.05123386  0.41685576\n",
      "  0.40903131  0.25244713  0.31533598 -0.06302635 -0.14502981  0.29799464\n",
      "  0.5552215  -0.29193474  0.62262762 -0.09239544  0.15318877  0.28991952\n",
      "  0.35240618  0.18857664  0.46635333 -0.15451457 -0.02796295  0.31332008\n",
      "  0.20305792  0.38917771  0.25090006  0.21736861  0.2804925  -0.29771028\n",
      " -0.17106941 -0.08220706  0.31573997]\n",
      "[[ 6.07422269e-01  6.34478660e-03  5.55381965e-01  5.39644012e-01\n",
      "   5.86247412e-01  4.77582716e-01  1.10851403e+00  6.72886982e-01\n",
      "   1.16679009e+00  5.49662632e-01  5.38004651e-01  7.86988106e-01\n",
      "   7.28416835e-05  1.37788506e+00  2.53948592e-01  5.44943999e-01\n",
      "  -2.81136312e-04  4.86773496e-02  5.90076214e-01  8.53693542e-02\n",
      "   4.01426419e-01  5.74142086e-01  4.69844275e-01  1.42250640e+00\n",
      "   7.76324016e-02  1.05920529e+00  6.70127556e-01  6.66030913e-01\n",
      "   6.35532289e-01  9.45784604e-01 -9.61689205e-07 -2.86979441e-04\n",
      "   1.61050798e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.30000000000000004\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 158230.8576025154\n",
      "[0.37996751 0.34342775 0.27832483 0.3691744  0.38296519 0.35798987\n",
      " 0.37967938 0.27843999 0.29368832 0.37934911 0.36434501 0.28687965\n",
      " 0.32481673 0.20829466 0.35139317 0.37668478 0.34562775 0.34031961\n",
      " 0.35967217 0.2889247  0.36333132 0.22229116 0.36553009 0.29098236\n",
      " 0.33577666 0.3648377  0.37565112 0.37531205 0.37014064 0.20572798\n",
      " 0.20996846 0.21077036 0.34639249]\n",
      "[[ 0.97087239 -0.33081447  0.56781852  0.42659461  0.94668593  0.65462575\n",
      "   1.27185112  0.67686703  1.23756019  0.74056699  0.34464464  0.84380411\n",
      "   0.36394637  0.98242736  0.62328725  0.62978379 -0.13386153  0.05030121\n",
      "   0.72205853 -0.21744142  0.61782442  0.28272267  0.41273754  1.33718619\n",
      "  -0.03339614  1.30552507  0.76578516  0.76870498  0.7219135   0.54488779\n",
      "  -0.29283769 -0.21101261  0.26909278]]\n",
      "{0: 3, 1: 885}\n",
      "acc 0.3367117117117117\n",
      "(0.3344632768361582, 1.0, 0.5012701100762066, None)\n",
      "\n",
      "1 loss 150557.21726026956\n",
      "[0.47995842 0.364387   0.28945161 0.27719186 0.44084535 0.39875083\n",
      " 0.42887323 0.28752712 0.31573569 0.35346114 0.2622661  0.30590907\n",
      " 0.42536363 0.11072875 0.45196198 0.31967385 0.2786578  0.35662916\n",
      " 0.387783   0.37605366 0.41490736 0.14921706 0.31443325 0.31091709\n",
      " 0.3322555  0.4013949  0.39307106 0.38994967 0.38619673 0.10676211\n",
      " 0.1201695  0.1229955  0.36807024]\n",
      "[[ 0.87324453 -0.23239892  0.57094274  0.46397727  0.86369747  0.61102976\n",
      "   1.22048738  0.68193281  1.22640156  0.6600525   0.41549404  0.83591343\n",
      "   0.26350068  1.07741703  0.52443508  0.5570222  -0.03412743  0.0507313\n",
      "   0.67367965 -0.11740316  0.56400802  0.35760282  0.41137063  1.35842723\n",
      "   0.01012747  1.23307779  0.73062838  0.73209517  0.69362291  0.64301247\n",
      "  -0.1972503  -0.11546638  0.24054359]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "2 loss 143949.76473273567\n",
      "[0.57534593 0.3796589  0.30456284 0.1742067  0.34932062 0.43811126\n",
      " 0.45938648 0.300031   0.33917595 0.24460459 0.15969775 0.32738691\n",
      " 0.51758757 0.0106771  0.54670957 0.214218   0.26425675 0.37169492\n",
      " 0.41073716 0.44557599 0.46557209 0.07760553 0.24275822 0.33342438\n",
      " 0.31379799 0.43021271 0.38721931 0.37802033 0.38889847 0.00579245\n",
      " 0.03126009 0.04327738 0.38699449]\n",
      "[[ 7.79494522e-01 -1.33851361e-01  5.72205587e-01  4.89664168e-01\n",
      "   7.65002338e-01  5.67910817e-01  1.18356906e+00  6.85482446e-01\n",
      "   1.21484060e+00  5.66167674e-01  4.83560930e-01  8.26866236e-01\n",
      "   1.68708844e-01  1.17539759e+00  4.30171666e-01  5.03484102e-01\n",
      "   2.71949244e-05  5.87371938e-02  6.38544102e-01 -1.74886698e-02\n",
      "   5.10601134e-01  4.31996000e-01  4.30502080e-01  1.37966086e+00\n",
      "   3.54061994e-02  1.16184141e+00  7.10417974e-01  7.10671443e-01\n",
      "   6.74913905e-01  7.43218150e-01 -1.04006611e-01 -2.21031377e-02\n",
      "   2.13384842e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "3 loss 138351.21822445234\n",
      "[ 0.67018798  0.39066621  0.32083579  0.07019544  0.23676213  0.47664154\n",
      "  0.47956046  0.31353703  0.36188654  0.13901478  0.0557546   0.34881505\n",
      "  0.60576423 -0.08982273  0.64073603  0.11021054  0.28522337  0.38697906\n",
      "  0.43155078  0.40947789  0.5156207   0.00811199  0.16670376  0.35612024\n",
      "  0.29913878  0.45386152  0.37382289  0.35534474  0.38755852 -0.09536803\n",
      " -0.04608658  0.0337622   0.40493201]\n",
      "[[ 6.86756098e-01 -3.52963403e-02  5.73070064e-01  5.19318914e-01\n",
      "   6.68204380e-01  5.25443343e-01  1.15517703e+00  6.88715876e-01\n",
      "   1.20415223e+00  5.38697676e-01  5.15676636e-01  8.18138329e-01\n",
      "   7.40028268e-02  1.27426787e+00  3.36546138e-01  5.30371928e-01\n",
      "   2.96077428e-05  7.06950288e-02  6.14417701e-01  9.13157192e-03\n",
      "   4.57749775e-01  5.05207391e-01  4.51014886e-01  1.40090681e+00\n",
      "   5.97289671e-02  1.09269098e+00  6.96415994e-01  6.96471952e-01\n",
      "   6.60045223e-01  8.43760044e-01 -1.44452810e-02 -2.75238702e-06\n",
      "   1.86890877e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 loss 133762.9095588707\n",
      "[ 0.76405485  0.38963446  0.33876305 -0.0338205   0.1301764   0.51394123\n",
      "  0.49185489  0.32865407  0.38447934  0.03506455 -0.04827279  0.37078606\n",
      "  0.65608201 -0.19031188  0.73337443  0.00622732  0.34985146  0.40156668\n",
      "  0.45009059  0.2969208   0.5648575  -0.05862738  0.08504983  0.37934155\n",
      "  0.28797269  0.47216511  0.35193262  0.32103444  0.38090983 -0.19635323\n",
      " -0.05155111  0.077202    0.42116959]\n",
      "[[ 5.95780176e-01  6.94262350e-03  5.73286868e-01  5.55951159e-01\n",
      "   5.99971725e-01  4.83870668e-01  1.13261886e+00  6.91311744e-01\n",
      "   1.19393267e+00  5.71341013e-01  5.53219694e-01  8.09365382e-01\n",
      "   1.97426212e-05  1.37336495e+00  2.44106098e-01  5.64404219e-01\n",
      "   2.70436973e-05  8.94204296e-02  6.05960079e-01  7.71437344e-02\n",
      "   4.05560286e-01  5.76831863e-01  4.76675361e-01  1.42216254e+00\n",
      "   8.24734160e-02  1.02857842e+00  6.87863002e-01  6.88962729e-01\n",
      "   6.48644184e-01  9.44206408e-01 -3.10785412e-07  2.14776825e-06\n",
      "   1.61394026e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.76405485  0.38963446  0.33876305 -0.0338205   0.1301764   0.51394123\n",
      "  0.49185489  0.32865407  0.38447934  0.03506455 -0.04827279  0.37078606\n",
      "  0.65608201 -0.19031188  0.73337443  0.00622732  0.34985146  0.40156668\n",
      "  0.45009059  0.2969208   0.5648575  -0.05862738  0.08504983  0.37934155\n",
      "  0.28797269  0.47216511  0.35193262  0.32103444  0.38090983 -0.19635323\n",
      " -0.05155111  0.077202    0.42116959]\n",
      "[[ 5.95780176e-01  6.94262350e-03  5.73286868e-01  5.55951159e-01\n",
      "   5.99971725e-01  4.83870668e-01  1.13261886e+00  6.91311744e-01\n",
      "   1.19393267e+00  5.71341013e-01  5.53219694e-01  8.09365382e-01\n",
      "   1.97426212e-05  1.37336495e+00  2.44106098e-01  5.64404219e-01\n",
      "   2.70436973e-05  8.94204296e-02  6.05960079e-01  7.71437344e-02\n",
      "   4.05560286e-01  5.76831863e-01  4.76675361e-01  1.42216254e+00\n",
      "   8.24734160e-02  1.02857842e+00  6.87863002e-01  6.88962729e-01\n",
      "   6.48644184e-01  9.44206408e-01 -3.10785412e-07  2.14776825e-06\n",
      "   1.61394026e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.4\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 158403.06082663927\n",
      "[0.5009498  0.43941338 0.37973994 0.46940977 0.48362282 0.45239124\n",
      " 0.47610904 0.37962111 0.39253022 0.48001455 0.46410171 0.38726144\n",
      " 0.43641674 0.31067158 0.46607195 0.4773077  0.44330017 0.43616335\n",
      " 0.45566554 0.39476535 0.4590294  0.323843   0.46483645 0.38906408\n",
      " 0.43259539 0.45839795 0.47464842 0.47377916 0.4666898  0.30796685\n",
      " 0.31188365 0.31264572 0.44236755]\n",
      "[[ 0.97595582 -0.33020613  0.56796918  0.42649456  0.94655876  0.66006767\n",
      "   1.27819751  0.67716326  1.23992394  0.74039404  0.34471964  0.84472503\n",
      "   0.3513627   0.97767008  0.60631742  0.62959807 -0.13362076  0.05125181\n",
      "   0.71745022 -0.21765731  0.6222215   0.28143982  0.41437545  1.33722464\n",
      "  -0.03271499  1.30170795  0.76846665  0.77238485  0.72794994  0.5421691\n",
      "  -0.2918841  -0.21002024  0.27258847]]\n",
      "{0: 5, 1: 883}\n",
      "acc 0.33896396396396394\n",
      "(0.3352208380520951, 1.0, 0.5021204410517388, None)\n",
      "\n",
      "1 loss 151174.1418170664\n",
      "[0.6000701  0.46192444 0.38345766 0.37551971 0.5337608  0.49235155\n",
      " 0.52059218 0.38152239 0.40528387 0.45126228 0.35989165 0.39785555\n",
      " 0.53622108 0.21405144 0.56590618 0.41885346 0.38190102 0.45586838\n",
      " 0.48330064 0.47814873 0.51000886 0.25007433 0.4182309  0.39886901\n",
      " 0.42775557 0.49140753 0.4911765  0.48728637 0.48239921 0.20986271\n",
      " 0.22269591 0.22552137 0.46550067]\n",
      "[[ 0.87969565 -0.23130302  0.57601275  0.46834461  0.86514736  0.61797349\n",
      "   1.23308256  0.68709084  1.23599153  0.66084735  0.41895713  0.84300623\n",
      "   0.25179839  1.07018431  0.50851464  0.55851263 -0.03379075  0.07874067\n",
      "   0.6569846  -0.11764085  0.5695266   0.35669569  0.4140083   1.35857898\n",
      "   0.00997844  1.21602124  0.73744473  0.74125628  0.70252217  0.639169\n",
      "  -0.19485101 -0.11299957  0.24382785]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "2 loss 144807.77484939326\n",
      "[0.69462718 0.47978376 0.39113792 0.272173   0.43001959 0.53194421\n",
      " 0.54688954 0.38673545 0.41982632 0.3424418  0.2570271  0.41111164\n",
      " 0.62789359 0.11431688 0.66006021 0.31318271 0.36870733 0.47583957\n",
      " 0.50302853 0.54442722 0.56094447 0.17718162 0.35128666 0.411775\n",
      " 0.40539079 0.51286447 0.48580041 0.47669342 0.48598265 0.10914587\n",
      " 0.13415674 0.14743762 0.48667379]\n",
      "[[ 7.87683731e-01 -1.32316194e-01  5.82139167e-01  4.95570736e-01\n",
      "   7.64574510e-01  5.75407357e-01  1.20147537e+00  6.95500851e-01\n",
      "   1.23111541e+00  5.67476479e-01  4.87783743e-01  8.39787999e-01\n",
      "   1.57833961e-01  1.16705660e+00  4.15477191e-01  5.14209532e-01\n",
      "   8.71898567e-06  1.30999193e-01  6.32531800e-01 -1.76963815e-02\n",
      "   5.16457298e-01  4.31926092e-01  4.32645279e-01  1.37991882e+00\n",
      "   3.67691891e-02  1.12274259e+00  7.21015677e-01  7.24593969e-01\n",
      "   6.85685720e-01  7.39057828e-01 -9.92279223e-02 -1.72353845e-02\n",
      "   2.15930179e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "3 loss 139407.09569339032\n",
      "[0.78809427 0.49377405 0.40051099 0.16812764 0.31879558 0.5711355\n",
      " 0.56393258 0.39338124 0.43441813 0.2370964  0.15306368 0.42498061\n",
      " 0.71467549 0.01381618 0.75306873 0.20915317 0.39528952 0.49711709\n",
      " 0.50297014 0.51759774 0.61162981 0.10611142 0.27999262 0.42571903\n",
      " 0.38698791 0.5100181  0.47384104 0.45666853 0.48602059 0.00794238\n",
      " 0.0612145  0.12859297 0.50722578]\n",
      "[[ 6.97810030e-01 -3.33155535e-02  5.87491943e-01  5.15083851e-01\n",
      "   6.66927036e-01  5.33110529e-01  1.17739181e+00  7.03277975e-01\n",
      "   1.22632631e+00  5.41179818e-01  5.10135366e-01  8.36260798e-01\n",
      "   6.44333499e-02  1.26549455e+00  3.23857706e-01  5.29884663e-01\n",
      "   2.57525402e-06  2.12114072e-01  7.08316061e-01  5.91031802e-03\n",
      "   4.63662365e-01  5.06172205e-01  4.53307803e-01  1.40125840e+00\n",
      "   6.28217723e-02  1.02976609e+00  7.10168239e-01  7.14419171e-01\n",
      "   6.72273936e-01  8.39645753e-01 -5.59283359e-03 -3.67740124e-04\n",
      "   1.88453324e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 135050.05236225922\n",
      "[ 0.87946441  0.49118143  0.41168564  0.0639666   0.21204635  0.60959535\n",
      "  0.57478235  0.40174037  0.44935832  0.13298369  0.04889166  0.43969973\n",
      "  0.75731373 -0.08674877  0.84400976  0.10502195  0.45701936  0.50278795\n",
      "  0.5029005   0.40740468  0.66188171  0.03746976  0.20369257  0.44069573\n",
      "  0.37166091  0.50202193  0.45577661  0.42786589  0.48219678 -0.09315182\n",
      "  0.06033685  0.04696838  0.52677225]\n",
      "[[ 6.12325188e-01  7.45452033e-03  5.92026589e-01  5.50991367e-01\n",
      "   6.01842191e-01  4.91399028e-01  1.15805089e+00  7.10268508e-01\n",
      "   1.22146346e+00  5.71868008e-01  5.47058626e-01  8.32309111e-01\n",
      "   3.30972937e-05  1.36444734e+00  2.34999552e-01  5.62813462e-01\n",
      "   3.94454821e-05  3.11862783e-01  7.93286109e-01  6.58251041e-02\n",
      "   4.11331530e-01  5.78995693e-01  4.77270698e-01  1.42259824e+00\n",
      "   8.75568057e-02  9.38480209e-01  7.03534025e-01  7.09406404e-01\n",
      "   6.61588615e-01  9.40244264e-01  1.45067075e-06 -2.90177714e-04\n",
      "   1.61605207e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[ 0.87946441  0.49118143  0.41168564  0.0639666   0.21204635  0.60959535\n",
      "  0.57478235  0.40174037  0.44935832  0.13298369  0.04889166  0.43969973\n",
      "  0.75731373 -0.08674877  0.84400976  0.10502195  0.45701936  0.50278795\n",
      "  0.5029005   0.40740468  0.66188171  0.03746976  0.20369257  0.44069573\n",
      "  0.37166091  0.50202193  0.45577661  0.42786589  0.48219678 -0.09315182\n",
      "  0.06033685  0.04696838  0.52677225]\n",
      "[[ 6.12325188e-01  7.45452033e-03  5.92026589e-01  5.50991367e-01\n",
      "   6.01842191e-01  4.91399028e-01  1.15805089e+00  7.10268508e-01\n",
      "   1.22146346e+00  5.71868008e-01  5.47058626e-01  8.32309111e-01\n",
      "   3.30972937e-05  1.36444734e+00  2.34999552e-01  5.62813462e-01\n",
      "   3.94454821e-05  3.11862783e-01  7.93286109e-01  6.58251041e-02\n",
      "   4.11331530e-01  5.78995693e-01  4.77270698e-01  1.42259824e+00\n",
      "   8.75568057e-02  9.38480209e-01  7.03534025e-01  7.09406404e-01\n",
      "   6.61588615e-01  9.40244264e-01  1.45067075e-06 -2.90177714e-04\n",
      "   1.61605207e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.5\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 158714.45773483493\n",
      "[0.6221304  0.53527072 0.48045015 0.56752696 0.58347294 0.54437105\n",
      " 0.56851871 0.48031631 0.49101135 0.57965898 0.56092623 0.48708586\n",
      " 0.55063716 0.41616082 0.58296411 0.57670186 0.53864919 0.50411478\n",
      " 0.57261805 0.50511759 0.55138457 0.42723249 0.5623959  0.48674664\n",
      " 0.52924545 0.58755554 0.5714796  0.56984722 0.55946245 0.41339478\n",
      " 0.41663435 0.41730417 0.53700522]\n",
      "[[ 0.98273892 -0.32970096  0.56841082  0.42762999  0.94709251  0.6673274\n",
      "   1.28872786  0.67752884  1.24225985  0.7409801   0.34642165  0.84575382\n",
      "   0.3366388   0.96699632  0.58720708  0.6302634  -0.13346617  0.07764335\n",
      "   0.71537822 -0.2177884   0.6297356   0.27841736  0.41695688  1.33725808\n",
      "  -0.03215757  1.30043734  0.77343635  0.77848422  0.73748558  0.53562008\n",
      "  -0.29117533 -0.20928692  0.27707636]]\n",
      "{0: 13, 1: 875}\n",
      "acc 0.34797297297297297\n",
      "(0.3382857142857143, 1.0, 0.5055508112724167, None)\n",
      "\n",
      "1 loss 151963.83463100393\n",
      "[0.71961912 0.55897368 0.47724997 0.46952898 0.6251339  0.58218604\n",
      " 0.60701749 0.47557027 0.49492968 0.54472471 0.45345841 0.48980604\n",
      " 0.64886564 0.32217821 0.68131957 0.51305054 0.48233605 0.50280192\n",
      " 0.53241913 0.58349813 0.60037892 0.35367171 0.51853403 0.48645082\n",
      " 0.5233414  0.664963   0.5864699  0.58149599 0.57401408 0.31774915\n",
      " 0.32935371 0.3320528  0.5608525 ]\n",
      "[[ 0.88898432 -0.23037746  0.58086471  0.46972063  0.86689353  0.62796448\n",
      "   1.2508956   0.69173906  1.24477384  0.66045297  0.42430398  0.84949162\n",
      "   0.23890551  1.05367835  0.49142599  0.55684856 -0.03356044  0.18660846\n",
      "   0.63537495 -0.11778945  0.57947384  0.35327179  0.41829163  1.3586963\n",
      "   0.00933112  1.21715347  0.74626538  0.75276545  0.71520967  0.62942112\n",
      "  -0.19313533 -0.11125116  0.24865945]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "2 loss 145890.503935734\n",
      "[0.8125235  0.57945654 0.47719631 0.36544679 0.51725262 0.62171417\n",
      " 0.63049225 0.47327207 0.50071552 0.43506585 0.35009191 0.49452996\n",
      " 0.739605   0.22333996 0.77436016 0.4065571  0.47066817 0.50289937\n",
      " 0.50313229 0.64453202 0.65134615 0.27964742 0.45864746 0.48922531\n",
      " 0.49702427 0.67755985 0.58294484 0.57396487 0.57937791 0.21781863\n",
      " 0.24146501 0.25553587 0.58424092]\n",
      "[[ 8.00432808e-01 -1.31016679e-01  5.91761483e-01  4.82036466e-01\n",
      "   7.63840484e-01  5.86279741e-01  1.22349615e+00  7.04859621e-01\n",
      "   1.24629456e+00  5.64985583e-01  4.74376362e-01  8.51876930e-01\n",
      "   1.46538406e-01  1.14804090e+00  4.00736856e-01  5.05472938e-01\n",
      "   2.42561079e-05  2.85371792e-01  5.72948152e-01 -1.78211980e-02\n",
      "   5.27029097e-01  4.29205641e-01  4.33602896e-01  1.38011934e+00\n",
      "   3.74992465e-02  1.12870505e+00  7.32249418e-01  7.39268141e-01\n",
      "   6.99420275e-01  7.28222116e-01 -9.60146497e-02 -1.40086411e-02\n",
      "   2.20062715e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "3 loss 140711.24455813642\n",
      "[0.90248904 0.59671137 0.47879758 0.26162192 0.40696625 0.66177393\n",
      " 0.64632314 0.47228081 0.51349161 0.32969565 0.24627271 0.49995864\n",
      " 0.82414296 0.12293848 0.86507122 0.30273181 0.5017955  0.50291944\n",
      " 0.50311487 0.62887487 0.70282055 0.20676045 0.39608514 0.4935781\n",
      " 0.47437295 0.57625688 0.57487948 0.55977612 0.58239123 0.11665742\n",
      " 0.17252237 0.25531443 0.60775542]\n",
      "[[ 7.17887810e-01 -3.16431426e-02  6.01832584e-01  4.89369316e-01\n",
      "   6.63264378e-01  5.44074698e-01  1.20236999e+00  7.17356781e-01\n",
      "   1.24779832e+00  5.20438902e-01  4.83268538e-01  8.53678344e-01\n",
      "   5.60812989e-02  1.24554641e+00  3.13813277e-01  5.07352557e-01\n",
      "   2.74388167e-05  3.83576836e-01  6.31756686e-01  3.13234146e-03\n",
      "   4.74272765e-01  5.04617732e-01  4.50178687e-01  1.40153541e+00\n",
      "   6.54919730e-02  1.02075283e+00  7.22791474e-01  7.30832080e-01\n",
      "   6.86305730e-01  8.28660085e-01 -4.55866773e-06  1.33853196e-04\n",
      "   1.91405285e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 136595.57500569543\n",
      "[0.98243601 0.59231629 0.48243052 0.15707536 0.29890492 0.70159422\n",
      " 0.6570869  0.47315781 0.52684838 0.2251857  0.14171581 0.51317247\n",
      " 0.86103333 0.02225049 0.95066568 0.19821235 0.52238279 0.50292618\n",
      " 0.50287635 0.52492326 0.75422146 0.13592398 0.32832278 0.49966081\n",
      " 0.45443441 0.50242264 0.56226175 0.53854878 0.58250152 0.015405\n",
      " 0.17254929 0.32932964 0.63082089]\n",
      "[[ 6.56938759e-01  7.66113337e-03  6.10893796e-01  5.25262671e-01\n",
      "   5.87286335e-01  5.02109846e-01  1.18522735e+00  7.28915932e-01\n",
      "   1.24932968e+00  5.50727974e-01  5.20272406e-01  8.54900138e-01\n",
      "   1.09406209e-04  1.34420444e+00  2.37167721e-01  5.39952791e-01\n",
      "   1.33208528e-05  4.82874356e-01  7.14273058e-01  5.05085492e-02\n",
      "   4.21799716e-01  5.78802134e-01  4.69881893e-01  1.42294455e+00\n",
      "   9.24326701e-02  9.19364569e-01  7.16834037e-01  7.26811744e-01\n",
      "   6.75445701e-01  9.29409431e-01 -1.97269119e-06  9.63220904e-08\n",
      "   1.63060144e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[0.98243601 0.59231629 0.48243052 0.15707536 0.29890492 0.70159422\n",
      " 0.6570869  0.47315781 0.52684838 0.2251857  0.14171581 0.51317247\n",
      " 0.86103333 0.02225049 0.95066568 0.19821235 0.52238279 0.50292618\n",
      " 0.50287635 0.52492326 0.75422146 0.13592398 0.32832278 0.49966081\n",
      " 0.45443441 0.50242264 0.56226175 0.53854878 0.58250152 0.015405\n",
      " 0.17254929 0.32932964 0.63082089]\n",
      "[[ 6.56938759e-01  7.66113337e-03  6.10893796e-01  5.25262671e-01\n",
      "   5.87286335e-01  5.02109846e-01  1.18522735e+00  7.28915932e-01\n",
      "   1.24932968e+00  5.50727974e-01  5.20272406e-01  8.54900138e-01\n",
      "   1.09406209e-04  1.34420444e+00  2.37167721e-01  5.39952791e-01\n",
      "   1.33208528e-05  4.82874356e-01  7.14273058e-01  5.05085492e-02\n",
      "   4.21799716e-01  5.78802134e-01  4.69881893e-01  1.42294455e+00\n",
      "   9.24326701e-02  9.19364569e-01  7.16834037e-01  7.26811744e-01\n",
      "   6.75445701e-01  9.29409431e-01 -1.97269119e-06  9.63220904e-08\n",
      "   1.63060144e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.6000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 159098.08303401002\n",
      "[0.70896293 0.63169433 0.58002021 0.6692557  0.68417803 0.63339622\n",
      " 0.65542353 0.58001159 0.59173542 0.6806707  0.66272411 0.58860348\n",
      " 0.66767224 0.53611279 0.71059332 0.67793246 0.63737977 0.60805281\n",
      " 0.68011913 0.61581608 0.63709109 0.53534229 0.66273204 0.58248809\n",
      " 0.62571259 0.68791603 0.66652714 0.66457741 0.64818419 0.52888433\n",
      " 0.53064204 0.53107954 0.6293518 ]\n",
      "[[ 0.9876986  -0.32933001  0.56929025  0.4270951   0.94744019  0.67720772\n",
      "   1.30423194  0.67817652  1.24573985  0.74109378  0.34560332  0.84798437\n",
      "   0.31606154  0.94735944  0.59742041  0.63020427 -0.13336816  0.0828214\n",
      "   0.71591101 -0.21786889  0.64357768  0.27071499  0.4176856   1.33728157\n",
      "  -0.03175053  1.30103992  0.78130289  0.7867873   0.75059395  0.51981136\n",
      "  -0.29057983 -0.20867279  0.28395332]]\n",
      "{0: 37, 1: 851}\n",
      "acc 0.35923423423423423\n",
      "(0.3396004700352526, 0.9763513513513513, 0.5039232781168265, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss 152907.24504237503\n",
      "[0.80451608 0.65660472 0.57006638 0.57501338 0.72826575 0.66626391\n",
      " 0.6873046  0.56900072 0.59152981 0.65827296 0.55718004 0.58727459\n",
      " 0.76431492 0.45286707 0.80694194 0.62396557 0.58901129 0.50283797\n",
      " 0.64988438 0.68733681 0.67967573 0.46513757 0.6301752  0.57218755\n",
      " 0.61950401 0.76286451 0.68495648 0.67916453 0.66353236 0.44243736\n",
      " 0.45111877 0.45336717 0.65279362]\n",
      "[[ 0.89718401 -0.22967618  0.58580293  0.44071792  0.86983266  0.64361125\n",
      "   1.27528148  0.696071    1.2552309   0.66086667  0.41108729  0.85765962\n",
      "   0.22083383  1.01691618  0.50542189  0.55275452 -0.03341114  0.19556896\n",
      "   0.63574837 -0.11787357  0.60033812  0.3421216   0.41609083  1.35877775\n",
      "   0.00790069  1.22099519  0.75522238  0.76405269  0.73108081  0.60270788\n",
      "  -0.19164537 -0.10974208  0.25733478]]\n",
      "{0: 2, 1: 886}\n",
      "acc 0.3355855855855856\n",
      "(0.3340857787810384, 1.0, 0.5008460236886634, None)\n",
      "\n",
      "2 loss 147241.31650832092\n",
      "[0.89458664 0.67958805 0.56240841 0.47014216 0.62747599 0.70421696\n",
      " 0.70990551 0.55949413 0.59259494 0.54763946 0.45291276 0.58780442\n",
      " 0.85293447 0.35753838 0.89686973 0.51574005 0.57900476 0.50295611\n",
      " 0.53970608 0.74032057 0.72905886 0.39100104 0.58208695 0.56487679\n",
      " 0.58954778 0.75508586 0.68532817 0.67732903 0.67192866 0.3457753\n",
      " 0.36576087 0.37994258 0.67788254]\n",
      "[[ 8.14437432e-01 -1.30005638e-01  6.01179245e-01  4.34393612e-01\n",
      "   7.63931954e-01  6.04404158e-01  1.25097852e+00  7.13458734e-01\n",
      "   1.26423035e+00  5.58608738e-01  4.24074513e-01  8.66489753e-01\n",
      "   1.32348858e-01  1.10279272e+00  4.21684135e-01  4.69695465e-01\n",
      "   6.42326075e-05  2.92945682e-01  5.35353766e-01 -1.78886953e-02\n",
      "   5.50165255e-01  4.17803698e-01  4.25769519e-01  1.38026064e+00\n",
      "   3.68734429e-02  1.13104374e+00  7.42006308e-01  7.51930015e-01\n",
      "   7.15496779e-01  6.97055152e-01 -9.33972650e-02 -1.14099076e-02\n",
      "   2.28590730e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "3 loss 142411.33483155468\n",
      "[0.97615581 0.69976471 0.55645711 0.36505897 0.51264535 0.74453845\n",
      " 0.72552708 0.55119691 0.59448734 0.43927447 0.34782019 0.58953605\n",
      " 0.93336908 0.25813164 0.97800366 0.4103605  0.60950734 0.5030749\n",
      " 0.50312491 0.73377435 0.78081685 0.31653039 0.53021975 0.55983748\n",
      " 0.56302323 0.65819241 0.68115342 0.6687912  0.67801152 0.24547109\n",
      " 0.30078944 0.37712751 0.70391666]\n",
      "[[ 7.50290231e-01 -3.03348208e-02  6.15672461e-01  4.53291647e-01\n",
      "   6.60450753e-01  5.62858054e-01  1.23184284e+00  7.30304203e-01\n",
      "   1.27289668e+00  4.94283240e-01  4.45098828e-01  8.74766711e-01\n",
      "   5.03812673e-02  1.19688007e+00  3.56829884e-01  4.77414759e-01\n",
      "   4.51554801e-05  3.90635433e-01  5.28701277e-01  1.32939976e-03\n",
      "   4.97947530e-01  4.94237598e-01  4.38191205e-01  1.40173340e+00\n",
      "   6.65443890e-02  1.02088733e+00  7.33478103e-01  7.44922852e-01\n",
      "   7.02286874e-01  7.96145454e-01 -1.58099426e-07 -1.41429872e-06\n",
      "   1.99024818e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 138668.90062131485\n",
      "[1.         0.69398694 0.55281455 0.25994059 0.402711   0.78542467\n",
      " 0.73639048 0.544913   0.59784094 0.33413012 0.24269128 0.59305232\n",
      " 0.97108558 0.15741836 1.         0.30526983 0.66385975 0.50311437\n",
      " 0.50308955 0.64081188 0.83316186 0.24348234 0.47202433 0.55733433\n",
      " 0.53887991 0.53608542 0.67288114 0.65369426 0.68145489 0.14412636\n",
      " 0.30066012 0.41614825 0.72996442]\n",
      "[[7.42394277e-01 7.16989314e-03 6.28976530e-01 4.84915378e-01\n",
      "  5.73500842e-01 5.20846897e-01 1.21632970e+00 7.46121060e-01\n",
      "  1.28088937e+00 5.18854712e-01 4.78172425e-01 8.82191181e-01\n",
      "  3.14203249e-04 1.29438262e+00 3.49622418e-01 5.04622712e-01\n",
      "  2.41883401e-05 4.89258882e-01 6.13382996e-01 3.36746222e-02\n",
      "  4.45584972e-01 5.69881291e-01 4.55626070e-01 1.42319506e+00\n",
      "  9.55071792e-02 9.12739298e-01 7.28279757e-01 7.42207949e-01\n",
      "  6.91179862e-01 8.96797517e-01 1.30390425e-06 4.57296196e-05\n",
      "  1.69423974e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[1.         0.69398694 0.55281455 0.25994059 0.402711   0.78542467\n",
      " 0.73639048 0.544913   0.59784094 0.33413012 0.24269128 0.59305232\n",
      " 0.97108558 0.15741836 1.         0.30526983 0.66385975 0.50311437\n",
      " 0.50308955 0.64081188 0.83316186 0.24348234 0.47202433 0.55733433\n",
      " 0.53887991 0.53608542 0.67288114 0.65369426 0.68145489 0.14412636\n",
      " 0.30066012 0.41614825 0.72996442]\n",
      "[[7.42394277e-01 7.16989314e-03 6.28976530e-01 4.84915378e-01\n",
      "  5.73500842e-01 5.20846897e-01 1.21632970e+00 7.46121060e-01\n",
      "  1.28088937e+00 5.18854712e-01 4.78172425e-01 8.82191181e-01\n",
      "  3.14203249e-04 1.29438262e+00 3.49622418e-01 5.04622712e-01\n",
      "  2.41883401e-05 4.89258882e-01 6.13382996e-01 3.36746222e-02\n",
      "  4.45584972e-01 5.69881291e-01 4.55626070e-01 1.42319506e+00\n",
      "  9.55071792e-02 9.12739298e-01 7.28279757e-01 7.42207949e-01\n",
      "  6.91179862e-01 8.96797517e-01 1.30390425e-06 4.57296196e-05\n",
      "  1.69423974e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.7000000000000001\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 159404.5094101522\n",
      "[0.80183742 0.72888758 0.67884138 0.77347706 0.78492485 0.72103156\n",
      " 0.73634319 0.67906323 0.68773211 0.7821371  0.76867248 0.68592778\n",
      " 0.78924414 0.66772555 0.80775346 0.78000612 0.74081758 0.71123015\n",
      " 0.78170474 0.72622677 0.71502541 0.65415529 0.75944278 0.67797681\n",
      " 0.72158843 0.78797663 0.75304161 0.75467055 0.73055335 0.66863731\n",
      " 0.65965129 0.6588     0.71672058]\n",
      "[[ 0.99344568 -0.32908352  0.5704008   0.42600792  0.94850611  0.68821937\n",
      "   1.3220146   0.67901563  1.24899885  0.7416825   0.34346314  0.85027072\n",
      "   0.29815986  0.9220322   0.60644828  0.63040615 -0.13331875  0.07886063\n",
      "   0.71642267 -0.21790631  0.66455591  0.25235294  0.42619813  1.33729181\n",
      "  -0.03149648  1.30257847  0.79720127  0.79992231  0.76700405  0.46786612\n",
      "  -0.29015609 -0.20823153  0.29573061]]\n",
      "{0: 218, 1: 670}\n",
      "acc 0.49099099099099097\n",
      "(0.3835820895522388, 0.8682432432432432, 0.5320910973084886, None)\n",
      "\n",
      "1 loss 153685.64529532337\n",
      "[0.89335442 0.75478307 0.66291367 0.69701486 0.83804191 0.74282853\n",
      " 0.75653739 0.66253582 0.67889981 0.79292282 0.67578802 0.67672686\n",
      " 0.88284599 0.62024528 0.90022545 0.75752515 0.69626316 0.58996739\n",
      " 0.78512919 0.78958914 0.73643745 0.60022976 0.74473252 0.65900415\n",
      " 0.7169241  0.8597626  0.77440932 0.77297786 0.74292737 0.61380819\n",
      " 0.6073206  0.60812261 0.7328247 ]\n",
      "[[ 0.91005334 -0.22921866  0.59044176  0.37081351  0.87630078  0.66645673\n",
      "   1.30627501  0.69998257  1.26402228  0.66642492  0.34582916  0.86480814\n",
      "   0.20903261  0.95872284  0.52184243  0.55235777 -0.03333278  0.19191861\n",
      "   0.64039756 -0.11790977  0.64312011  0.3075641   0.41836592  1.35881934\n",
      "   0.00527765  1.22856006  0.77533681  0.78048859  0.75475691  0.50427364\n",
      "  -0.19042631 -0.10850018  0.27804141]]\n",
      "{0: 87, 1: 801}\n",
      "acc 0.3997747747747748\n",
      "(0.352059925093633, 0.9527027027027027, 0.5141294439380127, None)\n",
      "\n",
      "2 loss 148668.58748858812\n",
      "[0.97351031 0.77937946 0.64856083 0.59190102 0.7799441  0.77099556\n",
      " 0.77488493 0.6470792  0.67112392 0.69306337 0.57109373 0.6688468\n",
      " 0.96408802 0.55330115 0.97950587 0.64976248 0.68676261 0.50372879\n",
      " 0.68227312 0.83427698 0.77058524 0.53576067 0.71298643 0.64236215\n",
      " 0.68630855 0.85735072 0.78253913 0.7793141  0.75334389 0.54095719\n",
      " 0.54196773 0.55294438 0.75395573]\n",
      "[[ 8.47181198e-01 -1.29344875e-01  6.09633007e-01  3.29877299e-01\n",
      "   7.71879409e-01  6.37909627e-01  1.28972226e+00  7.20567006e-01\n",
      "   1.27851794e+00  5.55139757e-01  3.16823322e-01  8.78665744e-01\n",
      "   1.37155874e-01  1.01125871e+00  4.60435351e-01  4.44165953e-01\n",
      "   4.74070499e-05  2.72697265e-01  5.29328823e-01 -1.79152866e-02\n",
      "   6.08616806e-01  3.73561752e-01  4.18655740e-01  1.38033976e+00\n",
      "   3.20480757e-02  1.14170577e+00  7.59432811e-01  7.67893067e-01\n",
      "   7.41350313e-01  5.70851022e-01 -9.09869302e-02 -9.03049079e-03\n",
      "   2.54710715e-01]]\n",
      "{0: 14, 1: 874}\n",
      "acc 0.3490990990990991\n",
      "(0.33867276887871856, 1.0, 0.505982905982906, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loss 144664.45909875765\n",
      "[1.         0.80157046 0.63548678 0.48580811 0.66420649 0.80602113\n",
      " 0.79210633 0.63244537 0.66417163 0.57426502 0.46499335 0.66198967\n",
      " 0.99999998 0.46797392 1.         0.53815478 0.72123049 0.50376205\n",
      " 0.56526405 0.83333677 0.81578783 0.46397294 0.67886664 0.62787157\n",
      " 0.657702   0.79489623 0.78426002 0.77926852 0.76312375 0.45246648\n",
      " 0.48791626 0.55182962 0.77953434]\n",
      "[[ 8.38108555e-01 -2.94620943e-02  6.28132832e-01  3.43045257e-01\n",
      "   6.59976711e-01  6.02484200e-01  1.27205130e+00  7.40867571e-01\n",
      "   1.29259463e+00  4.52834673e-01  3.32919975e-01  8.91992293e-01\n",
      "   1.19650239e-01  1.08176349e+00  4.54319176e-01  3.72142703e-01\n",
      "   2.60561328e-04  3.35625276e-01  4.29792622e-01  3.74634823e-04\n",
      "   5.63356999e-01  4.46981771e-01  4.21662442e-01  1.40185305e+00\n",
      "   6.16364723e-02  1.03067270e+00  7.49004692e-01  7.59632225e-01\n",
      "   7.27484167e-01  6.55540575e-01 -2.82462018e-07 -7.39981762e-08\n",
      "   2.26967996e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "4 loss 141834.661646692\n",
      "[1.         0.7955118  0.62407348 0.37944696 0.53982516 0.84586015\n",
      " 0.80553723 0.61891333 0.65843002 0.46411808 0.35862086 0.65658795\n",
      " 1.         0.37065727 1.         0.43166567 0.76178324 0.50371851\n",
      " 0.50327428 0.7599608  0.86700335 0.38921601 0.63822025 0.61597184\n",
      " 0.62964728 0.69629476 0.78220069 0.77366311 0.77098695 0.35407354\n",
      " 0.49162399 0.59364324 0.80745024]\n",
      "[[8.38108555e-01 5.51227391e-03 6.45788626e-01 3.73296246e-01\n",
      "  5.56008907e-01 5.62456723e-01 1.25627846e+00 7.60772404e-01\n",
      "  1.30607620e+00 4.14459325e-01 3.64896345e-01 9.04594100e-01\n",
      "  1.19650239e-01 1.16877582e+00 4.54319176e-01 3.97432966e-01\n",
      "  1.45989195e-04 4.15502870e-01 4.32845977e-01 1.51753121e-02\n",
      "  5.12765665e-01 5.23587436e-01 4.30097201e-01 1.42335694e+00\n",
      "  9.21245199e-02 9.18727432e-01 7.42265233e-01 7.55237465e-01\n",
      "  7.14959967e-01 7.51456465e-01 1.60169180e-06 3.75635201e-04\n",
      "  1.97000574e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "[1.         0.7955118  0.62407348 0.37944696 0.53982516 0.84586015\n",
      " 0.80553723 0.61891333 0.65843002 0.46411808 0.35862086 0.65658795\n",
      " 1.         0.37065727 1.         0.43166567 0.76178324 0.50371851\n",
      " 0.50327428 0.7599608  0.86700335 0.38921601 0.63822025 0.61597184\n",
      " 0.62964728 0.69629476 0.78220069 0.77366311 0.77098695 0.35407354\n",
      " 0.49162399 0.59364324 0.80745024]\n",
      "[[8.38108555e-01 5.51227391e-03 6.45788626e-01 3.73296246e-01\n",
      "  5.56008907e-01 5.62456723e-01 1.25627846e+00 7.60772404e-01\n",
      "  1.30607620e+00 4.14459325e-01 3.64896345e-01 9.04594100e-01\n",
      "  1.19650239e-01 1.16877582e+00 4.54319176e-01 3.97432966e-01\n",
      "  1.45989195e-04 4.15502870e-01 4.32845977e-01 1.51753121e-02\n",
      "  5.12765665e-01 5.23587436e-01 4.30097201e-01 1.42335694e+00\n",
      "  9.21245199e-02 9.18727432e-01 7.42265233e-01 7.55237465e-01\n",
      "  7.14959967e-01 7.51456465e-01 1.60169180e-06 3.75635201e-04\n",
      "  1.97000574e-01]]\n",
      "{1: 888}\n",
      "acc 0.3333333333333333\n",
      "acc 0.3333333333333333\n",
      "[[  0 592]\n",
      " [  0 296]]\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "alpha-mean 0.8\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 159528.68272148076\n",
      "[0.8945948  0.8266402  0.77760091 0.87211498 0.88231577 0.81715668\n",
      " 0.81984997 0.7777046  0.78386875 0.87987869 0.86763503 0.78331593\n",
      " 0.90504714 0.77898953 0.89679018 0.8779989  0.84064011 0.80068453\n",
      " 0.87949865 0.8431722  0.80740301 0.76620374 0.84625735 0.774269\n",
      " 0.81906582 0.88497022 0.8352185  0.83761782 0.81453538 0.82106774\n",
      " 0.79831255 0.79480602 0.80869859]\n",
      "[[ 1.00197658 -0.32896644  0.57158018  0.42970945  0.95362254  0.69092522\n",
      "   1.33418489  0.68018491  1.25203555  0.74643541  0.34664318  0.8524223\n",
      "   0.32008187  0.91410389  0.61637364  0.63488731 -0.13329938  0.0876929\n",
      "   0.72111989 -0.2179185   0.67088795  0.24059879  0.44029967  1.33728666\n",
      "  -0.03137412  1.3080912   0.81324907  0.81587819  0.77883781  0.4355801\n",
      "  -0.29005623 -0.20810922  0.30255794]]\n",
      "{0: 434, 1: 454}\n",
      "acc 0.6396396396396397\n",
      "(0.473568281938326, 0.7263513513513513, 0.5733333333333334, None)\n",
      "\n",
      "1 loss 153963.8149699714\n",
      "[0.97360596 0.85293434 0.75730859 0.81120629 0.9229669  0.8339642\n",
      " 0.82978107 0.75691505 0.76832561 0.89766133 0.78361257 0.76822505\n",
      " 0.98467572 0.75702009 0.97694848 0.87501241 0.7943003  0.68520908\n",
      " 0.89322944 0.88694009 0.81628176 0.72929437 0.84758745 0.74913774\n",
      " 0.81661838 0.94133354 0.84751978 0.85171829 0.82048663 0.85393928\n",
      " 0.7990026  0.795515   0.81700839]\n",
      "[[ 0.94116451 -0.22899999  0.59434187  0.34556266  0.89302608  0.67438681\n",
      "   1.32761334  0.70369895  1.27145807  0.67844705  0.26924284  0.87071722\n",
      "   0.2598763   0.93524987  0.55482595  0.56039154 -0.03330237  0.1801428\n",
      "   0.65177316 -0.11791974  0.66223547  0.27839774  0.43444153  1.35882027\n",
      "   0.00286603  1.25046831  0.80363856  0.80360206  0.77436925  0.41736309\n",
      "  -0.19006203 -0.10810928  0.29381131]]\n",
      "{0: 395, 1: 493}\n",
      "acc 0.625\n",
      "(0.46247464503042596, 0.7702702702702703, 0.5779467680608366, None)\n",
      "\n",
      "2 loss 149185.1598459034\n",
      "[1.         0.87839455 0.73799846 0.70100675 0.89132438 0.8524864\n",
      " 0.83828248 0.73683528 0.75363801 0.84543638 0.67666456 0.75400369\n",
      " 1.         0.73131222 1.         0.79966276 0.78424155 0.5804666\n",
      " 0.8369605  0.91701332 0.82866417 0.68735856 0.83968128 0.72561992\n",
      " 0.78726302 0.92757496 0.8543297  0.85922419 0.82514201 0.87800857\n",
      " 0.79770578 0.79508013 0.82678728]\n",
      "[[ 9.32636226e-01 -1.29032996e-01  6.16499737e-01  2.42610578e-01\n",
      "   7.86152094e-01  6.55966525e-01  1.32142855e+00  7.26887879e-01\n",
      "   1.29034712e+00  5.66095296e-01  2.13471902e-01  8.88483609e-01\n",
      "   2.56142537e-01  9.58152609e-01  5.47745884e-01  4.46047337e-01\n",
      "   1.29057786e-04  1.71522819e-01  5.38957398e-01 -1.79207164e-02\n",
      "   6.49857637e-01  3.21562417e-01  4.32559121e-01  1.38035351e+00\n",
      "   2.55119253e-02  1.15728715e+00  7.96474129e-01  7.95297712e-01\n",
      "   7.69938518e-01  4.06204873e-01 -9.00700522e-02 -8.11106174e-03\n",
      "   2.83236134e-01]]\n",
      "{0: 305, 1: 583}\n",
      "acc 0.588963963963964\n",
      "(0.44082332761578047, 0.8682432432432432, 0.5847554038680319, None)\n",
      "\n",
      "3 loss 145650.1808489729\n",
      "[1.         0.90156297 0.7195003  0.59478274 0.85176864 0.87398094\n",
      " 0.84666366 0.71736189 0.73967133 0.76924808 0.57226063 0.74048466\n",
      " 1.         0.69407098 1.         0.68127838 0.82198509 0.50385192\n",
      " 0.75222909 0.91733648 0.84684752 0.63869272 0.82551171 0.70370328\n",
      " 0.76073378 0.90715446 0.85860223 0.86352732 0.8302213  0.89110038\n",
      " 0.79500565 0.79551521 0.83961104]\n",
      "[[ 9.32636226e-01 -2.90629897e-02  6.38146875e-01  2.03005320e-01\n",
      "   6.77192182e-01  6.34556993e-01  1.31462247e+00  7.49780281e-01\n",
      "   1.30877069e+00  4.55062965e-01  1.94157018e-01  9.05808658e-01\n",
      "   2.56142537e-01  9.85315820e-01  5.47745884e-01  3.34171545e-01\n",
      "   7.52045126e-05  1.71689531e-01  4.27520134e-01  1.54276026e-04\n",
      "   6.31611238e-01  3.71748846e-01  4.32260891e-01  1.40188531e+00\n",
      "   5.15568154e-02  1.04976868e+00  7.89621117e-01  7.88771611e-01\n",
      "   7.64226451e-01  4.02950532e-01  1.01502774e-04  3.20546855e-04\n",
      "   2.69449366e-01]]\n",
      "{0: 155, 1: 733}\n",
      "acc 0.48986486486486486\n",
      "(0.39290586630286495, 0.972972972972973, 0.5597667638483965, None)\n",
      "\n",
      "4 loss 143804.70711589025\n",
      "[1.         0.89834402 0.70189521 0.49030516 0.78781152 0.90010989\n",
      " 0.85597807 0.698607   0.72651853 0.63683285 0.46778103 0.72775197\n",
      " 1.         0.6421573  1.         0.55806093 0.85640121 0.50407067\n",
      " 0.61565832 0.86513568 0.87421853 0.58119035 0.80587618 0.68346733\n",
      " 0.73393354 0.87836729 0.86151218 0.86582353 0.83671997 0.89469759\n",
      " 0.79713841 0.80037992 0.85734633]\n",
      "[[9.32636226e-01 2.54392722e-03 6.59234733e-01 1.95766074e-01\n",
      "  5.67211278e-01 6.08697016e-01 1.30610489e+00 7.72296151e-01\n",
      "  1.32667190e+00 3.43775494e-01 1.89458060e-01 9.22642718e-01\n",
      "  2.56142536e-01 1.02202089e+00 5.47745884e-01 2.37420506e-01\n",
      "  3.13925228e-04 1.95554620e-01 3.17606388e-01 4.26426469e-03\n",
      "  6.04361861e-01 4.30995163e-01 4.32475828e-01 1.42341429e+00\n",
      "  7.89167103e-02 9.41859837e-01 7.81937122e-01 7.83160334e-01\n",
      "  7.56283724e-01 4.06131440e-01 1.00588739e-05 2.96656946e-03\n",
      "  2.50716308e-01]]\n",
      "{0: 39, 1: 849}\n",
      "acc 0.37725225225225223\n",
      "(0.3486454652532391, 1.0, 0.5170305676855895, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.89834402 0.70189521 0.49030516 0.78781152 0.90010989\n",
      " 0.85597807 0.698607   0.72651853 0.63683285 0.46778103 0.72775197\n",
      " 1.         0.6421573  1.         0.55806093 0.85640121 0.50407067\n",
      " 0.61565832 0.86513568 0.87421853 0.58119035 0.80587618 0.68346733\n",
      " 0.73393354 0.87836729 0.86151218 0.86582353 0.83671997 0.89469759\n",
      " 0.79713841 0.80037992 0.85734633]\n",
      "[[9.32636226e-01 2.54392722e-03 6.59234733e-01 1.95766074e-01\n",
      "  5.67211278e-01 6.08697016e-01 1.30610489e+00 7.72296151e-01\n",
      "  1.32667190e+00 3.43775494e-01 1.89458060e-01 9.22642718e-01\n",
      "  2.56142536e-01 1.02202089e+00 5.47745884e-01 2.37420506e-01\n",
      "  3.13925228e-04 1.95554620e-01 3.17606388e-01 4.26426469e-03\n",
      "  6.04361861e-01 4.30995163e-01 4.32475828e-01 1.42341429e+00\n",
      "  7.89167103e-02 9.41859837e-01 7.81937122e-01 7.83160334e-01\n",
      "  7.56283724e-01 4.06131440e-01 1.00588739e-05 2.96656946e-03\n",
      "  2.50716308e-01]]\n",
      "{0: 39, 1: 849}\n",
      "acc 0.37725225225225223\n",
      "acc 0.37725225225225223\n",
      "[[ 39 553]\n",
      " [  0 296]]\n",
      "(0.3486454652532391, 1.0, 0.5170305676855895, None)\n",
      "\n",
      "alpha-mean 0.9\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 159649.89173539382\n",
      "[0.97863913 0.92559533 0.8757189  0.95303932 0.96661222 0.91998329\n",
      " 0.90921238 0.87543719 0.87964507 0.96380954 0.94578132 0.88026607\n",
      " 0.98287717 0.88108996 0.97997629 0.961458   0.92337427 0.86457796\n",
      " 0.96334762 0.93408237 0.91218217 0.86804138 0.93117127 0.87158923\n",
      " 0.91767567 0.96936422 0.9199701  0.91828095 0.90527429 0.91363153\n",
      " 0.90542546 0.91079034 0.90862236]\n",
      "[[ 1.02668427 -0.3289305   0.57333392  0.44779672  0.97435147  0.68765433\n",
      "   1.34079044  0.68223019  1.25539918  0.76648448  0.36362174  0.85493278\n",
      "   0.34786189  0.91338063  0.64182274  0.65443961 -0.13329389  0.11632217\n",
      "   0.74106676 -0.21792     0.66540119  0.23903945  0.45272469  1.33726933\n",
      "  -0.03133445  1.32961445  0.82515145  0.82998045  0.78456221  0.44455022\n",
      "  -0.29004887 -0.20811239  0.30195839]]\n",
      "{0: 462, 1: 426}\n",
      "acc 0.6801801801801802\n",
      "(0.5140845070422535, 0.7398648648648649, 0.6066481994459835, None)\n",
      "\n",
      "1 loss 154114.535652056\n",
      "[1.         0.9525135  0.8520845  0.88799485 0.95912537 0.94048913\n",
      " 0.9131444  0.85093911 0.85867723 0.94558705 0.85277314 0.86064613\n",
      " 1.         0.86167709 1.         0.93362972 0.88998859 0.75200645\n",
      " 0.94327524 0.95550444 0.92733685 0.83190588 0.93022928 0.84259866\n",
      " 0.9176377  0.97122663 0.92418102 0.92466754 0.90788538 0.92556513\n",
      " 0.90428066 0.92179702 0.91894873]\n",
      "[[ 1.02112145e+00 -2.28935799e-01  5.98545846e-01  3.39844180e-01\n",
      "   9.05895819e-01  6.67785915e-01  1.33806863e+00  7.08487411e-01\n",
      "   1.27870987e+00  6.85368012e-01  2.48408925e-01  8.76437520e-01\n",
      "   3.43769244e-01  9.33874110e-01  6.36729925e-01  5.65184036e-01\n",
      "  -3.32942625e-02  1.21396436e-01  6.58200392e-01 -1.17920175e-01\n",
      "   6.50555620e-01  2.75935931e-01  4.51544304e-01  1.35879061e+00\n",
      "   1.10347230e-03  1.27799178e+00  8.22275098e-01  8.24388823e-01\n",
      "   7.82583629e-01  4.40414069e-01 -1.90050109e-01 -1.08112508e-01\n",
      "   2.91605096e-01]]\n",
      "{0: 424, 1: 464}\n",
      "acc 0.6779279279279279\n",
      "(0.5107758620689655, 0.8006756756756757, 0.6236842105263158, None)\n",
      "\n",
      "2 loss 149349.15590521018\n",
      "[1.         0.97889434 0.82928536 0.77037402 0.93705381 0.96267677\n",
      " 0.91684639 0.8270869  0.83864971 0.91141113 0.73311993 0.84183043\n",
      " 1.         0.84174173 1.         0.88597169 0.88230764 0.6441288\n",
      " 0.90671802 0.96914102 0.94627804 0.79073557 0.92423666 0.81467194\n",
      " 0.88560573 0.95767524 0.9262196  0.92805678 0.91054508 0.9319086\n",
      " 0.90304365 0.92805679 0.93096176]\n",
      "[[ 1.02112145e+00 -1.28944823e-01  6.23120601e-01  2.21736759e-01\n",
      "   7.92638813e-01  6.46649527e-01  1.33515107e+00  7.34313766e-01\n",
      "   1.30130758e+00  5.70793272e-01  1.53435989e-01  8.97342940e-01\n",
      "   3.43769244e-01  9.55048833e-01  6.36729925e-01  4.49419176e-01\n",
      "   9.66003421e-05  1.17550220e-01  5.43400154e-01 -1.79202652e-02\n",
      "   6.32138654e-01  3.18042740e-01  4.52331545e-01  1.38031674e+00\n",
      "   2.08195634e-02  1.16616963e+00  8.19920580e-01  8.20532477e-01\n",
      "   7.79974205e-01  4.40701473e-01 -9.00515114e-02 -8.11262077e-03\n",
      "   2.79516715e-01]]\n",
      "{0: 300, 1: 588}\n",
      "acc 0.6486486486486487\n",
      "(0.48639455782312924, 0.9662162162162162, 0.6470588235294117, None)\n",
      "\n",
      "3 loss 145840.01751767402\n",
      "[1.         0.99010132 0.80725599 0.65017432 0.90951756 0.98688228\n",
      " 0.92068736 0.80389447 0.81950373 0.86189885 0.62641751 0.82376346\n",
      " 1.         0.82044061 1.         0.8051052  0.90655034 0.53824125\n",
      " 0.85223446 0.96945035 0.96945516 0.744573   0.91403395 0.78796072\n",
      " 0.85708907 0.9426094  0.92694003 0.92992318 0.91363685 0.93382479\n",
      " 0.90263867 0.92735761 0.94500942]\n",
      "[[ 1.02112145e+00 -2.89507858e-02  6.47135294e-01  1.45935909e-01\n",
      "   6.81882378e-01  6.25067377e-01  1.33172404e+00  7.59708392e-01\n",
      "   1.32326300e+00  4.58210057e-01  1.37575330e-01  9.17710855e-01\n",
      "   3.43769244e-01  9.77007822e-01  6.36729925e-01  3.34845939e-01\n",
      "   3.62619325e-04  1.15731205e-01  4.30464381e-01  1.17678785e-04\n",
      "   6.10248849e-01  3.65354208e-01  4.54499086e-01  1.40184641e+00\n",
      "   4.79668855e-02  1.05683892e+00  8.17412318e-01  8.17516641e-01\n",
      "   7.76480514e-01  4.44158390e-01  1.12584702e-04  1.35563837e-04\n",
      "   2.65466626e-01]]\n",
      "{0: 279, 1: 609}\n",
      "acc 0.6407657657657657\n",
      "(0.48111658456486045, 0.9898648648648649, 0.6475138121546962, None)\n",
      "\n",
      "4 loss 144068.4249587876\n",
      "[1.         0.9900713  0.78596097 0.54550464 0.87414986 0.990075\n",
      " 0.92505739 0.78135204 0.80120495 0.7808791  0.52203183 0.80641479\n",
      " 1.         0.79628592 1.         0.66416695 0.93583803 0.50434449\n",
      " 0.7585691  0.95765802 0.99009056 0.6935761  0.90213246 0.76258021\n",
      " 0.82920962 0.9261626  0.92699399 0.93106224 0.91745303 0.93288612\n",
      " 0.90363387 0.91860849 0.96133231]\n",
      "[[1.02112145e+00 3.30494467e-04 6.70628359e-01 1.34339700e-01\n",
      "  5.72568994e-01 6.00424162e-01 1.32744687e+00 7.84685440e-01\n",
      "  1.34461254e+00 3.45962359e-01 1.29109928e-01 9.37571699e-01\n",
      "  3.43769244e-01 1.00007859e+00 6.36729925e-01 2.22501234e-01\n",
      "  3.47241031e-04 1.23293243e-01 3.17580105e-01 1.78866040e-04\n",
      "  5.86639415e-01 4.17735689e-01 4.56886101e-01 1.42337815e+00\n",
      "  7.56913294e-02 9.49344673e-01 8.14579336e-01 8.14863888e-01\n",
      "  7.71952293e-01 4.49547077e-01 2.61657144e-04 4.74851779e-04\n",
      "  2.49413882e-01]]\n",
      "{0: 259, 1: 629}\n",
      "acc 0.6204954954954955\n",
      "(0.46740858505564387, 0.9932432432432432, 0.6356756756756756, None)\n",
      "\n",
      "[1.         0.9900713  0.78596097 0.54550464 0.87414986 0.990075\n",
      " 0.92505739 0.78135204 0.80120495 0.7808791  0.52203183 0.80641479\n",
      " 1.         0.79628592 1.         0.66416695 0.93583803 0.50434449\n",
      " 0.7585691  0.95765802 0.99009056 0.6935761  0.90213246 0.76258021\n",
      " 0.82920962 0.9261626  0.92699399 0.93106224 0.91745303 0.93288612\n",
      " 0.90363387 0.91860849 0.96133231]\n",
      "[[1.02112145e+00 3.30494467e-04 6.70628359e-01 1.34339700e-01\n",
      "  5.72568994e-01 6.00424162e-01 1.32744687e+00 7.84685440e-01\n",
      "  1.34461254e+00 3.45962359e-01 1.29109928e-01 9.37571699e-01\n",
      "  3.43769244e-01 1.00007859e+00 6.36729925e-01 2.22501234e-01\n",
      "  3.47241031e-04 1.23293243e-01 3.17580105e-01 1.78866040e-04\n",
      "  5.86639415e-01 4.17735689e-01 4.56886101e-01 1.42337815e+00\n",
      "  7.56913294e-02 9.49344673e-01 8.14579336e-01 8.14863888e-01\n",
      "  7.71952293e-01 4.49547077e-01 2.61657144e-04 4.74851779e-04\n",
      "  2.49413882e-01]]\n",
      "{0: 259, 1: 629}\n",
      "acc 0.6204954954954955\n",
      "acc 0.6204954954954955\n",
      "[[257 335]\n",
      " [  2 294]]\n",
      "(0.46740858505564387, 0.9932432432432432, 0.6356756756756756, None)\n",
      "\n",
      "alpha-mean 1.0\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 159752.50031546722\n",
      "[1.14985856 1.         1.14996198 1.         1.14795612 1.14419846\n",
      " 1.1464229  1.15020164 1.14911851 1.14813356 1.         1.14979837\n",
      " 0.90135902 1.14860973 1.15099026 1.14824735 1.         1.\n",
      " 1.14815919 0.95440094 1.14423229 0.96710983 1.         1.14813732\n",
      " 1.         1.14777165 1.14710067 1.14769953 1.1461097  0.98057584\n",
      " 0.98583677 0.9729823  1.        ]\n",
      "[[ 1.24391186 -0.32892728  0.62933448  0.50312153  1.18646331  0.68719111\n",
      "   1.38294311  0.74141008  1.327495    0.97753591  0.41785319  0.92252819\n",
      "   0.29366863  0.92001505  0.85955772  0.86460948 -0.1332933   0.09967063\n",
      "   0.95194552 -0.21791993  0.66439735  0.23471164  0.48275231  1.337684\n",
      "  -0.03133251  1.54277986  0.94208221  0.95537695  0.83619976  0.46191953\n",
      "  -0.29004304 -0.2081127   0.30066218]]\n",
      "{0: 349, 1: 539}\n",
      "acc 0.6542792792792793\n",
      "(0.4897959183673469, 0.8918918918918919, 0.6323353293413174, None)\n",
      "\n",
      "1 loss 154143.7041498833\n",
      "[1.26699316 1.         1.2691074  1.         1.26326831 1.25549143\n",
      " 1.26158299 1.26874503 1.26615492 1.26360126 1.         1.26768902\n",
      " 0.75013104 1.26569705 1.26903161 1.26383856 1.         1.\n",
      " 1.26365182 0.86627034 1.25596463 0.94041914 1.         1.26374531\n",
      " 0.99999999 1.26294581 1.26265756 1.26315652 1.26170314 0.95323314\n",
      " 0.98165358 0.92716212 1.        ]\n",
      "[[ 1.36585026e+00 -2.28927154e-01  7.51113679e-01  5.03121531e-01\n",
      "   1.30699007e+00  6.96744384e-01  1.49812867e+00  8.63038774e-01\n",
      "   1.44891783e+00  1.09807914e+00  4.17853194e-01  1.04419342e+00\n",
      "   2.91419917e-01  1.00214924e+00  9.81947848e-01  9.85178764e-01\n",
      "  -3.32933038e-02  9.96706299e-02  1.07249322e+00 -1.17921416e-01\n",
      "   6.87453833e-01  2.62808749e-01  4.82567081e-01  1.35918526e+00\n",
      "   1.44012729e-04  1.66332912e+00  1.06178644e+00  1.07516519e+00\n",
      "   9.52140277e-01  4.82937980e-01 -1.90043372e-01 -1.08113193e-01\n",
      "   2.89753532e-01]]\n",
      "{0: 292, 1: 596}\n",
      "acc 0.6486486486486487\n",
      "(0.4865771812080537, 0.9797297297297297, 0.6502242152466368, None)\n",
      "\n",
      "2 loss 148925.0688361668\n",
      "[1.37864174 1.         1.3820322  1.         1.37294099 1.36256764\n",
      " 1.37129964 1.38129555 1.37774054 1.37328975 1.         1.37981182\n",
      " 0.63107047 1.37785411 1.38125971 1.37354865 1.         1.\n",
      " 1.3733444  0.76886376 1.36386358 0.91062108 1.         1.37408597\n",
      " 0.97432634 1.37265782 1.37237824 1.37286031 1.37147131 0.90878329\n",
      " 0.97630427 0.87529674 1.        ]\n",
      "[[ 1.48028159e+00 -1.28927006e-01  8.66014104e-01  5.03121531e-01\n",
      "   1.41980794e+00  7.60978475e-01  1.61083515e+00  9.77758335e-01\n",
      "   1.56329656e+00  1.21082330e+00  4.17853194e-01  1.15875690e+00\n",
      "   4.19033103e-01  1.11234881e+00  1.09659569e+00  1.09788895e+00\n",
      "   1.21229604e-04  9.96706301e-02  1.18522928e+00 -1.79329434e-02\n",
      "   7.66855884e-01  2.93150509e-01  4.82189313e-01  1.38073251e+00\n",
      "   4.02434024e-03  1.77630364e+00  1.17451381e+00  1.18789616e+00\n",
      "   1.06466619e+00  5.12913432e-01 -9.00438698e-02 -8.11691925e-03\n",
      "   2.78009042e-01]]\n",
      "{0: 285, 1: 603}\n",
      "acc 0.6497747747747747\n",
      "(0.48756218905472637, 0.9932432432432432, 0.6540600667408232, None)\n",
      "\n",
      "3 loss 144170.79779291645\n",
      "[1.48820595 1.         1.4923133  1.00000054 1.48012361 1.46765362\n",
      " 1.47864388 1.49134363 1.48729021 1.48030834 1.         1.48960339\n",
      " 0.51588487 1.48771616 1.49111478 1.48046147 0.91103681 1.\n",
      " 1.48034082 0.76624906 1.46945989 0.87605436 1.         1.48238152\n",
      " 0.95285691 1.48012473 1.47937537 1.47986402 1.47836786 0.84507125\n",
      " 0.97140146 0.87765134 1.        ]\n",
      "[[ 1.59178140e+00 -2.89268182e-02  9.77619504e-01  5.03121529e-01\n",
      "   1.52926226e+00  8.57205594e-01  1.72046758e+00  1.08928028e+00\n",
      "   1.67482146e+00  1.32008318e+00  4.17853194e-01  1.27022548e+00\n",
      "   5.39684033e-01  1.22328381e+00  1.20807703e+00  1.20703884e+00\n",
      "   3.57276334e-03  9.96706298e-02  1.29446481e+00  1.32830461e-04\n",
      "   8.68695905e-01  3.27242221e-01  4.81600610e-01  1.40239070e+00\n",
      "   2.10844247e-02  1.88610798e+00  1.28375126e+00  1.29714054e+00\n",
      "   1.17378534e+00  5.54750529e-01  1.13148405e-04  1.01068555e-04\n",
      "   2.65078036e-01]]\n",
      "{0: 288, 1: 600}\n",
      "acc 0.6531531531531531\n",
      "(0.49, 0.9932432432432432, 0.65625, None)\n",
      "\n",
      "4 loss 139658.9550458144\n",
      "[1.59696609 0.96724405 1.60120122 1.         1.58587345 1.56901199\n",
      " 1.58474738 1.60011881 1.59611138 1.58563509 0.99999965 1.5982931\n",
      " 0.40362894 1.59642804 1.59981725 1.58549812 0.76372159 1.\n",
      " 1.58560771 0.84411501 1.5714392  0.83416948 0.99999995 1.58969669\n",
      " 0.92401275 1.58654232 1.58461718 1.58513775 1.58330035 0.7684697\n",
      " 0.97411527 0.94075707 1.00000009]\n",
      "[[1.70204865e+00 2.13272481e-04 1.08749395e+00 5.03121511e-01\n",
      "  1.63680936e+00 9.59356866e-01 1.82836385e+00 1.19916693e+00\n",
      "  1.78519281e+00 1.42725783e+00 4.17853182e-01 1.38019720e+00\n",
      "  6.54799192e-01 1.33312135e+00 1.31801100e+00 1.31399556e+00\n",
      "  1.11371986e-01 9.96706292e-02 1.40159189e+00 3.49327816e-05\n",
      "  9.72667133e-01 3.67054001e-01 4.80793238e-01 1.42428262e+00\n",
      "  4.40233690e-02 1.99432218e+00 1.39086452e+00 1.40427604e+00\n",
      "  1.28068041e+00 6.09188293e-01 1.22569084e-04 6.63007470e-05\n",
      "  2.50518967e-01]]\n",
      "{0: 284, 1: 604}\n",
      "acc 0.6486486486486487\n",
      "(0.4867549668874172, 0.9932432432432432, 0.6533333333333333, None)\n",
      "\n",
      "[1.59696609 0.96724405 1.60120122 1.         1.58587345 1.56901199\n",
      " 1.58474738 1.60011881 1.59611138 1.58563509 0.99999965 1.5982931\n",
      " 0.40362894 1.59642804 1.59981725 1.58549812 0.76372159 1.\n",
      " 1.58560771 0.84411501 1.5714392  0.83416948 0.99999995 1.58969669\n",
      " 0.92401275 1.58654232 1.58461718 1.58513775 1.58330035 0.7684697\n",
      " 0.97411527 0.94075707 1.00000009]\n",
      "[[1.70204865e+00 2.13272481e-04 1.08749395e+00 5.03121511e-01\n",
      "  1.63680936e+00 9.59356866e-01 1.82836385e+00 1.19916693e+00\n",
      "  1.78519281e+00 1.42725783e+00 4.17853182e-01 1.38019720e+00\n",
      "  6.54799192e-01 1.33312135e+00 1.31801100e+00 1.31399556e+00\n",
      "  1.11371986e-01 9.96706292e-02 1.40159189e+00 3.49327816e-05\n",
      "  9.72667133e-01 3.67054001e-01 4.80793238e-01 1.42428262e+00\n",
      "  4.40233690e-02 1.99432218e+00 1.39086452e+00 1.40427604e+00\n",
      "  1.28068041e+00 6.09188293e-01 1.22569084e-04 6.63007470e-05\n",
      "  2.50518967e-01]]\n",
      "{0: 284, 1: 604}\n",
      "acc 0.6486486486486487\n",
      "acc 0.6486486486486487\n",
      "[[282 310]\n",
      " [  2 294]]\n",
      "(0.4867549668874172, 0.9932432432432432, 0.6533333333333333, None)\n"
     ]
    }
   ],
   "source": [
    "#66\n",
    "for i in np.linspace(0,1,11):\n",
    "    print()\n",
    "    print(\"alpha-mean\",i)\n",
    "    train(0.1/len(train_L_S),5,th = tf.truncated_normal_initializer(0.5,0.5,seed),\\\n",
    "                            af = tf.truncated_normal_initializer(i,0.001,seed),\n",
    "                          pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(33,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(33,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "0 loss 150433.37563495414\n",
      "[1.15026467 0.97423146 1.15174359 1.         1.14725284 1.14718193\n",
      " 1.14725798 1.15170146 1.14996134 1.14723255 1.         1.15100618\n",
      " 0.86493413 1.15080316 1.15150744 1.1471853  1.         1.\n",
      " 1.14722605 0.9599889  1.14715398 0.9752856  1.         0.97033362\n",
      " 1.00012637 1.147259   1.14723604 1.14723643 1.14722278 0.98035959\n",
      " 0.99194805 0.98310753 1.00014589]\n",
      "[[ 1.24423227 -0.34568358  0.70258425  0.50312153  1.18595225  0.85932965\n",
      "   1.50095224  0.8118074   1.38835225  0.97688594  0.41785319  0.98924068\n",
      "   0.28473034  1.0491667   0.8599813   0.86384663 -0.23328816  0.09967064\n",
      "   0.95127308 -0.34566523  0.82930857  0.23668849  0.48089166  1.33768471\n",
      "  -0.13055658  1.54240465  0.99377375  0.99589335  0.94047724  0.4703787\n",
      "  -0.34568152 -0.31982394  0.31043472]]\n",
      "{0: 364, 1: 524}\n",
      "acc 0.6328828828828829\n",
      "(0.4713740458015267, 0.8344594594594594, 0.6024390243902439, None)\n",
      "\n",
      "1 loss 149009.39562741242\n",
      "[1.26770101 1.00002989 1.27078053 1.         1.26167144 1.26133603\n",
      " 1.26178384 1.27024386 1.26717474 1.26152059 1.         1.26898861\n",
      " 0.7315706  1.26863531 1.26987709 1.26134597 1.         1.\n",
      " 1.26149039 0.86971205 1.26126061 0.94953087 1.         0.93906515\n",
      " 1.00000001 1.26179618 1.26153843 1.26154057 1.26147636 0.94491383\n",
      " 0.98767128 0.92120325 1.        ]\n",
      "[[ 1.36641973 -0.29154122  0.82562958  0.50312153  1.30586734  0.97897781\n",
      "   1.62102831  0.93456337  1.51043099  1.0966516   0.41785319  1.11170477\n",
      "   0.37985415  1.17155379  0.98264601  0.98349997 -0.23328816  0.09967064\n",
      "   1.07101589 -0.29154245  0.94892071  0.26413407  0.48017568  1.35918231\n",
      "  -0.13055114  1.6624992   1.11355378  1.11567515  1.06021     0.49664205\n",
      "  -0.29154406 -0.29154378  0.31041237]]\n",
      "{0: 300, 1: 588}\n",
      "acc 0.6666666666666666\n",
      "(0.5, 0.9932432432432432, 0.665158371040724, None)\n",
      "\n",
      "2 loss 147332.41760565678\n",
      "[1.37950677 1.         1.38353904 1.         1.37041252 1.36940005\n",
      " 1.37081733 1.38271201 1.37886042 1.3699236  1.         1.38111583\n",
      " 0.61321758 1.38067194 1.38224016 1.36942668 1.         1.\n",
      " 1.36983335 0.76161804 1.36920321 0.92042348 1.         0.90727138\n",
      " 1.         1.37086089 1.36997794 1.3699845  1.36979212 0.88689801\n",
      " 0.98177297 0.85547851 1.        ]\n",
      "[[ 1.48099875 -0.24159687  0.94058393  0.50312153  1.41806199  1.0907266\n",
      "   1.73349849  1.04938432  1.62498467  1.20859877  0.41785319  1.22639006\n",
      "   0.50387614  1.28620599  1.09742361  1.09525772 -0.23328816  0.09967064\n",
      "   1.18292506 -0.24159744  1.06060633  0.29375556  0.47874797  1.38068912\n",
      "  -0.1294252   1.7750022   1.22552478  1.22764908  1.17210241  0.53563321\n",
      "  -0.24159351 -0.24159721  0.30783274]]\n",
      "{0: 300, 1: 588}\n",
      "acc 0.6666666666666666\n",
      "(0.5, 0.9932432432432432, 0.665158371040724, None)\n",
      "\n",
      "[1.37950677 1.         1.38353904 1.         1.37041252 1.36940005\n",
      " 1.37081733 1.38271201 1.37886042 1.3699236  1.         1.38111583\n",
      " 0.61321758 1.38067194 1.38224016 1.36942668 1.         1.\n",
      " 1.36983335 0.76161804 1.36920321 0.92042348 1.         0.90727138\n",
      " 1.         1.37086089 1.36997794 1.3699845  1.36979212 0.88689801\n",
      " 0.98177297 0.85547851 1.        ]\n",
      "[[ 1.48099875 -0.24159687  0.94058393  0.50312153  1.41806199  1.0907266\n",
      "   1.73349849  1.04938432  1.62498467  1.20859877  0.41785319  1.22639006\n",
      "   0.50387614  1.28620599  1.09742361  1.09525772 -0.23328816  0.09967064\n",
      "   1.18292506 -0.24159744  1.06060633  0.29375556  0.47874797  1.38068912\n",
      "  -0.1294252   1.7750022   1.22552478  1.22764908  1.17210241  0.53563321\n",
      "  -0.24159351 -0.24159721  0.30783274]]\n",
      "{0: 300, 1: 588}\n",
      "acc 0.6666666666666666\n",
      "acc 0.6666666666666666\n",
      "[[298 294]\n",
      " [  2 294]]\n",
      "(0.5, 0.9932432432432432, 0.665158371040724, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train(0.1/len(train_L_S),3,th = tf.truncated_normal_initializer(0.5,0.5,seed),\\\n",
    "#                             af = tf.truncated_normal_initializer(1,0.001,seed),\n",
    "#                           pcl=np.array([-1,1],dtype=np.float64),smooth=True,penalty=2,alp=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
