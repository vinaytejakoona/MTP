{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8272 888\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()\n",
    "print(len(train_cands),len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 592\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from six.moves.cPickle import load\n",
    "\n",
    "with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "    ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "##### Discrete #########\n",
    "\n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "def LF_d_induced_by_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,1) if (\n",
    "        re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "def LF_d_treat_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d_wide(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "def LF_uncertain(c):\n",
    "    if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induced_other(c):\n",
    "    if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)  \n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_therapy(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "    LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "    LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "    LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "    LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "    LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "    LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "]\n",
    "\n",
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distanceCD(c):\n",
    "    dist = 0\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    return dist/5000\n",
    "\n",
    "\n",
    "def distanceCD_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{A}})(.*)('+w+r')(.*)({{B}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "def distanceDC_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{B}})(.*)('+w+r')(.*)({{A}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "   \n",
    "\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "# ##### Smooth LFs #########\n",
    "\n",
    "# def cand_in_ctd_unspecified(c):\n",
    "#     return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "# def cand_in_ctd_therapy(c):\n",
    "#     return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "# def cand_in_ctd_marker(c):\n",
    "#     return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "# def LF_in_ctd_unspecified(c):\n",
    "#     if(cand_in_ctd_unspecified(c)==1):\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_in_ctd_therapy(c):\n",
    "#     if(cand_in_ctd_therapy(c)==1):\n",
    "#         return (-1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# def LF_in_ctd_marker(c):\n",
    "#     if(cand_in_ctd_marker(c)==1):\n",
    "#         return (1,1)\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_tagged_text,\n",
    "#     rule_regex_search_tagged_text,\n",
    "#     rule_regex_search_btw_AB,\n",
    "#     rule_regex_search_btw_BA,\n",
    "#     rule_regex_search_before_A,\n",
    "#     rule_regex_search_before_B,\n",
    "# )\n",
    "\n",
    "# import re\n",
    "# from snorkel.lf_helpers import (\n",
    "#     get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "#     get_text_between, get_tagged_text,\n",
    "# )\n",
    "\n",
    "# # List to parenthetical\n",
    "# def ltp(x):\n",
    "#     return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# # def LF_induce(c):\n",
    "# #     return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_induce(c):\n",
    "#     return (1,distanceCD_(c,['induc'])) if re.search(r'{{A}}.*induc.*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# causal_past = ['induced', 'caused', 'due']\n",
    "# # def LF_d_induced_by_c(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "# #         return (1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_d_induced_by_c(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "#     for w in causal_past:\n",
    "#         sc=max(sc,get_similarity(word_vectors,w))\n",
    "#     return (1,sc)\n",
    "\n",
    "# # def LF_d_induced_by_c_tight(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "# #         return (1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_d_induced_by_c_tight(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.*' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "#         return (1,(1-distanceDC_(c,causal_past)))\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_induce_name(c):\n",
    "#     return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "# causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "# # def LF_c_cause_d(c):\n",
    "# #     return (1,1) if (\n",
    "# #         re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "# #         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "# #     ) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_c_cause_d(c):\n",
    "#     return (1,(1-distanceCD_(c,causal))) if (\n",
    "#         re.search(r'{{A}}.* ' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "#         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#     ) else (0,0)\n",
    "\n",
    "\n",
    "# treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "# # def LF_d_treat_c(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_d_treat_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1-distanceDC_(c,treat))\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "# # def LF_c_treat_d(c):\n",
    "# #     if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_c_treat_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1-distanceCD_(c,treat))\n",
    "#     return (0,0)\n",
    "\n",
    "# # def LF_treat_d(c):\n",
    "# #     if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_treat_d(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "#     for w in treat:\n",
    "#         sc=max(sc,get_similarity(word_vectors,w))\n",
    "#     if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "#         return (0,0)\n",
    "#     else:\n",
    "#         return (-1,sc)\n",
    "\n",
    "# # def LF_c_treat_d_wide(c):\n",
    "# #     if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_c_treat_d_wide(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "#         return (-1,1-distanceCD_(c,treat))\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_c_d(c):\n",
    "#     return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "# def LF_c_induced_d(c):\n",
    "#     return (1,1) if (\n",
    "#         ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "#         (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "#         ) else (0,0)\n",
    "\n",
    "# # def LF_improve_before_disease(c):\n",
    "# #     if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "\n",
    "# def distanceImproveBeforeDisease(c):\n",
    "#     m=re.search(r'(improv)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "#     if(m):\n",
    "#         return len(m.group(2))/5000\n",
    "#     return 0\n",
    "\n",
    "\n",
    "# def LF_improve_before_disease(c):\n",
    "#     if(rule_regex_search_before_B(c, 'improv.*', -1) == -1):\n",
    "#         return (-1,1-distanceImproveBeforeDisease(c))\n",
    "#     else:\n",
    "#         return (0,0)\n",
    "\n",
    "\n",
    "# pat_terms = ['in a patient with ', 'in patients with']\n",
    "# def LF_in_patient_with(c):\n",
    "#     return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# uncertain = ['combin', 'possible', 'unlikely']\n",
    "# # def LF_uncertain(c):\n",
    "# #     if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_uncertain(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "#     for w in uncertain:\n",
    "#         sc=max(sc,get_similarity(word_vectors,w))\n",
    "#     if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "#         return (0,0)\n",
    "#     else:\n",
    "#         return (-1,sc)\n",
    "\n",
    "# # def LF_induced_other(c):\n",
    "# #     if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)  \n",
    "\n",
    "# def LF_induced_other(c):\n",
    "#     if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "#         return (-1,distanceCD(c))\n",
    "#     return (0,0)  \n",
    "\n",
    "# # def LF_far_c_d(c):\n",
    "# #     if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_far_c_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,distanceCD(c))\n",
    "#     return (0,0)\n",
    "\n",
    "# # def LF_far_d_c(c):\n",
    "# #     if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "# #         return (-1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "# def LF_far_d_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,distanceCD(c))\n",
    "#     return (0,0)\n",
    "\n",
    "# #without deps\n",
    "# gen_model.weights.lf_accuracy\n",
    "# # def LF_risk_d(c):\n",
    "# #     if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "# #         return (1,1)\n",
    "# #     return (0,0)\n",
    "\n",
    "\n",
    "# def LF_risk_d(c):\n",
    "#     sc = 0\n",
    "#     word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "#     sc=max(sc,get_similarity(word_vectors,'risk'))\n",
    "#     return (1,sc)\n",
    "\n",
    "\n",
    "# # def LF_develop_d_following_c(c):\n",
    "# #     return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceDevFol(c):\n",
    "#     dist = 0\n",
    "#     matchObj = re.search(r'(develop)(.*)({{B}})(.*)(following)(.*)({{A}})', get_tagged_text(c), flags=re.I)\n",
    "#     if(matchObj):\n",
    "#         match_groups = matchObj.group(2,4,6)\n",
    "#         dist = sum([len(mg) for mg in match_groups])\n",
    "#     return dist/5000\n",
    "\n",
    "# def LF_develop_d_following_c(c):\n",
    "#     return (1,1-distanceDevFol(c)) if re.search(r'develop.*{{B}}.*following.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# procedure, following = ['inject', 'administrat'], ['following']\n",
    "# # def LF_d_following_c(c):\n",
    "# #     return (1,distanceDFollC(c)) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_d_following_c(c):\n",
    "#     return (1,1-distanceDC_(c,following)) if re.search('{{B}}.*' + ltp(following) + '.*{{A}}.*' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# # def LF_measure(c):\n",
    "# #     return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceMeasureA(c):\n",
    "#     m = re.search('(measur)(.*)({{A}})', get_tagged_text(c), flags=re.I) \n",
    "#     if(m):\n",
    "#         return (5000-len(m.group(2)))/5000\n",
    "#     return 0\n",
    "\n",
    "# def LF_measure(c):\n",
    "#     return (-1,distanceMeasureA(c)) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# # def LF_level(c):\n",
    "# #     return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceLevel(c):\n",
    "#     m = re.search('({{A}})(.*)(level)', get_tagged_text(c), flags=re.I)\n",
    "#     if(m):\n",
    "#         return (5000-len(m.group(2)))/5000\n",
    "#     return 0\n",
    "\n",
    "# def LF_level(c):\n",
    "#     return (-1,distanceLevel(c)) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# # def LF_neg_d(c):\n",
    "# #     return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def distanceNeg(c):\n",
    "#     m = re.search('(none|not|no)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "#     if(m):\n",
    "#         return (5000-len(m.group(2)))/5000\n",
    "#     return 0\n",
    "\n",
    "\n",
    "# def LF_neg_d(c):\n",
    "#     return (-1,distanceNeg(c)) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "#                 'seems', 'suggests', 'risk', 'implicated',\n",
    "#                'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "# WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "# def LF_weak_assertions(c):\n",
    "#     return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     l,s = LF_c_d(c)\n",
    "#     cl = cand_in_ctd_marker(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     l1,s1 = LF_c_induced_d(c)\n",
    "#     l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "#     cl = cand_in_ctd_marker(c)\n",
    "#     return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     l,s = LF_c_treat_d_wide(c)\n",
    "#     cl = cand_in_ctd_therapy(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     l,s = LF_c_treat_d_wide(c)\n",
    "#     cl = cand_in_ctd_unspecified(c)\n",
    "#     return (l*cl,s*cl)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     l1,s1 = LF_c_induced_d(c)\n",
    "#     l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "#     cl = cand_in_ctd_unspecified(c)\n",
    "#     return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# # def LF_ctd_marker_c_d(c):\n",
    "# #     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# # def LF_ctd_marker_induce(c):\n",
    "# #     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# # def LF_ctd_therapy_treat(c):\n",
    "# #     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# # def LF_ctd_unspecified_treat(c):\n",
    "# #     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# # def LF_ctd_unspecified_induce(c):\n",
    "# #     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_closer_chem(c):\n",
    "#     # Get distance between chemical and disease\n",
    "#     chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "#     dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "#     if dis_start < chem_start:\n",
    "#         dist = chem_start - dis_end\n",
    "#     else:\n",
    "#         dist = dis_start - chem_end\n",
    "#     # Try to find chemical closer than @dist/2 in either direction\n",
    "#     sent = c.get_parent()\n",
    "#     closest_other_chem = float('inf')\n",
    "#     for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "#             return (-1,1)\n",
    "#     for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "#             return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "# def LF_closer_dis(c):\n",
    "#     # Get distance between chemical and disease\n",
    "#     chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "#     dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "#     if dis_start < chem_start:\n",
    "#         dist = chem_start - dis_end\n",
    "#     else:\n",
    "#         dist = dis_start - chem_end\n",
    "#     # Try to find chemical disease than @dist/8 in either direction\n",
    "#     sent = c.get_parent()\n",
    "#     for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "#             return (-1,1)\n",
    "#     for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "#         et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "#         if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "#             return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "# LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "#     LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "#     LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "#     LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "#     LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "#     LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "#     LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "# ]\n",
    "\n",
    "# LF_l = [\n",
    "#     1,1,1,-1,-1,-1,\n",
    "#     -1,1,1,-1,\n",
    "#     -1,1,1,\n",
    "#     1,1,-1,1,\n",
    "#     -1,-1,-1,-1,\n",
    "#     1,-1,1,1,-1,\n",
    "#     -1,-1,-1,1,-1,-1,-1\n",
    "# ]\n",
    "# print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "np.save(\"dev_L_S_discrete\",np.array(dev_L_S))\n",
    "\n",
    "# np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_discrete\",np.array(train_L_S))\n",
    "\n",
    "# np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "print(\"time taken: \",str(datetime.timedelta(seconds=(time.time() - start_time))))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "def drawLossVsF1(y_loss,x_f1s,text,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x_f1s, y_loss)\n",
    "\n",
    "    plt.xlabel('f1-score')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(title)\n",
    "    \n",
    "    for i, txt in enumerate(text):\n",
    "        ax.annotate(txt, (x_f1s[i],y_loss[i]))    \n",
    "    \n",
    "    plt.savefig(title+\".png\")\n",
    "    \n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "#     print(\"thresholds\",thresholds,len(thresholds))\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 2, 33) (8272, 2, 33)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "# dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized training with penalty reduce_sum(max(0,-theta))\n",
    "\n",
    "def train_nl_p(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) +\\\n",
    "                        tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559d3be48>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559d3be48>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559d3be48>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 184944.8631493242\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.35040972e-03  3.72062295e-02  9.26079949e-03  4.41183199e-01\n",
      "   5.07256268e-01  2.29545728e-02  2.17302295e-01  3.68721808e-02\n",
      "   7.49754170e-03  4.11665241e-01  4.67499346e-01  9.67192100e-03\n",
      "   1.81926628e-03  9.21830200e-03  5.91999710e-04  4.54736602e-01\n",
      "   3.04169564e-03  1.96577871e-01  2.75644815e-01  4.84976536e-01\n",
      "   7.46998660e-02  7.02662998e+00  4.92939993e-01  1.00424884e-02\n",
      "   2.12364148e-01  3.42415123e-01  4.82431263e-01  4.85540342e-01\n",
      "   4.51501311e-01  1.94542435e-03  4.50250959e-01  2.96873062e-01\n",
      "   2.05902426e-01]]\n",
      "{0: 339, 1: 549}\n",
      "(0.5355191256830601, 0.9932432432432432, 0.6958579881656806, None)\n",
      "\n",
      "1 loss 184081.8900858081\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.41352019e-02  3.03870796e-02  7.22528348e-05  4.75934091e-01\n",
      "   5.46913470e-01  4.92239554e-03  2.57088514e-01  3.06478602e-02\n",
      "   2.79873836e-03  4.46376800e-01  5.05617725e-01  7.89843651e-03\n",
      "   8.71062634e-04  7.34060485e-04  1.62989026e-03  4.90085042e-01\n",
      "   2.48576559e-03  2.39365350e-01  3.07070927e-01  5.24326990e-01\n",
      "   7.52782048e-02  7.72893903e+00  5.34633036e-01 -8.03776851e-05\n",
      "   1.83522422e-01  3.75900736e-01  5.21102775e-01  5.25274533e-01\n",
      "   4.87582022e-01  1.08114996e-03  4.84830636e-01  3.34267033e-01\n",
      "   2.44305800e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "2 loss 184060.25272180155\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.31235864e-03 2.86935238e-02 6.70065946e-03 4.81727924e-01\n",
      "  5.53028482e-01 1.28970100e-02 2.63140197e-01 2.89140266e-02\n",
      "  1.32642646e-03 4.51990065e-01 5.11597826e-01 3.49528972e-04\n",
      "  1.58430630e-03 1.04868164e-03 4.68449930e-03 4.96319403e-01\n",
      "  5.13733353e-04 2.45775647e-01 3.12247243e-01 5.32972080e-01\n",
      "  6.53872681e-02 7.84032886e+00 5.46885947e-01 1.67541155e-03\n",
      "  1.82569197e-01 3.81439133e-01 5.29147266e-01 5.34101043e-01\n",
      "  4.94088080e-01 1.04061600e-02 4.91035436e-01 3.40339864e-01\n",
      "  2.50242670e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "3 loss 184056.81369310291\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.57648784e-04  1.10521426e-02  2.29029885e-03  4.82637326e-01\n",
      "   5.54304824e-01  1.49055213e-02  2.65781770e-01  3.12627304e-02\n",
      "   7.55969215e-03  4.52991242e-01  5.12735286e-01  5.00117143e-03\n",
      "   7.88810915e-04  1.51683150e-04  2.54558882e-03  4.97147014e-01\n",
      "   1.16023526e-02  2.47382802e-01  3.13131370e-01  5.34043355e-01\n",
      "   6.81292721e-02  7.85839330e+00  5.47614110e-01  1.23227859e-03\n",
      "   1.79842255e-01  3.82343509e-01  5.30116559e-01  5.35246469e-01\n",
      "   4.94936022e-01  2.92951654e-03  4.91780178e-01  3.41119185e-01\n",
      "   2.51432874e-01]]\n",
      "{0: 339, 1: 549}\n",
      "(0.5355191256830601, 0.9932432432432432, 0.6958579881656806, None)\n",
      "\n",
      "4 loss 184057.39740124193\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.54505476e-02  2.77017320e-02  2.25722866e-03  4.82850910e-01\n",
      "   5.54715547e-01  1.49863790e-02  2.65147305e-01  1.15668429e-02\n",
      "   7.09198229e-03  4.53304002e-01  5.13117210e-01  5.38678790e-03\n",
      "   4.66465981e-04  2.57280227e-03  8.51590823e-03  4.97042663e-01\n",
      "   3.39612811e-03  2.47653420e-01  3.13349864e-01  5.32094527e-01\n",
      "   6.55644941e-02  7.86396053e+00  5.42097741e-01 -1.42884489e-03\n",
      "   1.76567334e-01  3.82626552e-01  5.28771999e-01  5.33068693e-01\n",
      "   4.94902469e-01 -1.03895914e-04  4.91735100e-01  3.41331545e-01\n",
      "   2.51790833e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.54505476e-02  2.77017320e-02  2.25722866e-03  4.82850910e-01\n",
      "   5.54715547e-01  1.49863790e-02  2.65147305e-01  1.15668429e-02\n",
      "   7.09198229e-03  4.53304002e-01  5.13117210e-01  5.38678790e-03\n",
      "   4.66465981e-04  2.57280227e-03  8.51590823e-03  4.97042663e-01\n",
      "   3.39612811e-03  2.47653420e-01  3.13349864e-01  5.32094527e-01\n",
      "   6.55644941e-02  7.86396053e+00  5.42097741e-01 -1.42884489e-03\n",
      "   1.76567334e-01  3.82626552e-01  5.28771999e-01  5.33068693e-01\n",
      "   4.94902469e-01 -1.03895914e-04  4.91735100e-01  3.41331545e-01\n",
      "   2.51790833e-01]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.568\n",
      "Precision            0.535\n",
      "Recall               0.993\n",
      "F1                   0.695\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 256 | TN: 336 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 338, 1: 550}\n",
      "acc 0.7094594594594594\n",
      "(array([0.99408284, 0.53454545]), array([0.56756757, 0.99324324]), array([0.72258065, 0.69503546]), array([592, 296]))\n",
      "(0.7643141473910704, 0.7804054054054054, 0.708808053077099, None)\n",
      "[[336 256]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5345454545454545 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty init at 1\n",
    "\n",
    "train_nl_p(0.01,5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554d61ba58>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554d61ba58>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554d61ba58>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 185073.43457291505\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-9.49072667e-04  3.87572759e-02  7.35150854e-03  4.38577778e-01\n",
      "   5.04631613e-01  2.25939336e-02  2.14936822e-01  1.89911271e-02\n",
      "   9.84975808e-03  4.09224078e-01  4.64844240e-01  1.05869486e-02\n",
      "   1.16250934e-03  4.51306612e-03  3.54368444e-04  4.52023167e-01\n",
      "   2.48112913e-03  1.93530358e-01  2.73520965e-01  4.81651370e-01\n",
      "   6.63125451e-02  6.98508409e+00  4.89756209e-01  1.89106935e-03\n",
      "   2.14361128e-01  3.40024656e-01  4.79199514e-01  4.82204989e-01\n",
      "   4.48686417e-01  2.48402646e-03  4.47552578e-01  2.93760435e-01\n",
      "   2.03193578e-01]]\n",
      "{0: 339, 1: 549}\n",
      "(0.5355191256830601, 0.9932432432432432, 0.6958579881656806, None)\n",
      "\n",
      "1 loss 184081.41714089812\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 3.32058171e-03  3.06974852e-02 -5.87784793e-04  4.75500957e-01\n",
      "   5.46392012e-01  3.91974754e-03  2.55825002e-01  3.07978248e-02\n",
      "   1.93500650e-03  4.45951570e-01  5.05123165e-01 -1.13002869e-03\n",
      "  -2.30268988e-04 -1.97153548e-03  2.32817079e-03  4.89718119e-01\n",
      "   3.01245044e-03  2.38620950e-01  3.06777764e-01  5.24173327e-01\n",
      "   7.18706150e-02  7.72128941e+00  5.35846380e-01  8.93951963e-03\n",
      "   1.85411541e-01  3.75530243e-01  5.20888507e-01  5.25110529e-01\n",
      "   4.87276339e-01  1.92320959e-03  4.84542433e-01  3.33604784e-01\n",
      "   2.43739916e-01]]\n",
      "{0: 337, 1: 551}\n",
      "(0.5335753176043557, 0.9932432432432432, 0.6942148760330579, None)\n",
      "\n",
      "2 loss 184060.3547582218\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.00960988e-02  2.86707202e-02  1.00106413e-04  4.81769462e-01\n",
      "   5.53202781e-01  1.37803957e-02  2.64179033e-01  2.93100517e-02\n",
      "   1.66325312e-03  4.52184217e-01  5.11780514e-01  5.76760051e-03\n",
      "   1.10490638e-02 -2.68352912e-05 -8.08577597e-04  4.96154499e-01\n",
      "   3.87099066e-03  2.46192517e-01  3.12373832e-01  5.32305769e-01\n",
      "   6.73018922e-02  7.84210064e+00  5.44195174e-01  1.18053322e-03\n",
      "   1.77623518e-01  3.81542542e-01  5.28740556e-01  5.33321372e-01\n",
      "   4.94197629e-01  9.85718202e-04  4.91096628e-01  3.40686387e-01\n",
      "   2.50612831e-01]]\n",
      "{0: 339, 1: 549}\n",
      "(0.5355191256830601, 0.9932432432432432, 0.6958579881656806, None)\n",
      "\n",
      "3 loss 184056.78784569682\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 3.08459809e-03  1.11489478e-02  4.19374034e-03  4.83039633e-01\n",
      "   5.55015812e-01  1.53544990e-02  2.65023776e-01  2.99218915e-02\n",
      "  -2.32849286e-04  4.53601344e-01  5.13364504e-01  3.64984747e-03\n",
      "  -1.11037130e-03  7.76903031e-04 -8.00730558e-04  4.97415104e-01\n",
      "   1.15517451e-02  2.47971702e-01  3.13797210e-01  5.32845670e-01\n",
      "   6.20779871e-02  7.86049565e+00  5.44397564e-01  3.24701604e-03\n",
      "   1.80411211e-01  3.82844199e-01  5.29358007e-01  5.33858075e-01\n",
      "   4.95178489e-01  6.28515417e-03  4.91983192e-01  3.41733395e-01\n",
      "   2.52015543e-01]]\n",
      "{0: 339, 1: 549}\n",
      "(0.5355191256830601, 0.9932432432432432, 0.6958579881656806, None)\n",
      "\n",
      "4 loss 184058.22785393236\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.36012415e-02  1.15505407e-02  7.30741827e-03  4.82515524e-01\n",
      "   5.54441328e-01  1.47891592e-02  2.64899161e-01  2.80779914e-02\n",
      "  -1.28475106e-04  4.53031511e-01  5.12858976e-01  5.20715698e-03\n",
      "  -2.51671228e-04  1.95993437e-03  1.38681209e-05  4.96719339e-01\n",
      "   3.76270082e-04  2.47513917e-01  3.13038649e-01  5.32138948e-01\n",
      "   6.52677006e-02  7.86395740e+00  5.42892078e-01  2.51289326e-03\n",
      "   1.78683114e-01  3.82236651e-01  5.28681875e-01  5.33146187e-01\n",
      "   4.94487063e-01 -9.44716120e-04  4.91391748e-01  3.41056934e-01\n",
      "   2.51401588e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "5 loss 184055.78487533875\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.08002582e-02  1.10289098e-02  4.70834146e-04  4.83056510e-01\n",
      "   5.54853876e-01  1.50905374e-02  2.66070195e-01  1.13847399e-02\n",
      "   7.32013798e-03  4.53498835e-01  5.13309613e-01  5.01439536e-03\n",
      "   1.03185241e-03  4.23150462e-03  3.88414496e-03  4.97357213e-01\n",
      "  -6.45454727e-04  2.47829153e-01  3.13510655e-01  5.32689234e-01\n",
      "   6.55235481e-02  7.86415469e+00  5.42955873e-01  4.41949089e-04\n",
      "   1.79578430e-01  3.82797277e-01  5.29289196e-01  5.33665376e-01\n",
      "   4.95199257e-01 -7.84059584e-04  4.92061953e-01  3.41598469e-01\n",
      "   2.51919212e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "6 loss 184056.28025308158\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 4.80907284e-03  1.13645787e-02  5.54065326e-03  4.83177020e-01\n",
      "   5.54853077e-01  1.49953945e-02  2.65732005e-01  1.06298017e-02\n",
      "   2.92884347e-04  4.53584724e-01  5.13329768e-01  4.61759407e-03\n",
      "  -2.14549953e-03  1.67884530e-03  7.49537037e-04  4.97587529e-01\n",
      "  -4.01657241e-04  2.47838723e-01  3.13598147e-01  5.33258796e-01\n",
      "   6.54186951e-02  7.86756446e+00  5.43942904e-01  2.75387492e-03\n",
      "   1.79187633e-01  3.82914426e-01  5.29750045e-01  5.34288207e-01\n",
      "   4.95448812e-01  1.15034683e-04  4.92272169e-01  3.41661268e-01\n",
      "   2.51835898e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 loss 184057.10055977525\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.30437048e-02  1.12100948e-02  8.89568010e-04  4.82786293e-01\n",
      "   5.54783543e-01  1.50231690e-02  2.65410688e-01  1.13825547e-02\n",
      "   7.10813702e-03  4.53296682e-01  5.13137025e-01  3.28979756e-03\n",
      "   2.59279107e-03  4.93691489e-03  7.39804194e-04  4.96847838e-01\n",
      "  -4.94331571e-04  2.47845512e-01  3.13378264e-01  5.31649077e-01\n",
      "   6.58525892e-02  7.86479025e+00  5.40472820e-01 -1.39946238e-04\n",
      "   1.81684091e-01  3.82534121e-01  5.28432288e-01  5.32554133e-01\n",
      "   4.94731645e-01  2.65877423e-03  4.91489361e-01  3.41304058e-01\n",
      "   2.51744030e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "8 loss 184056.31144398733\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 1.14823293e-02  1.16742288e-02  5.97048309e-03  4.82684993e-01\n",
      "   5.54759147e-01  1.53059696e-02  2.65253386e-01  1.32473972e-02\n",
      "   7.45091392e-03  4.53166865e-01  5.13063490e-01  4.11787601e-03\n",
      "  -1.47894255e-04  2.42121618e-03  3.48047819e-03  4.96729433e-01\n",
      "   2.81084815e-03  2.47939171e-01  3.13270063e-01  5.30948016e-01\n",
      "   6.56937537e-02  7.86221833e+00  5.39002025e-01  2.64357624e-03\n",
      "   1.81204794e-01  3.82398173e-01  5.27885255e-01  5.31776440e-01\n",
      "   4.94532803e-01  4.61825296e-04  4.91337876e-01  3.41031684e-01\n",
      "   2.51792617e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "9 loss 184056.97840367287\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-7.30684713e-04  1.15151013e-02  1.67810698e-03  4.82657133e-01\n",
      "   5.54556224e-01  1.50674850e-02  2.65446824e-01  1.08152853e-02\n",
      "   7.59446092e-03  4.53148600e-01  5.12942300e-01  5.27184742e-03\n",
      "   3.26486429e-03  1.64581198e-03 -1.87975069e-03  4.96955687e-01\n",
      "   1.55313446e-03  2.47628308e-01  3.13196409e-01  5.32458971e-01\n",
      "   6.55620997e-02  7.86482079e+00  5.43612986e-01  3.65848775e-03\n",
      "   1.81386299e-01  3.82406264e-01  5.28998073e-01  5.33486873e-01\n",
      "   4.94805102e-01  2.13193280e-03  4.91557081e-01  3.41316308e-01\n",
      "   2.51602561e-01]]\n",
      "{0: 340, 1: 548}\n",
      "(0.5364963503649635, 0.9932432432432432, 0.6966824644549762, None)\n",
      "\n",
      "10 loss 184055.63393243792\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 9.18788948e-03  1.15460964e-02  6.26755248e-03  4.82524512e-01\n",
      "   5.54453687e-01  1.55802073e-02  2.64655088e-01  2.91539251e-02\n",
      "   7.24798458e-03  4.52968254e-01  5.12821148e-01  2.51298640e-03\n",
      "  -7.87524591e-04  3.69196094e-04  1.71740728e-03  4.96500438e-01\n",
      "   1.61846602e-03  2.47673416e-01  3.13169102e-01  5.31219117e-01\n",
      "   6.73766111e-02  7.86339539e+00  5.41020177e-01  8.45093380e-04\n",
      "   1.79834179e-01  3.82230803e-01  5.27974600e-01  5.32218595e-01\n",
      "   4.94344761e-01  1.28261970e-02  4.91243997e-01  3.41076613e-01\n",
      "   2.51715781e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "11 loss 184056.584922691\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 3.56289168e-03  1.15829969e-02  8.61953440e-04  4.82361986e-01\n",
      "   5.54521212e-01  1.49695516e-02  2.64649127e-01  1.18479411e-02\n",
      "   7.43160285e-03  4.52946487e-01  5.12829771e-01  5.48354874e-03\n",
      "   2.62566418e-03 -5.26417406e-05  1.56377097e-03  4.96236262e-01\n",
      "   1.03668951e-02  2.47585241e-01  3.13095891e-01  5.30378687e-01\n",
      "   6.58723631e-02  7.86411332e+00  5.39331530e-01  2.11901316e-03\n",
      "   1.82057945e-01  3.82145302e-01  5.27273151e-01  5.31238223e-01\n",
      "   4.94050329e-01  6.41525684e-03  4.90913540e-01  3.40752259e-01\n",
      "   2.51542998e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "12 loss 184056.84362180842\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 7.85886070e-03  2.82133261e-02  7.08068862e-03  4.82717736e-01\n",
      "   5.54467723e-01  1.45423066e-02  2.65406891e-01  1.14649801e-02\n",
      "   7.16080164e-03  4.53136283e-01  5.12914723e-01  8.02807596e-03\n",
      "  -8.37074749e-05  2.09695268e-03  1.90867563e-03  4.97099400e-01\n",
      "   1.01673027e-02  2.47302772e-01  3.13208183e-01  5.33180200e-01\n",
      "   8.17484911e-02  7.86317919e+00  5.45358388e-01  2.91449003e-03\n",
      "   1.81476282e-01  3.82414012e-01  5.29569451e-01  5.34293219e-01\n",
      "   4.94934432e-01  8.04183139e-03  4.91839859e-01  3.41452243e-01\n",
      "   2.51438848e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "13 loss 184056.22324721116\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 8.89059720e-03  1.10706737e-02  8.73812080e-03  4.83417274e-01\n",
      "   5.54990138e-01  1.51451269e-02  2.66520161e-01  1.11163995e-02\n",
      "   1.47948012e-03  4.53770666e-01  5.13499517e-01  4.53170738e-03\n",
      "  -1.15671355e-03  4.87255140e-03  3.35460735e-03  4.97974867e-01\n",
      "   1.00218626e-02  2.47951137e-01  3.13800127e-01  5.34337511e-01\n",
      "   6.54263656e-02  7.86432325e+00  5.46199889e-01  3.54430071e-03\n",
      "   1.81406680e-01  3.83101156e-01  5.30694483e-01  5.35417783e-01\n",
      "   4.95780699e-01  8.48131527e-03  4.92646741e-01  3.41983389e-01\n",
      "   2.52024984e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "14 loss 184055.76881096573\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 9.18009108e-03  1.15237536e-02  2.51024439e-03  4.82714898e-01\n",
      "   5.54591411e-01  1.57746522e-02  2.65168066e-01  1.37058345e-02\n",
      "   7.55282762e-03  4.53128659e-01  5.12988153e-01  5.02169168e-03\n",
      "   1.16278764e-02  5.61108237e-03 -1.45256806e-03  4.96762558e-01\n",
      "   3.53326951e-03  2.47853497e-01  3.13283851e-01  5.31264737e-01\n",
      "   6.79922350e-02  7.86445356e+00  5.40321682e-01  2.74655654e-03\n",
      "   1.81332670e-01  3.82422894e-01  5.28121567e-01  5.32187800e-01\n",
      "   4.94700619e-01  2.81432480e-03  4.91472791e-01  3.41345353e-01\n",
      "   2.51857089e-01]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 9.18009108e-03  1.15237536e-02  2.51024439e-03  4.82714898e-01\n",
      "   5.54591411e-01  1.57746522e-02  2.65168066e-01  1.37058345e-02\n",
      "   7.55282762e-03  4.53128659e-01  5.12988153e-01  5.02169168e-03\n",
      "   1.16278764e-02  5.61108237e-03 -1.45256806e-03  4.96762558e-01\n",
      "   3.53326951e-03  2.47853497e-01  3.13283851e-01  5.31264737e-01\n",
      "   6.79922350e-02  7.86445356e+00  5.40321682e-01  2.74655654e-03\n",
      "   1.81332670e-01  3.82422894e-01  5.28121567e-01  5.32187800e-01\n",
      "   4.94700619e-01  2.81432480e-03  4.91472791e-01  3.41345353e-01\n",
      "   2.51857089e-01]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.568\n",
      "Precision            0.535\n",
      "Recall               0.993\n",
      "F1                   0.695\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 256 | TN: 336 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 338, 1: 550}\n",
      "acc 0.7094594594594594\n",
      "(array([0.99408284, 0.53454545]), array([0.56756757, 0.99324324]), array([0.72258065, 0.69503546]), array([592, 296]))\n",
      "(0.7643141473910704, 0.7804054054054054, 0.708808053077099, None)\n",
      "[[336 256]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5345454545454545 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty init at 0\n",
    "\n",
    "train_nl_p(0.01,15,tf.truncated_normal_initializer(0,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized training with penalty2  -tf.minimum( tf.reduce_min(theta),0)\n",
    "\n",
    "def train_nl_p2(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     -tf.minimum( tf.reduce_min(thetas),0.0)\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554d598828>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554d598828>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554d598828>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 185016.0622531975\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-2.76159826e-02  2.78109141e-03 -1.81968660e-02  4.31140228e-01\n",
      "   4.98690691e-01 -2.97617995e-02  2.04904423e-01  6.27099591e-03\n",
      "  -3.03776191e-02  4.02396090e-01  4.58246291e-01 -2.01891520e-02\n",
      "  -3.01364750e-02 -3.04932640e-02 -1.08293951e-02  4.43067813e-01\n",
      "  -3.01361238e-02  1.87636532e-01  2.67426768e-01  4.70638516e-01\n",
      "  -7.77401252e-03  6.98817745e+00  4.79269994e-01 -2.99767794e-02\n",
      "   2.17223168e-01  3.32997287e-01  4.68233505e-01  4.71133366e-01\n",
      "   4.39646251e-01 -2.20422446e-02  4.38755498e-01  2.85102189e-01\n",
      "   1.97300864e-01]]\n",
      "{0: 282, 1: 606}\n",
      "(0.48514851485148514, 0.9932432432432432, 0.6518847006651884, None)\n",
      "\n",
      "1 loss 184097.01227976294\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.65924924e-02 -2.14015809e-02 -4.34870369e-02  4.60464271e-01\n",
      "   5.36742533e-01 -3.43067708e-03  2.35791907e-01 -2.09434494e-02\n",
      "  -2.73991768e-02  4.32735476e-01  4.93486383e-01 -3.97198183e-02\n",
      "  -2.56731011e-02 -2.73805403e-02 -4.30997680e-02  4.69851063e-01\n",
      "  -3.64513667e-02  2.29415500e-01  2.95074656e-01  4.96568448e-01\n",
      "   3.00318755e-02  7.81057419e+00  5.05403355e-01 -3.08662440e-02\n",
      "   1.91188845e-01  3.61588506e-01  4.94322659e-01  4.97215395e-01\n",
      "   4.67321109e-01 -3.90227792e-02  4.65050961e-01  3.15288762e-01\n",
      "   2.33191080e-01]]\n",
      "{0: 331, 1: 557}\n",
      "(0.5278276481149012, 0.9932432432432432, 0.6893317702227432, None)\n",
      "\n",
      "2 loss 184075.97268363222\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.24370842e-02 -2.90989445e-02 -3.89161617e-02  4.69875031e-01\n",
      "   5.46858738e-01  9.58419910e-03  2.46764555e-01 -1.22174303e-02\n",
      "  -2.12790747e-02  4.42022345e-01  5.03385471e-01 -3.59357435e-02\n",
      "  -3.55463854e-02 -3.90632877e-02 -3.68920039e-02  4.79585783e-01\n",
      "  -3.55627087e-02  2.40293475e-01  3.03551329e-01  5.06408680e-01\n",
      "   6.83901189e-03  7.96623497e+00  5.13139205e-01 -3.61152392e-02\n",
      "   1.90489486e-01  3.70676384e-01  5.04343899e-01  5.06998838e-01\n",
      "   4.77198169e-01 -3.69941982e-02  4.74599686e-01  3.25534601e-01\n",
      "   2.43239330e-01]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "3 loss 184074.72969344154\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.82166240e-02 -3.14957073e-02 -3.55197617e-02  4.69088436e-01\n",
      "   5.46815133e-01  1.05011665e-02  2.45007160e-01 -2.91932323e-03\n",
      "  -4.44341866e-02  4.41602448e-01  5.03119521e-01 -3.23161423e-02\n",
      "  -4.31099254e-02 -4.16217619e-02 -4.09020832e-02  4.78420585e-01\n",
      "  -4.32527537e-02  2.40529888e-01  3.03223395e-01  5.04263429e-01\n",
      "   2.55790785e-02  8.00449796e+00  5.11947007e-01 -3.94455741e-02\n",
      "   1.87446326e-01  3.70179256e-01  5.02299410e-01  5.04843867e-01\n",
      "   4.76071023e-01 -4.28330053e-02  4.73531161e-01  3.24423428e-01\n",
      "   2.43184295e-01]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "4 loss 184073.6007418165\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.0382872  -0.01475473 -0.0339493   0.47157356  0.54891096  0.01269326\n",
      "   0.24823308 -0.01520926 -0.03530512  0.44380979  0.50532352 -0.03441466\n",
      "  -0.03976593 -0.03234621 -0.04083317  0.48123503 -0.03667775  0.24251459\n",
      "   0.30520436  0.50786668  0.01020349  8.00808402  0.51453995 -0.04112163\n",
      "   0.18889093  0.37240778  0.50589718  0.50842836  0.47900394 -0.02970853\n",
      "   0.47630996  0.32739971  0.24521476]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "5 loss 184075.5460714199\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.01486273 -0.01451443 -0.04429311  0.47197564  0.54942334  0.01270208\n",
      "   0.24867845 -0.01300677 -0.02212923  0.44417757  0.50583347 -0.03834027\n",
      "  -0.03690581 -0.03354285 -0.03861511  0.48142801 -0.04694281  0.24287015\n",
      "   0.3056129   0.50774965  0.01011941  8.00868704  0.51439945 -0.04106269\n",
      "   0.18370526  0.37280272  0.5057791   0.50830109  0.47919246 -0.03918839\n",
      "   0.47653883  0.32759166  0.24570076]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "6 loss 184075.11179247365\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03624244 -0.02797588 -0.03353737  0.46982936  0.54759004  0.01151956\n",
      "   0.24596097 -0.01540616 -0.04024225  0.44218369  0.50384477 -0.03730111\n",
      "  -0.0394679  -0.03781173 -0.03742313  0.4790477  -0.03556835  0.2411825\n",
      "   0.3037487   0.50542722  0.02475819  8.01135826  0.51304936 -0.03836697\n",
      "   0.1867271   0.37073848  0.50339075  0.50603611  0.47678095 -0.03100515\n",
      "   0.47408834  0.32521303  0.24379454]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "7 loss 184072.78285592317\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-2.27128552e-02 -4.67473741e-03 -4.21127194e-02  4.71798356e-01\n",
      "   5.48994514e-01  1.28451140e-02  2.48492741e-01 -3.18038766e-02\n",
      "  -3.99164540e-02  4.44021671e-01  5.05493824e-01 -3.43990838e-02\n",
      "  -3.67578780e-02 -2.88125307e-02 -2.23910440e-02  4.81485031e-01\n",
      "  -4.19615965e-02  2.42711293e-01  3.05410964e-01  5.07981537e-01\n",
      "   1.11626018e-02  8.01465321e+00  5.14739325e-01 -3.91620725e-02\n",
      "   1.84106278e-01  3.72625167e-01  5.06037969e-01  5.08545715e-01\n",
      "   4.79188540e-01 -4.36210780e-02  4.76488936e-01  3.27590146e-01\n",
      "   2.45434618e-01]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss 184073.90664247665\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03576569 -0.02988826 -0.03601125  0.46965074  0.54706208  0.0108315\n",
      "   0.24613811 -0.02956726 -0.03609676  0.4419729   0.50341544 -0.02426699\n",
      "  -0.04157829 -0.04116216 -0.03408445  0.47919222 -0.03099129  0.24063148\n",
      "   0.30357489  0.50577234  0.02235002  8.00865169  0.51336419 -0.03847091\n",
      "   0.18542929  0.37062334  0.50370365  0.50638678  0.47685022 -0.0338509\n",
      "   0.47422154  0.32525154  0.24334574]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "9 loss 184075.81286854026\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.01516656 -0.03076104 -0.03443008  0.47119332  0.54883732  0.01243749\n",
      "   0.24731888 -0.02992262 -0.03776197  0.44351178  0.50519549 -0.03259922\n",
      "  -0.03526813 -0.03848896 -0.02768519  0.4805092  -0.0288377   0.24238601\n",
      "   0.30496279  0.50625202  0.02794035  8.00765786  0.51338621 -0.03582221\n",
      "   0.18523404  0.3720884   0.50441058  0.50678831  0.47821021 -0.03238122\n",
      "   0.4755753   0.32655799  0.24507824]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "10 loss 184074.07186949888\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03905608 -0.03059774 -0.04298981  0.47053474  0.54808744  0.012158\n",
      "   0.24695368 -0.02122338 -0.03194212  0.44293247  0.5044017  -0.02766662\n",
      "  -0.0398247  -0.04180906 -0.03480274  0.48018477 -0.03004339  0.24178087\n",
      "   0.30447605  0.50681335  0.00888165  8.00662471  0.51412729 -0.03641276\n",
      "   0.18513651  0.3715261   0.50477474  0.5073972   0.47782426 -0.02703218\n",
      "   0.47521333  0.32629065  0.24453942]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "11 loss 184074.8952901887\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.93405750e-02 -3.22850737e-02 -4.12177502e-02  4.71779593e-01\n",
      "   5.49234987e-01  1.28792745e-02  2.48227944e-01 -6.48650417e-03\n",
      "  -2.86155446e-02  4.44004244e-01  5.05591929e-01 -3.68075956e-02\n",
      "  -3.82598695e-02 -3.66908980e-02 -3.21406451e-02  4.81302398e-01\n",
      "  -3.77959108e-02  2.42686552e-01  3.05460640e-01  5.07765329e-01\n",
      "   8.78714883e-03  8.00973435e+00  5.14503721e-01 -1.73322681e-02\n",
      "   1.85364800e-01  3.72618887e-01  5.05827435e-01  5.08319895e-01\n",
      "   4.79136965e-01 -4.02908045e-02  4.76421475e-01  3.27430723e-01\n",
      "   2.45445316e-01]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "12 loss 184075.1316862504\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.04176361 -0.02886851 -0.04274103  0.4696345   0.54713663  0.0112933\n",
      "   0.24641011 -0.03131255 -0.03082835  0.44197772  0.50344138 -0.03370064\n",
      "  -0.03566809 -0.03184519 -0.03581915  0.47927312 -0.03687369  0.24095154\n",
      "   0.30348616  0.5062915   0.01021631  8.01109768  0.51347457 -0.04077381\n",
      "   0.18812097  0.37057889  0.50413405  0.50690977  0.47690421 -0.0339552\n",
      "   0.47426319  0.3252394   0.24363233]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "13 loss 184073.66566181747\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.0425284  -0.03158615 -0.02695941  0.46967358  0.54742804  0.01182068\n",
      "   0.24596846 -0.0295193  -0.03649977  0.44210137  0.50367366 -0.03898995\n",
      "  -0.04229693 -0.02898221 -0.02618654  0.47896092 -0.02665273  0.24118588\n",
      "   0.30362223  0.505249    0.02971836  8.01510251  0.5123935  -0.03983046\n",
      "   0.18602174  0.37065718  0.50328673  0.50584101  0.47675144 -0.03898432\n",
      "   0.4740134   0.32507084  0.24386353]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "14 loss 184075.14218988773\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.0356859  -0.01543016 -0.03065487  0.47137642  0.5492537   0.01346313\n",
      "   0.24713779 -0.03073637 -0.03748121  0.44379265  0.50554065 -0.03820073\n",
      "  -0.0265695  -0.03902371 -0.03037478  0.48050154 -0.04079011  0.2428664\n",
      "   0.30519968  0.50615525  0.02913933  8.01044429  0.51335122 -0.02022713\n",
      "   0.18981826  0.37228543  0.504264    0.50667292  0.4782493  -0.03924736\n",
      "   0.47557244  0.32681207  0.24542838]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.0356859  -0.01543016 -0.03065487  0.47137642  0.5492537   0.01346313\n",
      "   0.24713779 -0.03073637 -0.03748121  0.44379265  0.50554065 -0.03820073\n",
      "  -0.0265695  -0.03902371 -0.03037478  0.48050154 -0.04079011  0.2428664\n",
      "   0.30519968  0.50615525  0.02913933  8.01044429  0.51335122 -0.02022713\n",
      "   0.18981826  0.37228543  0.504264    0.50667292  0.4782493  -0.03924736\n",
      "   0.47557244  0.32681207  0.24542838]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.581\n",
      "Precision            0.542\n",
      "Recall               0.993\n",
      "F1                   0.702\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 248 | TN: 344 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 346, 1: 542}\n",
      "acc 0.7184684684684685\n",
      "(array([0.99421965, 0.54243542]), array([0.58108108, 0.99324324]), array([0.73347548, 0.70167064]), array([592, 296]))\n",
      "(0.7683275387667171, 0.7871621621621621, 0.7175730620677723, None)\n",
      "[[344 248]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5424354243542435 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 2 init at 0\n",
    "\n",
    "train_nl_p2(0.01,15,tf.truncated_normal_initializer(0,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c27b38>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c27b38>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c27b38>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 185160.84729805464\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.18243321e-02  1.45532264e-03 -3.28805445e-02  4.32504269e-01\n",
      "   5.00697451e-01 -1.78684060e-02  2.05561769e-01 -2.93119423e-03\n",
      "  -2.16712826e-02  4.03759841e-01  4.60099295e-01 -2.67613800e-02\n",
      "  -1.28965029e-02 -2.74126395e-02 -2.57653825e-02  4.43908040e-01\n",
      "  -2.93913428e-02  1.89824217e-01  2.68754457e-01  4.71194499e-01\n",
      "  -5.88631415e-04  7.02282855e+00  4.80788169e-01 -2.36189072e-02\n",
      "   2.23214583e-01  3.34246816e-01  4.68714046e-01  4.71674408e-01\n",
      "   4.40478126e-01 -3.09324616e-02  4.39537096e-01  2.86414511e-01\n",
      "   1.99227076e-01]]\n",
      "{0: 282, 1: 606}\n",
      "(0.48514851485148514, 0.9932432432432432, 0.6518847006651884, None)\n",
      "\n",
      "1 loss 184095.9311958604\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.72955561e-02 -2.46166449e-02 -3.41606863e-02  4.63824804e-01\n",
      "   5.39650896e-01 -3.58804019e-04  2.39650219e-01 -2.39113049e-02\n",
      "  -2.10216595e-02  4.35881554e-01  4.96592638e-01 -3.57752122e-02\n",
      "  -2.12840571e-02 -3.40849971e-02 -2.37120355e-02  4.73632460e-01\n",
      "  -3.39150639e-02  2.32391547e-01  2.98043192e-01  5.00297536e-01\n",
      "   1.44845070e-02  7.81211593e+00  5.07995405e-01 -2.93102869e-02\n",
      "   1.92499070e-01  3.64792788e-01  4.98223164e-01  5.00885611e-01\n",
      "   4.71183326e-01 -3.03719733e-02  4.68860512e-01  3.19156039e-01\n",
      "   2.36233739e-01]]\n",
      "{0: 331, 1: 557}\n",
      "(0.5278276481149012, 0.9932432432432432, 0.6893317702227432, None)\n",
      "\n",
      "2 loss 184077.76811626065\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.74654896e-02 -3.04629063e-02 -3.82366396e-02  4.69669996e-01\n",
      "   5.46233110e-01  8.79120931e-03  2.46674475e-01 -1.31476545e-02\n",
      "  -4.07209614e-02  4.41731692e-01  5.02917091e-01 -2.84621799e-02\n",
      "  -3.32019851e-02 -3.58495336e-02 -3.56667991e-02  4.79530557e-01\n",
      "  -3.18347355e-02  2.39667879e-01  3.03248697e-01  5.06465240e-01\n",
      "   6.29523638e-03  7.96957032e+00  5.13052826e-01 -3.52724325e-02\n",
      "   1.85690979e-01  3.70481286e-01  5.04444761e-01  5.07060780e-01\n",
      "   4.77256020e-01 -3.15393420e-02  4.74648803e-01  3.25495699e-01\n",
      "   2.42689773e-01]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "3 loss 184075.69561594783\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.04007342 -0.03009211 -0.04399871  0.4707212   0.54822328  0.01190825\n",
      "   0.24755572 -0.01560052 -0.02688465  0.4430823   0.5045627  -0.04073575\n",
      "  -0.03286593 -0.03914421 -0.03133302  0.48039165 -0.03513749  0.24176299\n",
      "   0.30448611  0.50733268  0.0103617   8.00520632  0.51414719 -0.02847123\n",
      "   0.18560952  0.37163363  0.505257    0.50789708  0.47818969 -0.03316648\n",
      "   0.47550115  0.32648333  0.24449385]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "4 loss 184074.32622841964\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03478743 -0.01218055 -0.02597035  0.47236521  0.54939482  0.01299039\n",
      "   0.2490927  -0.0137723  -0.03824495  0.4444127   0.50593305 -0.03900618\n",
      "  -0.03795087 -0.03414962 -0.03666969  0.48209781 -0.03688362  0.24301856\n",
      "   0.30582981  0.50844464  0.01143825  8.01473096  0.51494271 -0.03667869\n",
      "   0.18556167  0.37309315  0.50654889  0.50899009  0.47977558 -0.0361336\n",
      "   0.47713486  0.32812335  0.24582919]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "5 loss 184074.304141006\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.79849453e-02 -1.50709607e-02 -3.34517996e-02  4.72513241e-01\n",
      "   5.49943564e-01  1.37250021e-02  2.48761931e-01 -7.28571780e-03\n",
      "  -3.47774708e-02  4.44770335e-01  5.06382672e-01 -4.09486330e-02\n",
      "  -3.64227067e-02 -4.01221237e-02 -3.65454511e-02  4.81981872e-01\n",
      "  -3.47849911e-02  2.43544912e-01  3.06105359e-01  5.08133742e-01\n",
      "   1.22800256e-02  8.01421629e+00  5.14949811e-01 -3.07474096e-02\n",
      "   1.89747186e-01  3.73327031e-01  5.06231111e-01  5.08690199e-01\n",
      "   4.79788399e-01 -3.46953709e-02  4.77001162e-01  3.28201695e-01\n",
      "   2.46228827e-01]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "6 loss 184073.8838315618\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03675934 -0.03010398 -0.04241368  0.47006663  0.54782973  0.01158996\n",
      "   0.24576021 -0.03249236 -0.02540835  0.44241773  0.50408214 -0.04035233\n",
      "  -0.01241406 -0.03937704 -0.03960897  0.47924993 -0.03922279  0.24138867\n",
      "   0.30406522  0.50528228  0.02599478  8.01379131  0.51303024 -0.0350588\n",
      "   0.18360572  0.37102949  0.50329044  0.50589281  0.47692463 -0.0314095\n",
      "   0.47429714  0.3252226   0.24407282]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "7 loss 184075.57111589992\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03426944 -0.01450217 -0.03868049  0.47015301  0.54789824  0.01211224\n",
      "   0.24622444 -0.03124466 -0.03835695  0.44243412  0.50412242 -0.03601927\n",
      "  -0.0431017  -0.03724357 -0.02216511  0.47946944 -0.03890508  0.24157376\n",
      "   0.30404989  0.50589139  0.01015002  8.01007493  0.51353105 -0.0425506\n",
      "   0.19008546  0.37105032  0.50384007  0.50653026  0.47712322 -0.04191844\n",
      "   0.47451222  0.32527493  0.24416845]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss 184074.00492684141\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.04140712 -0.01388598 -0.02847346  0.47125175  0.54897307  0.01236048\n",
      "   0.24710177 -0.01493443 -0.04031432  0.44365854  0.50527412 -0.03668002\n",
      "  -0.0220881  -0.04205851 -0.03018213  0.48067397 -0.03940079  0.24253276\n",
      "   0.30521563  0.50648752  0.02483747  8.003674    0.51395503 -0.0385454\n",
      "   0.19210929  0.37219175  0.50457201  0.50703091  0.4783006  -0.03838424\n",
      "   0.47566866  0.32664931  0.24510798]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "9 loss 184073.94014400485\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03874572 -0.01684914 -0.03337709  0.47134788  0.54911406  0.01288815\n",
      "   0.24741412 -0.03242969 -0.03541105  0.44379185  0.50544131 -0.0336627\n",
      "  -0.03443815 -0.03795035 -0.03324436  0.48067064 -0.03114516  0.24278702\n",
      "   0.3053335   0.50633072  0.0275334   8.00939135  0.51343883 -0.02822792\n",
      "   0.18958137  0.37235929  0.50445413  0.50687602  0.47835494 -0.02702278\n",
      "   0.47578979  0.32689307  0.24541342]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "10 loss 184075.95499247435\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03679897 -0.03122701 -0.02750086  0.46918565  0.54712129  0.01131101\n",
      "   0.24506258 -0.03130265 -0.03501671  0.4417699   0.50333767 -0.03252691\n",
      "  -0.03525199 -0.03939257 -0.03685102  0.47839186 -0.02919354  0.24094407\n",
      "   0.30339267  0.50435793  0.02846139  8.00756388  0.51199842 -0.04026599\n",
      "   0.18780648  0.3703113   0.50237024  0.5049639   0.47608172 -0.03725894\n",
      "   0.47352157  0.32454086  0.24344874]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "11 loss 184074.1633556071\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.0409892  -0.03199625 -0.02513484  0.47139949  0.54889688  0.01281362\n",
      "   0.2476866  -0.03244139 -0.036787    0.44374594  0.50527443 -0.03118705\n",
      "  -0.02835255 -0.03322832 -0.02363063  0.48095899 -0.03967819  0.2426356\n",
      "   0.30521279  0.50714398  0.01271412  8.01266138  0.51421603 -0.04045789\n",
      "   0.18266847  0.37233489  0.5052242   0.50770335  0.47867294 -0.02625085\n",
      "   0.4760599   0.32705638  0.24527416]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "12 loss 184074.45304946633\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03560678 -0.02011072 -0.02948065  0.47134615  0.54866534  0.01216019\n",
      "   0.24787157 -0.01500945 -0.03404796  0.44354749  0.50507841 -0.03979459\n",
      "  -0.03762256 -0.03414967 -0.04025053  0.48090648 -0.03570953  0.24224109\n",
      "   0.30503995  0.50742304  0.0087434   8.00882682  0.51450118 -0.03808427\n",
      "   0.18563756  0.37218986  0.50543367  0.50802797  0.47863752 -0.03660954\n",
      "   0.47589776  0.32679041  0.24499348]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "13 loss 184075.1792532667\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.04014971 -0.01479072 -0.03755278  0.47072694  0.54872947  0.01260218\n",
      "   0.24623711 -0.03157435 -0.03616037  0.44326528  0.50494435 -0.03992442\n",
      "  -0.03809204 -0.02553765 -0.03955701  0.47991582 -0.02908845  0.24234298\n",
      "   0.30475963  0.50553585  0.02711111  8.00969322  0.51339535 -0.02438206\n",
      "   0.1873126   0.37176092  0.50363991  0.50611929  0.47757822 -0.03615781\n",
      "   0.47499259  0.32604955  0.24489018]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "14 loss 184076.3317588023\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-4.02981947e-02 -1.43941117e-02 -2.61275957e-02  4.72671873e-01\n",
      "   5.50013881e-01  1.34094404e-02  2.48948774e-01 -1.75642177e-02\n",
      "  -3.89380095e-02  4.44867112e-01  5.06421137e-01 -3.36443232e-02\n",
      "  -3.43405412e-02 -4.05279483e-02 -2.42534794e-02  4.82395633e-01\n",
      "  -3.67936855e-02  2.43461382e-01  3.06344259e-01  5.08663053e-01\n",
      "   7.74470693e-03  8.00190359e+00  5.15294266e-01 -3.43201512e-02\n",
      "   1.89883595e-01  3.73467912e-01  5.06769195e-01  5.09203793e-01\n",
      "   4.80118780e-01 -3.04199722e-02  4.77414458e-01  3.28406966e-01\n",
      "   2.46226953e-01]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-4.02981947e-02 -1.43941117e-02 -2.61275957e-02  4.72671873e-01\n",
      "   5.50013881e-01  1.34094404e-02  2.48948774e-01 -1.75642177e-02\n",
      "  -3.89380095e-02  4.44867112e-01  5.06421137e-01 -3.36443232e-02\n",
      "  -3.43405412e-02 -4.05279483e-02 -2.42534794e-02  4.82395633e-01\n",
      "  -3.67936855e-02  2.43461382e-01  3.06344259e-01  5.08663053e-01\n",
      "   7.74470693e-03  8.00190359e+00  5.15294266e-01 -3.43201512e-02\n",
      "   1.89883595e-01  3.73467912e-01  5.06769195e-01  5.09203793e-01\n",
      "   4.80118780e-01 -3.04199722e-02  4.77414458e-01  3.28406966e-01\n",
      "   2.46226953e-01]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.581\n",
      "Precision            0.542\n",
      "Recall               0.993\n",
      "F1                   0.702\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 248 | TN: 344 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 346, 1: 542}\n",
      "acc 0.7184684684684685\n",
      "(array([0.99421965, 0.54243542]), array([0.58108108, 0.99324324]), array([0.73347548, 0.70167064]), array([592, 296]))\n",
      "(0.7683275387667171, 0.7871621621621621, 0.7175730620677723, None)\n",
      "[[344 248]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5424354243542435 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 2 init at 1\n",
    "\n",
    "train_nl_p2(0.01,15,tf.truncated_normal_initializer(1,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c27400>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c27400>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c27400>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"sub_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 185019.1808580617\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-2.97004790e-02  3.40415613e-03 -3.01599583e-02  4.30219146e-01\n",
      "   4.97951111e-01 -1.71109063e-02  2.03537461e-01  4.01915067e-03\n",
      "  -3.33700532e-02  4.01467329e-01  4.57478415e-01 -2.58622238e-02\n",
      "  -3.59678560e-02 -1.56412049e-02 -3.51813849e-02  4.41685736e-01\n",
      "  -3.28875387e-02  1.87050668e-01  2.66611783e-01  4.68940489e-01\n",
      "  -3.59863311e-03  6.99235117e+00  4.77808386e-01 -3.35508639e-02\n",
      "   2.23322890e-01  3.32044881e-01  4.66546160e-01  4.69459113e-01\n",
      "   4.38213658e-01 -1.07721581e-02  4.37418430e-01  2.84022540e-01\n",
      "   1.96699148e-01]]\n",
      "{0: 282, 1: 606}\n",
      "(0.48514851485148514, 0.9932432432432432, 0.6518847006651884, None)\n",
      "\n",
      "1 loss 184098.24631785662\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-4.10632919e-02 -2.20223790e-02 -3.20340210e-02  4.60760752e-01\n",
      "   5.36606624e-01 -3.65271370e-03  2.36207588e-01 -2.23684595e-02\n",
      "  -2.76453832e-02  4.32971418e-01  4.93477121e-01 -3.77134393e-02\n",
      "  -3.29967723e-02 -3.81676744e-02 -3.51848270e-02  4.70542613e-01\n",
      "  -3.73613538e-02  2.29432222e-01  2.95323915e-01  4.97307042e-01\n",
      "   2.74088438e-02  7.80439205e+00  5.06064280e-01 -3.47007852e-02\n",
      "   1.91623964e-01  3.61967607e-01  4.95079796e-01  4.97963920e-01\n",
      "   4.67981787e-01 -2.28453881e-02  4.65628331e-01  3.15660159e-01\n",
      "   2.33373624e-01]]\n",
      "{0: 331, 1: 557}\n",
      "(0.5278276481149012, 0.9932432432432432, 0.6893317702227432, None)\n",
      "\n",
      "2 loss 184076.33617693814\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03719551 -0.02980716 -0.02487364  0.46910614  0.5463477   0.00927219\n",
      "   0.2454726  -0.02942687 -0.02610505  0.44147978  0.50283325 -0.02891611\n",
      "  -0.03978765 -0.04095165 -0.03390313  0.47864875 -0.02728497  0.23975378\n",
      "   0.30309216  0.50497266  0.02101822  7.96639001  0.51228528 -0.03593535\n",
      "   0.18884098  0.37008621  0.50295413  0.50555508  0.47636365 -0.03397818\n",
      "   0.47371376  0.32458286  0.24268838]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "3 loss 184075.67230073962\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.04000579 -0.02929014 -0.0416808   0.46773449  0.54597162  0.01036018\n",
      "   0.24353707 -0.02947228 -0.02905742  0.44043733  0.50202344 -0.04354925\n",
      "  -0.02634854 -0.04150618 -0.04061767  0.47691512 -0.04335959  0.23971151\n",
      "   0.30201398  0.50345046  0.02500319  8.00582072  0.51154305 -0.035034\n",
      "   0.19270217  0.36891756  0.50125296  0.50410436  0.47453877 -0.04332848\n",
      "   0.47190294  0.32301819  0.24227728]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "4 loss 184074.09480719012\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03272515 -0.03028879 -0.02935123  0.4691231   0.54692951  0.01060621\n",
      "   0.24483561 -0.02994183 -0.03837465  0.44154809  0.50318547 -0.03645254\n",
      "  -0.0382104  -0.04183762 -0.0395394   0.47818151 -0.03313195  0.24046424\n",
      "   0.30313813  0.5042176   0.04058152  8.0071091   0.51225526 -0.02943779\n",
      "   0.18729839  0.37008394  0.50222602  0.50483434  0.47590542 -0.02744389\n",
      "   0.4732858   0.32441545  0.24310304]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "5 loss 184074.69442650315\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03026418 -0.03177738 -0.03310403  0.46999379  0.54752346  0.01129466\n",
      "   0.24626004 -0.03101791 -0.03800875  0.44231784  0.50387201 -0.02990233\n",
      "  -0.03707408 -0.03029629 -0.03664463  0.47950725 -0.0369264   0.2411329\n",
      "   0.30382723  0.50575188  0.02530273  8.01203429  0.51304455 -0.03784305\n",
      "   0.18534166  0.37090506  0.50377783  0.50634331  0.4771397  -0.03271905\n",
      "   0.47452079  0.3254376   0.24375998]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "6 loss 184075.81222054674\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.04214348 -0.01363894 -0.04229492  0.47005912  0.5482978   0.01194111\n",
      "   0.24550109 -0.0130485  -0.04234248  0.44271651  0.50449525 -0.02750517\n",
      "  -0.03866325 -0.03237141 -0.03691245  0.47889755 -0.03128719  0.24171977\n",
      "   0.30422389  0.50404903  0.043032    8.01088417  0.51175679 -0.04075407\n",
      "   0.19136053  0.37114088  0.50220226  0.50459397  0.47666021 -0.03667382\n",
      "   0.47406695  0.3252742   0.24427325]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "7 loss 184073.6538553713\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03830226 -0.03124056 -0.03574147  0.47075967  0.54816489  0.0119156\n",
      "   0.24724748 -0.03040333 -0.02598976  0.44310658  0.50459085 -0.03526461\n",
      "  -0.03036376 -0.03285122 -0.03411445  0.48034726 -0.03348566  0.24190698\n",
      "   0.30455225  0.5068177   0.00870511  8.00942288  0.514349   -0.0378006\n",
      "   0.18388141  0.37167596  0.50480576  0.50739867  0.47802572 -0.04000233\n",
      "   0.47543268  0.326421    0.24453894]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss 184075.35465314926\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03747283 -0.03086687 -0.03313061  0.47154549  0.54946232  0.01285635\n",
      "   0.24737934 -0.01488155 -0.03674967  0.4439993   0.50572409 -0.0368102\n",
      "  -0.03375719 -0.03829721 -0.03101491  0.48073403 -0.02867235  0.24291939\n",
      "   0.30542197  0.50615582  0.02590858  8.00685861  0.51340009 -0.03402302\n",
      "   0.18993527  0.37252336  0.50433613  0.50668766  0.47847292 -0.02531054\n",
      "   0.47579621  0.32683271  0.24553263]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "9 loss 184075.0425320094\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03722898 -0.03136025 -0.03246129  0.47056368  0.54817309  0.01204425\n",
      "   0.247105   -0.02961021 -0.02983603  0.44285051  0.50447151 -0.02270029\n",
      "  -0.03493177 -0.03873293 -0.03574525  0.48011541 -0.03836254  0.24171564\n",
      "   0.30434466  0.50668603  0.00910798  8.00458594  0.51374893 -0.03282894\n",
      "   0.18663326  0.37144231  0.50464262  0.50724336  0.47771347 -0.04015549\n",
      "   0.47510767  0.3260944   0.24447049]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "10 loss 184074.6878426222\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.04047492 -0.01401787 -0.04182022  0.47231535  0.54953101  0.01274488\n",
      "   0.24865161 -0.01515972 -0.0374405   0.44453376  0.50607422 -0.04293005\n",
      "  -0.03303275 -0.0192695  -0.03015338  0.48191821 -0.0360775   0.24292541\n",
      "   0.30592296  0.50780314  0.00947546  8.00746425  0.51467569 -0.02611762\n",
      "   0.18561266  0.37317031  0.50592522  0.50834094  0.47960561 -0.04315551\n",
      "   0.47696677  0.32795702  0.24579871]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "11 loss 184074.18317999493\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03347115 -0.03039098 -0.04182734  0.47020483  0.54786092  0.01202673\n",
      "   0.24681775 -0.03116962 -0.03263449  0.44260849  0.50415154 -0.02845724\n",
      "  -0.02848683 -0.03630008 -0.03860476  0.47980112 -0.03893293  0.24155588\n",
      "   0.30408598  0.50654748  0.00939724  8.00856268  0.51384299 -0.0408034\n",
      "   0.18247022  0.37118822  0.50445416  0.50715884  0.47750971 -0.02571848\n",
      "   0.47488176  0.32584874  0.24417027]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "12 loss 184076.2212753663\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03988126 -0.02968545 -0.02939736  0.46954028  0.54763971  0.01137389\n",
      "   0.24479894 -0.03045954 -0.04223743  0.44212976  0.50384314 -0.03520748\n",
      "  -0.02277096 -0.03442688 -0.03881909  0.47860993 -0.04269373  0.24124056\n",
      "   0.30369656  0.50388315  0.04333452  8.00572744  0.51183404 -0.01880674\n",
      "   0.18981005  0.37065809  0.50200946  0.50443071  0.47623468 -0.03296655\n",
      "   0.4737184   0.32461917  0.24377479]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "13 loss 184074.984916148\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03546544 -0.03032071 -0.04009047  0.4698368   0.54746501  0.0111245\n",
      "   0.24623461 -0.03170327 -0.04143663  0.44233695  0.50376134 -0.03240191\n",
      "  -0.02961944 -0.01723793 -0.03283327  0.47945405 -0.03532291  0.24119451\n",
      "   0.30385639  0.50572037  0.02635251  8.01048673  0.51297277 -0.03825507\n",
      "   0.18527263  0.37090341  0.50371816  0.50630196  0.47710683 -0.03947691\n",
      "   0.47442806  0.32534081  0.24388593]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "14 loss 184075.0093095277\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03634859 -0.03100653 -0.02783093  0.47023041  0.5479867   0.01190698\n",
      "   0.24622544 -0.02129636 -0.02826619  0.44265819  0.5042776  -0.03683723\n",
      "  -0.04206125 -0.03297314 -0.03802875  0.47948641 -0.03428453  0.24162946\n",
      "   0.30424461  0.50543967  0.02581523  8.0105839   0.51298408 -0.03827974\n",
      "   0.18559293  0.37125195  0.50342032  0.50602783  0.4772285  -0.03611104\n",
      "   0.47460624  0.32558126  0.24424884]]\n",
      "{0: 346, 1: 542}\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.03634859 -0.03100653 -0.02783093  0.47023041  0.5479867   0.01190698\n",
      "   0.24622544 -0.02129636 -0.02826619  0.44265819  0.5042776  -0.03683723\n",
      "  -0.04206125 -0.03297314 -0.03802875  0.47948641 -0.03428453  0.24162946\n",
      "   0.30424461  0.50543967  0.02581523  8.0105839   0.51298408 -0.03827974\n",
      "   0.18559293  0.37125195  0.50342032  0.50602783  0.4772285  -0.03611104\n",
      "   0.47460624  0.32558126  0.24424884]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.581\n",
      "Precision            0.542\n",
      "Recall               0.993\n",
      "F1                   0.702\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 248 | TN: 344 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 346, 1: 542}\n",
      "acc 0.7184684684684685\n",
      "(array([0.99421965, 0.54243542]), array([0.58108108, 0.99324324]), array([0.73347548, 0.70167064]), array([592, 296]))\n",
      "(0.7683275387667171, 0.7871621621621621, 0.7175730620677723, None)\n",
      "[[344 248]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5424354243542435 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5424354243542435, 0.9932432432432432, 0.7016706443914081, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 2 init at 0.2\n",
    "\n",
    "train_nl_p2(0.01,15,tf.truncated_normal_initializer(0.2,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized training with penalty3 log(1+e^(-x))\n",
    "\n",
    "def train_nl_p3(lr,ep,th,pk=0):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz)) \\\n",
    "                     +tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f553c2cbe48>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f553c2cbe48>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f553c2cbe48>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 291606.0732735043\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.95318103  1.0826686   0.99504176  1.3541312   1.41512944  1.03895243\n",
      "   1.13567176  1.08117037  1.01088726  1.31735104  1.37685975  0.99241078\n",
      "   0.86709682  0.87891464  0.86447469  1.35745838  0.86751074  1.19850012\n",
      "   1.24252827  1.36317369  1.03618529 12.09697115  1.35942826  0.9380688\n",
      "   1.3542583   1.28011781  1.36479751  1.36227615  1.36130268  0.86401791\n",
      "   1.35498379  1.22062306  1.20637663]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "1 loss 288019.9942856593\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.89128263  1.01623627  0.93099761  1.43023724  1.49822573  1.11713271\n",
      "   1.21318471  1.01489165  0.94584622  1.39396244  1.45570949  0.9284899\n",
      "   0.80855497  0.81931503  0.80605663  1.43306333  0.80883551  1.2764648\n",
      "   1.31266362  1.44107795  1.11578854 14.47279002  1.43685045  0.87729875\n",
      "   1.28103959  1.35297718  1.44242115  1.44021165  1.43852153  0.80596796\n",
      "   1.43049302  1.2959493   1.28023029]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "2 loss 287889.6733951078\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.88059115  1.00481854  0.91996122  1.44396918  1.51329517  1.13127474\n",
      "   1.22710667  1.00349593  0.93464929  1.4078194   1.46999034  0.91747423\n",
      "   0.7983951   0.80900924  0.79594189  1.44667978  0.79866033  1.29055985\n",
      "   1.32528593  1.45514901  1.13017054 14.89491877  1.45084866  0.8667885\n",
      "   1.2684493   1.36612113  1.45642479  1.45429053  1.45245165  0.79590766\n",
      "   1.44409679  1.3095408   1.29355134]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "3 loss 287880.5681594515\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.87868765  1.00278743  0.91799711  1.44643278  1.51600169  1.13381387\n",
      "   1.22960259  1.00146866  0.93265697  1.41030694  1.47255449  0.91551381\n",
      "   0.79658498  0.80717408  0.79414036  1.44912203  0.79684747  1.29309006\n",
      "   1.32754954  1.45767399  1.13275191 14.97045755  1.45336114  0.86491685\n",
      "   1.26620963  1.36847929  1.45893723  1.45681701  1.45495089  0.7941156\n",
      "   1.44653691  1.31197968  1.29594142]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "4 loss 287879.38528843503\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.87834678  1.00242376  0.91764542  1.44687455  1.51648712  1.13426926\n",
      "   1.23005011  1.00110568  0.93230023  1.41075306  1.47301436  0.91516277\n",
      "   0.79626079  0.80684544  0.79381773  1.44955995  0.79652279  1.29354383\n",
      "   1.32795542  1.45812679  1.13321484 14.98399699  1.45381172  0.86458167\n",
      "   1.26580863  1.36890216  1.45938776  1.45727008  1.45539905  0.79379466\n",
      "   1.44697446  1.31241703  1.29637002]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.87834678  1.00242376  0.91764542  1.44687455  1.51648712  1.13426926\n",
      "   1.23005011  1.00110568  0.93230023  1.41075306  1.47301436  0.91516277\n",
      "   0.79626079  0.80684544  0.79381773  1.44955995  0.79652279  1.29354383\n",
      "   1.32795542  1.45812679  1.13321484 14.98399699  1.45381172  0.86458167\n",
      "   1.26580863  1.36890216  1.45938776  1.45727008  1.45539905  0.79379466\n",
      "   1.44697446  1.31241703  1.29637002]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.568\n",
      "Precision            0.535\n",
      "Recall               0.993\n",
      "F1                   0.695\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 256 | TN: 336 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 338, 1: 550}\n",
      "acc 0.7094594594594594\n",
      "(array([0.99408284, 0.53454545]), array([0.56756757, 0.99324324]), array([0.72258065, 0.69503546]), array([592, 296]))\n",
      "(0.7643141473910704, 0.7804054054054054, 0.708808053077099, None)\n",
      "[[336 256]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5345454545454545 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 3 init at 0.0\n",
    "\n",
    "train_nl_p3(0.01,5,tf.truncated_normal_initializer(0.0,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c273c8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c273c8>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c273c8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 290770.90924393653\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.94913024  1.07829776  0.99084087  1.35893491  1.42035779  1.04389281\n",
      "   1.14057216  1.07681867  1.00662058  1.32218092  1.38182356  0.98822025\n",
      "   0.86330721  0.87501933  0.86066177  1.36224323  0.86367949  1.20341864\n",
      "   1.24696483  1.3680727   1.04121183 12.24901797  1.36429625  0.93409604\n",
      "   1.3494613   1.28471899  1.3696973   1.36718061  1.36618045  0.86024017\n",
      "   1.35976273  1.22537199  1.21104299]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "1 loss 288007.6202959855\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.89059835  1.01550504  0.93029103  1.43111079  1.49918353  1.1180318\n",
      "   1.21407082  1.01416185  0.94512926  1.39484351  1.45661737  0.92778465\n",
      "   0.80790511  0.81865554  0.80540948  1.43392971  0.80818465  1.27736103\n",
      "   1.31346683  1.4419729   1.11670311 14.49969835  1.43774061  0.8766262\n",
      "   1.28023326  1.35381331  1.44331195  1.44110707  1.43940766  0.80532436\n",
      "   1.43135855  1.29681377  1.28107765]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "2 loss 287888.9666838523\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.88046987  1.00468912  0.91983608  1.44412597  1.51346739  1.13143632\n",
      "   1.22726553  1.00336676  0.93452235  1.4079777   1.47015351  0.91734932\n",
      "   0.79827979  0.80889233  0.79582712  1.44683522  0.79854484  1.29072086\n",
      "   1.32543     1.4553097   1.13033481 14.89972795  1.45100855  0.86666926\n",
      "   1.26830659  1.36627121  1.45658469  1.45445131  1.4526107   0.7957935\n",
      "   1.44425209  1.30969601  1.29370345]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "3 loss 287880.48881181644\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.87866595  1.00276428  0.91797472  1.44646089  1.51603259  1.13384286\n",
      "   1.22963108  1.00144556  0.93263426  1.41033534  1.47258376  0.91549146\n",
      "   0.79656434  0.80715316  0.79411982  1.4491499   0.7968268   1.29311894\n",
      "   1.32757537  1.45770281  1.13278138 14.97131934  1.45338982  0.86489551\n",
      "   1.26618411  1.36850621  1.4589659   1.45684584  1.45497941  0.79409517\n",
      "   1.44656476  1.31200751  1.2959687 ]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "4 loss 287879.37257568026\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.87834289  1.00241961  0.9176414   1.44687959  1.51649266  1.13427445\n",
      "   1.23005522  1.00110154  0.93229616  1.41075815  1.47301961  0.91515876\n",
      "   0.79625709  0.80684169  0.79381405  1.44956495  0.79651908  1.29354901\n",
      "   1.32796005  1.45813196  1.13322013 14.98415149  1.45381686  0.86457784\n",
      "   1.26580405  1.36890698  1.4593929   1.45727525  1.45540417  0.793791\n",
      "   1.44697945  1.31242202  1.29637491]]\n",
      "{0: 338, 1: 550}\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.87834289  1.00241961  0.9176414   1.44687959  1.51649266  1.13427445\n",
      "   1.23005522  1.00110154  0.93229616  1.41075815  1.47301961  0.91515876\n",
      "   0.79625709  0.80684169  0.79381405  1.44956495  0.79651908  1.29354901\n",
      "   1.32796005  1.45813196  1.13322013 14.98415149  1.45381686  0.86457784\n",
      "   1.26580405  1.36890698  1.4593929   1.45727525  1.45540417  0.793791\n",
      "   1.44697945  1.31242202  1.29637491]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.568\n",
      "Precision            0.535\n",
      "Recall               0.993\n",
      "F1                   0.695\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 256 | TN: 336 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 338, 1: 550}\n",
      "acc 0.7094594594594594\n",
      "(array([0.99408284, 0.53454545]), array([0.56756757, 0.99324324]), array([0.72258065, 0.69503546]), array([592, 296]))\n",
      "(0.7643141473910704, 0.7804054054054054, 0.708808053077099, None)\n",
      "[[336 256]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5345454545454545 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5345454545454545, 0.9932432432432432, 0.6950354609929078, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 3 init at 1\n",
    "\n",
    "train_nl_p3(0.01,5,tf.truncated_normal_initializer(1.0,0.5,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c272b0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c272b0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c272b0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 198162.27563383782\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.55078094e-02  1.88884098e-01  8.02988336e-02  4.48477437e-01\n",
      "   5.07728066e-01 -9.34814250e-04  2.06723569e-01  1.88253623e-01\n",
      "   1.06386193e-01  4.15480844e-01  4.70919336e-01  7.75370876e-02\n",
      "  -1.23174180e-01 -8.65573322e-02 -1.15918413e-01  4.58018179e-01\n",
      "  -1.22186143e-01  2.19479712e-01  3.12869811e-01  4.66633858e-01\n",
      "  -6.38711451e-02  6.64706142e+00  4.64915115e-01 -4.53742872e-02\n",
      "   4.66235708e-01  3.61701887e-01  4.67206745e-01  4.66399114e-01\n",
      "   4.53159443e-01 -1.23945187e-01  4.53851983e-01  3.00702849e-01\n",
      "   2.42891362e-01]]\n",
      "{0: 281, 1: 607}\n",
      "(0.4843492586490939, 0.9932432432432432, 0.6511627906976744, None)\n",
      "\n",
      "1 loss 194067.69649343874\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.11024912  0.09561018 -0.01115632  0.54909697  0.62776129  0.15289865\n",
      "   0.31931849  0.09537398  0.01458902  0.51863151  0.58340945 -0.01356479\n",
      "  -0.21164596 -0.17430501 -0.20346421  0.55423913 -0.21029864  0.34635748\n",
      "   0.40463715  0.56729313  0.09268576 10.02692071  0.5656384  -0.13807017\n",
      "   0.36328663  0.45856207  0.56742848  0.56698275  0.55269521 -0.21108973\n",
      "   0.54965382  0.40495467  0.35450301]]\n",
      "{0: 342, 1: 546}\n",
      "(0.5384615384615384, 0.9932432432432432, 0.6983372921615201, None)\n",
      "\n",
      "2 loss 193795.6698417469\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.35667167e-01  7.04596936e-02 -3.58599652e-02  5.77010924e-01\n",
      "   6.59746168e-01  1.91512591e-01  3.49047109e-01  7.02888362e-02\n",
      "  -1.02386702e-02  5.47047374e-01  6.13557262e-01 -3.81978700e-02\n",
      "  -2.35643086e-01 -1.98129004e-01 -2.27230356e-01  5.81019962e-01\n",
      "  -2.34203319e-01  3.79556146e-01  4.30409777e-01  5.95271216e-01\n",
      "   1.32686911e-01  1.09428376e+01  5.93664716e-01 -1.63117926e-01\n",
      "   3.35668404e-01  4.85757110e-01  5.95284858e-01  5.94981486e-01\n",
      "   5.80509893e-01 -2.34773488e-01  5.76500558e-01  4.33482064e-01\n",
      "   3.84739943e-01]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "3 loss 193766.6226634336\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14254264  0.06364781 -0.04255437  0.5846363   0.66843648  0.20184171\n",
      "   0.35711641  0.06349068 -0.01696937  0.55481085  0.62175485 -0.04487565\n",
      "  -0.24215786 -0.20459427 -0.23368155  0.58834101 -0.24069327  0.38853999\n",
      "   0.437468    0.60292015  0.14342533 11.19178067  0.60132857 -0.16990886\n",
      "   0.32818607  0.49319979  0.60289923  0.60263809  0.58811983 -0.2412043\n",
      "   0.58385053  0.44126372  0.39295937]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "4 loss 193762.02612330267\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14440849  0.0617987  -0.04437183  0.58671117  0.67079815  0.20463715\n",
      "   0.35930836  0.06164498 -0.01879693  0.55692349  0.62398303 -0.04668881\n",
      "  -0.2439275  -0.20635028 -0.23543384  0.5903335  -0.24245618  0.39097876\n",
      "   0.43938977  0.60500186  0.14633406 11.25942783  0.60341449 -0.17175283\n",
      "   0.32615477  0.49522581  0.60497143  0.60472203  0.59019131 -0.2429512\n",
      "   0.58585158  0.44338033  0.39519298]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14440849  0.0617987  -0.04437183  0.58671117  0.67079815  0.20463715\n",
      "   0.35930836  0.06164498 -0.01879693  0.55692349  0.62398303 -0.04668881\n",
      "  -0.2439275  -0.20635028 -0.23543384  0.5903335  -0.24245618  0.39097876\n",
      "   0.43938977  0.60500186  0.14633406 11.25942783  0.60341449 -0.17175283\n",
      "   0.32615477  0.49522581  0.60497143  0.60472203  0.59019131 -0.2429512\n",
      "   0.58585158  0.44338033  0.39519298]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.576\n",
      "Precision            0.539\n",
      "Recall               0.993\n",
      "F1                   0.699\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 251 | TN: 341 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 343, 1: 545}\n",
      "acc 0.7150900900900901\n",
      "(array([0.9941691 , 0.53944954]), array([0.57601351, 0.99324324]), array([0.72941176, 0.69916766]), array([592, 296]))\n",
      "(0.7668093187471581, 0.7846283783783783, 0.7142897111282087, None)\n",
      "[[341 251]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5394495412844037 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 3 \n",
    "\n",
    "train_nl_p3(0.01,5,tf.truncated_normal_initializer(0.0,0.5,12),3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c271d0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c271d0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f5559c271d0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 195386.12621331043\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.07587055  0.12950425  0.02209998  0.51204082  0.58473213  0.09940877\n",
      "   0.27942629  0.12915159  0.04800138  0.48088444  0.54292738  0.01958143\n",
      "  -0.17942775 -0.14229457 -0.17155024  0.51870715 -0.17821215  0.30137768\n",
      "   0.37059457  0.53020628  0.0376914   8.79882603  0.52849608 -0.10436175\n",
      "   0.40049427  0.42262931  0.53049411  0.52989464  0.51588118 -0.17930639\n",
      "   0.5141674   0.36697057  0.31397971]]\n",
      "{0: 342, 1: 546}\n",
      "(0.5384615384615384, 0.9932432432432432, 0.6983372921615201, None)\n",
      "\n",
      "1 loss 193864.59510188442\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.26422086e-01  7.96139864e-02 -2.68658573e-02  5.66807718e-01\n",
      "   6.48088786e-01  1.77549561e-01  3.38215794e-01  7.94218794e-02\n",
      "  -1.19779633e-03  5.36660628e-01  6.02564864e-01 -2.92278921e-02\n",
      "  -2.26898608e-01 -1.89449277e-01 -2.18570555e-01  5.71227687e-01\n",
      "  -2.25492322e-01  3.67480692e-01  4.20977021e-01  5.85040526e-01\n",
      "   1.18195195e-01  1.06089055e+01  5.83415281e-01 -1.53996624e-01\n",
      "   3.45722099e-01  4.75806822e-01  5.85099459e-01  5.84741801e-01\n",
      "   5.70334961e-01 -2.26142357e-01  5.66676379e-01  4.23062504e-01\n",
      "   3.73715140e-01]]\n",
      "{0: 342, 1: 546}\n",
      "(0.5384615384615384, 0.9932432432432432, 0.6983372921615201, None)\n",
      "\n",
      "2 loss 193775.00181542008\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.1400379   0.06612973 -0.04011508  0.58185468  0.66526841  0.19808402\n",
      "   0.35417534  0.06596782 -0.0145167   0.55197873  0.61876613 -0.04244228\n",
      "  -0.2397834  -0.20223798 -0.23133031  0.58567011 -0.23832784  0.38526671\n",
      "   0.43489244  0.60012964  0.13951701 11.10103069  0.59853252 -0.1674342\n",
      "   0.3309124   0.49048423  0.6001214   0.59984469  0.58534329 -0.2388604\n",
      "   0.58116861  0.43842561  0.389963  ]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "3 loss 193763.53853158277\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14372844  0.06247267 -0.04370938  0.58595467  0.66993722  0.20361867\n",
      "   0.35850935  0.06231772 -0.01813079  0.55615321  0.62317075 -0.04602793\n",
      "  -0.24328244 -0.20571019 -0.23479511  0.58960702 -0.24181357  0.39008986\n",
      "   0.43868903  0.60424285  0.14527418 11.2347681   0.60265393 -0.17108071\n",
      "   0.32689515  0.49448708  0.60421589  0.6039622   0.589436   -0.24231443\n",
      "   0.58512194  0.44260865  0.39437874]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "4 loss 193761.37506614238\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14473057  0.0614795  -0.04468558  0.58706957  0.67120596  0.20511937\n",
      "   0.35968682  0.06132634 -0.01911244  0.55728842  0.62436781 -0.04700184\n",
      "  -0.24423304 -0.20665346 -0.23573638  0.59067769 -0.24276056  0.39139977\n",
      "   0.43972177  0.60536146  0.14683593 11.27110879  0.60377482 -0.17207117\n",
      "   0.3258041   0.4955758   0.60532938  0.60508202  0.59054915 -0.24325281\n",
      "   0.58619727  0.44374591  0.39557867]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14473057  0.0614795  -0.04468558  0.58706957  0.67120596  0.20511937\n",
      "   0.35968682  0.06132634 -0.01911244  0.55728842  0.62436781 -0.04700184\n",
      "  -0.24423304 -0.20665346 -0.23573638  0.59067769 -0.24276056  0.39139977\n",
      "   0.43972177  0.60536146  0.14683593 11.27110879  0.60377482 -0.17207117\n",
      "   0.3258041   0.4955758   0.60532938  0.60508202  0.59054915 -0.24325281\n",
      "   0.58619727  0.44374591  0.39557867]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.576\n",
      "Precision            0.539\n",
      "Recall               0.993\n",
      "F1                   0.699\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 251 | TN: 341 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 343, 1: 545}\n",
      "acc 0.7150900900900901\n",
      "(array([0.9941691 , 0.53944954]), array([0.57601351, 0.99324324]), array([0.72941176, 0.69916766]), array([592, 296]))\n",
      "(0.7668093187471581, 0.7846283783783783, 0.7142897111282087, None)\n",
      "[[341 251]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5394495412844037 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 3 \n",
    "\n",
    "train_nl_p3(0.01,5,tf.truncated_normal_initializer(1.0,0.5,12),3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554c7ee588>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554c7ee588>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f554c7ee588>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"add_1:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 195242.18244008135\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.07476236  0.13059897  0.02317173  0.51085488  0.58333493  0.09763957\n",
      "   0.27808041  0.1302412   0.04907678  0.47966932  0.541615    0.02064881\n",
      "  -0.17839076 -0.14126689 -0.17052564  0.51757683 -0.17717598  0.29990965\n",
      "   0.36950995  0.52901836  0.03588348  8.75924634  0.52730963 -0.10327645\n",
      "   0.40170036  0.42148278  0.52931266  0.52870457  0.51470925 -0.17828536\n",
      "   0.51303633  0.36574566  0.3126701 ]]\n",
      "{0: 342, 1: 546}\n",
      "(0.5384615384615384, 0.9932432432432432, 0.6983372921615201, None)\n",
      "\n",
      "1 loss 193867.38908306375\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-1.26123891e-01  7.99091415e-02 -2.65759131e-02  5.66479577e-01\n",
      "   6.47713276e-01  1.77097704e-01  3.37866770e-01  7.97162983e-02\n",
      "  -9.06379294e-04  5.36326594e-01  6.02210855e-01 -2.89387580e-02\n",
      "  -2.26616862e-01 -1.89169587e-01 -2.18291528e-01  5.70912831e-01\n",
      "  -2.25211656e-01  3.67091250e-01  4.20673885e-01  5.84711576e-01\n",
      "   1.17726722e-01  1.05981499e+01  5.83085751e-01 -1.53702622e-01\n",
      "   3.46046228e-01  4.75486988e-01  5.84771950e-01  5.84412586e-01\n",
      "   5.70007879e-01 -2.25864279e-01  5.66360634e-01  4.22727257e-01\n",
      "   3.73360057e-01]]\n",
      "{0: 342, 1: 546}\n",
      "(0.5384615384615384, 0.9932432432432432, 0.6983372921615201, None)\n",
      "\n",
      "2 loss 193775.31386634527\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.13995719  0.0662097  -0.04003649  0.58176511  0.66516637  0.19796284\n",
      "   0.3540806   0.06604763 -0.01443768  0.55188754  0.61866986 -0.04236388\n",
      "  -0.23970691 -0.20216207 -0.23125457  0.58558412 -0.23825164  0.38516124\n",
      "   0.43480952  0.60003979  0.139391   11.09810755  0.5984425  -0.16735447\n",
      "   0.33100024  0.4903968   0.60003196  0.59975476  0.5852539  -0.23878489\n",
      "   0.58108227  0.43833421  0.38986648]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "3 loss 193763.59036067565\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14370654  0.06249438 -0.04368804  0.58593031  0.66990949  0.20358585\n",
      "   0.35848362  0.06233939 -0.01810933  0.5561284   0.62314458 -0.04600664\n",
      "  -0.24326166 -0.20568958 -0.23477453  0.58958362 -0.24179287  0.39006123\n",
      "   0.43866647  0.6042184   0.14524003 11.23397378  0.60262944 -0.17105906\n",
      "   0.326919    0.49446329  0.60419155  0.60393773  0.58941168 -0.24229391\n",
      "   0.58509844  0.4425838   0.39435252]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "4 loss 193761.38671536115\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14472462  0.06148539 -0.04467978  0.58706294  0.67119842  0.20511046\n",
      "   0.35967982  0.06133223 -0.01910661  0.55728168  0.6243607  -0.04699605\n",
      "  -0.2442274  -0.20664785 -0.23573079  0.59067133 -0.24275493  0.39139199\n",
      "   0.43971564  0.60535482  0.14682666 11.27089295  0.60376816 -0.17206529\n",
      "   0.32581058  0.49556933  0.60532276  0.60507536  0.59054254 -0.24324724\n",
      "   0.58619088  0.44373915  0.39557154]]\n",
      "{0: 343, 1: 545}\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.14472462  0.06148539 -0.04467978  0.58706294  0.67119842  0.20511046\n",
      "   0.35967982  0.06133223 -0.01910661  0.55728168  0.6243607  -0.04699605\n",
      "  -0.2442274  -0.20664785 -0.23573079  0.59067133 -0.24275493  0.39139199\n",
      "   0.43971564  0.60535482  0.14682666 11.27089295  0.60376816 -0.17206529\n",
      "   0.32581058  0.49556933  0.60532276  0.60507536  0.59054254 -0.24324724\n",
      "   0.58619088  0.44373915  0.39557154]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.576\n",
      "Precision            0.539\n",
      "Recall               0.993\n",
      "F1                   0.699\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 251 | TN: 341 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 343, 1: 545}\n",
      "acc 0.7150900900900901\n",
      "(array([0.9941691 , 0.53944954]), array([0.57601351, 0.99324324]), array([0.72941176, 0.69916766]), array([592, 296]))\n",
      "(0.7668093187471581, 0.7846283783783783, 0.7142897111282087, None)\n",
      "[[341 251]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5394495412844037 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5394495412844037, 0.9932432432432432, 0.6991676575505351, None)\n"
     ]
    }
   ],
   "source": [
    "# normalized with penalty 3 \n",
    "\n",
    "train_nl_p3(0.01,5,tf.truncated_normal_initializer(0.2,0.5,12),3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with different params\n",
    "\n",
    "def train_nl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000e80>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000e80>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000e80>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 222251.99457482004\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.19735832 0.89093251 1.08341589 0.90657515 1.01595663 0.95712761\n",
      "  1.0718031  1.10396375 1.21637515 0.97085348 0.89072741 1.13944674\n",
      "  1.07750003 1.15783256 1.12498591 0.94512725 0.94895473 0.83027009\n",
      "  0.9660881  0.73811549 0.95577461 1.00048532 0.89772938 1.24737166\n",
      "  0.94662978 1.08218756 0.97002825 0.97021497 0.96005081 1.08451997\n",
      "  0.72511578 0.7414887  0.87067795]]\n",
      "{0: 497, 1: 391}\n",
      "(0.5626598465473146, 0.7432432432432432, 0.6404657933042213, None)\n",
      "\n",
      "1 loss 212826.0550733279\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.19986342 0.92684934 1.10601022 0.82347062 0.93928867 0.89076637\n",
      "  0.98121448 1.12834962 1.23522837 0.88800828 0.81018832 1.15861665\n",
      "  1.05435969 1.16556198 1.12735123 0.85559547 0.94537935 0.75567467\n",
      "  0.88317277 0.64689519 0.89506406 1.03723377 0.80560614 1.23698994\n",
      "  0.97796818 0.99574693 0.87851571 0.87807301 0.86969867 1.07134332\n",
      "  0.63686953 0.6532503  0.79151595]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "2 loss 208341.68852991555\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.12216561 0.90307919 1.06229464 0.75718361 0.88462406 0.85359606\n",
      "  0.89414176 1.09052303 1.19174484 0.82130057 0.74916439 1.11115252\n",
      "  0.9469431  1.09395703 1.04425275 0.77178695 0.83727233 0.70600894\n",
      "  0.81558615 0.55726965 0.8621418  1.04570765 0.71383022 1.1483844\n",
      "  0.95177768 0.91930221 0.78881497 0.78653747 0.78328761 0.96787395\n",
      "  0.55766455 0.57396648 0.73156854]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "3 loss 205131.39371999548\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.03760964 0.85929508 1.00249919 0.70928398 0.84854295 0.83543432\n",
      "  0.81105673 1.03597741 1.13310701 0.77166386 0.70767827 1.04868021\n",
      "  0.84604031 1.01329413 0.95615575 0.69585316 0.73656994 0.67714538\n",
      "  0.76385563 0.46866481 0.84516941 1.0423398  0.62093558 1.05706592\n",
      "  0.90698064 0.85408755 0.70075811 0.69471109 0.70199891 0.868642\n",
      "  0.49333011 0.50930535 0.68872922]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "4 loss 202398.267004147\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.95343364 0.81297596 0.94149116 0.67107614 0.82024536 0.82344968\n",
      "  0.7314926  0.97978353 1.07312539 0.73116397 0.67555079 0.98518327\n",
      "  0.74704113 0.93282672 0.8690693  0.62677984 0.63799974 0.65640189\n",
      "  0.72024458 0.38284805 0.83311145 1.03645349 0.52764139 0.96662658\n",
      "  0.85969069 0.79605945 0.61487888 0.60353434 0.62579174 0.77113025\n",
      "  0.44161321 0.45694962 0.65350723]]\n",
      "{0: 493, 1: 395}\n",
      "(0.5569620253164557, 0.7432432432432432, 0.6367583212735166, None)\n",
      "\n",
      "5 loss 200006.34934833934\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.87080905 0.76867401 0.88273641 0.63637229 0.79399378 0.81213849\n",
      "  0.65463977 0.92563387 1.0151024  0.69419624 0.64634662 0.9238756\n",
      "  0.64880058 0.85423024 0.78390677 0.56226975 0.54036065 0.63711839\n",
      "  0.67928514 0.30307705 0.82152169 1.03142305 0.43562755 0.87743914\n",
      "  0.81419941 0.7416082  0.5316332  0.51428645 0.5536968  0.67449719\n",
      "  0.39658045 0.41120497 0.6204228 ]]\n",
      "{0: 493, 1: 395}\n",
      "(0.5569620253164557, 0.7432432432432432, 0.6367583212735166, None)\n",
      "\n",
      "6 loss 197924.81181714358\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.79029325 0.72761936 0.82730654 0.60305026 0.76801791 0.79987141\n",
      "  0.58025714 0.87460765 0.96003419 0.65883265 0.61797256 0.86577804\n",
      "  0.55106931 0.77824706 0.70129491 0.50126412 0.44345544 0.61727124\n",
      "  0.63907382 0.23326782 0.809081   1.02829399 0.34666309 0.78982277\n",
      "  0.77171379 0.68933577 0.45157826 0.42810064 0.48534462 0.57863892\n",
      "  0.35525716 0.36932064 0.58770481]]\n",
      "{0: 492, 1: 396}\n",
      "(0.5606060606060606, 0.75, 0.6416184971098265, None)\n",
      "\n",
      "7 loss 196138.18967765084\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.71243402 0.69022101 0.77570798 0.57039136 0.74175727 0.78609865\n",
      "  0.50842407 0.82715596 0.90837336 0.62441922 0.58974907 0.81140328\n",
      "  0.45391788 0.70552238 0.62197376 0.44338522 0.34750283 0.59623675\n",
      "  0.59895385 0.17707439 0.79532617 1.02750849 0.26322902 0.70423621\n",
      "  0.73268521 0.63871963 0.37544158 0.34630485 0.42075576 0.48378558\n",
      "  0.31638542 0.33012814 0.55476788]]\n",
      "{0: 487, 1: 401}\n",
      "(0.5635910224438903, 0.7635135135135135, 0.6484935437589671, None)\n",
      "\n",
      "8 loss 194630.1233630186\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.63785697 0.65662807 0.72825318 0.53808522 0.7149826  0.77054947\n",
      "  0.43938978 0.7835252  0.86039594 0.59067397 0.56139577 0.76109105\n",
      "  0.35757901 0.63672811 0.54683316 0.38847922 0.25307253 0.57377516\n",
      "  0.55863873 0.13492767 0.78001578 1.02936776 0.18939979 0.62129029\n",
      "  0.69733438 0.58950695 0.30417311 0.2706795  0.36007292 0.39042567\n",
      "  0.27932106 0.29300698 0.52136614]]\n",
      "{0: 486, 1: 402}\n",
      "(0.5646766169154229, 0.7668918918918919, 0.6504297994269341, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 loss 193380.9969215407\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.56724157 0.62679623 0.68507462 0.50606059 0.68765004 0.75310817\n",
      "  0.37362748 0.74379033 0.81622303 0.55753435 0.5328595  0.71501386\n",
      "  0.26250493 0.57251679 0.47685315 0.33658955 0.1614721  0.54986543\n",
      "  0.51807673 0.1032013  0.76303524 1.0340914  0.13029139 0.54175207\n",
      "  0.66570719 0.54163415 0.23903025 0.2034868  0.30357179 0.29946105\n",
      "  0.24385042 0.25772627 0.48746065]]\n",
      "{0: 478, 1: 410}\n",
      "(0.5609756097560976, 0.777027027027027, 0.6515580736543909, None)\n",
      "\n",
      "10 loss 192367.70312715552\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.50124602 0.60048716 0.64609239 0.47445537 0.65987473 0.73380082\n",
      "  0.31192263 0.70783536 0.77579543 0.52512738 0.50428134 0.67313851\n",
      "  0.16964534 0.51341579 0.41295428 0.28800699 0.07624489 0.52467811\n",
      "  0.4774393  0.07827164 0.74438737 1.04180889 0.08847536 0.46651983\n",
      "  0.63766336 0.49523326 0.18160002 0.14715073 0.25172364 0.21262278\n",
      "  0.2101646  0.22442884 0.45320326]]\n",
      "{0: 469, 1: 419}\n",
      "(0.5560859188544153, 0.7871621621621622, 0.6517482517482518, None)\n",
      "\n",
      "11 loss 191563.6786683813\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.44028936 0.57718802 0.61091095 0.44372394 0.63201646 0.71288822\n",
      "  0.25561593 0.67526236 0.7387803  0.49386259 0.47609559 0.63511813\n",
      "  0.08132376 0.45959779 0.35565253 0.24348217 0.0077669  0.49867528\n",
      "  0.43724449 0.05992174 0.72426421 1.05247742 0.06266945 0.39643701\n",
      "  0.61278542 0.45076675 0.13399601 0.10392874 0.20543852 0.13317865\n",
      "  0.17905144 0.19380651 0.41903458]]\n",
      "{0: 469, 1: 419}\n",
      "(0.5560859188544153, 0.7871621621621622, 0.6517482517482518, None)\n",
      "\n",
      "12 loss 190937.37802831345\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.38427503  0.55606757  0.5787581   0.41472131  0.60475255  0.69095155\n",
      "   0.20679881  0.64533702  0.70451129  0.46450239  0.44911198  0.60022493\n",
      "   0.00457325  0.41065202  0.30466849  0.20437097 -0.02905698  0.47269329\n",
      "   0.3984603   0.05061968  0.70311078  1.06579564  0.05091148  0.33195278\n",
      "   0.59031971  0.40914501  0.09890356  0.07563602  0.16624716  0.06624673\n",
      "   0.15200531  0.16719583  0.38575538]]\n",
      "{0: 469, 1: 419}\n",
      "(0.5560859188544153, 0.7871621621621622, 0.6517482517482518, None)\n",
      "\n",
      "13 loss 190449.69180167522\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.33281826  0.53633853  0.54884776  0.38824665  0.5787279   0.6685854\n",
      "   0.16740356  0.61732839  0.67230824  0.43773888  0.42408793  0.56770203\n",
      "  -0.04156786  0.36590823  0.25923197  0.17185446 -0.043103    0.44754002\n",
      "   0.36205666  0.05012579  0.68137806  1.08139063  0.0499691   0.27316655\n",
      "   0.56951917  0.37127821  0.07755876  0.06144032  0.13536331  0.0165302\n",
      "   0.1302695   0.14570204  0.35413594]]\n",
      "{0: 449, 1: 439}\n",
      "(0.5603644646924829, 0.831081081081081, 0.6693877551020407, None)\n",
      "\n",
      "14 loss 190061.11191754616\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.28575421  0.51761933  0.5207703   0.36454638  0.55417525  0.64604823\n",
      "   0.13772734  0.5908674   0.64182507  0.41374798  0.40126751  0.53715197\n",
      "  -0.05536051  0.32497394  0.21877467  0.14599839 -0.05084094  0.423559\n",
      "   0.32848078  0.05423492  0.65924925  1.0990666   0.05418686  0.22025056\n",
      "   0.55000554  0.33751599  0.06750857  0.05696271  0.11251414 -0.01585396\n",
      "   0.11384963  0.12932497  0.32449617]]\n",
      "{0: 433, 1: 455}\n",
      "(0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.28575421  0.51761933  0.5207703   0.36454638  0.55417525  0.64604823\n",
      "   0.13772734  0.5908674   0.64182507  0.41374798  0.40126751  0.53715197\n",
      "  -0.05536051  0.32497394  0.21877467  0.14599839 -0.05084094  0.423559\n",
      "   0.32848078  0.05423492  0.65924925  1.0990666   0.05418686  0.22025056\n",
      "   0.55000554  0.33751599  0.06750857  0.05696271  0.11251414 -0.01585396\n",
      "   0.11384963  0.12932497  0.32449617]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.851\n",
      "Neg. class accuracy: 0.657\n",
      "Precision            0.554\n",
      "Recall               0.851\n",
      "F1                   0.671\n",
      "----------------------------------------\n",
      "TP: 252 | FP: 203 | TN: 389 | FN: 44\n",
      "========================================\n",
      "\n",
      "{0: 433, 1: 455}\n",
      "acc 0.7218468468468469\n",
      "(array([0.89838337, 0.55384615]), array([0.65709459, 0.85135135]), array([0.75902439, 0.67110519]), array([592, 296]))\n",
      "(0.7261147628353171, 0.754222972972973, 0.7150647916599007, None)\n",
      "[[389 203]\n",
      " [ 44 252]]\n",
      "prec: tp/(tp+fp) 0.5538461538461539 recall: tp/(tp+fn) 0.8513513513513513\n",
      "(0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None)\n"
     ]
    }
   ],
   "source": [
    "train_nl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000b00>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000b00>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000b00>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 184222.4562227816\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.27082211 -0.01928787 -0.14063245  0.37856253  0.43681819 -0.15844807\n",
      "   0.13280198 -0.01935702 -0.10775934  0.34390113  0.39762823 -0.14286955\n",
      "  -0.39588527 -0.33699178 -0.37821404  0.38378715 -0.39537146  0.11504936\n",
      "   0.21906794  0.39699417 -0.27113816  8.13838832  0.39548336 -0.31328908\n",
      "   0.25503373  0.28019293  0.39734506  0.39700564  0.37866251 -0.39156514\n",
      "   0.38002959  0.21917987  0.13605525]]\n",
      "{0: 253, 1: 635}\n",
      "(0.462992125984252, 0.9932432432432432, 0.6315789473684211, None)\n",
      "\n",
      "1 loss 182926.8350570609\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.32930229 -0.072816   -0.19433839  0.43560275  0.51398692 -0.0365946\n",
      "   0.19811178 -0.0729731  -0.16134781  0.40620045  0.46910414 -0.1965869\n",
      "  -0.45399143 -0.39169289 -0.43424009  0.43969335 -0.45314662  0.20099496\n",
      "   0.27412612  0.45449307 -0.13677994 10.2419757   0.45333961 -0.37154561\n",
      "   0.1955587   0.33629475  0.4545094   0.45440313  0.43567656 -0.4480074\n",
      "   0.43491147  0.28277911  0.20811263]]\n",
      "{0: 290, 1: 598}\n",
      "(0.4916387959866221, 0.9932432432432432, 0.6577181208053691, None)\n",
      "\n",
      "2 loss 182800.53538895075\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.50033750e-01 -9.20612978e-02 -2.13641010e-01  4.56135636e-01\n",
      "   5.39595897e-01  1.96766814e-03  2.20784600e-01 -9.22805491e-02\n",
      "  -1.80658821e-01  4.27914802e-01  4.93027965e-01 -2.15923034e-01\n",
      "  -4.74703809e-01 -4.11355405e-01 -4.54355973e-01  4.59505852e-01\n",
      "  -4.73775237e-01  2.29612258e-01  2.93641733e-01  4.75037473e-01\n",
      "  -9.49920742e-02  1.09749630e+01  4.73964547e-01 -3.92341571e-01\n",
      "   1.74156761e-01  3.56763707e-01  4.74947673e-01  4.74935290e-01\n",
      "   4.56160538e-01 -4.68232180e-01  4.54509106e-01  3.04807646e-01\n",
      "   2.33081953e-01]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "3 loss 182779.13781656895\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35716556 -0.09869963 -0.22030067  0.46322675  0.54827254  0.01475931\n",
      "   0.2285349  -0.09894358 -0.18732523  0.43535596  0.50115104 -0.22259733\n",
      "  -0.48184533 -0.41814551 -0.46130134  0.46632557 -0.48089044  0.2392675\n",
      "   0.30036809  0.48212319 -0.08115096 11.22596956  0.48107634 -0.39950962\n",
      "   0.16677415  0.36384845  0.48199793  0.48202006  0.46323675 -0.47521353\n",
      "   0.46127745  0.31235554  0.24160462]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "4 loss 182774.49158656417\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "5 loss 182773.22108715939\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36042604 -0.10173693 -0.22334809  0.46647291  0.55222009  0.02053171\n",
      "   0.2320707  -0.10199269 -0.19037627  0.43875415  0.50484937 -0.22565194\n",
      "  -0.48511306 -0.42125391 -0.46448062  0.4694444  -0.48414652  0.24365198\n",
      "   0.30344598  0.48536566 -0.07490621 11.34054866  0.48433056 -0.4027889\n",
      "   0.16339637  0.3670936   0.48522431  0.48526262  0.46647665 -0.47840914\n",
      "   0.46437656  0.31580217  0.24548993]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "6 loss 182772.82518048465\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.3607074  -0.10199908 -0.22361113  0.46675314  0.55256019  0.02102764\n",
      "   0.23237559 -0.10225588 -0.19063964  0.43904729  0.50516807 -0.22591562\n",
      "  -0.48539512 -0.42152225 -0.46475509  0.46971356 -0.48442758  0.24402947\n",
      "   0.30371166  0.48564555 -0.07436971 11.3504309   0.48461146 -0.40307194\n",
      "   0.16310482  0.3673738   0.48550281  0.48554253  0.46675636 -0.478685\n",
      "   0.46464413  0.31609947  0.24582487]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "7 loss 182772.69479950634\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36080301 -0.10208818 -0.22370052  0.46684838  0.55267575  0.0211961\n",
      "   0.2324792  -0.10234532 -0.19072914  0.4391469   0.50527636 -0.22600522\n",
      "  -0.48549097 -0.42161345 -0.46484836  0.46980503 -0.4845231   0.24415773\n",
      "   0.30380195  0.48574066 -0.07418748 11.35378896  0.48470692 -0.40316812\n",
      "   0.16300575  0.36746903  0.48559746  0.48563766  0.46685142 -0.47877875\n",
      "   0.46473506  0.3162005   0.24593869]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss 182772.6509773922\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36083549 -0.10211845 -0.22373089  0.46688074  0.55271501  0.02125333\n",
      "   0.2325144  -0.10237572 -0.19075956  0.43918074  0.50531316 -0.22603567\n",
      "  -0.48552354 -0.42164444 -0.46488005  0.46983611 -0.48455556  0.2442013\n",
      "   0.30383263  0.48577298 -0.07412557 11.35492999  0.48473936 -0.40320081\n",
      "   0.16297208  0.36750138  0.48562962  0.48566998  0.46688372 -0.47881061\n",
      "   0.46476596  0.31623483  0.24597736]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "9 loss 182772.63614294853\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36084653 -0.10212873 -0.22374122  0.46689174  0.55272836  0.02127277\n",
      "   0.23252636 -0.10238604 -0.19076989  0.43919224  0.50532566 -0.22604602\n",
      "  -0.48553461 -0.42165497 -0.46489082  0.46984667 -0.48456658  0.24421611\n",
      "   0.30384305  0.48578396 -0.07410453 11.35531768  0.48475038 -0.40321191\n",
      "   0.16296064  0.36751238  0.48564055  0.48568096  0.4668947  -0.47882143\n",
      "   0.46477646  0.31624649  0.2459905 ]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "10 loss 182772.63110895432\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085028 -0.10213223 -0.22374472  0.46689547  0.55273289  0.02127938\n",
      "   0.23253043 -0.10238955 -0.1907734   0.43919615  0.50532991 -0.22604953\n",
      "  -0.48553837 -0.42165854 -0.46489448  0.46985026 -0.48457033  0.24422114\n",
      "   0.30384659  0.48578769 -0.07409739 11.35544941  0.48475412 -0.40321569\n",
      "   0.16295676  0.36751611  0.48564426  0.48568469  0.46689843 -0.47882511\n",
      "   0.46478003  0.31625046  0.24599496]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "11 loss 182772.62939926746\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085156 -0.10213342 -0.22374591  0.46689674  0.55273443  0.02128162\n",
      "   0.23253181 -0.10239074 -0.19077459  0.43919748  0.50533135 -0.22605073\n",
      "  -0.48553965 -0.42165976 -0.46489572  0.46985148 -0.4845716   0.24422285\n",
      "   0.3038478   0.48578896 -0.07409496 11.35549417  0.48475539 -0.40321697\n",
      "   0.16295544  0.36751738  0.48564552  0.48568596  0.4668997  -0.47882636\n",
      "   0.46478124  0.3162518   0.24599648]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "12 loss 182772.62881844505\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085199 -0.10213382 -0.22374632  0.46689717  0.55273495  0.02128239\n",
      "   0.23253228 -0.10239115 -0.190775    0.43919793  0.50533184 -0.22605113\n",
      "  -0.48554008 -0.42166017 -0.46489615  0.4698519  -0.48457204  0.24422343\n",
      "   0.3038482   0.48578939 -0.07409413 11.35550938  0.48475583 -0.4032174\n",
      "   0.16295499  0.36751782  0.48564595  0.48568639  0.46690013 -0.47882679\n",
      "   0.46478165  0.31625226  0.24599699]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "13 loss 182772.62862110534\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085214 -0.10213396 -0.22374646  0.46689732  0.55273513  0.02128265\n",
      "   0.23253244 -0.10239129 -0.19077514  0.43919808  0.50533201 -0.22605127\n",
      "  -0.48554023 -0.42166031 -0.46489629  0.46985204 -0.48457218  0.24422362\n",
      "   0.30384834  0.48578954 -0.07409385 11.35551454  0.48475597 -0.40321755\n",
      "   0.16295483  0.36751796  0.4856461   0.48568654  0.46690027 -0.47882693\n",
      "   0.46478179  0.31625242  0.24599717]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "14 loss 182772.62855405567\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085219 -0.102134   -0.2237465   0.46689737  0.55273519  0.02128273\n",
      "   0.23253249 -0.10239133 -0.19077518  0.43919814  0.50533206 -0.22605132\n",
      "  -0.48554028 -0.42166036 -0.46489634  0.46985208 -0.48457223  0.24422369\n",
      "   0.30384839  0.48578959 -0.07409376 11.3555163   0.48475602 -0.4032176\n",
      "   0.16295478  0.36751801  0.48564615  0.48568659  0.46690032 -0.47882698\n",
      "   0.46478184  0.31625247  0.24599723]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085219 -0.102134   -0.2237465   0.46689737  0.55273519  0.02128273\n",
      "   0.23253249 -0.10239133 -0.19077518  0.43919814  0.50533206 -0.22605132\n",
      "  -0.48554028 -0.42166036 -0.46489634  0.46985208 -0.48457223  0.24422369\n",
      "   0.30384839  0.48578959 -0.07409376 11.3555163   0.48475602 -0.4032176\n",
      "   0.16295478  0.36751801  0.48564615  0.48568659  0.46690032 -0.47882698\n",
      "   0.46478184  0.31625247  0.24599723]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.512\n",
      "Precision            0.504\n",
      "Recall               0.993\n",
      "F1                   0.669\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 289 | TN: 303 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 305, 1: 583}\n",
      "acc 0.6722972972972973\n",
      "(array([0.99344262, 0.50428816]), array([0.51182432, 0.99324324]), array([0.67558528, 0.66894198]), array([592, 296]))\n",
      "(0.7488653938081714, 0.7525337837837838, 0.6722636319015605, None)\n",
      "[[303 289]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5042881646655232 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n"
     ]
    }
   ],
   "source": [
    "train_nl(0.01,15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 33)\n",
      "(888, 2)\n",
      "(8272, 33)\n",
      "(8272, 2)\n"
     ]
    }
   ],
   "source": [
    "#input L_S:train_L_S, K: no of classes\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def get_maj_prior(L_S,K):\n",
    "    maj_prior = []\n",
    "    \n",
    "    print(L_S[:,0,:].shape)\n",
    "    for row in np.nditer(L_S[:,0,:],flags=['external_loop'], order='C'):\n",
    "        p = np.ones(K)/K\n",
    "        unique, counts = np.unique(row, return_counts=True)\n",
    "        unique = [int(x) for x in unique]\n",
    "        rc = dict(zip(unique, counts))\n",
    "        tnz = np.count_nonzero(row)\n",
    "        if -1 in rc:\n",
    "            p[0] = rc[-1]\n",
    "        if 1 in rc:\n",
    "            p[1] = rc[1]\n",
    "        p = softmax(p)\n",
    "        maj_prior.append(p)\n",
    "    return np.array(maj_prior)\n",
    "\n",
    "dev_maj_pl=get_maj_prior(dev_L_S,2)\n",
    "print(dev_maj_pl.shape)\n",
    "\n",
    "\n",
    "train_maj_pl=get_maj_prior(train_L_S,2)\n",
    "print(train_maj_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized loss with majority prior\n",
    "\n",
    "\n",
    "\n",
    "def train_nlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout-logz),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e0ddda0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e0ddda0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e0ddda0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 474906.2210111349\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.21090337 0.9091335  1.10355165 0.90243832 1.0081237  0.94312556\n",
      "  1.07091421 1.12508078 1.23950664 0.96649523 0.88539626 1.16028402\n",
      "  1.0765182  1.17218054 1.13463138 0.94402255 0.94809364 0.82180501\n",
      "  0.96140002 0.73834929 0.93716715 1.03577908 0.89803169 1.25607099\n",
      "  0.96839541 1.07918097 0.96985551 0.9702773  0.95925285 1.08443836\n",
      "  0.72393615 0.74030921 0.86399097]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "1 loss 455130.415053864\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.15415736 0.89930916 1.05950407 0.80839578 0.91379878 0.84897\n",
      "  0.97643772 1.07827194 1.17974936 0.87227725 0.79140275 1.10921272\n",
      "  1.0361324  1.11973129 1.08663692 0.84986456 0.92946895 0.72800393\n",
      "  0.8671955  0.64482544 0.84302779 1.00140161 0.80400172 1.19462518\n",
      "  0.94569095 0.98468535 0.8756287  0.87604938 0.86505402 1.04295548\n",
      "  0.63046326 0.64677849 0.77006054]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "2 loss 442206.0464598437\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.04786187 0.78735601 0.95198471 0.71181537 0.81697258 0.75229033\n",
      "  0.87948252 0.97104168 1.07370455 0.77554326 0.6948658  1.00240375\n",
      "  0.9282105  1.01305199 0.97952691 0.75318277 0.81876778 0.63163981\n",
      "  0.77047319 0.54871715 0.74636227 0.89277402 0.70743244 1.08871446\n",
      "  0.83555249 0.887714   0.77888705 0.77930678 0.76833667 0.93515644\n",
      "  0.53440294 0.55066377 0.67357983]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "3 loss 430960.91597695864\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.9433463  0.68076126 0.84692289 0.61708767 0.72172657 0.65734949\n",
      "  0.78397673 0.8661027  0.96930994 0.68048802 0.60023311 0.89765044\n",
      "  0.82298344 0.90835748 0.87464009 0.65823743 0.71255225 0.53739571\n",
      "  0.67544242 0.45508192 0.65145151 0.78727239 0.61272897 0.98438593\n",
      "  0.72951875 0.79217626 0.68381581 0.68423353 0.67331629 0.82997912\n",
      "  0.44088667 0.45701272 0.57907168]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "4 loss 421531.6071310269\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84041909 0.57695228 0.74374506 0.5257729  0.62901552 0.56543534\n",
      "  0.69063769 0.76297997 0.8664402  0.58826708 0.50919803 0.79461237\n",
      "  0.71973262 0.80534658 0.77154098 0.56631103 0.6088902  0.44759525\n",
      "  0.58328628 0.36757581 0.55961967 0.68390305 0.52148476 0.88154791\n",
      "  0.62592919 0.69876292 0.59155276 0.59196523 0.58118779 0.72675013\n",
      "  0.35389259 0.36944025 0.48841583]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84041909 0.57695228 0.74374506 0.5257729  0.62901552 0.56543534\n",
      "  0.69063769 0.76297997 0.8664402  0.58826708 0.50919803 0.79461237\n",
      "  0.71973262 0.80534658 0.77154098 0.56631103 0.6088902  0.44759525\n",
      "  0.58328628 0.36757581 0.55961967 0.68390305 0.52148476 0.88154791\n",
      "  0.62592919 0.69876292 0.59155276 0.59196523 0.58118779 0.72675013\n",
      "  0.35389259 0.36944025 0.48841583]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.743\n",
      "Neg. class accuracy: 0.706\n",
      "Precision            0.558\n",
      "Recall               0.743\n",
      "F1                   0.638\n",
      "----------------------------------------\n",
      "TP: 220 | FP: 174 | TN: 418 | FN: 76\n",
      "========================================\n",
      "\n",
      "{0: 494, 1: 394}\n",
      "acc 0.7184684684684685\n",
      "(array([0.84615385, 0.55837563]), array([0.70608108, 0.74324324]), array([0.76979742, 0.63768116]), array([592, 296]))\n",
      "(0.7022647403358063, 0.7246621621621621, 0.7037392905757066, None)\n",
      "[[418 174]\n",
      " [ 76 220]]\n",
      "prec: tp/(tp+fp) 0.5583756345177665 recall: tp/(tp+fn) 0.7432432432432432\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n"
     ]
    }
   ],
   "source": [
    "train_nlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized loss with majority bias un normalized\n",
    "\n",
    "\n",
    "\n",
    "def train_unlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7a29e8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7a29e8>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7a29e8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "1 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "2 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "3 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "4 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.581\n",
      "Neg. class accuracy: 0.799\n",
      "Precision            0.591\n",
      "Recall               0.581\n",
      "F1                   0.586\n",
      "----------------------------------------\n",
      "TP: 172 | FP: 119 | TN: 473 | FN: 124\n",
      "========================================\n",
      "\n",
      "{0: 597, 1: 291}\n",
      "acc 0.7263513513513513\n",
      "(array([0.79229481, 0.59106529]), array([0.79898649, 0.58108108]), array([0.79562658, 0.58603066]), array([592, 296]))\n",
      "(0.6916800497332021, 0.6900337837837838, 0.6908286206753274, None)\n",
      "[[473 119]\n",
      " [124 172]]\n",
      "prec: tp/(tp+fp) 0.5910652920962199 recall: tp/(tp+fn) 0.581081081081081\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n"
     ]
    }
   ],
   "source": [
    "train_unlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized loss with prior from other LFs\n",
    "\n",
    "def train_nlp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        \n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        \n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "               \n",
    "        \n",
    "        pout = tf.map_fn(lambda li: tf.map_fn(lambda lij:li*lij,li ),l)\n",
    "#         print(\"nls\",nls_)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        \n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        \n",
    "\n",
    "        sumy = t_pout-logz\n",
    "        print(\"sumy\",sumy)\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(t_pout-logz,axis=1) ))\n",
    "\n",
    "        \n",
    "        def index_along_every_row(array, index):\n",
    "            N, _ = array.shape\n",
    "            return array[np.arange(N), index]\n",
    "\n",
    "        #Best LF\n",
    "        blf = tf.argmax(t_pout,axis=1)\n",
    "        print(\"blf\",blf)\n",
    "        print(\"normloss\",normloss)\n",
    "        \n",
    "        \n",
    "        marginals = tf.py_func(index_along_every_row, [tf.squeeze(t_pout), tf.squeeze(blf)], [tf.float64])[0]\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict1 = tf.gather(k,tf.squeeze(blf))\n",
    "        \n",
    "        predict = tf.where(tf.equal(predict1,1),tf.ones_like(predict1),tf.zeros_like(predict1))\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "                print(\"blfs\")\n",
    "                unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "            \n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "            print(\"blfs\")\n",
    "            unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "        \n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d4e95c0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d4e95c0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d4e95c0>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"sub:0\", shape=(?, 33, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 7786500.991153705\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.21092702 0.9079748  1.10199065 0.90247263 1.00824309 0.94324036\n",
      "  1.07090337 1.1235188  1.2375242  0.96658725 0.88550743 1.15869341\n",
      "  1.07658607 1.17200763 1.13447974 0.9440065  0.94815552 0.8219294\n",
      "  0.96139843 0.73833518 0.93738841 1.03379794 0.89801413 1.25616534\n",
      "  0.96708367 1.07922646 0.96984457 0.97027363 0.95925583 1.08451359\n",
      "  0.72392792 0.74032389 0.86403909]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "1 loss 7461453.1901169885\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.15369472 0.917885   1.07998007 0.80938249 0.91498167 0.85013375\n",
      "  0.97712107 1.09966941 1.20062707 0.87336857 0.7925665  1.12799642\n",
      "  1.03241013 1.1217056  1.08774363 0.85058571 0.92173995 0.72918238\n",
      "  0.86798609 0.6456306  0.84454906 1.0282505  0.80472107 1.1939616\n",
      "  0.96544151 0.98550882 0.8763339  0.87675713 0.86579571 1.03948456\n",
      "  0.63131025 0.64768603 0.77103463]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "2 loss 7258744.6239661025\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.04734456 0.82799416 0.9825643  0.71448193 0.81996089 0.75523862\n",
      "  0.88128207 1.00215457 1.10190608 0.77835699 0.69789624 1.02905092\n",
      "  0.92453976 1.01586514 0.98159338 0.75513949 0.81201328 0.63469438\n",
      "  0.77258213 0.55102196 0.75024745 0.93770523 0.7094074  1.08778988\n",
      "  0.87193258 0.88978986 0.78077964 0.78119608 0.77031188 0.93171223\n",
      "  0.53683755 0.55323295 0.67620465]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "3 loss 7084913.234376455\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.94370674 0.72897264 0.88224226 0.62234041 0.72748445 0.66309835\n",
      "  0.78697883 0.90185113 1.00143958 0.68592025 0.60634431 0.92830805\n",
      "  0.8203522  0.91242789 0.87803017 0.66168168 0.7072573  0.5436828\n",
      "  0.67922642 0.45968365 0.65952025 0.83962502 0.61625132 0.98421728\n",
      "  0.77225974 0.79574628 0.68707945 0.68747952 0.67677096 0.82755862\n",
      "  0.44589415 0.46234421 0.58425375]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "4 loss 6940041.820261128\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "acc 0.5990990990990991\n",
      "(array([0.80412371, 0.44      ]), array([0.52702703, 0.74324324]), array([0.63673469, 0.55276382]), array([592, 296]))\n",
      "(0.6220618556701031, 0.6351351351351351, 0.5947492564865142, None)\n",
      "[[312 280]\n",
      " [ 76 220]]\n",
      "prec: tp/(tp+fp) 0.44 recall: tp/(tp+fn) 0.7432432432432432\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n"
     ]
    }
   ],
   "source": [
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d652208>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d652208>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d652208>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"sub:0\", shape=(?, 33, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 7786500.991153705\n",
      "[[1.21092702 0.9079748  1.10199065 0.90247263 1.00824309 0.94324036\n",
      "  1.07090337 1.1235188  1.2375242  0.96658725 0.88550743 1.15869341\n",
      "  1.07658607 1.17200763 1.13447974 0.9440065  0.94815552 0.8219294\n",
      "  0.96139843 0.73833518 0.93738841 1.03379794 0.89801413 1.25616534\n",
      "  0.96708367 1.07922646 0.96984457 0.97027363 0.95925583 1.08451359\n",
      "  0.72392792 0.74032389 0.86403909]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "1 loss 7461453.1901169885\n",
      "[[1.15369472 0.917885   1.07998007 0.80938249 0.91498167 0.85013375\n",
      "  0.97712107 1.09966941 1.20062707 0.87336857 0.7925665  1.12799642\n",
      "  1.03241013 1.1217056  1.08774363 0.85058571 0.92173995 0.72918238\n",
      "  0.86798609 0.6456306  0.84454906 1.0282505  0.80472107 1.1939616\n",
      "  0.96544151 0.98550882 0.8763339  0.87675713 0.86579571 1.03948456\n",
      "  0.63131025 0.64768603 0.77103463]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "2 loss 7258744.6239661025\n",
      "[[1.04734456 0.82799416 0.9825643  0.71448193 0.81996089 0.75523862\n",
      "  0.88128207 1.00215457 1.10190608 0.77835699 0.69789624 1.02905092\n",
      "  0.92453976 1.01586514 0.98159338 0.75513949 0.81201328 0.63469438\n",
      "  0.77258213 0.55102196 0.75024745 0.93770523 0.7094074  1.08778988\n",
      "  0.87193258 0.88978986 0.78077964 0.78119608 0.77031188 0.93171223\n",
      "  0.53683755 0.55323295 0.67620465]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "3 loss 7084913.234376455\n",
      "[[0.94370674 0.72897264 0.88224226 0.62234041 0.72748445 0.66309835\n",
      "  0.78697883 0.90185113 1.00143958 0.68592025 0.60634431 0.92830805\n",
      "  0.8203522  0.91242789 0.87803017 0.66168168 0.7072573  0.5436828\n",
      "  0.67922642 0.45968365 0.65952025 0.83962502 0.61625132 0.98421728\n",
      "  0.77225974 0.79574628 0.68707945 0.68747952 0.67677096 0.82755862\n",
      "  0.44589415 0.46234421 0.58425375]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "4 loss 6940041.820261128\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "acc 0.5990990990990991\n",
      "(array([0.80412371, 0.44      ]), array([0.52702703, 0.74324324]), array([0.63673469, 0.55276382]), array([592, 296]))\n",
      "(0.6220618556701031, 0.6351351351351351, 0.5947492564865142, None)\n",
      "[[312 280]\n",
      " [ 76 220]]\n",
      "prec: tp/(tp+fp) 0.44 recall: tp/(tp+fn) 0.7432432432432432\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n"
     ]
    }
   ],
   "source": [
    "# print blf\n",
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf0b8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf0b8>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf0b8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -15869.846624424299\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.13216794 0.84431419 1.03577597 1.02109161 1.13526783 1.08015688\n",
      "  1.18145719 1.06028046 1.1751276  1.08588497 1.0068098  1.09248317\n",
      "  0.98336898 1.09600627 1.05379678 1.05549242 0.85541465 0.95083779\n",
      "  1.08461451 0.83868296 1.07003751 0.96766079 0.9986     1.17431885\n",
      "  0.8993755  1.19734509 1.07500128 1.07051312 1.06684234 0.99202436\n",
      "  0.83212316 0.84560774 0.98149621]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "1 loss -16214.65212550975\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.14667932 0.87444394 1.0621771  1.04219316 1.16423881 1.11874316\n",
      "  1.19391738 1.0893804  1.2040518  1.10710219 1.02995553 1.11852526\n",
      "  0.98455667 1.11352671 1.06690143 1.06898106 0.85754985 0.9809955\n",
      "  1.11006471 0.84087479 1.10447784 0.99378945 1.00097057 1.1853921\n",
      "  0.92531294 1.21670697 1.08174958 1.07226547 1.07621178 0.99367552\n",
      "  0.8422764  0.85277171 1.00083577]]\n",
      "{0: 598, 1: 290}\n",
      "(0.5896551724137931, 0.5777027027027027, 0.5836177474402731, None)\n",
      "\n",
      "2 loss -16561.204610821405\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.16114906 0.90458204 1.08857823 1.06329845 1.19321574 1.15745091\n",
      "  1.20635982 1.11848117 1.23297601 1.12831942 1.05310373 1.14456737\n",
      "  0.98574439 1.13101501 1.08000609 1.08238921 0.85968747 1.01118612\n",
      "  1.13551144 0.8430677  1.13909544 1.0198811  1.00330216 1.19635177\n",
      "  0.95123975 1.23606015 1.08848559 1.0739708  1.08551093 0.99524307\n",
      "  0.85239606 0.85990408 1.02015758]]\n",
      "{0: 598, 1: 290}\n",
      "(0.5896551724137931, 0.5777027027027027, 0.5836177474402731, None)\n",
      "\n",
      "3 loss -16909.370373253547\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.17557735 0.93472783 1.11497933 1.08440726 1.22219832 1.19627504\n",
      "  1.21878436 1.14758265 1.2619002  1.14953663 1.07625426 1.17060947\n",
      "  0.98693211 1.14847091 1.09311076 1.09571581 0.86182743 1.04140786\n",
      "  1.16095438 0.84526168 1.17388574 1.04593168 1.00559409 1.20719777\n",
      "  0.97715462 1.25540463 1.09520911 1.07562957 1.09473977 0.9967292\n",
      "  0.86248187 0.86700551 1.03946073]]\n",
      "{0: 589, 1: 299}\n",
      "(0.5785953177257525, 0.5844594594594594, 0.5815126050420167, None)\n",
      "\n",
      "4 loss -17259.067448019156\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.18996435 0.96488071 1.14138042 1.10551945 1.25118627 1.23521082\n",
      "  1.23119086 1.17668474 1.29082439 1.17075384 1.09940701 1.19665158\n",
      "  0.98811983 1.16589421 1.10621544 1.10895989 0.86396965 1.07165904\n",
      "  1.18639337 0.84745674 1.20884454 1.07193723 1.00784581 1.21793007\n",
      "  1.00305638 1.27474035 1.10191991 1.07724231 1.10389827 0.99813616\n",
      "  0.8725336  0.87407662 1.05874442]]\n",
      "{0: 591, 1: 297}\n",
      "(0.5791245791245792, 0.581081081081081, 0.5801011804384485, None)\n",
      "\n",
      "5 loss -17610.21866257076\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.20431021 0.9950401  1.16778148 1.12663489 1.28017934 1.27425383\n",
      "  1.24357918 1.20578736 1.31974857 1.19197105 1.12256186 1.22269368\n",
      "  0.98930754 1.18328472 1.11932011 1.1221205  0.86611403 1.10193822\n",
      "  1.21182829 0.84965286 1.24396798 1.09789382 1.0100569  1.22854873\n",
      "  1.02894394 1.29406732 1.10861781 1.07880957 1.11298635 0.99946623\n",
      "  0.88255107 0.88111805 1.07800789]]\n",
      "{0: 612, 1: 276}\n",
      "(0.5978260869565217, 0.5574324324324325, 0.5769230769230769, None)\n",
      "\n",
      "6 loss -17962.751295727656\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.21861509 1.02520549 1.19418253 1.14775349 1.30917731 1.31339993\n",
      "  1.25594923 1.23489043 1.34867274 1.21318824 1.1457187  1.24873578\n",
      "  0.99049525 1.20064233 1.13242479 1.13519673 0.86826051 1.13224413\n",
      "  1.23725909 0.85185005 1.27925249 1.12379757 1.0122271  1.23905389\n",
      "  1.05481632 1.31338553 1.11530263 1.08033194 1.12200393 1.00072173\n",
      "  0.89253418 0.88813041 1.09725047]]\n",
      "{0: 607, 1: 281}\n",
      "(0.594306049822064, 0.5641891891891891, 0.5788561525129982, None)\n",
      "\n",
      "7 loss -18316.596780126394\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.23287916 1.05537641 1.22058357 1.16887514 1.33817997 1.35264522\n",
      "  1.26830087 1.26399391 1.37759691 1.23440544 1.16887744 1.27477788\n",
      "  0.99168296 1.21796692 1.14552947 1.14818773 0.87040901 1.16257567\n",
      "  1.26268579 0.85404829 1.31469477 1.14964466 1.01435626 1.24944582\n",
      "  1.08067259 1.332695   1.12197421 1.08181008 1.13095094 1.00190502\n",
      "  0.90248285 0.89511431 1.11647156]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "8 loss -18671.690429301383\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.24710257 1.08555241 1.2469846  1.18999973 1.36718713 1.39198601\n",
      "  1.28063403 1.29309773 1.40652107 1.25562263 1.19203799 1.30081998\n",
      "  0.99287066 1.23525843 1.15863416 1.16109268 0.87255945 1.19293191\n",
      "  1.28810845 0.85624759 1.35029177 1.17543128 1.01644438 1.25972491\n",
      "  1.10651191 1.35199578 1.1286324  1.08324468 1.1398273  1.00301849\n",
      "  0.91239704 0.90207034 1.13567062]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 loss -19027.97118752855\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.26128554 1.11573311 1.27338561 1.21112719 1.39619863 1.43141881\n",
      "  1.29294861 1.32220184 1.43544524 1.27683982 1.21520026 1.32686209\n",
      "  0.99405836 1.25251682 1.17173884 1.17391081 0.87471176 1.22331206\n",
      "  1.31352721 0.85844793 1.38604064 1.20115365 1.01849161 1.26989168\n",
      "  1.13233348 1.37128792 1.13527709 1.08463649 1.14863295 1.00406452\n",
      "  0.92227679 0.90899909 1.15484714]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "10 loss -19385.38140054044\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.27542826 1.14591812 1.29978662 1.23225741 1.42521429 1.47094027\n",
      "  1.30524454 1.35130623 1.46436939 1.29805701 1.23836417 1.35290419\n",
      "  0.99524606 1.26974212 1.18484352 1.18664141 0.87686586 1.25371543\n",
      "  1.33894224 0.86064932 1.42193873 1.22680803 1.02049819 1.27994681\n",
      "  1.15813657 1.39057148 1.14190818 1.0859863  1.15736784 1.00504551\n",
      "  0.93212216 0.91590114 1.17400072]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "11 loss -19743.8666052283\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.28953098 1.17610711 1.32618763 1.25339032 1.45423398 1.51054723\n",
      "  1.31752175 1.38041084 1.49329355 1.31927419 1.26152964 1.37894629\n",
      "  0.99643375 1.28693435 1.19794821 1.19928385 0.87902169 1.28414148\n",
      "  1.36435376 0.86285174 1.45798354 1.25239072 1.02246452 1.28989117\n",
      "  1.18392052 1.40984657 1.14852557 1.08729495 1.16603199 1.00596385\n",
      "  0.94193327 0.92277706 1.19313096]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "12 loss -20103.375336483656\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.30359398 1.20629975 1.35258863 1.27452584 1.48325755 1.55023664\n",
      "  1.32978017 1.40951565 1.5222177  1.34049137 1.28469661 1.40498839\n",
      "  0.99762144 1.30409359 1.21105289 1.21183753 0.88117919 1.31458975\n",
      "  1.38976203 0.86505519 1.49417274 1.27789802 1.02439107 1.29972576\n",
      "  1.20968467 1.42911325 1.15512922 1.08856334 1.17462541 1.00682191\n",
      "  0.95171027 0.92962742 1.21223755]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "13 loss -20463.85894941081\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.31761754 1.23649576 1.37898962 1.29566389 1.51228487 1.59000558\n",
      "  1.34201976 1.43862065 1.55114186 1.36170856 1.307865   1.43103049\n",
      "  0.99880914 1.32121995 1.22415757 1.22430193 0.88333827 1.34505988\n",
      "  1.41516736 0.86725966 1.53050411 1.30332629 1.02627845 1.30945179\n",
      "  1.23542845 1.44837165 1.16171908 1.0897924  1.18314818 1.00762204\n",
      "  0.96145335 0.93645277 1.2313202 ]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "14 loss -20825.27145525223\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.331602   1.26669486 1.40539061 1.31680439 1.54131584 1.62985126\n",
      "  1.35424048 1.46772579 1.58006601 1.38292574 1.33103475 1.4570726\n",
      "  0.99999682 1.33831358 1.23726225 1.23667661 0.8854989  1.37555159\n",
      "  1.4405701  0.86946515 1.56697556 1.32867193 1.02812733 1.31907064\n",
      "  1.26115131 1.46762187 1.16829513 1.0909831  1.19160042 1.00836655\n",
      "  0.97116276 0.94325368 1.2503787 ]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.331602   1.26669486 1.40539061 1.31680439 1.54131584 1.62985126\n",
      "  1.35424048 1.46772579 1.58006601 1.38292574 1.33103475 1.4570726\n",
      "  0.99999682 1.33831358 1.23726225 1.23667661 0.8854989  1.37555159\n",
      "  1.4405701  0.86946515 1.56697556 1.32867193 1.02812733 1.31907064\n",
      "  1.26115131 1.46762187 1.16829513 1.0909831  1.19160042 1.00836655\n",
      "  0.97116276 0.94325368 1.2503787 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.574\n",
      "Neg. class accuracy: 0.806\n",
      "Precision            0.596\n",
      "Recall               0.574\n",
      "F1                   0.585\n",
      "----------------------------------------\n",
      "TP: 170 | FP: 115 | TN: 477 | FN: 126\n",
      "========================================\n",
      "\n",
      "{0: 603, 1: 285}\n",
      "acc 0.7286036036036037\n",
      "(array([0.79104478, 0.59649123]), array([0.80574324, 0.57432432]), array([0.79832636, 0.58519793]), array([592, 296]))\n",
      "(0.6937680020947892, 0.6900337837837838, 0.6917621472140805, None)\n",
      "[[477 115]\n",
      " [126 170]]\n",
      "prec: tp/(tp+fp) 0.5964912280701754 recall: tp/(tp+fn) 0.5743243243243243\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n"
     ]
    }
   ],
   "source": [
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf1d0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf1d0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf1d0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -227408.91347570377\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 14.67744856  25.90583748  22.69587475  12.78045065  14.39716461\n",
      "    0.44035997  -3.2445657   25.08846134  24.96396903  13.96286426\n",
      "   11.82652493  22.46522151   1.79888645  16.7243985   11.77938823\n",
      "   -3.43412061   2.63100655   1.29551022  -6.77434535  -0.59566388\n",
      "  -10.15505727  61.98589432   1.58045045  12.01429343  28.43790004\n",
      "    3.19948727   1.48865339  -1.91094183  -4.65161603   5.80171757\n",
      "   -2.764758     1.47742811 -15.12888705]]\n",
      "{0: 186, 1: 702}\n",
      "(0.42165242165242167, 1.0, 0.5931863727454909, None)\n",
      "\n",
      "1 loss -795568.3743622616\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 29.1382024   50.98953277  44.5347361   11.90304728   7.97769311\n",
      "  -33.36295911 -13.78367756  49.16531977  48.89000862  10.12194851\n",
      "    5.60645624  44.00724617   2.78127445  32.43543394  22.61954728\n",
      "  -17.15336115   4.44616891 -22.67077969 -30.0618316   -3.05561389\n",
      "  -49.7209123  126.61153624   0.64822066  23.66716585  56.89541963\n",
      "  -11.66315922  -4.53226146  -6.20451658 -14.59742147  10.94639346\n",
      "  -14.72857165  -6.39233907 -41.37367621]]\n",
      "{0: 111, 1: 777}\n",
      "(0.38095238095238093, 1.0, 0.5517241379310345, None)\n",
      "\n",
      "2 loss -1497595.2865301068\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 43.82313598  76.0732257   66.37361624  -2.83511525 -13.74309888\n",
      "  -69.6174265  -24.53515019  73.24217802  72.81605934  -7.43529202\n",
      "  -12.32494795  65.54928463   3.76369901  48.14644276  33.45971014\n",
      "  -30.81955012   6.26133908 -49.74514454 -53.31057794  -5.5021359\n",
      "  -94.71278943 191.23319492  -0.91417375  35.31874093  85.40157055\n",
      "  -28.300968   -11.01874319 -10.49390156 -25.39489506  16.0910497\n",
      "  -26.65253492 -16.80352722 -68.29906796]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "3 loss -2229361.6521006986\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[  58.50780431  101.15691863   88.21249639  -20.37177447  -37.87423146\n",
      "  -105.87164134  -35.28655981   97.31903626   96.74211007  -24.98614902\n",
      "   -31.50792165   87.0913231     4.74612358   63.85745158   44.29987301\n",
      "   -44.48572589    8.07650926  -76.81881532  -76.55931453   -7.94837273\n",
      "  -139.70217543  255.85485259   -3.48628196   46.97031568  113.90771963\n",
      "   -44.93630527  -17.50395884  -14.7832795   -36.19236384   21.23570594\n",
      "   -38.57648808  -27.46786758  -95.22415065]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "4 loss -2962754.465933687\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[  73.19247258  126.24061155  110.05137654  -37.90572763  -62.00373818\n",
      "  -142.12585613  -46.03796941  121.3958945   120.66816079  -42.53700441\n",
      "   -50.6908199   108.63336157    5.72854814   79.5684604    55.14003587\n",
      "   -58.15190167    9.89167944 -103.89248593  -99.80805111  -10.394607\n",
      "  -184.6915608   320.47651026   -6.0035901    58.62189042  142.4138687\n",
      "   -61.5716419   -23.98917416  -19.07265745  -46.98983261   26.38036218\n",
      "   -50.50044124  -38.13220779 -122.14923326]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "5 loss -3696146.969814301\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[  87.87714084  151.32430448  131.89025669  -55.4396801   -86.13324448\n",
      "  -178.38007091  -56.789379    145.47275274  144.59421152  -60.08785979\n",
      "   -69.87371813  130.17540004    6.71097271   95.27946922   65.98019873\n",
      "   -71.81807744   11.70684961 -130.96615653 -123.05678769  -12.84084125\n",
      "  -229.68094617  385.09816792   -8.52040318   70.27346517  170.92001777\n",
      "   -78.20697853  -30.47438948  -23.36203539  -57.78730139   31.52501842\n",
      "   -62.4243944   -48.796548   -149.07431587]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "6 loss -4429539.47297085\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 102.56180911  176.40799741  153.72913684  -72.97363256 -110.26275078\n",
      "  -214.6342857   -67.5407886   169.54961099  168.52026224  -77.63871517\n",
      "   -89.05661637  151.71743851    7.69339727  110.99047804   76.82036159\n",
      "   -85.48425321   13.52201979 -158.03982714 -146.30552427  -15.2870755\n",
      "  -274.67033154  449.71982559  -11.03721297   81.92503992  199.42616684\n",
      "   -94.84231517  -36.95960479  -27.65141333  -68.58477017   36.66967466\n",
      "   -74.34834756  -59.46088821 -175.99939849]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "7 loss -5162931.976122954\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 117.24647737  201.49169034  175.56801699  -90.50758503 -134.39225709\n",
      "  -250.88850048  -78.2921982   193.62646923  192.44631297  -95.18957056\n",
      "  -108.2395146   173.25947698    8.67582184  126.70148685   87.66052445\n",
      "   -99.15042899   15.33718997 -185.11349774 -169.55426085  -17.73330975\n",
      "  -319.65971691  514.34148325  -13.55402274   93.57661466  227.93231591\n",
      "  -111.4776518   -43.44482011  -31.94079128  -79.38223894   41.8143309\n",
      "   -86.27230072  -70.12522842 -202.9244811 ]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss -5896324.47927504\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 131.93114564  226.57538327  197.40689714 -108.0415375  -158.52176339\n",
      "  -287.14271527  -89.0436078   217.70332747  216.37236369 -112.74042594\n",
      "  -127.42241284  194.80151545    9.6582464   142.41249567   98.50068732\n",
      "  -112.81660476   17.15236014 -212.18716835 -192.80299743  -20.179544\n",
      "  -364.64910227  578.96314092  -16.07083251  105.22818941  256.43846498\n",
      "  -128.11298843  -49.93003543  -36.23016922  -90.17970772   46.95898715\n",
      "   -98.19625388  -80.78956863 -229.84956371]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "9 loss -6629716.9824271165\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 146.6158139   251.6590762   219.24577729 -125.57548997 -182.65126969\n",
      "  -323.39693005  -99.7950174   241.78018572  240.29841441 -130.29128132\n",
      "  -146.60531107  216.34355392   10.64067097  158.12350449  109.34085018\n",
      "  -126.48278053   18.96753032 -239.26083895 -216.05173401  -22.62577825\n",
      "  -409.63848764  643.58479858  -18.58764228  116.87976416  284.94461405\n",
      "  -144.74832507  -56.41525074  -40.51954717 -100.9771765    52.10364339\n",
      "  -110.12020704  -91.45390884 -256.77464632]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "10 loss -7363109.485579201\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 161.30048217  276.74276913  241.08465744 -143.10944244 -206.780776\n",
      "  -359.65114484 -110.54642699  265.85704396  264.22446514 -147.8421367\n",
      "  -165.78820931  237.88559238   11.62309553  173.83451331  120.18101304\n",
      "  -140.1489563    20.7827005  -266.33450956 -239.3004706   -25.0720125\n",
      "  -454.62787301  708.20645625  -21.10445205  128.5313389   313.45076312\n",
      "  -161.3836617   -62.90046606  -44.80892511 -111.77464527   57.24829963\n",
      "  -122.0441602  -102.11824905 -283.69972894]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "11 loss -8096501.988731309\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 175.98515043  301.82646205  262.92353759 -160.64339491 -230.9102823\n",
      "  -395.90535962 -121.29783659  289.9339022   288.15051586 -165.39299209\n",
      "  -184.97110754  259.42763085   12.6055201   189.54552213  131.0211759\n",
      "  -153.81513208   22.59787068 -293.40818016 -262.54920718  -27.51824675\n",
      "  -499.61725838  772.82811392  -23.62126182  140.18291365  341.95691219\n",
      "  -178.01899834  -69.38568138  -49.09830305 -122.57211405   62.39295587\n",
      "  -133.96811336 -112.78258926 -310.62481155]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "12 loss -8829894.49188337\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 190.6698187   326.91015498  284.76241774 -178.17734738 -255.0397886\n",
      "  -432.1595744  -132.04924619  314.01076044  312.07656659 -182.94384747\n",
      "  -204.15400578  280.96966932   13.58794466  205.25653095  141.86133876\n",
      "  -167.48130785   24.41304085 -320.48185076 -285.79794376  -29.964481\n",
      "  -544.60664375  837.44977158  -26.13807159  151.8344884   370.46306126\n",
      "  -194.65433497  -75.87089669  -53.387681   -133.36958282   67.53761211\n",
      "  -145.89206652 -123.44692947 -337.54989416]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "13 loss -9563286.99503545\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 205.35448696  351.99384791  306.60129788 -195.71129985 -279.16929491\n",
      "  -468.41378919 -142.80065579  338.08761869  336.00261731 -200.49470285\n",
      "  -223.33690401  302.51170779   14.57036923  220.96753976  152.70150163\n",
      "  -181.14748362   26.22821103 -347.55552137 -309.04668034  -32.41071525\n",
      "  -589.59602912  902.07142925  -28.65488136  163.48606314  398.96921033\n",
      "  -211.2896716   -82.35611201  -57.67705894 -144.1670516    72.68226835\n",
      "  -157.81601968 -134.11126968 -364.47497677]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "14 loss -10296679.498187572\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 220.03915523  377.07754084  328.44017803 -213.24525232 -303.29880121\n",
      "  -504.66800397 -153.55206539  362.16447693  359.92866804 -218.04555823\n",
      "  -242.51980225  324.05374626   15.55279379  236.67854858  163.54166449\n",
      "  -194.8136594    28.04338121 -374.62919197 -332.29541692  -34.8569495\n",
      "  -634.58541449  966.69308691  -31.17169113  175.13763789  427.4753594\n",
      "  -227.92500824  -88.84132733  -61.96643688 -154.96452038   77.82692459\n",
      "  -169.73997284 -144.77560989 -391.40005938]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 220.03915523  377.07754084  328.44017803 -213.24525232 -303.29880121\n",
      "  -504.66800397 -153.55206539  362.16447693  359.92866804 -218.04555823\n",
      "  -242.51980225  324.05374626   15.55279379  236.67854858  163.54166449\n",
      "  -194.8136594    28.04338121 -374.62919197 -332.29541692  -34.8569495\n",
      "  -634.58541449  966.69308691  -31.17169113  175.13763789  427.4753594\n",
      "  -227.92500824  -88.84132733  -61.96643688 -154.96452038   77.82692459\n",
      "  -169.73997284 -144.77560989 -391.40005938]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 1.0\n",
      "Neg. class accuracy: 0.179\n",
      "Precision            0.379\n",
      "Recall               1.0\n",
      "F1                   0.549\n",
      "----------------------------------------\n",
      "TP: 296 | FP: 486 | TN: 106 | FN: 0\n",
      "========================================\n",
      "\n",
      "{0: 106, 1: 782}\n",
      "acc 0.4527027027027027\n",
      "(array([1.        , 0.37851662]), array([0.17905405, 1.        ]), array([0.30372493, 0.54916512]), array([592, 296]))\n",
      "(0.6892583120204604, 0.589527027027027, 0.42644502448022714, None)\n",
      "[[106 486]\n",
      " [  0 296]]\n",
      "prec: tp/(tp+fp) 0.37851662404092073 recall: tp/(tp+fn) 1.0\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n"
     ]
    }
   ],
   "source": [
    "train_unl(0.01,15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee21978>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee21978>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee21978>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 190172.47853772584\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.2053655  0.30936395 0.28997198 0.2367527  0.28647343 0.32020677\n",
      "  0.19884814 0.30623267 0.30723075 0.2419398  0.25581996 0.28299078\n",
      "  0.18634078 0.21796294 0.20752234 0.19268362 0.18442374 0.26176987\n",
      "  0.22819245 0.18263638 0.30204006 0.36018888 0.18171976 0.19192962\n",
      "  0.30156119 0.22449297 0.18345572 0.18210333 0.18615508 0.18474442\n",
      "  0.19665109 0.19592285 0.21894825]]\n",
      "dev loss 20432.207128112896\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.733\n",
      "Neg. class accuracy: 0.715\n",
      "Precision            0.562\n",
      "Recall               0.733\n",
      "F1                   0.636\n",
      "----------------------------------------\n",
      "TP: 217 | FP: 169 | TN: 423 | FN: 79\n",
      "========================================\n",
      "\n",
      "{0: 502, 1: 386}\n",
      "acc 0.7207207207207207\n",
      "(array([0.84262948, 0.56217617]), array([0.71452703, 0.73310811]), array([0.77330896, 0.63636364]), array([592, 296]))\n",
      "(0.7024028239374109, 0.7238175675675675, 0.7048362971580522, None)\n",
      "[[423 169]\n",
      " [ 79 217]]\n",
      "prec: tp/(tp+fp) 0.5621761658031088 recall: tp/(tp+fn) 0.7331081081081081\n",
      "(0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)\n",
      "un-norma thetas ep15 lr 0.1/len(train)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee20278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee20278>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee20278>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 247125.96744431488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.331602   1.26669486 1.40539061 1.31680439 1.54131584 1.62985126\n",
      "  1.35424048 1.46772579 1.58006601 1.38292574 1.33103475 1.4570726\n",
      "  0.99999682 1.33831358 1.23726225 1.23667661 0.8854989  1.37555159\n",
      "  1.4405701  0.86946515 1.56697556 1.32867193 1.02812733 1.31907064\n",
      "  1.26115131 1.46762187 1.16829513 1.0909831  1.19160042 1.00836655\n",
      "  0.97116276 0.94325368 1.2503787 ]]\n",
      "dev loss 26587.082646325307\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.574\n",
      "Neg. class accuracy: 0.806\n",
      "Precision            0.596\n",
      "Recall               0.574\n",
      "F1                   0.585\n",
      "----------------------------------------\n",
      "TP: 170 | FP: 115 | TN: 477 | FN: 126\n",
      "========================================\n",
      "\n",
      "{0: 603, 1: 285}\n",
      "acc 0.7286036036036037\n",
      "(array([0.79104478, 0.59649123]), array([0.80574324, 0.57432432]), array([0.79832636, 0.58519793]), array([592, 296]))\n",
      "(0.6937680020947892, 0.6900337837837838, 0.6917621472140805, None)\n",
      "[[477 115]\n",
      " [126 170]]\n",
      "prec: tp/(tp+fp) 0.5964912280701754 recall: tp/(tp+fn) 0.5743243243243243\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "[(190172.47853772584, 20432.207128112896, (0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)), (247125.96744431488, 26587.082646325307, (0.5964912280701754, 0.5743243243243243, 0.585197934595525, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value Normalized\n",
    "\n",
    "def getNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getNLObjValue(np.array([[0.2053655 , 0.30936395, 0.28997198, 0.2367527 , 0.28647343,\n",
    "       0.32020677, 0.19884814, 0.30623267, 0.30723075, 0.2419398 ,\n",
    "       0.25581996, 0.28299078, 0.18634078, 0.21796294, 0.20752234,\n",
    "       0.19268362, 0.18442374, 0.26176987, 0.22819245, 0.18263638,\n",
    "       0.30204006, 0.36018888, 0.18171976, 0.19192962, 0.30156119,\n",
    "       0.22449297, 0.18345572, 0.18210333, 0.18615508, 0.18474442,\n",
    "       0.19665109, 0.19592285, 0.21894825]])))\n",
    " \n",
    "            \n",
    "print(\"un-norma thetas ep15 lr 0.1/len(train)\")\n",
    "\n",
    "l_f1s.append(getNLObjValue(np.array([[1.331602,   1.26669486, 1.40539061, 1.31680439, 1.54131584, 1.62985126,\n",
    "  1.35424048, 1.46772579, 1.58006601, 1.38292574, 1.33103475, 1.4570726,\n",
    "  0.99999682, 1.33831358, 1.23726225, 1.23667661, 0.8854989 , 1.37555159,\n",
    "  1.4405701 , 0.86946515, 1.56697556, 1.32867193, 1.02812733, 1.31907064,\n",
    "  1.26115131, 1.46762187, 1.16829513, 1.0909831 , 1.19160042, 1.00836655,\n",
    "  0.97116276, 0.94325368, 1.2503787 ]])))\n",
    "\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAEWCAYAAAAKI89vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucVXW9//HXmwEBQcALFQIKKqiAXJwRVI6XtADTk5zMW6Zgpln6S7NITE4Smll21MPR9NhRQeWEgoZ4tBDNWxrkICSCoKAQt1K5CMiIXD6/P9Z3xg3OzB4G9jDI+/l4rMes/Vnf9V3f7x6Y/dnf9V1rKSIwMzMzq06Dnd0AMzMzq/+cMJiZmVleThjMzMwsLycMZmZmlpcTBjMzM8vLCYOZmZnl5YTBbBcl6URJi3Nez5J04g4+xihJN+zIOs1s1+SEwXYZkr4hqVTSWknLJP1B0r9IGi5pg6Q1aXlT0u2S2uTse6KkzWnfNZLmSrowz/FGSQpJvXNih0iqlzcviYiuEfFcXR5T0gJJX6rLY5rZzuGEwXYJkq4CbgNuBD4PHAD8Bjg9FXkoIvYC9gH+DfgCMC03aQCWRkRzoAXwA+C3kg7Nc+gVwA75hi2p4Y6ox8xsZ3DCYPWepJbACOCyiHg0Ij6MiA0R8XhEDMktm+KzgLOB94Afbl1fZJ4kSwa65zn8aKC7pBOqaNv+kiZKWiFpnqSLc7YNlzRe0oOSVgODU2xciq2RNFNSZ0nXSHpX0iJJ/XLquFDSG6ns25K+U837VPFtX9KqNJqyVtKHaaSkQ9p2mqQZqczLkrrn1NFL0qvpeA8BTfK8P1WSdHF6T1ak92j/FJekW1N/V6f3oFva9hVJs9Pxl0j6UW2Pb2Y7lhMG2xUcQ/bB9fua7hARm4DHgOO23iapgaSvAvsB8/JUtY5sVOPnVWwfCywG9ge+Dtwo6aSc7acD44FWwJgU+1fgAWBvYDowiez/YluyxOi/c/Z/FziNbFTkQuBWSUfmaTMR0SoimqcRlf8EXgSWSOoF3At8B9g3HWuipMaS9gAmpLbtA4wDzsh3rMqk9+AXwFlAG2Ah2XsF0A84HugMtExllqdt9wDfSaNF3YA/1eb4ZrbjOWGwXcG+wPsRsXEb91tK9sFXbn9Jq4AysuTjqoiYXoN6/hs4QNIpuUFJ7YG+wNUR8VFEzAD+B7ggp9hfImJCRGyOiLIUezEiJqX+jANaAzdFxAayD9UOkloBRMQTETE/jYo8DzxFJUlQVSSdDXwDOCPVfwnw3xExNSI2RcRoYD1wdFoaAbelkZrxwCs1PdZWzgPujYhXI2I9cA1wTBrl2ADsBRwGKCLeiIhlab8NQBdJLSJiZUS8Wsvjm9kO5oTBdgXLgf1qMQegLdlph3JLI6IV2bf1kUDFSICkn+QM4d+VW0n6wLs+Lbn2B1ZExJqc2MJ03HKLKmnXP3PWy8iSoU05rwGap3adImlKGtZfBXyFbGQkrzSacDvwbxHxXgofCPwwnY5Ylepsn/qyP7Aktnwi3cKc+v6Q8x6dl+fw++fuGxFryX6PbSPiT6lddwDvSrpbUotU9IzUx4WSnpd0TE36amaF54TBdgV/IfsWPLCmO0hqQDb0/+LW21ICcDVwhKSBKXZj+RB+RFxaSZX3kZ1W+FpObCmwj6S9cmIHAEtyD1fTNlfSh8bAI8Cvgc+nZOdJQDXY93Nkpxcu22oUZRHw83TKonzZMyJ+BywD2krKrf+Aio5EnJLzHo2hekvJkpPy9jQjGylakuoaGRHFQBeyUxNDUvyViDgdKG//w/n6amZ1wwmD1XsR8QHwU+AOSQMl7SmpUfr2/avcspIaSjoc+B3ZlRK3VFHnx8B/pHpr0oaNwHVkiUZ5bBHwMvALSU3S5MGLgAe3uZOV2wNoTDZ5c2M6JdKv+l0qrsYYDzwYEVt/4P4WuFRSnzT5sJmkU1PS8xdgI/D99P5+DehNfo1S/8uXhmTv/4WSeqbE50ZgakQskHRUOn4j4EPgI2CzpD0knSepZTp9shrYXIPjm1kdcMJgu4SI+A/gKmAY2QfoIuBysm+hAGdLWgt8AEwkG/4ujoil1VR7L9nchH+tYTPKv4XnOhfoQPaN+vfAdRHxdA3rq1Y61fF9sm/ZK8nmIkyswa7tyOY5XJlzCmGtpAMiohS4mOyUwEqySZ+D0/E+JhtBGUx2Kuds4NEaHO9JslMp5cvw9B78O9kIyTLgYOCcVL4FWeKykuy0xXLg5rTtfGCBsqtKLiWbC2Fm9YC2PF1pZmZm9mkeYTAzM7O8nDCYmZlZXgVLGCS1l/RsumvbLElXpPjwdAe3GWn5Ss4+16Q7w82V1D8nPiDF5kkamhPvKGlqij+UbjxDugnNQyk+NV37bWZmZrVUyBGGjcAPI6IL2Q1hLpPUJW27NSJ6puVJgLTtHKArMAD4jaQiSUVk12ufQnYJ1rk59fwy1XUI2QSqi1L8ImBlit+aypmZmVktFexhOOnObcvS+hpJb7DlDW22djowNl0j/46keXxySde8iHgbQNJY4PRU30lkM8chu+f/cODOVNfwFB8P3C5JUc0Mz/322y86dOiwrd00M9utTZs27f2IaL2z22GFVydPz0unBHoBU8lupXu5pAuAUrJRiJVkycSUnN0W80mCsWireB+ym8CsyrldcG75tuX7RMRGSR+k8u9v1a5LyG6VywEHHEBpaen2dtXMbLciaWH+UvZZUPBJj5Kak12LfWVErCYbATgY6Ek2AvEfhW5DVSLi7ogoiYiS1q2dIJuZmVWloAlDupPbI8CYiHgUICL+mR56s5ns5i3lpx2WkN3Tvly7FKsqvhxolfN8gfL4FnWl7S355Gl4u53nnnsOSTz++OMVsdNOO43nnnuuTtvRoUMH3n8/G+Q59thjt7u+UaNGcfnll1e67cYbb6xYX7BgAd26ddumuidMmMDs2bO3q321NWbMGHr27FmxNGjQgBkzZuyUtpiZlSvkVRIie1TtGxFxS068TU6xfwNeT+sTgXPSFQ4dgU7AX8meltcpXRGxB9nEyIlpPsKzZI8UBhhE9jjj8roGpfWvA3+qbv7C7qBdu3b8/OdVPaE5v02bNuUvtA1efvnlHVrf1nIThtrYmQnDeeedx4wZM5gxYwYPPPAAHTt2pGfPnjulLWZm5Qo5wtCX7DavJ211CeWvJM2U9BrwReAHABExi+wWuLOBP5I9NGdTmqNwOTAJeAN4OJWF7L7+V6UJkvuSJSikn/um+FVAxaWYnwVbf2P+9a9/zfDhwznxxBO5+uqr6d27N507d+bFFz957lKPHj1o2bIlkydP/lR9zzzzDL169eKII47gW9/6FuvXrweyEYGrr76aI488knHjxnHiiSfygx/8gJKSEg4//HBeeeUVvva1r9GpUyeGDRtWUd/AgQMpLi6ma9eu3H333ZX2oXnz5gD89Kc/rfgm3bZtWy688EIAHnzwQXr37k3Pnj35zne+U5Gw3HfffXTu3JnevXvz0ksvVVr30KFDKSsro2fPnpx3XnZn4U2bNnHxxRfTtWtX+vXrR1lZ9lDI+fPnM2DAAIqLiznuuOOYM2cOL7/8MhMnTmTIkCH07NmT+fPn89vf/pajjjqKHj16cMYZZ7Bu3ToAxo0bR7du3ejRowfHH398pe2p7BgAgwcP5tJLL6WkpITOnTvzf//3f5/a93e/+x3nnHPOp+JmZnUuIrxEUFxcHLuKd955J7p27Vrx+uabb47rrrsuTjjhhLjqqqsiIuKJJ56Ik08+OSIinn322Tj11FPj+eefj+OPPz4iIk499dR49tlno6ysLNq1axdz586NiIjzzz8/br311oiIOPDAA+OXv/xlxXFOOOGE+PGPfxwREbfddlu0adMmli5dGh999FG0bds23n///YiIWL58eURErFu3Lrp27VoRP/DAA+O9996LiIhmzZpt0aeVK1dGt27dorS0NGbPnh2nnXZafPzxxxER8d3vfjdGjx4dS5cujfbt28e7774b69evj2OPPTYuu+yySt+j3PrfeeedKCoqiunTp0dExJlnnhkPPPBAREScdNJJ8eabb0ZExJQpU+KLX/xiREQMGjQoxo0bV1FHeR8iIq699toYOXJkRER069YtFi9eXNGHylR3jP79+8emTZvizTffjLZt20ZZWdkW+x500EExc+bMSus1qw+A0qgHf8O9FH6pk6skbPtNmL6EmyfNZemqMvaJD1j90cZKy33ta9nTl4uLi1mwYMEW28q/Af/5z3+uiM2dO5eOHTvSuXNnAAYNGsQdd9zBlVdeCcDZZ5+9RR1f/epXATjiiCPo2rUrbdpkZ5gOOuggFi1axL777svIkSP5/e9/D8CiRYt466232HfffavsW0TwzW9+k6uuuori4mJuv/12pk2bxlFHHQVAWVkZn/vc55g6dSonnngi5RNUzz77bN58880871wmd1i//L1Zu3YtL7/8MmeeeWZFufLRla29/vrrDBs2jFWrVrF27Vr698/uK9a3b18GDx7MWWedVfHe58p3jLPOOosGDRrQqVMnDjroIObMmVPRzqlTp7Lnnntu8/wLM7NCcMKwC5gwfQnXPDqTsg3ZsPw/12zgvQ/WMWH6Egb2astHH31UUbZx48YAFBUVsXHjp5OKa6+9lhtuuIGGDWv2q2/WrNkWr8vrb9CgQcV6+euNGzfy3HPP8fTTT/OXv/yFPffckxNPPHGL9lVm+PDhtGvXruJ0REQwaNAgfvGLX2z5PkyYUNnubNq0ieLiYiBLaEaMGPGpMrltLSoqoqysjM2bN9OqVasaTSgcPHgwEyZMoEePHowaNapiwuhdd93F1KlTeeKJJyguLmbatGn86Ec/Yvr06ey///6MHTu22mNkU30qfz127FjOPffcvG0zM6sLfpbELuDmSXMrkgWAomat2PjhB9z46F9Zv359pee+q9KvXz9WrlzJa6+9BsChhx7KggULmDdvHgAPPPAAJ5xwQq3b+sEHH7D33nuz5557MmfOHKZMmVJt+ccff5ynn36akSNHVsROPvlkxo8fz7vvvgvAihUrWLhwIX369OH5559n+fLlbNiwgXHjxgFZAlA+SbA8WWjUqBEbNmyo9tgtWrSgY8eOFfVEBH/7298A2GuvvVizZk1F2TVr1tCmTRs2bNjAmDFjKuLz58+nT58+jBgxgtatW7No0SLuu+8+ZsyYwZNPPlntMSCbA7F582bmz5/P22+/zaGHHgrA5s2befjhhz1/wczqDScMu4Clq8q2eK2ihrQ89hym3/49vvzlL3PYYYdtU33XXnstixZl98Jq0qQJ9913H2eeeSZHHHEEDRo04NJLL611WwcMGMDGjRs5/PDDGTp0KEcffXS15W+55RaWLFlSMcHxpz/9KV26dOGGG26gX79+dO/enS9/+cssW7aMNm3aMHz4cI455hj69u3L4YcfXmW9l1xyCd27d6+Y9FiVMWPGcM8999CjRw+6du3KY49lF9qcc8453HzzzfTq1Yv58+dz/fXX06dPH/r27bvF+z1kyBCOOOIIunXrxrHHHkuPHj1qfAzIbhjWu3dvTjnlFO666y6aNGkCwAsvvED79u056KCDqm2/mVldUTZnxUpKSqK+3umx701/YslWSQNA21ZNeWnoSTuhRbYjDB48mNNOO42vf/3r+Qub1VOSpkVEyc5uhxWeRxh2AUP6H0rTRkVbxJo2KmJI/0N3UovMzGx340mPu4CBvbJHZJRfJbF/q6YM6X9oRdx2TaNGjdrZTTAzqzEnDLuIgb3aOkEwM7OdxqckzMzMLC8nDGZmZpaXEwYzMzPLywmDmZmZ5eWEwczMzPJywmBmZmZ5OWEwMzOzvJwwmJmZWV5OGMzMzCwvJwxmZmaWlxMGMzMzy8sJg5mZmeXlhMHMzMzycsJgZmZmeRUsYZDUXtKzkmZLmiXpiq22/1BSSNovvZakkZLmSXpN0pE5ZQdJeistg3LixZJmpn1GSlKK7yNpcio/WdLeheqnmZnZ7qCQIwwbgR9GRBfgaOAySV0gSyaAfsDfc8qfAnRKyyXAnansPsB1QB+gN3BdTgJwJ3Bxzn4DUnwo8ExEdAKeSa/NzMyslgqWMETEsoh4Na2vAd4A2qbNtwI/BiJnl9OB+yMzBWglqQ3QH5gcESsiYiUwGRiQtrWIiCkREcD9wMCcukan9dE5cTMzM6uFOpnDIKkD0AuYKul0YElE/G2rYm2BRTmvF6dYdfHFlcQBPh8Ry9L6P4DPV9GuSySVSip97733trVbZmZmu42CJwySmgOPAFeSnab4CfDTQh+3XBp9iCq23R0RJRFR0rp167pqkpmZ2S6noAmDpEZkycKYiHgUOBjoCPxN0gKgHfCqpC8AS4D2Obu3S7Hq4u0qiQP8M52yIP18d8f2zMzMbPdSyKskBNwDvBERtwBExMyI+FxEdIiIDmSnEY6MiH8AE4EL0tUSRwMfpNMKk4B+kvZOkx37AZPSttWSjk7HugB4LB1+IlB+NcWgnLiZmZnVQsMC1t0XOB+YKWlGiv0kIp6sovyTwFeAecA64EKAiFgh6XrglVRuRESsSOvfA0YBTYE/pAXgJuBhSRcBC4GzdlSnzMzMdkfKTvFbSUlJlJaW7uxmmJntUiRNi4iSnd0OKzzf6dHMzMzycsJgZmZmeTlhMDMzs7ycMJiZmVleThjMzMwsLycMZmZmlpcTBjMzM8vLCYOZmZnl5YTBzMzM8nLCYGZmZnk5YTAzM7O8nDCYmZlZXk4YzMzMLC8nDGZmZpaXEwYzMzPLywmDmZmZ5eWEwczMzPJywmBmZmZ5OWEwMzOzvJwwmJmZWV5OGMzMzCwvJwxmZmaWV8ESBkntJT0rabakWZKuSPHrJb0maYakpyTtn+KSNFLSvLT9yJy6Bkl6Ky2DcuLFkmamfUZKUorvI2lyKj9Z0t6F6qeZmdnuoJAjDBuBH0ZEF+Bo4DJJXYCbI6J7RPQE/g/4aSp/CtApLZcAd0L24Q9cB/QBegPX5SQAdwIX5+w3IMWHAs9ERCfgmfTazMzMaqlgCUNELIuIV9P6GuANoG1ErM4p1gyItH46cH9kpgCtJLUB+gOTI2JFRKwEJgMD0rYWETElIgK4HxiYU9fotD46J25mZma10LAuDiKpA9ALmJpe/xy4APgA+GIq1hZYlLPb4hSrLr64kjjA5yNiWVr/B/D5Ktp1CdloBgcccMA298vMzGx3UfBJj5KaA48AV5aPLkTEtRHRHhgDXF7I46fRh6hi290RURIRJa1bty5kM8zMzHZpBU0YJDUiSxbGRMSjlRQZA5yR1pcA7XO2tUux6uLtKokD/DOdsiD9fHf7emJmZrZ7K+RVEgLuAd6IiFty4p1yip0OzEnrE4EL0tUSRwMfpNMKk4B+kvZOkx37AZPSttWSjk7HugB4LKeu8qspBuXEzczMrBYKOYehL3A+MFPSjBT7CXCRpEOBzcBC4NK07UngK8A8YB1wIUBErJB0PfBKKjciIlak9e8Bo4CmwB/SAnAT8LCki9IxzipEB83MzHYXyk7xW0lJSZSWlu7sZpiZ7VIkTYuIkp3dDis83+nRzMzM8nLCYGZmZnk5YTAzM7O8nDCYmZlZXk4YzMzMLC8nDGZmZpaXEwYzMzPLywmDmZmZ5eWEwczMzPJywmBmZmZ5OWEwMzOzvJwwmJmZWV5OGMzMzCwvJwxmZmaWlxMGMzMzy8sJg5mZmeXlhMHMzMzycsJgZmZmeTlhMDMzs7ycMJiZmVleThjMzMwsr4IlDJLaS3pW0mxJsyRdkeI3S5oj6TVJv5fUKmefayTNkzRXUv+c+IAUmydpaE68o6SpKf6QpD1SvHF6PS9t71CofpqZme0OCjnCsBH4YUR0AY4GLpPUBZgMdIuI7sCbwDUAads5QFdgAPAbSUWSioA7gFOALsC5qSzAL4FbI+IQYCVwUYpfBKxM8VtTOTMzM6ulgiUMEbEsIl5N62uAN4C2EfFURGxMxaYA7dL66cDYiFgfEe8A84DeaZkXEW9HxMfAWOB0SQJOAsan/UcDA3PqGp3WxwMnp/JmZmZWC3UyhyGdEugFTN1q07eAP6T1tsCinG2LU6yq+L7Aqpzkozy+RV1p+wepvJmZmdVCwRMGSc2BR4ArI2J1TvxastMWYwrdhmradomkUkml77333s5qhpmZWb1X0IRBUiOyZGFMRDyaEx8MnAacFxGRwkuA9jm7t0uxquLLgVaSGm4V36KutL1lKr+FiLg7IkoioqR169bb0VMzM7PPtkJeJSHgHuCNiLglJz4A+DHw1YhYl7PLROCcdIVDR6AT8FfgFaBTuiJiD7KJkRNTovEs8PW0/yDgsZy6BqX1rwN/yklMzMzMbBs1zF+k1voC5wMzJc1IsZ8AI4HGwOQ0D3FKRFwaEbMkPQzMJjtVcVlEbAKQdDkwCSgC7o2IWam+q4Gxkm4AppMlKKSfD0iaB6wgSzLMzMysluQv3pmSkpIoLS3d2c0wM9ulSJoWESU7ux1WeL7To5mZmeXlhMHMzMzycsJgZmZmeTlhMDMzs7xqlDBIukJSC2XukfSqpH6FbpyZmZnVDzUdYfhWuktjP2BvssslbypYq8zMzKxeqWnCUP7gpq8AD6T7IPhhTmZmZruJmiYM0yQ9RZYwTJK0F7C5cM0yMzOz+qSmd3q8COgJvB0R6yTtA1xYuGaZmZlZfVLTEYZjgLkRsUrSN4FhZI+MNjMzs91ATROGO4F1knoAPwTmA/cXrFVmZmZWr9Q0YdiYnvZ4OnB7RNwB7FW4ZpmZmVl9UtM5DGskXUN2OeVxkhoAjQrXLDMzM6tPajrCcDawnux+DP8A2gE3F6xVZmZmVq/UKGFIScIYoKWk04CPIsJzGMzMzHYTNb019FnAX4EzgbOAqZK+XsiGmZmZWf1R0zkM1wJHRcS7AJJaA08D4wvVMDMzM6s/ajqHoUF5spAs34Z9zczMbBdX0xGGP0qaBPwuvT4beLIwTTIzM7P6pkYJQ0QMkXQG0DeF7o6I3xeuWWZmZlaf1HSEgYh4BHikgG0xMzOzeqrahEHSGiAq2wRERLQoSKvMzMysXql24mJE7BURLSpZ9sqXLEhqL+lZSbMlzZJ0RYqfmV5vllSy1T7XSJonaa6k/jnxASk2T9LQnHhHSVNT/CFJe6R44/R6XtreYdvfGjMzMytXyCsdNgI/jIguwNHAZZK6AK8DXwNeyC2ctp0DdAUGAL+RVCSpCLgDOAXoApybygL8Erg1Ig4BVpI9hpv0c2WK35rKmZmZWS0VLGGIiGUR8WpaXwO8AbSNiDciYm4lu5wOjI2I9RHxDjAP6J2WeRHxdkR8DIwFTpck4CQ+uRfEaGBgTl2j0/p44ORU3szMzGqhTu6lkE4J9AKmVlOsLbAo5/XiFKsqvi+wKiI2bhXfoq60/YNUfut2XSKpVFLpe++9t22dMjMz240UPGGQ1Jzs6oorI2J1oY+3LSLi7ogoiYiS1q1b7+zmmJmZ1VsFTRgkNSJLFsZExKN5ii8B2ue8bpdiVcWXA60kNdwqvkVdaXvLVN7MzMxqoWAJQ5ozcA/wRkTcUoNdJgLnpCscOgKdyB549QrQKV0RsQfZxMiJERHAs0D5Q7AGAY/l1DUorX8d+FMqb2ZmZrVQ4xs31UJf4HxgpqQZKfYToDHwX0Br4AlJMyKif0TMkvQwMJvsCovLImITgKTLgUlAEXBvRMxK9V0NjJV0AzCdLEEh/XxA0jxgBVmSYWZmZrUkf/HOlJSURGlp6c5uhpnZLkXStIgoyV/SdnV+4qSZmZnl5YTBzMzM8nLCYGZmZnk5YTAzM7O8nDCYmZlZXk4YzMzMLC8nDGZmZpaXEwYzMzPLywmDmZmZ5eWEwczMzPJywmBmZmZ5OWEwMzOzvJwwmJmZWV5OGMzMzCwvJwxmZmaWlxMGMzMzy8sJg5mZmeXlhMHMzMzycsJgZmZmeTlhMDMzs7ycMJiZmVleThjMzMwsr4IlDJLaS3pW0mxJsyRdkeL7SJos6a30c+8Ul6SRkuZJek3SkTl1DUrl35I0KCdeLGlm2mekJFV3DDMzM6udQo4wbAR+GBFdgKOByyR1AYYCz0REJ+CZ9BrgFKBTWi4B7oTswx+4DugD9Aauy0kA7gQuztlvQIpXdQwzMzOrhYIlDBGxLCJeTetrgDeAtsDpwOhUbDQwMK2fDtwfmSlAK0ltgP7A5IhYERErgcnAgLStRURMiYgA7t+qrsqOYWZmZrVQJ3MYJHUAegFTgc9HxLK06R/A59N6W2BRzm6LU6y6+OJK4lRzjK3bdYmkUkml77333rZ3zMzMbDdR8IRBUnPgEeDKiFiduy2NDEQhj1/dMSLi7ogoiYiS1q1bF7IZZmZmu7SCJgySGpElC2Mi4tEU/mc6nUD6+W6KLwHa5+zeLsWqi7erJF7dMczMzKwWCnmVhIB7gDci4pacTROB8isdBgGP5cQvSFdLHA18kE4rTAL6Sdo7TXbsB0xK21ZLOjod64Kt6qrsGGZmZlYLDQtYd1/gfGCmpBkp9hPgJuBhSRcBC4Gz0rYnga8A84B1wIUAEbFC0vXAK6nciIhYkda/B4wCmgJ/SAvVHMPMzMxqQdkpfispKYnS0tKd3Qwzs12KpGkRUbKz22GF5zs9mpmZWV5OGMzMzCwvJwxmZmaWlxMGMzMzy8sJg5mZmeXlhMHMzMzycsJgZmZmeTlhMDMzs7ycMJiZmVleThjMzMwsLycMZmZmlpcTBjMzM8vLCYOZmZnl5YTBzMzM8nLCYGZmZnk5YTAzM7O8nDCYmZlZXk4YzKzOjBo1issvv3yHlL/xxhsr1hcsWEC3bt22qS0TJkxg9uzZ27SP2e7MCYOZ1YmNGzfu0PpyE4bacMJgtm2cMJhZpT788ENOPfVUevToQbdu3XjooYfo0KED1113HUceeSRHHHEEc+bMAWDFihUMHDiQ7t27c/TRR/Paa68BMHz4cM4//3z69u3L+eefv0X9TzzxBMcccwzvv/8+7733HmeccQZHHXUURx11FC+99FK1bRs6dChlZWX07NmT8847D4BNmzZx8cUX07VrV/pRbs5BAAAWU0lEQVT160dZWRkA8+fPZ8CAARQXF3PccccxZ84cXn75ZSZOnMiQIUPo2bMn8+fP57e//S1HHXUUPXr04IwzzmDdunUAjBs3jm7dutGjRw+OP/74Hfoem+1SIsJLBMXFxWFmnxg/fnx8+9vfrni9atWqOPDAA2PkyJEREXHHHXfERRddFBERl19+eQwfPjwiIp555pno0aNHRERcd911ceSRR8a6desiIuK+++6Lyy67LB599NH4l3/5l1ixYkVERJx77rnx4osvRkTEwoUL47DDDtuifGWaNWtWsf7OO+9EUVFRTJ8+PSIizjzzzHjggQciIuKkk06KN998MyIipkyZEl/84hcjImLQoEExbty4ijref//9ivVrr722op/dunWLxYsXR0TEypUra/z+7S6A0qgHf8O9FH5puLMTFjOrPyZMX8LNk+aydFUZe29Yy+In/sg+V1/NaaedxnHHHQfA1772NQCKi4t59NFHAfjzn//MI488AsBJJ53E8uXLWb16NQBf/epXadq0acUx/vSnP1FaWspTTz1FixYtAHj66ae3OD2wevVq1q5du01t79ixIz179qxo24IFC1i7di0vv/wyZ555ZkW59evXV7r/66+/zrBhw1i1ahVr166lf//+APTt25fBgwdz1llnVfTdbHdUsIRB0r3AacC7EdEtxXoAdwHNgQXAeRGxOm27BrgI2AR8PyImpfgA4D+BIuB/IuKmFO8IjAX2BaYB50fEx5IaA/cDxcBy4OyIWFCofpp9VkyYvoRrHp1J2YZNAKxotB+tvnEL6/daxrBhwzj55JMBaNy4MQBFRUU1mpfQrFmzLV4ffPDBvP3227z55puUlJQAsHnzZqZMmUKTJk0qrWPTpk0UFxcDWQIyYsSIT5Upb1d528rKyti8eTOtWrVixowZeds5ePBgJkyYQI8ePRg1ahTPPfccAHfddRdTp07liSeeoLi4mGnTprHvvvvmrc/ss6aQcxhGAQO2iv0PMDQijgB+DwwBkNQFOAfomvb5jaQiSUXAHcApQBfg3FQW4JfArRFxCLCSLNkg/VyZ4remcmaWx82T5lYkCwAb1yxnPQ15pWE3hgwZwquvvlrlvscddxxjxowB4LnnnmO//farGD3Y2oEHHsgjjzzCBRdcwKxZswDo168f//Vf/1VRZusP+KKiImbMmMGMGTMqkoVGjRqxYcOGavvUokULOnbsyLhx44DsFOzf/vY3APbaay/WrFlTUXbNmjW0adOGDRs2VPQFsjkQffr0YcSIEbRu3ZpFixZVe0yzz6qCJQwR8QKwYqtwZ+CFtD4ZOCOtnw6MjYj1EfEOMA/onZZ5EfF2RHxMNqJwuiQBJwHj0/6jgYE5dY1O6+OBk1N5M6vG0lVlW7ze8N4Clt1/Fa/c+m1+9rOfMWzYsCr3HT58ONOmTaN79+4MHTqU0aNHV1kW4LDDDmPMmDGceeaZzJ8/n5EjR1JaWkr37t3p0qULd911V972XnLJJXTv3r1i0mNVxowZwz333EOPHj3o2rUrjz32GADnnHMON998M7169WL+/Plcf/319OnTh759+3LYYYdV7D9kyBCOOOIIunXrxrHHHkuPHj3yts3ss0gRUbjKpQ7A/+WckngZ+FVETJB0FfCziNhL0u3AlIh4MJW7B/hDqmZARHw7xc8H+gDDU/lDUrw98IeI6Cbp9bTP4rRtPtAnIt6vpH2XAJcAHHDAAcULFy4sxNtgtkvoe9OfWLJV0gDQtlVTXhp60k5oke0KJE2LiJKd3Q4rvLq+rPJbwPckTQP2Aj6u4+NvISLujoiSiChp3br1zmyK2U43pP+hNG1UtEWsaaMihvQ/dCe1yMzqkzq9SiIi5gD9ACR1Bk5Nm5YA7XOKtksxqogvB1pJahgRG7cqX17XYkkNgZapvJlVY2CvtgAVV0ns36opQ/ofWhE3s91bnSYMkj4XEe9KagAMI7tiAmAi8L+SbgH2BzoBfwUEdEpXRCwhmxj5jYgISc8CXyeb1zAIeCynrkHAX9L2P0Uhz7uYfYYM7NXWCYKZVaqQl1X+DjgR2E/SYuA6oLmky1KRR4H7ACJilqSHgdnARuCyiNiU6rkcmER2WeW9ETEr7X81MFbSDcB04J4Uvwd4QNI8skmX5xSqj2ZmZruLgk563JWUlJREaWnpzm6GmdkuxZMedx9+loSZmZnl5YTBzMzM8nLCYGZmZnk5YTAzM7O8nDCYmZlZXk4YzMzMLC8nDGZmZpaXEwYzMzPLywmDmZmZ5eWEwczMPqVVq1bcf//9AHzuc59j4cKF21WfpCsl/bOKbb+XtG8t6nxe0pDtatgnda2SdH4l8bMl/TTn9XOSHt/GuiftiDbWhqSXJJVJCklLJc1I8Q4pPiMtd+Wrq04fPmVmZoX30Ucf0aRJkx1W37vvvrvD6qrCvwL7UsmThSU1iogNle0UEScUumHAl4HewIjtqKPfDmpLbXwb2Az8N7AYeDNn2/yI6FnTijzCYGZWD/35z3+mcePGHHbYYTRp0oR9992XFStW8NBDD9G8eXOaNm3K/vvvzzvvvANkIwK9evWiWbNmnHnmmRxyyCF07dqV5s2b06hRI2677TY6d+5M48aNOeSQQyqO07VrV5o1a0aTJk044YTKP38bNmzI3Llz+cY3vkHTpk1p2rQpjRo1Yu+99wZA0lBJayStk7RI0udT/FpJ6yWtAwZXVrek8WQPF3xN0soUC0mlksqAb0t6RtKHkj6S9IYkpXLzJP06rW9M3/7XpXKnpHhrSW9KWpu2/TzF95a0MLVvKZV8gZbULLW7W/o2flvadGgakdiQ2l9e/jfpOGWSZktqJOkvaVuZpHfS+rKc/jyQYo1Sfz5Ky4Qq3q9PHSPnPXs17btC0mEAEfFGRMxNu38J+F2lv+SaiAgvERQXF4eZWX3x4osvBhBjx46NiIh27drFpZdeGk2aNInbbrstIiKOO+646NmzZ0REtGzZMrp27Vqx/8EHHxzt27ePTZs2xTXXXBNAjB8/PjZs2BBNmzatqHfevHkREbF+/fpo2bJljBs3rqK+0aNHR0REUVFRzJkzp6LuDz/8MFq0aBHDhg0LYCawCmgd2cMMnwSeAVqSPX34S4CARcA/o5K/v6lc55zXAdya8/qgnPW3gRFpfR7w65w6xqX1scCctP4S8Ju0fiDwMdAaeAx4M8XPSMc8v5K2/Q/wWs7r54DVwF5AZ7Jv702BU4F/AE1TudeB/y7vz1Z1HpR+7g18BBwCnAcszylzYCVtqfYYwB1p/ZncNqfYdGB2zusOwIcp/jxwXGW/m9zFIwxmZvXIsAkzOfiaJ/n6nS9DgyJmNu4CQLdu3ZgzZw4bNmzgiiuuAGD48OHMnTu3Yt+LL754i7pOO+00GjRoQL9+/WjUqBFnnHEGDRs2pE2bNsyYMQOAf//3f2fPPfekZcuWrF69mhdeeCFvG3v37k3Pnj25/vrrAfYBWgB/TyMCJwPtgAHA2oh4OrJPqNHb+Fb8OGf9u+lb9UdkH/pHVbHPTennU2RJAUBP4FupbXPIRtZ7p+UugIh4BCjbhrZNiYg1EfEmsAHoCgxKx1yRjtUZOLSK/e9NZZYCjYETyBKbFpL+JulastMHW8t3jKvSzxFAp632/RyQO5diGXBARPRK+/2vpBbVddoJg5lZPTFswkwenPJ3NmXfAEENeHDK3xk2YSZFRUV88MEH1e5ffoqgXNOmTYHslEKDBp/8uZfExx9/zAsvvMC4ceOYNWsWZWVlHHTQQaxbt67aY3z729/m3Xff5ZlnnqmoDlgUEU3T0jgiqvqgLD/++2lIfU5VZSLNW5DUkuwD7aSIaAK8SPaNvjJr0s8NZKc5yg3MaV/DiHiimrbdlNpWVtkkyGR9blOBJmTvw9Sc4+wRESdWUv+VQC+gXUQ0BT4A9oqIBWSJ1lPA5cAbknrntGVMTY+R067yYzYkSzQmV2yMWB8Ry9P6NGA+WQJSJScMZmb1xO+mLqo23qJFCxo1asTtt98OwIgRIzjssMNqfbx//OMfNGzYkPbt2/P666/z9ttvV1v+wQcf5MEHH2Tq1Kk0bFhxyn850FbSSVAxZ6Af8EeguaQvpnIVH74RsV/6wCtv/Ebg81UctlX6+VaaG3H0NnZzBvCrnHkP56T4X4FLUmwgKQmJiKE5H8gPACuBPWtwnPuBEkldUp0HSTo2bQtJ5UlOa6AsIpaneRYtU/nOQMOIGAJ8F2gfEX/Nact5eY4B8Kv0cxjwVk78S8A6oGL2avo9FZXXQzYiUe0/ACcMZmb1RMXIQjXxUaNGMXToUJo2bcrcuXN55JFHan28s846i7Zt27Lnnnty/PHH84UvfKHa8j/72c/YsGEDXbp0oWnTphx66KGQfdv+MfB4Gib/O3B8RHwAXAf8MU16XFFN1Y8Dz5RPeswVEQuBl8k+7N4imwuxLU4jm9BYlk5p/GeKDwaaSloP3EF2Pr8ydwJttpr0+CkR8TjwG2Baeh9mAoenzX8FVqVJjzcBRem495KNMAD0IEuKysgmJt64jccA+JfUx2LgbABJ/wY8AjQDntAnl3geTzbRdAYwHrg0Iqr7HaGo4h/o7qakpCRKS0t3djPMbDd28DVPVpo0FEnM/8VXdkKL8pM0LSJKdnY7dneSIiJUyGN4hMHMrJ44t0/7bYqb1SXfuMnMrJ64YeARQDZnYVMERRLn9mlfETerSqFHF8CnJCr4lISZ2bbzKYndR8FOSUi6V9K7kl7PifWUNEXZfatLJfVOcUkame5y9ZqkI3P2GSTprbQMyokXS5qZ9hmZMwN2H0mTU/nJkra8zsjMzMy2WSHnMIwiu3FHrl8BP4vs3tU/5ZNLQE4hu6SjE9llLndC9uFPNsu2D9lNNq7LSQDuBC7O2a/8WEOBZyKiE9ndrobu6I6ZmZntbgqWMETEC3z6MpoguyMYZNeeLk3rpwP3p7uOTgFaSWoD9AcmR8SKiFhJdtOJAWlbi4iYku4gdj8wMKeu8juKjc6Jm5mZWS3V9aTHK4FJyh4W0gAov+FEW7a8tnZxilUXX1xJHODzEbEsrf+Dqm8GgqRLSDfuOOCAA2rRHTMzs91DXV9W+V3gBxHRHvgBcE8hD5ZGH6qc1RkRd0dESUSUtG7duqpiZmZmu726HmEYBFyR1seRPQUMYAmQe6FxuxRbApy4Vfy5FG9XSXmAf0pqExHL0qmLGj3Ifdq0ae9LWljjnmyf/YD36+hY9cHu1l/Y/frs/n72VdXnA+u6IbZz1HXCsJTsqVzPASfxyb2uJwKXSxpLNsHxg/SBPwm4MWeiYz/gmohYIWm1pKOBqcAFwH/l1DWI7Pabg8geYZpXRNTZEIOk0t3pMqTdrb+w+/XZ/f3s2x37bFsqWMIg6XdkowP7SVpMdrXDxcB/pidnfUSaP0D2/PSvkD3bfB1wIUBKDK4HXknlRuTc6/p7ZFdiNAX+kBbIEoWHJV0ELATOKlAXzczMdhsFSxgi4twqNhVXUjaAy6qo516yB3RsHS8FulUSX072PHYzMzPbQfwsiZ3j7p3dgDq2u/UXdr8+u7+ffbtjny2Hbw1tZmZmeXmEwczMzPJywmBmZmZ5OWHYwSQNkDQ3PRTrU8+xkDRY0nvpAVwzJH07Z9uvJM2S9EbuA7Xqs3z9TWXOkjQ79e1/c+KVPlisPqttf9OD1/6SYq9JOrtuW1572/M7TttaSFos6fa6afH22c5/0wdIeir9H54tqUNdtbu2trO/u9zfLNsOEeFlBy1AETAfOAjYA/gb0GWrMoOB2yvZ91jgpVRHEfAX4MSd3acd0N9OwHRg7/T6c+nnPsDb6efeaX3vnd2nAva3M9Apre8PLANa7ew+FbLPOdv/E/jfyv7d17dle/tLdo+ZL6f15sCeO7tPhervrvg3y8v2LR5h2LF6A/Mi4u2I+BgYS/YwrJoIoAnZf9rGQCPgnwVp5Y5Tk/5eDNwR2cPDiIjyO29W+mCxOmp3bdW6vxHxZkS8ldaXkt2BdFe4H/n2/I6RVEz2PJen6qi926vW/ZXUBWgYEZNTfG1ErKu7ptfK9vx+d8W/WbYdnDDsWFU9LGtrZ6Rh6fGS2gNExF+AZ8m+eS4DJkXEG4Vu8HaqSX87A50lvSRpiqQB27BvfbM9/a0gqTfZH9n5BWvpjlPrPktqAPwH8KM6aemOsT2/487AKkmPSpou6WZJRXXQ5u1R6/7uon+zbDvU9a2hDR4HfhcR6yV9h+wR3CdJOgQ4nE+ekTFZ0nER8eLOaugO0pBsSPNEsr69IOmIndqiwqq0vxGxCiA93+QBYFBEbN5prdyxqvodfxN4MiIWf8ZObVfV34bAcUAv4O/AQ2SnIAv6kL06UFV/9+Oz+TfLquARhh2rqodoVYiI5RGxPr38Hz658+W/AVPSMOZasltdH1Pg9m6vvP0l+8YyMSI2RMQ7wJtkf3xqsm99sz39RVIL4Ang2oiYUgft3RG2p8/HkD0jZgHwa+ACSTcVvsnbZXv6uxiYkYb3NwITgCProM3bY3v6uyv+zbLt4IRhx3oF6CSpo6Q9gHPIHoZVIX3DLPdVoHwI7+/ACZIaSmpE9pCu+j68l7e/ZH80TwSQtB/Z8ObbwCSgn6S9lT1crF+K1We17m8q/3vg/ogYX3dN3m617nNEnBcRB0REB7LTEvdHRKWz8OuR7fk3/QrQSlL53JSTgNl10ejtsD393RX/Ztl28CmJHSgiNkq6nOyDrwi4NyJmSRoBlEbEROD7kr4KbARWkA1ZAown+wMzk2wy0R8j4vG67sO2qGF/yxOD2cAmYEhkz/tAVT9YrF7anv5K+iZwPLCvpMGpysERMaPue1Jz2/s73tXsgH/TPwKeSZcXTgN+u1M6UkPb+W96l/ubZdvHt4Y2MzOzvHxKwszMzPJywmBmZmZ5OWEwMzOzvJwwmJmZWV5OGMzMzCwvJwxmBSDp++kJfo8oe0rl+nTJnZnZLsn3YTArjO8BXwI+Bg4EBtblwSU1THcbNDPbITzCYLaDSbqL7HHBfwDOi4hXgA159jlB0oy0TJe0V4pfLWmmpL+V31ZZUs/0EKDXJP0+3SkTSc9Juk1SKXCFpNZphOOVtPQtaMfN7DPNIwxmO1hEXJqe6PfFiHi/hrv9CLgsIl6S1Bz4SNIpZI8a7hMR6yTtk8reD/y/iHg+3ZHvOuDKtG2PiCgBkPS/wK0R8WdJB5Ddse/wHdNLM9vdOGEwqx9eAm6RNAZ4ND3h8UvAfRGxDiAiVkhqCbSKiOfTfqOBcTn1PJSz/iWgS86TIltIap4eFGRmtk2cMJjtBJIuAy5OL78SETdJegL4CvCSpP61rPrDnPUGwNER8dF2NNXMDPAcBrOdIiLuiIieaVkq6eCImBkRvyR7INdhwGTgQkl7AkjaJyI+AFZKOi5VdT7wfKUHgaeA/1f+QlLPgnXIzD7zPMJgVkCSvgCUAi2AzZKuBLpExOqtil4p6YvAZmAW8IeIWJ8+5EslfQw8CfwEGATclRKJt4ELqzj894E7JL1G9n/9BeDSHdtDM9td+GmVZmZmlpdPSZiZmVleThjMzMwsLycMZmZmlpcTBjMzM8vLCYOZmZnl5YTBzMzM8nLCYGZmZnn9f+FpKzg3sKsCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Normalized loss plot loss vs f1s\n",
    "    \n",
    "y_loss=[190172.47,247125.96,182772.825,182772.62]\n",
    "x_f1s =[0.636,0.585,0.668,0.668]\n",
    "text=[\"snorkel-thetas\",\"unNormalized-thetas-ep7\",\"normalized-trained-thetas-ep7\",\"normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"CDR-Normalized-Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31db88518>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31db88518>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31db88518>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -7326.533290003919\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.2053655  0.30936395 0.28997198 0.2367527  0.28647343 0.32020677\n",
      "  0.19884814 0.30623267 0.30723075 0.2419398  0.25581996 0.28299078\n",
      "  0.18634078 0.21796294 0.20752234 0.19268362 0.18442374 0.26176987\n",
      "  0.22819245 0.18263638 0.30204006 0.36018888 0.18171976 0.19192962\n",
      "  0.30156119 0.22449297 0.18345572 0.18210333 0.18615508 0.18474442\n",
      "  0.19665109 0.19592285 0.21894825]]\n",
      "dev loss -769.3308920775862\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.733\n",
      "Neg. class accuracy: 0.715\n",
      "Precision            0.562\n",
      "Recall               0.733\n",
      "F1                   0.636\n",
      "----------------------------------------\n",
      "TP: 217 | FP: 169 | TN: 423 | FN: 79\n",
      "========================================\n",
      "\n",
      "{0: 502, 1: 386}\n",
      "acc 0.7207207207207207\n",
      "(array([0.84262948, 0.56217617]), array([0.71452703, 0.73310811]), array([0.77330896, 0.63636364]), array([592, 296]))\n",
      "(0.7024028239374109, 0.7238175675675675, 0.7048362971580522, None)\n",
      "[[423 169]\n",
      " [ 79 217]]\n",
      "prec: tp/(tp+fp) 0.5621761658031088 recall: tp/(tp+fn) 0.7331081081081081\n",
      "(0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)\n",
      "normalized thetas ep15 lr0.01\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31eaf0710>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31eaf0710>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31eaf0710>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -10754.901547424994\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.28575421  0.51761933  0.5207703   0.36454638  0.55417525  0.64604823\n",
      "   0.13772734  0.5908674   0.64182507  0.41374798  0.40126751  0.53715197\n",
      "  -0.05536051  0.32497394  0.21877467  0.14599839 -0.05084094  0.423559\n",
      "   0.32848078  0.05423492  0.65924925  1.0990666   0.05418686  0.22025056\n",
      "   0.55000554  0.33751599  0.06750857  0.05696271  0.11251414 -0.01585396\n",
      "   0.11384963  0.12932497  0.32449617]]\n",
      "dev loss -1083.2420373622351\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.851\n",
      "Neg. class accuracy: 0.657\n",
      "Precision            0.554\n",
      "Recall               0.851\n",
      "F1                   0.671\n",
      "----------------------------------------\n",
      "TP: 252 | FP: 203 | TN: 389 | FN: 44\n",
      "========================================\n",
      "\n",
      "{0: 433, 1: 455}\n",
      "acc 0.7218468468468469\n",
      "(array([0.89838337, 0.55384615]), array([0.65709459, 0.85135135]), array([0.75902439, 0.67110519]), array([592, 296]))\n",
      "(0.7261147628353171, 0.754222972972973, 0.7150647916599007, None)\n",
      "[[389 203]\n",
      " [ 44 252]]\n",
      "prec: tp/(tp+fp) 0.5538461538461539 recall: tp/(tp+fn) 0.8513513513513513\n",
      "(0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None)\n",
      "[(-7326.533290003919, -769.3308920775862, (0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)), (-10754.901547424994, -1083.2420373622351, (0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas Unnormalized # remove logz from obj\n",
    "\n",
    "def getUNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getUNLObjValue(np.array([[0.2053655 , 0.30936395, 0.28997198, 0.2367527 , 0.28647343,\n",
    "       0.32020677, 0.19884814, 0.30623267, 0.30723075, 0.2419398 ,\n",
    "       0.25581996, 0.28299078, 0.18634078, 0.21796294, 0.20752234,\n",
    "       0.19268362, 0.18442374, 0.26176987, 0.22819245, 0.18263638,\n",
    "       0.30204006, 0.36018888, 0.18171976, 0.19192962, 0.30156119,\n",
    "       0.22449297, 0.18345572, 0.18210333, 0.18615508, 0.18474442,\n",
    "       0.19665109, 0.19592285, 0.21894825]])))\n",
    "\n",
    "            \n",
    "print(\"normalized thetas ep15 lr0.01\")\n",
    "\n",
    "l_f1s.append(getUNLObjValue(np.array([[ 0.28575421,  0.51761933 , 0.5207703 ,  0.36454638,  0.55417525,  0.64604823,\n",
    "   0.13772734,  0.5908674,   0.64182507,  0.41374798,  0.40126751,  0.53715197,\n",
    "  -0.05536051,  0.32497394,  0.21877467,  0.14599839, -0.05084094,  0.423559,\n",
    "   0.32848078,  0.05423492,  0.65924925,  1.0990666 ,  0.05418686,  0.22025056,\n",
    "   0.55000554,  0.33751599,  0.06750857,  0.05696271,  0.11251414, -0.01585396,\n",
    "   0.11384963,  0.12932497,  0.32449617]])))\n",
    "\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## UN-Normalized loss plot loss vs f1s\n",
    "    \n",
    "y_loss=[-7326.53,-10754.9,-20825.271]\n",
    "x_f1s =[0.636,0.671,0.585]\n",
    "text=[\"snorkel-thetas\",\"normalized-thetas-ep15\",\"Un-normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"CDR-Un-Normalized-Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 145767.7856663791\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "1 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "2 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "3 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "4 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "5 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "6 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "7 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "8 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "9 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "10 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "11 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "12 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "13 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "14 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "15 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "16 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "17 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "18 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "19 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "21 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "22 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "23 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "24 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "25 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "26 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "27 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "28 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "29 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "acc 0.7949538024164889\n",
      "(array([0.93353091, 0.07079646]), array([0.84      , 0.16931217]), array([0.88429918, 0.09984399]), array([2625,  189]))\n",
      "(0.5021636830944227, 0.5046560846560846, 0.49207158581109633, None)\n",
      "[[2205  420]\n",
      " [ 157   32]]\n",
      "prec: tp/(tp+fp) 0.07079646017699115 recall: tp/(tp+fn) 0.1693121693121693\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n"
     ]
    }
   ],
   "source": [
    "## same network that didn't train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(30):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 16 29 14 13 23 20  0  5 11  2  8  7  1 17  6 32 18 31 24 25  9  3 28\n",
      " 30 15 22 19 27 26 10  4 21]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([-0.27082211, -0.01928787, -0.14063245,  0.37856253,  0.43681819, -0.15844807,\n",
    "   0.13280198, -0.01935702, -0.10775934,  0.34390113,  0.39762823, -0.14286955,\n",
    "  -0.39588527, -0.33699178, -0.37821404,  0.38378715, -0.39537146,  0.11504936,\n",
    "   0.21906794,  0.39699417, -0.27113816,  8.13838832,  0.39548336, -0.31328908,\n",
    "   0.25503373,  0.28019293,  0.39734506,  0.39700564,  0.37866251, -0.39156514,\n",
    "   0.38002959,  0.21917987,  0.13605525])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00017390038913676534\n",
      "[1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99962997 1.         1.         1.         1.\n",
      " 1.         1.         1.00055981 0.99944019 1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99966171\n",
      " 1.00037556 1.00038181 1.        ]\n",
      "0 502\n",
      "0  d  (0.6458827900831905, 0.6613175675675675, 0.6176945431062317, None)\n",
      "\n",
      "-2.852652261056169e+32\n",
      "[1.00000000e+00 4.19096332e+01 1.61282266e+01 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 6.53761651e-01 2.98818871e+01\n",
      " 2.73636591e+01 1.00000000e+00 1.00000000e+00 1.37726648e+01\n",
      " 1.00000000e+00 1.09325114e+17 1.00000000e+00 1.00000000e+00\n",
      " 1.06648029e+00 9.51584748e-01 9.51436912e-01 9.84135000e-01\n",
      " 5.96544835e-01 4.02272777e+10 9.60992051e-01 6.00949033e+02\n",
      " 1.18230742e+02 9.52113532e-01 8.06269559e-01 8.73388071e-01\n",
      " 6.56039488e-01 1.04801446e+17 8.47703035e-01 8.44365498e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.0622783407234913e+67\n",
      "[1.00000000e+00 1.43344200e+03 2.48781988e+02 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 7.89037703e+02\n",
      " 8.81714699e+02 1.00000000e+00 1.00000000e+00 2.34544600e+02\n",
      " 1.00000000e+00 2.10967076e+34 1.00000000e+00 1.00000000e+00\n",
      " 1.13949202e+00 9.51575033e-01 9.51436912e-01 9.04171121e-01\n",
      " 5.96544835e-01 9.36474848e+20 9.03578251e-01 3.91911816e+04\n",
      " 1.12698180e+04 9.52113532e-01 6.74689330e-01 8.03451390e-01\n",
      " 6.01198782e-01 2.02237656e+34 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "0 -9.30043372417304e+72\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-4.6948196355860187e+67\n",
      "[1.00000000e+00 1.83829184e+03 2.94633493e+02 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 1.01188714e+03\n",
      " 1.06522370e+03 1.00000000e+00 1.00000000e+00 2.77785173e+02\n",
      " 1.00000000e+00 3.15957251e+35 1.00000000e+00 1.00000000e+00\n",
      " 1.13949202e+00 9.51575033e-01 9.51436912e-01 9.04171121e-01\n",
      " 5.96544835e-01 5.78505513e+21 9.03578251e-01 5.44237193e+04\n",
      " 1.69470154e+04 9.52113532e-01 6.74689330e-01 8.03451390e-01\n",
      " 6.01198782e-01 3.02883535e+35 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-8.872685835358324e+103\n",
      "[1.00000000e+00 8.55982116e+04 5.27812973e+03 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 3.35935250e+04\n",
      " 3.43245806e+04 1.00000000e+00 1.00000000e+00 4.50517914e+03\n",
      " 1.00000000e+00 6.09709656e+52 1.00000000e+00 1.00000000e+00\n",
      " 1.21998165e+00 9.51575033e-01 9.51436912e-01 8.90777925e-01\n",
      " 5.96544835e-01 2.78447456e+32 8.70677135e-01 1.99574851e+06\n",
      " 2.28839216e+06 9.52113532e-01 6.11409487e-01 7.19909702e-01\n",
      " 6.01198782e-01 5.84481018e+52 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-3.3040346752451336e+138\n",
      "[1.00000000e+00 2.92787378e+06 8.14437614e+04 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 8.87130519e+05\n",
      " 1.10602441e+06 1.00000000e+00 1.00000000e+00 7.67897630e+04\n",
      " 1.00000000e+00 1.17657013e+70 1.00000000e+00 1.00000000e+00\n",
      " 1.30797411e+00 9.51575033e-01 9.51436912e-01 8.23758607e-01\n",
      " 5.96544835e-01 6.48214481e+42 8.22564466e-01 1.26509258e+08\n",
      " 2.18132110e+08 9.52113532e-01 5.98005309e-01 6.77570161e-01\n",
      " 6.01198782e-01 1.12788587e+70 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "1 -2.892740475020717e+144\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-1.4602431655937904e+139\n",
      "[1.00000000e+00 3.75479899e+06 9.64542072e+04 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 1.13768455e+06\n",
      " 1.33619800e+06 1.00000000e+00 1.00000000e+00 9.09424694e+04\n",
      " 1.00000000e+00 1.76210369e+71 1.00000000e+00 1.00000000e+00\n",
      " 1.30797411e+00 9.51575033e-01 9.51436912e-01 8.23758607e-01\n",
      " 5.96544835e-01 4.00433233e+43 8.22564466e-01 1.75682150e+08\n",
      " 3.28016674e+08 9.52113532e-01 5.98005309e-01 6.77570161e-01\n",
      " 6.01198782e-01 1.68919116e+71 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-2.7596968269741615e+175\n",
      "[1.00000000e+00 1.74838445e+08 1.72790374e+06 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 3.77698622e+07\n",
      " 4.30556588e+07 1.00000000e+00 1.00000000e+00 1.47486216e+06\n",
      " 1.00000000e+00 3.40037025e+88 1.00000000e+00 1.00000000e+00\n",
      " 1.40451095e+00 9.51575033e-01 9.51436912e-01 8.12608427e-01\n",
      " 5.96544835e-01 1.92737343e+54 7.95205777e-01 6.44265545e+09\n",
      " 4.42928013e+10 9.52113532e-01 5.98005309e-01 6.32078701e-01\n",
      " 6.01198782e-01 3.25966933e+88 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-1.0276633455396451e+210\n",
      "[1.00000000e+000 5.98032235e+009 2.66622814e+007 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 9.97418324e+008\n",
      " 1.38736156e+009 1.00000000e+000 1.00000000e+000 2.51387068e+007\n",
      " 1.00000000e+000 6.56176925e+105 1.00000000e+000 1.00000000e+000\n",
      " 1.50971899e+000 9.51575033e-001 9.51436912e-001 7.57382372e-001\n",
      " 5.96544835e-001 4.48684783e+064 7.55568399e-001 4.08392478e+011\n",
      " 4.22203954e+012 9.52113532e-001 5.98005309e-001 6.13411799e-001\n",
      " 6.01198782e-001 6.29025559e+105 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "2 -8.997373352678785e+215\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-4.5418360409434207e+210\n",
      "[1.00000000e+000 7.66935668e+009 3.15762579e+007 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 1.27912116e+009\n",
      " 1.67608391e+009 1.00000000e+000 1.00000000e+000 2.97718818e+007\n",
      " 1.00000000e+000 9.82730866e+106 1.00000000e+000 1.00000000e+000\n",
      " 1.50971899e+000 9.51575033e-001 9.51436912e-001 7.57382372e-001\n",
      " 5.96544835e-001 2.77174151e+065 7.55568399e-001 5.67130581e+011\n",
      " 6.34890192e+012 9.52113532e-001 5.98005309e-001 6.13411799e-001\n",
      " 6.01198782e-001 9.42067313e+106 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-8.583563892751895e+246\n",
      "[1.00000000e+000 3.57115894e+011 5.65664638e+008 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 4.24654003e+010\n",
      " 5.40076366e+010 1.00000000e+000 1.00000000e+000 4.82826300e+008\n",
      " 1.00000000e+000 1.89639736e+124 1.00000000e+000 1.00000000e+000\n",
      " 1.62474903e+000 9.51575033e-001 9.51436912e-001 7.48284652e-001\n",
      " 5.96544835e-001 1.33410029e+076 7.33282203e-001 2.07979409e+013\n",
      " 8.57305964e+014 9.52113532e-001 5.98005309e-001 6.01745759e-001\n",
      " 6.01198782e-001 1.81792801e+124 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-3.196370666683136e+281\n",
      "[1.00000000e+000 1.22150947e+013 8.72844328e+009 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 1.12141708e+012\n",
      " 1.74026181e+012 1.00000000e+000 1.00000000e+000 8.22966996e+009\n",
      " 1.00000000e+000 3.65951970e+141 1.00000000e+000 1.00000000e+000\n",
      " 1.74984525e+000 9.51575033e-001 9.51436912e-001 7.03901522e-001\n",
      " 5.96544835e-001 3.10573183e+086 7.01438698e-001 1.31835742e+015\n",
      " 8.17193669e+016 9.52113532e-001 5.98005309e-001 6.01692613e-001\n",
      " 6.01198782e-001 3.50809566e+141 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "3 -2.7984787417514873e+287\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-1.4126602410376465e+282\n",
      "[1.00000000e+000 1.56650282e+013 1.03371340e+010 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 1.43814113e+012\n",
      " 2.10242586e+012 1.00000000e+000 1.00000000e+000 9.74643457e+009\n",
      " 1.00000000e+000 5.48072147e+142 1.00000000e+000 1.00000000e+000\n",
      " 1.74984525e+000 9.51575033e-001 9.51436912e-001 7.03901522e-001\n",
      " 5.96544835e-001 1.91855979e+087 7.01438698e-001 1.83078987e+015\n",
      " 1.22885691e+017 9.52113532e-001 5.98005309e-001 6.01692613e-001\n",
      " 6.01198782e-001 5.25393954e+142 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/ipykernel_launcher.py:60: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "0 0\n",
      "4000  d  (0.3333333333333333, 0.5, 0.4, None)\n",
      "\n",
      "nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "0 0\n",
      "8000  d  (0.3333333333333333, 0.5, 0.4, None)\n",
      "4 nan\n",
      "0 0\n",
      "(0.3333333333333333, 0.5, 0.4, None)\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
