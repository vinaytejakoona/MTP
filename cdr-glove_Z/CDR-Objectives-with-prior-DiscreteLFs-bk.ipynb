{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "8272 888\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev_cands = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()\n",
    "print(len(train_cands),len(dev_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "# from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "# L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "# gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 592\n",
      "888\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "# print(len(gold_labels_dev),len(gold_labels_test))\n",
    "# print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "# print(len(gold_labels_dev))\n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(0))\n",
    "print(len(gold_labels_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from six.moves.cPickle import load\n",
    "\n",
    "with bz2.BZ2File('data/ctd.pkl.bz2', 'rb') as ctd_f:\n",
    "    ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "##### Discrete #########\n",
    "\n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "def LF_d_induced_by_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,1) if (\n",
    "        re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "def LF_d_treat_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_treat_d_wide(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "def LF_uncertain(c):\n",
    "    if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induced_other(c):\n",
    "    if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)  \n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "        return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_therapy(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "    LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "    LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "    LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "    LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "    LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "    LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "]\n",
    "\n",
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distanceCD(c):\n",
    "    dist = 0\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    return dist/5000\n",
    "\n",
    "\n",
    "def distanceCD_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{A}})(.*)('+w+r')(.*)({{B}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "def distanceDC_(c,l):\n",
    "    dist = []\n",
    "    for w in l:\n",
    "        pattern = r'({{B}})(.*)('+w+r')(.*)({{A}})'\n",
    "        matchObj = re.search(pattern, get_tagged_text(c), flags=re.I)\n",
    "        if(matchObj):\n",
    "            match_groups = matchObj.group(2,4)\n",
    "            dist.append(sum([len(mg) for mg in match_groups]))\n",
    "    if(len(dist)>0):\n",
    "        return min(dist)\n",
    "    return 0\n",
    "\n",
    "   \n",
    "\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "##### Smooth LFs #########\n",
    "\n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n",
    "\n",
    "\n",
    "def LF_in_ctd_unspecified(c):\n",
    "    if(cand_in_ctd_unspecified(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_therapy(c):\n",
    "    if(cand_in_ctd_therapy(c)==1):\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_in_ctd_marker(c):\n",
    "    if(cand_in_ctd_marker(c)==1):\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_tagged_text,\n",
    "    rule_regex_search_tagged_text,\n",
    "    rule_regex_search_btw_AB,\n",
    "    rule_regex_search_btw_BA,\n",
    "    rule_regex_search_before_A,\n",
    "    rule_regex_search_before_B,\n",
    ")\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "# List to parenthetical\n",
    "def ltp(x):\n",
    "    return '(' + '|'.join(x) + ')'\n",
    "\n",
    "# def LF_induce(c):\n",
    "#     return (1,1) if re.search(r'{{A}}.{0,20}induc.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "def LF_induce(c):\n",
    "    return (1,distanceCD_(c,['induc'])) if re.search(r'{{A}}.*induc.*{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "causal_past = ['induced', 'caused', 'due']\n",
    "# def LF_d_induced_by_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + '.{0,9}(by|to).{0,50}', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_between_tokens(c))\n",
    "    for w in causal_past:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    return (1,sc)\n",
    "\n",
    "# def LF_d_induced_by_c_tight(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_d_induced_by_c_tight(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.*' + ltp(causal_past) + ' (by|to) ', 1)==1):\n",
    "        return (1,(1-distanceDC_(c,causal_past)))\n",
    "    return (0,0)\n",
    "\n",
    "def LF_induce_name(c):\n",
    "    return (1,1) if 'induc' in c.chemical.get_span().lower() else (0,0)     \n",
    "\n",
    "causal = ['cause[sd]?', 'induce[sd]?', 'associated with']\n",
    "# def LF_c_cause_d(c):\n",
    "#     return (1,1) if (\n",
    "#         re.search(r'{{A}}.{0,50} ' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#         and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "#     ) else (0,0)\n",
    "\n",
    "\n",
    "def LF_c_cause_d(c):\n",
    "    return (1,(1-distanceCD_(c,causal))) if (\n",
    "        re.search(r'{{A}}.* ' + ltp(causal) + '.*{{B}}', get_tagged_text(c), re.I)\n",
    "        and not re.search('{{A}}.{0,50}(not|no).{0,20}' + ltp(causal) + '.{0,50}{{B}}', get_tagged_text(c), re.I)\n",
    "    ) else (0,0)\n",
    "\n",
    "\n",
    "treat = ['treat', 'effective', 'prevent', 'resistant', 'slow', 'promise', 'therap']\n",
    "# def LF_d_treat_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_d_treat_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1-distanceDC_(c,treat))\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "# def LF_c_treat_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_c_treat_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,50}' + ltp(treat) + '.{0,50}', -1)==-1):\n",
    "        return (-1,1-distanceCD_(c,treat))\n",
    "    return (0,0)\n",
    "\n",
    "# def LF_treat_d(c):\n",
    "#     if (rule_regex_search_before_B(c, ltp(treat) + '.{0,50}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_treat_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "    for w in treat:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "\n",
    "# def LF_c_treat_d_wide(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_c_treat_d_wide(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{0,200}' + ltp(treat) + '.{0,200}', -1)==-1):\n",
    "        return (-1,1-distanceCD_(c,treat))\n",
    "    return (0,0)\n",
    "\n",
    "def LF_c_d(c):\n",
    "    return (1,1) if ('{{A}} {{B}}' in get_tagged_text(c)) else (0,0)\n",
    "\n",
    "def LF_c_induced_d(c):\n",
    "    return (1,1) if (\n",
    "        ('{{A}} {{B}}' in get_tagged_text(c)) and \n",
    "        (('-induc' in c[0].get_span().lower()) or ('-assoc' in c[0].get_span().lower()))\n",
    "        ) else (0,0)\n",
    "\n",
    "# def LF_improve_before_disease(c):\n",
    "#     if(rule_regex_search_before_B(c, 'improv.*', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "def distanceImproveBeforeDisease(c):\n",
    "    m=re.search(r'(improv)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "    if(m):\n",
    "        return len(m.group(2))/5000\n",
    "    return 0\n",
    "\n",
    "\n",
    "def LF_improve_before_disease(c):\n",
    "    if(rule_regex_search_before_B(c, 'improv.*', -1) == -1):\n",
    "        return (-1,1-distanceImproveBeforeDisease(c))\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "\n",
    "pat_terms = ['in a patient with ', 'in patients with']\n",
    "def LF_in_patient_with(c):\n",
    "    return (-1,1) if re.search(ltp(pat_terms) + '{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "uncertain = ['combin', 'possible', 'unlikely']\n",
    "# def LF_uncertain(c):\n",
    "#     if (rule_regex_search_before_A(c, ltp(uncertain) + '.*', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_uncertain(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "    for w in uncertain:\n",
    "        sc=max(sc,get_similarity(word_vectors,w))\n",
    "    if(re.search('(not|no|none) .* {{B}}', get_tagged_text(c), re.I)):\n",
    "        return (0,0)\n",
    "    else:\n",
    "        return (-1,sc)\n",
    "\n",
    "# def LF_induced_other(c):\n",
    "#     if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)  \n",
    "\n",
    "def LF_induced_other(c):\n",
    "    if (rule_regex_search_tagged_text(c, '{{A}}.{20,1000}-induced {{B}}', -1)==-1):\n",
    "        return (-1,distanceCD(c))\n",
    "    return (0,0)  \n",
    "\n",
    "# def LF_far_c_d(c):\n",
    "#     if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_far_c_d(c):\n",
    "    if (rule_regex_search_btw_AB(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,distanceCD(c))\n",
    "    return (0,0)\n",
    "\n",
    "# def LF_far_d_c(c):\n",
    "#     if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "#         return (-1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "def LF_far_d_c(c):\n",
    "    if (rule_regex_search_btw_BA(c, '.{100,5000}', -1)==-1):\n",
    "        return (-1,distanceCD(c))\n",
    "    return (0,0)\n",
    "\n",
    "#without deps\n",
    "gen_model.weights.lf_accuracy\n",
    "# def LF_risk_d(c):\n",
    "#     if (rule_regex_search_before_B(c, 'risk of ', 1)==1):\n",
    "#         return (1,1)\n",
    "#     return (0,0)\n",
    "\n",
    "\n",
    "def LF_risk_d(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(get_left_tokens(c[1],7))\n",
    "    sc=max(sc,get_similarity(word_vectors,'risk'))\n",
    "    return (1,sc)\n",
    "\n",
    "\n",
    "# def LF_develop_d_following_c(c):\n",
    "#     return (1,1) if re.search(r'develop.{0,25}{{B}}.{0,25}following.{0,25}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceDevFol(c):\n",
    "    dist = 0\n",
    "    matchObj = re.search(r'(develop)(.*)({{B}})(.*)(following)(.*)({{A}})', get_tagged_text(c), flags=re.I)\n",
    "    if(matchObj):\n",
    "        match_groups = matchObj.group(2,4,6)\n",
    "        dist = sum([len(mg) for mg in match_groups])\n",
    "    return dist/5000\n",
    "\n",
    "def LF_develop_d_following_c(c):\n",
    "    return (1,1-distanceDevFol(c)) if re.search(r'develop.*{{B}}.*following.*{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "procedure, following = ['inject', 'administrat'], ['following']\n",
    "# def LF_d_following_c(c):\n",
    "#     return (1,distanceDFollC(c)) if re.search('{{B}}.{0,50}' + ltp(following) + '.{0,20}{{A}}.{0,50}' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_d_following_c(c):\n",
    "    return (1,1-distanceDC_(c,following)) if re.search('{{B}}.*' + ltp(following) + '.*{{A}}.*' + ltp(procedure), get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_measure(c):\n",
    "#     return (-1,1) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceMeasureA(c):\n",
    "    m = re.search('(measur)(.*)({{A}})', get_tagged_text(c), flags=re.I) \n",
    "    if(m):\n",
    "        return (5000-len(m.group(2)))/5000\n",
    "    return 0\n",
    "\n",
    "def LF_measure(c):\n",
    "    return (-1,distanceMeasureA(c)) if re.search('measur.{0,75}{{A}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "# def LF_level(c):\n",
    "#     return (-1,1) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceLevel(c):\n",
    "    m = re.search('({{A}})(.*)(level)', get_tagged_text(c), flags=re.I)\n",
    "    if(m):\n",
    "        return (5000-len(m.group(2)))/5000\n",
    "    return 0\n",
    "\n",
    "def LF_level(c):\n",
    "    return (-1,distanceLevel(c)) if re.search('{{A}}.{0,25} level', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "# def LF_neg_d(c):\n",
    "#     return (-1,1) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def distanceNeg(c):\n",
    "    m = re.search('(none|not|no)(.*)({{B}})', get_tagged_text(c), flags=re.I)\n",
    "    if(m):\n",
    "        return (5000-len(m.group(2)))/5000\n",
    "    return 0\n",
    "\n",
    "\n",
    "def LF_neg_d(c):\n",
    "    return (-1,distanceNeg(c)) if re.search('(none|not|no) .{0,25}{{B}}', get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return (-1,1) if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else (0,0)\n",
    "\n",
    "\n",
    "def LF_ctd_marker_c_d(c):\n",
    "    l,s = LF_c_d(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_marker_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_marker(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "def LF_ctd_therapy_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_therapy(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_treat(c):\n",
    "    l,s = LF_c_treat_d_wide(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return (l*cl,s*cl)\n",
    "\n",
    "def LF_ctd_unspecified_induce(c):\n",
    "    l1,s1 = LF_c_induced_d(c)\n",
    "    l2,s2 = LF_d_induced_by_c_tight(c)\n",
    "    cl = cand_in_ctd_unspecified(c)\n",
    "    return ((l1 or l2)*cl,max(s1,s2)*cl)\n",
    "\n",
    "\n",
    "# def LF_ctd_marker_c_d(c):\n",
    "#     return LF_c_d(c) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_marker_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_marker(c)\n",
    "\n",
    "# def LF_ctd_therapy_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_therapy(c)\n",
    "\n",
    "# def LF_ctd_unspecified_treat(c):\n",
    "#     return LF_c_treat_d_wide(c) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "# def LF_ctd_unspecified_induce(c):\n",
    "#     return (LF_c_induced_d(c) or LF_d_induced_by_c_tight(c)) * cand_in_ctd_unspecified(c)\n",
    "\n",
    "def LF_closer_chem(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical closer than @dist/2 in either direction\n",
    "    sent = c.get_parent()\n",
    "    closest_other_chem = float('inf')\n",
    "    for i in range(dis_end, min(len(sent.words), dis_end + dist // 2)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, dis_start - dist // 2), dis_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Chemical' and cid != sent.entity_cids[chem_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_closer_dis(c):\n",
    "    # Get distance between chemical and disease\n",
    "    chem_start, chem_end = c.chemical.get_word_start(), c.chemical.get_word_end()\n",
    "    dis_start, dis_end = c.disease.get_word_start(), c.disease.get_word_end()\n",
    "    if dis_start < chem_start:\n",
    "        dist = chem_start - dis_end\n",
    "    else:\n",
    "        dist = dis_start - chem_end\n",
    "    # Try to find chemical disease than @dist/8 in either direction\n",
    "    sent = c.get_parent()\n",
    "    for i in range(chem_end, min(len(sent.words), chem_end + dist // 8)):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    for i in range(max(0, chem_start - dist // 8), chem_start):\n",
    "        et, cid = sent.entity_types[i], sent.entity_cids[i]\n",
    "        if et == 'Disease' and cid != sent.entity_cids[dis_start]:\n",
    "            return (-1,1)\n",
    "    return (0,0)\n",
    "\n",
    "\n",
    "LFs = [LF_c_cause_d,LF_c_d,LF_c_induced_d,LF_c_treat_d,LF_c_treat_d_wide,LF_closer_chem,\n",
    "    LF_closer_dis,LF_ctd_marker_c_d,LF_ctd_marker_induce,LF_ctd_therapy_treat,\n",
    "    LF_ctd_unspecified_treat,LF_ctd_unspecified_induce,LF_d_following_c,\n",
    "    LF_d_induced_by_c,LF_d_induced_by_c_tight,LF_d_treat_c,LF_develop_d_following_c,\n",
    "    LF_far_c_d,LF_far_d_c,LF_improve_before_disease,LF_in_ctd_therapy,\n",
    "    LF_in_ctd_marker,LF_in_patient_with,LF_induce,LF_induce_name,LF_induced_other,\n",
    "    LF_level,LF_measure,LF_neg_d,LF_risk_d,LF_treat_d,LF_uncertain,LF_weak_assertions\n",
    "]\n",
    "\n",
    "LF_l = [\n",
    "    1,1,1,-1,-1,-1,\n",
    "    -1,1,1,-1,\n",
    "    -1,1,1,\n",
    "    1,1,-1,1,\n",
    "    -1,-1,-1,-1,\n",
    "    1,-1,1,1,-1,\n",
    "    -1,-1,-1,1,-1,-1,-1\n",
    "]\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for i,ci in enumerate(cands):\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "        if(i%500==0 and i!=0):\n",
    "            print(str(i)+'data points labelled in',(time.time() - start_time)/60,'mins')\n",
    "    return L_S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 14-5-2018, 12:21:24\n",
      "500data points labelled in 0.1373719533284505 mins\n",
      "500data points labelled in 0.3836975653966268 mins\n",
      "1000data points labelled in 0.46697951952616373 mins\n",
      "1500data points labelled in 0.5392428477605183 mins\n",
      "2000data points labelled in 0.6080347259839376 mins\n",
      "2500data points labelled in 0.6639169692993164 mins\n",
      "3000data points labelled in 0.7215169946352641 mins\n",
      "3500data points labelled in 0.7739200472831727 mins\n",
      "4000data points labelled in 0.8187049786249797 mins\n",
      "4500data points labelled in 0.8593282024065654 mins\n",
      "5000data points labelled in 0.902047598361969 mins\n",
      "5500data points labelled in 0.9426584959030151 mins\n",
      "6000data points labelled in 1.0017528692881266 mins\n",
      "6500data points labelled in 1.036082379023234 mins\n",
      "7000data points labelled in 1.070180348555247 mins\n",
      "7500data points labelled in 1.1017404516537985 mins\n",
      "8000data points labelled in 1.1367191394170126 mins\n",
      "time taken:  0:01:09.329678\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "\n",
    "lt = time.localtime()\n",
    "\n",
    "print(\"started at: {}-{}-{}, {}:{}:{}\".format(lt.tm_mday,lt.tm_mon,lt.tm_year,lt.tm_hour,lt.tm_min,lt.tm_sec))\n",
    "\n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "np.save(\"dev_L_S_discrete\",np.array(dev_L_S))\n",
    "\n",
    "# np.save(\"dev_L_S_smooth\",np.array(dev_L_S))\n",
    "\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "np.save(\"train_L_S_discrete\",np.array(train_L_S))\n",
    "\n",
    "# np.save(\"train_L_S_smooth\",np.array(train_L_S))\n",
    "print(\"time taken: \",str(datetime.timedelta(seconds=(time.time() - start_time))))\n",
    "\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "    print(confusion_matrix(gold_labels_dev,pl))\n",
    "    draw2DArray(confusion_matrix(gold_labels_dev,pl))\n",
    "    return report2dict(classification_report(gold_labels_dev, pl))# target_names=class_names))\n",
    "    \n",
    "\n",
    "def drawLossVsF1(y_loss,x_f1s,text,title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x_f1s, y_loss)\n",
    "\n",
    "    plt.xlabel('f1-score')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(title)\n",
    "    \n",
    "    for i, txt in enumerate(text):\n",
    "        ax.annotate(txt, (x_f1s[i],y_loss[i]))    \n",
    "    \n",
    "    plt.savefig(title+\".png\")\n",
    "    \n",
    "def drawPRcurve(y_test,y_score,it_no):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    splt = fig.add_subplot(111)\n",
    "    \n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_score,pos_label=1)\n",
    "\n",
    "    splt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    splt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "#     print(\"thresholds\",thresholds,len(thresholds))\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.title('{0:d} Precision-Recall curve: AP={1:0.2f}'.format(it_no,\n",
    "              average_precision))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 2, 33) (8272, 2, 33)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dev_L_S = np.load(\"dev_L_S_discrete.npy\")\n",
    "train_L_S = np.load(\"train_L_S_discrete.npy\")\n",
    "\n",
    "# dev_L_S = np.load(\"dev_L_S_smooth.npy\")\n",
    "# train_L_S = np.load(\"train_L_S_smooth.npy\")\n",
    "print(dev_L_S.shape,train_L_S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this only once for a kernel startup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33\n"
     ]
    }
   ],
   "source": [
    "NoOfLFs= len(LFs)\n",
    "NoOfClasses = 2\n",
    "print(len(LFs),len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## normalized training with different params\n",
    "\n",
    "def train_nl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000e80>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000e80>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000e80>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 222251.99457482004\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.19735832 0.89093251 1.08341589 0.90657515 1.01595663 0.95712761\n",
      "  1.0718031  1.10396375 1.21637515 0.97085348 0.89072741 1.13944674\n",
      "  1.07750003 1.15783256 1.12498591 0.94512725 0.94895473 0.83027009\n",
      "  0.9660881  0.73811549 0.95577461 1.00048532 0.89772938 1.24737166\n",
      "  0.94662978 1.08218756 0.97002825 0.97021497 0.96005081 1.08451997\n",
      "  0.72511578 0.7414887  0.87067795]]\n",
      "{0: 497, 1: 391}\n",
      "(0.5626598465473146, 0.7432432432432432, 0.6404657933042213, None)\n",
      "\n",
      "1 loss 212826.0550733279\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.19986342 0.92684934 1.10601022 0.82347062 0.93928867 0.89076637\n",
      "  0.98121448 1.12834962 1.23522837 0.88800828 0.81018832 1.15861665\n",
      "  1.05435969 1.16556198 1.12735123 0.85559547 0.94537935 0.75567467\n",
      "  0.88317277 0.64689519 0.89506406 1.03723377 0.80560614 1.23698994\n",
      "  0.97796818 0.99574693 0.87851571 0.87807301 0.86969867 1.07134332\n",
      "  0.63686953 0.6532503  0.79151595]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "2 loss 208341.68852991555\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.12216561 0.90307919 1.06229464 0.75718361 0.88462406 0.85359606\n",
      "  0.89414176 1.09052303 1.19174484 0.82130057 0.74916439 1.11115252\n",
      "  0.9469431  1.09395703 1.04425275 0.77178695 0.83727233 0.70600894\n",
      "  0.81558615 0.55726965 0.8621418  1.04570765 0.71383022 1.1483844\n",
      "  0.95177768 0.91930221 0.78881497 0.78653747 0.78328761 0.96787395\n",
      "  0.55766455 0.57396648 0.73156854]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "3 loss 205131.39371999548\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.03760964 0.85929508 1.00249919 0.70928398 0.84854295 0.83543432\n",
      "  0.81105673 1.03597741 1.13310701 0.77166386 0.70767827 1.04868021\n",
      "  0.84604031 1.01329413 0.95615575 0.69585316 0.73656994 0.67714538\n",
      "  0.76385563 0.46866481 0.84516941 1.0423398  0.62093558 1.05706592\n",
      "  0.90698064 0.85408755 0.70075811 0.69471109 0.70199891 0.868642\n",
      "  0.49333011 0.50930535 0.68872922]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "4 loss 202398.267004147\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.95343364 0.81297596 0.94149116 0.67107614 0.82024536 0.82344968\n",
      "  0.7314926  0.97978353 1.07312539 0.73116397 0.67555079 0.98518327\n",
      "  0.74704113 0.93282672 0.8690693  0.62677984 0.63799974 0.65640189\n",
      "  0.72024458 0.38284805 0.83311145 1.03645349 0.52764139 0.96662658\n",
      "  0.85969069 0.79605945 0.61487888 0.60353434 0.62579174 0.77113025\n",
      "  0.44161321 0.45694962 0.65350723]]\n",
      "{0: 493, 1: 395}\n",
      "(0.5569620253164557, 0.7432432432432432, 0.6367583212735166, None)\n",
      "\n",
      "5 loss 200006.34934833934\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.87080905 0.76867401 0.88273641 0.63637229 0.79399378 0.81213849\n",
      "  0.65463977 0.92563387 1.0151024  0.69419624 0.64634662 0.9238756\n",
      "  0.64880058 0.85423024 0.78390677 0.56226975 0.54036065 0.63711839\n",
      "  0.67928514 0.30307705 0.82152169 1.03142305 0.43562755 0.87743914\n",
      "  0.81419941 0.7416082  0.5316332  0.51428645 0.5536968  0.67449719\n",
      "  0.39658045 0.41120497 0.6204228 ]]\n",
      "{0: 493, 1: 395}\n",
      "(0.5569620253164557, 0.7432432432432432, 0.6367583212735166, None)\n",
      "\n",
      "6 loss 197924.81181714358\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.79029325 0.72761936 0.82730654 0.60305026 0.76801791 0.79987141\n",
      "  0.58025714 0.87460765 0.96003419 0.65883265 0.61797256 0.86577804\n",
      "  0.55106931 0.77824706 0.70129491 0.50126412 0.44345544 0.61727124\n",
      "  0.63907382 0.23326782 0.809081   1.02829399 0.34666309 0.78982277\n",
      "  0.77171379 0.68933577 0.45157826 0.42810064 0.48534462 0.57863892\n",
      "  0.35525716 0.36932064 0.58770481]]\n",
      "{0: 492, 1: 396}\n",
      "(0.5606060606060606, 0.75, 0.6416184971098265, None)\n",
      "\n",
      "7 loss 196138.18967765084\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.71243402 0.69022101 0.77570798 0.57039136 0.74175727 0.78609865\n",
      "  0.50842407 0.82715596 0.90837336 0.62441922 0.58974907 0.81140328\n",
      "  0.45391788 0.70552238 0.62197376 0.44338522 0.34750283 0.59623675\n",
      "  0.59895385 0.17707439 0.79532617 1.02750849 0.26322902 0.70423621\n",
      "  0.73268521 0.63871963 0.37544158 0.34630485 0.42075576 0.48378558\n",
      "  0.31638542 0.33012814 0.55476788]]\n",
      "{0: 487, 1: 401}\n",
      "(0.5635910224438903, 0.7635135135135135, 0.6484935437589671, None)\n",
      "\n",
      "8 loss 194630.1233630186\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.63785697 0.65662807 0.72825318 0.53808522 0.7149826  0.77054947\n",
      "  0.43938978 0.7835252  0.86039594 0.59067397 0.56139577 0.76109105\n",
      "  0.35757901 0.63672811 0.54683316 0.38847922 0.25307253 0.57377516\n",
      "  0.55863873 0.13492767 0.78001578 1.02936776 0.18939979 0.62129029\n",
      "  0.69733438 0.58950695 0.30417311 0.2706795  0.36007292 0.39042567\n",
      "  0.27932106 0.29300698 0.52136614]]\n",
      "{0: 486, 1: 402}\n",
      "(0.5646766169154229, 0.7668918918918919, 0.6504297994269341, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 loss 193380.9969215407\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.56724157 0.62679623 0.68507462 0.50606059 0.68765004 0.75310817\n",
      "  0.37362748 0.74379033 0.81622303 0.55753435 0.5328595  0.71501386\n",
      "  0.26250493 0.57251679 0.47685315 0.33658955 0.1614721  0.54986543\n",
      "  0.51807673 0.1032013  0.76303524 1.0340914  0.13029139 0.54175207\n",
      "  0.66570719 0.54163415 0.23903025 0.2034868  0.30357179 0.29946105\n",
      "  0.24385042 0.25772627 0.48746065]]\n",
      "{0: 478, 1: 410}\n",
      "(0.5609756097560976, 0.777027027027027, 0.6515580736543909, None)\n",
      "\n",
      "10 loss 192367.70312715552\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.50124602 0.60048716 0.64609239 0.47445537 0.65987473 0.73380082\n",
      "  0.31192263 0.70783536 0.77579543 0.52512738 0.50428134 0.67313851\n",
      "  0.16964534 0.51341579 0.41295428 0.28800699 0.07624489 0.52467811\n",
      "  0.4774393  0.07827164 0.74438737 1.04180889 0.08847536 0.46651983\n",
      "  0.63766336 0.49523326 0.18160002 0.14715073 0.25172364 0.21262278\n",
      "  0.2101646  0.22442884 0.45320326]]\n",
      "{0: 469, 1: 419}\n",
      "(0.5560859188544153, 0.7871621621621622, 0.6517482517482518, None)\n",
      "\n",
      "11 loss 191563.6786683813\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.44028936 0.57718802 0.61091095 0.44372394 0.63201646 0.71288822\n",
      "  0.25561593 0.67526236 0.7387803  0.49386259 0.47609559 0.63511813\n",
      "  0.08132376 0.45959779 0.35565253 0.24348217 0.0077669  0.49867528\n",
      "  0.43724449 0.05992174 0.72426421 1.05247742 0.06266945 0.39643701\n",
      "  0.61278542 0.45076675 0.13399601 0.10392874 0.20543852 0.13317865\n",
      "  0.17905144 0.19380651 0.41903458]]\n",
      "{0: 469, 1: 419}\n",
      "(0.5560859188544153, 0.7871621621621622, 0.6517482517482518, None)\n",
      "\n",
      "12 loss 190937.37802831345\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.38427503  0.55606757  0.5787581   0.41472131  0.60475255  0.69095155\n",
      "   0.20679881  0.64533702  0.70451129  0.46450239  0.44911198  0.60022493\n",
      "   0.00457325  0.41065202  0.30466849  0.20437097 -0.02905698  0.47269329\n",
      "   0.3984603   0.05061968  0.70311078  1.06579564  0.05091148  0.33195278\n",
      "   0.59031971  0.40914501  0.09890356  0.07563602  0.16624716  0.06624673\n",
      "   0.15200531  0.16719583  0.38575538]]\n",
      "{0: 469, 1: 419}\n",
      "(0.5560859188544153, 0.7871621621621622, 0.6517482517482518, None)\n",
      "\n",
      "13 loss 190449.69180167522\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.33281826  0.53633853  0.54884776  0.38824665  0.5787279   0.6685854\n",
      "   0.16740356  0.61732839  0.67230824  0.43773888  0.42408793  0.56770203\n",
      "  -0.04156786  0.36590823  0.25923197  0.17185446 -0.043103    0.44754002\n",
      "   0.36205666  0.05012579  0.68137806  1.08139063  0.0499691   0.27316655\n",
      "   0.56951917  0.37127821  0.07755876  0.06144032  0.13536331  0.0165302\n",
      "   0.1302695   0.14570204  0.35413594]]\n",
      "{0: 449, 1: 439}\n",
      "(0.5603644646924829, 0.831081081081081, 0.6693877551020407, None)\n",
      "\n",
      "14 loss 190061.11191754616\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.28575421  0.51761933  0.5207703   0.36454638  0.55417525  0.64604823\n",
      "   0.13772734  0.5908674   0.64182507  0.41374798  0.40126751  0.53715197\n",
      "  -0.05536051  0.32497394  0.21877467  0.14599839 -0.05084094  0.423559\n",
      "   0.32848078  0.05423492  0.65924925  1.0990666   0.05418686  0.22025056\n",
      "   0.55000554  0.33751599  0.06750857  0.05696271  0.11251414 -0.01585396\n",
      "   0.11384963  0.12932497  0.32449617]]\n",
      "{0: 433, 1: 455}\n",
      "(0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.28575421  0.51761933  0.5207703   0.36454638  0.55417525  0.64604823\n",
      "   0.13772734  0.5908674   0.64182507  0.41374798  0.40126751  0.53715197\n",
      "  -0.05536051  0.32497394  0.21877467  0.14599839 -0.05084094  0.423559\n",
      "   0.32848078  0.05423492  0.65924925  1.0990666   0.05418686  0.22025056\n",
      "   0.55000554  0.33751599  0.06750857  0.05696271  0.11251414 -0.01585396\n",
      "   0.11384963  0.12932497  0.32449617]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.851\n",
      "Neg. class accuracy: 0.657\n",
      "Precision            0.554\n",
      "Recall               0.851\n",
      "F1                   0.671\n",
      "----------------------------------------\n",
      "TP: 252 | FP: 203 | TN: 389 | FN: 44\n",
      "========================================\n",
      "\n",
      "{0: 433, 1: 455}\n",
      "acc 0.7218468468468469\n",
      "(array([0.89838337, 0.55384615]), array([0.65709459, 0.85135135]), array([0.75902439, 0.67110519]), array([592, 296]))\n",
      "(0.7261147628353171, 0.754222972972973, 0.7150647916599007, None)\n",
      "[[389 203]\n",
      " [ 44 252]]\n",
      "prec: tp/(tp+fp) 0.5538461538461539 recall: tp/(tp+fn) 0.8513513513513513\n",
      "(0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None)\n"
     ]
    }
   ],
   "source": [
    "train_nl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000b00>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000b00>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31f000b00>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 184222.4562227816\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.27082211 -0.01928787 -0.14063245  0.37856253  0.43681819 -0.15844807\n",
      "   0.13280198 -0.01935702 -0.10775934  0.34390113  0.39762823 -0.14286955\n",
      "  -0.39588527 -0.33699178 -0.37821404  0.38378715 -0.39537146  0.11504936\n",
      "   0.21906794  0.39699417 -0.27113816  8.13838832  0.39548336 -0.31328908\n",
      "   0.25503373  0.28019293  0.39734506  0.39700564  0.37866251 -0.39156514\n",
      "   0.38002959  0.21917987  0.13605525]]\n",
      "{0: 253, 1: 635}\n",
      "(0.462992125984252, 0.9932432432432432, 0.6315789473684211, None)\n",
      "\n",
      "1 loss 182926.8350570609\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.32930229 -0.072816   -0.19433839  0.43560275  0.51398692 -0.0365946\n",
      "   0.19811178 -0.0729731  -0.16134781  0.40620045  0.46910414 -0.1965869\n",
      "  -0.45399143 -0.39169289 -0.43424009  0.43969335 -0.45314662  0.20099496\n",
      "   0.27412612  0.45449307 -0.13677994 10.2419757   0.45333961 -0.37154561\n",
      "   0.1955587   0.33629475  0.4545094   0.45440313  0.43567656 -0.4480074\n",
      "   0.43491147  0.28277911  0.20811263]]\n",
      "{0: 290, 1: 598}\n",
      "(0.4916387959866221, 0.9932432432432432, 0.6577181208053691, None)\n",
      "\n",
      "2 loss 182800.53538895075\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-3.50033750e-01 -9.20612978e-02 -2.13641010e-01  4.56135636e-01\n",
      "   5.39595897e-01  1.96766814e-03  2.20784600e-01 -9.22805491e-02\n",
      "  -1.80658821e-01  4.27914802e-01  4.93027965e-01 -2.15923034e-01\n",
      "  -4.74703809e-01 -4.11355405e-01 -4.54355973e-01  4.59505852e-01\n",
      "  -4.73775237e-01  2.29612258e-01  2.93641733e-01  4.75037473e-01\n",
      "  -9.49920742e-02  1.09749630e+01  4.73964547e-01 -3.92341571e-01\n",
      "   1.74156761e-01  3.56763707e-01  4.74947673e-01  4.74935290e-01\n",
      "   4.56160538e-01 -4.68232180e-01  4.54509106e-01  3.04807646e-01\n",
      "   2.33081953e-01]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "3 loss 182779.13781656895\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35716556 -0.09869963 -0.22030067  0.46322675  0.54827254  0.01475931\n",
      "   0.2285349  -0.09894358 -0.18732523  0.43535596  0.50115104 -0.22259733\n",
      "  -0.48184533 -0.41814551 -0.46130134  0.46632557 -0.48089044  0.2392675\n",
      "   0.30036809  0.48212319 -0.08115096 11.22596956  0.48107634 -0.39950962\n",
      "   0.16677415  0.36384845  0.48199793  0.48202006  0.46323675 -0.47521353\n",
      "   0.46127745  0.31235554  0.24160462]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "4 loss 182774.49158656417\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.35959836 -0.10096576 -0.22257433  0.46564861  0.55121907  0.01907076\n",
      "   0.23117355 -0.1012185  -0.18960156  0.43789171  0.5039114  -0.22487633\n",
      "  -0.48428336 -0.42046459 -0.46367331  0.4686526  -0.48331976  0.24254067\n",
      "   0.30266447  0.48454236 -0.07648666 11.3114723   0.48350429 -0.40195631\n",
      "   0.16425398  0.36626946  0.48440509  0.48443927  0.4656539  -0.47759769\n",
      "   0.46358955  0.31492745  0.24450427]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "5 loss 182773.22108715939\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36042604 -0.10173693 -0.22334809  0.46647291  0.55222009  0.02053171\n",
      "   0.2320707  -0.10199269 -0.19037627  0.43875415  0.50484937 -0.22565194\n",
      "  -0.48511306 -0.42125391 -0.46448062  0.4694444  -0.48414652  0.24365198\n",
      "   0.30344598  0.48536566 -0.07490621 11.34054866  0.48433056 -0.4027889\n",
      "   0.16339637  0.3670936   0.48522431  0.48526262  0.46647665 -0.47840914\n",
      "   0.46437656  0.31580217  0.24548993]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "6 loss 182772.82518048465\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.3607074  -0.10199908 -0.22361113  0.46675314  0.55256019  0.02102764\n",
      "   0.23237559 -0.10225588 -0.19063964  0.43904729  0.50516807 -0.22591562\n",
      "  -0.48539512 -0.42152225 -0.46475509  0.46971356 -0.48442758  0.24402947\n",
      "   0.30371166  0.48564555 -0.07436971 11.3504309   0.48461146 -0.40307194\n",
      "   0.16310482  0.3673738   0.48550281  0.48554253  0.46675636 -0.478685\n",
      "   0.46464413  0.31609947  0.24582487]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "7 loss 182772.69479950634\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36080301 -0.10208818 -0.22370052  0.46684838  0.55267575  0.0211961\n",
      "   0.2324792  -0.10234532 -0.19072914  0.4391469   0.50527636 -0.22600522\n",
      "  -0.48549097 -0.42161345 -0.46484836  0.46980503 -0.4845231   0.24415773\n",
      "   0.30380195  0.48574066 -0.07418748 11.35378896  0.48470692 -0.40316812\n",
      "   0.16300575  0.36746903  0.48559746  0.48563766  0.46685142 -0.47877875\n",
      "   0.46473506  0.3162005   0.24593869]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss 182772.6509773922\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36083549 -0.10211845 -0.22373089  0.46688074  0.55271501  0.02125333\n",
      "   0.2325144  -0.10237572 -0.19075956  0.43918074  0.50531316 -0.22603567\n",
      "  -0.48552354 -0.42164444 -0.46488005  0.46983611 -0.48455556  0.2442013\n",
      "   0.30383263  0.48577298 -0.07412557 11.35492999  0.48473936 -0.40320081\n",
      "   0.16297208  0.36750138  0.48562962  0.48566998  0.46688372 -0.47881061\n",
      "   0.46476596  0.31623483  0.24597736]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "9 loss 182772.63614294853\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36084653 -0.10212873 -0.22374122  0.46689174  0.55272836  0.02127277\n",
      "   0.23252636 -0.10238604 -0.19076989  0.43919224  0.50532566 -0.22604602\n",
      "  -0.48553461 -0.42165497 -0.46489082  0.46984667 -0.48456658  0.24421611\n",
      "   0.30384305  0.48578396 -0.07410453 11.35531768  0.48475038 -0.40321191\n",
      "   0.16296064  0.36751238  0.48564055  0.48568096  0.4668947  -0.47882143\n",
      "   0.46477646  0.31624649  0.2459905 ]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "10 loss 182772.63110895432\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085028 -0.10213223 -0.22374472  0.46689547  0.55273289  0.02127938\n",
      "   0.23253043 -0.10238955 -0.1907734   0.43919615  0.50532991 -0.22604953\n",
      "  -0.48553837 -0.42165854 -0.46489448  0.46985026 -0.48457033  0.24422114\n",
      "   0.30384659  0.48578769 -0.07409739 11.35544941  0.48475412 -0.40321569\n",
      "   0.16295676  0.36751611  0.48564426  0.48568469  0.46689843 -0.47882511\n",
      "   0.46478003  0.31625046  0.24599496]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "11 loss 182772.62939926746\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085156 -0.10213342 -0.22374591  0.46689674  0.55273443  0.02128162\n",
      "   0.23253181 -0.10239074 -0.19077459  0.43919748  0.50533135 -0.22605073\n",
      "  -0.48553965 -0.42165976 -0.46489572  0.46985148 -0.4845716   0.24422285\n",
      "   0.3038478   0.48578896 -0.07409496 11.35549417  0.48475539 -0.40321697\n",
      "   0.16295544  0.36751738  0.48564552  0.48568596  0.4668997  -0.47882636\n",
      "   0.46478124  0.3162518   0.24599648]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "12 loss 182772.62881844505\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085199 -0.10213382 -0.22374632  0.46689717  0.55273495  0.02128239\n",
      "   0.23253228 -0.10239115 -0.190775    0.43919793  0.50533184 -0.22605113\n",
      "  -0.48554008 -0.42166017 -0.46489615  0.4698519  -0.48457204  0.24422343\n",
      "   0.3038482   0.48578939 -0.07409413 11.35550938  0.48475583 -0.4032174\n",
      "   0.16295499  0.36751782  0.48564595  0.48568639  0.46690013 -0.47882679\n",
      "   0.46478165  0.31625226  0.24599699]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "13 loss 182772.62862110534\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085214 -0.10213396 -0.22374646  0.46689732  0.55273513  0.02128265\n",
      "   0.23253244 -0.10239129 -0.19077514  0.43919808  0.50533201 -0.22605127\n",
      "  -0.48554023 -0.42166031 -0.46489629  0.46985204 -0.48457218  0.24422362\n",
      "   0.30384834  0.48578954 -0.07409385 11.35551454  0.48475597 -0.40321755\n",
      "   0.16295483  0.36751796  0.4856461   0.48568654  0.46690027 -0.47882693\n",
      "   0.46478179  0.31625242  0.24599717]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "14 loss 182772.62855405567\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085219 -0.102134   -0.2237465   0.46689737  0.55273519  0.02128273\n",
      "   0.23253249 -0.10239133 -0.19077518  0.43919814  0.50533206 -0.22605132\n",
      "  -0.48554028 -0.42166036 -0.46489634  0.46985208 -0.48457223  0.24422369\n",
      "   0.30384839  0.48578959 -0.07409376 11.3555163   0.48475602 -0.4032176\n",
      "   0.16295478  0.36751801  0.48564615  0.48568659  0.46690032 -0.47882698\n",
      "   0.46478184  0.31625247  0.24599723]]\n",
      "{0: 305, 1: 583}\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[-0.36085219 -0.102134   -0.2237465   0.46689737  0.55273519  0.02128273\n",
      "   0.23253249 -0.10239133 -0.19077518  0.43919814  0.50533206 -0.22605132\n",
      "  -0.48554028 -0.42166036 -0.46489634  0.46985208 -0.48457223  0.24422369\n",
      "   0.30384839  0.48578959 -0.07409376 11.3555163   0.48475602 -0.4032176\n",
      "   0.16295478  0.36751801  0.48564615  0.48568659  0.46690032 -0.47882698\n",
      "   0.46478184  0.31625247  0.24599723]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.993\n",
      "Neg. class accuracy: 0.512\n",
      "Precision            0.504\n",
      "Recall               0.993\n",
      "F1                   0.669\n",
      "----------------------------------------\n",
      "TP: 294 | FP: 289 | TN: 303 | FN: 2\n",
      "========================================\n",
      "\n",
      "{0: 305, 1: 583}\n",
      "acc 0.6722972972972973\n",
      "(array([0.99344262, 0.50428816]), array([0.51182432, 0.99324324]), array([0.67558528, 0.66894198]), array([592, 296]))\n",
      "(0.7488653938081714, 0.7525337837837838, 0.6722636319015605, None)\n",
      "[[303 289]\n",
      " [  2 294]]\n",
      "prec: tp/(tp+fp) 0.5042881646655232 recall: tp/(tp+fn) 0.9932432432432432\n",
      "(0.5042881646655232, 0.9932432432432432, 0.6689419795221843, None)\n"
     ]
    }
   ],
   "source": [
    "train_nl(0.01,15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(888, 33)\n",
      "(888, 2)\n",
      "(8272, 33)\n",
      "(8272, 2)\n"
     ]
    }
   ],
   "source": [
    "#input L_S:train_L_S, K: no of classes\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def get_maj_prior(L_S,K):\n",
    "    maj_prior = []\n",
    "    \n",
    "    print(L_S[:,0,:].shape)\n",
    "    for row in np.nditer(L_S[:,0,:],flags=['external_loop'], order='C'):\n",
    "        p = np.ones(K)/K\n",
    "        unique, counts = np.unique(row, return_counts=True)\n",
    "        unique = [int(x) for x in unique]\n",
    "        rc = dict(zip(unique, counts))\n",
    "        tnz = np.count_nonzero(row)\n",
    "        if -1 in rc:\n",
    "            p[0] = rc[-1]\n",
    "        if 1 in rc:\n",
    "            p[1] = rc[1]\n",
    "        p = softmax(p)\n",
    "        maj_prior.append(p)\n",
    "    return np.array(maj_prior)\n",
    "\n",
    "dev_maj_pl=get_maj_prior(dev_L_S,2)\n",
    "print(dev_maj_pl.shape)\n",
    "\n",
    "\n",
    "train_maj_pl=get_maj_prior(train_L_S,2)\n",
    "print(train_maj_pl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized loss with majority prior\n",
    "\n",
    "\n",
    "\n",
    "def train_nlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout-logz),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e0ddda0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e0ddda0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e0ddda0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 474906.2210111349\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.21090337 0.9091335  1.10355165 0.90243832 1.0081237  0.94312556\n",
      "  1.07091421 1.12508078 1.23950664 0.96649523 0.88539626 1.16028402\n",
      "  1.0765182  1.17218054 1.13463138 0.94402255 0.94809364 0.82180501\n",
      "  0.96140002 0.73834929 0.93716715 1.03577908 0.89803169 1.25607099\n",
      "  0.96839541 1.07918097 0.96985551 0.9702773  0.95925285 1.08443836\n",
      "  0.72393615 0.74030921 0.86399097]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "1 loss 455130.415053864\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.15415736 0.89930916 1.05950407 0.80839578 0.91379878 0.84897\n",
      "  0.97643772 1.07827194 1.17974936 0.87227725 0.79140275 1.10921272\n",
      "  1.0361324  1.11973129 1.08663692 0.84986456 0.92946895 0.72800393\n",
      "  0.8671955  0.64482544 0.84302779 1.00140161 0.80400172 1.19462518\n",
      "  0.94569095 0.98468535 0.8756287  0.87604938 0.86505402 1.04295548\n",
      "  0.63046326 0.64677849 0.77006054]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "2 loss 442206.0464598437\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.04786187 0.78735601 0.95198471 0.71181537 0.81697258 0.75229033\n",
      "  0.87948252 0.97104168 1.07370455 0.77554326 0.6948658  1.00240375\n",
      "  0.9282105  1.01305199 0.97952691 0.75318277 0.81876778 0.63163981\n",
      "  0.77047319 0.54871715 0.74636227 0.89277402 0.70743244 1.08871446\n",
      "  0.83555249 0.887714   0.77888705 0.77930678 0.76833667 0.93515644\n",
      "  0.53440294 0.55066377 0.67357983]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "3 loss 430960.91597695864\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.9433463  0.68076126 0.84692289 0.61708767 0.72172657 0.65734949\n",
      "  0.78397673 0.8661027  0.96930994 0.68048802 0.60023311 0.89765044\n",
      "  0.82298344 0.90835748 0.87464009 0.65823743 0.71255225 0.53739571\n",
      "  0.67544242 0.45508192 0.65145151 0.78727239 0.61272897 0.98438593\n",
      "  0.72951875 0.79217626 0.68381581 0.68423353 0.67331629 0.82997912\n",
      "  0.44088667 0.45701272 0.57907168]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "4 loss 421531.6071310269\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84041909 0.57695228 0.74374506 0.5257729  0.62901552 0.56543534\n",
      "  0.69063769 0.76297997 0.8664402  0.58826708 0.50919803 0.79461237\n",
      "  0.71973262 0.80534658 0.77154098 0.56631103 0.6088902  0.44759525\n",
      "  0.58328628 0.36757581 0.55961967 0.68390305 0.52148476 0.88154791\n",
      "  0.62592919 0.69876292 0.59155276 0.59196523 0.58118779 0.72675013\n",
      "  0.35389259 0.36944025 0.48841583]]\n",
      "{0: 494, 1: 394}\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84041909 0.57695228 0.74374506 0.5257729  0.62901552 0.56543534\n",
      "  0.69063769 0.76297997 0.8664402  0.58826708 0.50919803 0.79461237\n",
      "  0.71973262 0.80534658 0.77154098 0.56631103 0.6088902  0.44759525\n",
      "  0.58328628 0.36757581 0.55961967 0.68390305 0.52148476 0.88154791\n",
      "  0.62592919 0.69876292 0.59155276 0.59196523 0.58118779 0.72675013\n",
      "  0.35389259 0.36944025 0.48841583]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.743\n",
      "Neg. class accuracy: 0.706\n",
      "Precision            0.558\n",
      "Recall               0.743\n",
      "F1                   0.638\n",
      "----------------------------------------\n",
      "TP: 220 | FP: 174 | TN: 418 | FN: 76\n",
      "========================================\n",
      "\n",
      "{0: 494, 1: 394}\n",
      "acc 0.7184684684684685\n",
      "(array([0.84615385, 0.55837563]), array([0.70608108, 0.74324324]), array([0.76979742, 0.63768116]), array([592, 296]))\n",
      "(0.7022647403358063, 0.7246621621621621, 0.7037392905757066, None)\n",
      "[[418 174]\n",
      " [ 76 220]]\n",
      "prec: tp/(tp+fp) 0.5583756345177665 recall: tp/(tp+fn) 0.7432432432432432\n",
      "(0.5583756345177665, 0.7432432432432432, 0.6376811594202899, None)\n"
     ]
    }
   ],
   "source": [
    "train_nlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized loss with majority bias un normalized\n",
    "\n",
    "\n",
    "\n",
    "def train_unlmp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "        \n",
    "        maj_train_dataset = tf.data.Dataset.from_tensor_slices(train_maj_pl).batch(BATCH_SIZE)\n",
    "        maj_dev_dataset = tf.data.Dataset.from_tensor_slices(dev_maj_pl).batch(dev_maj_pl.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        maj_iterator = tf.data.Iterator.from_structure(maj_train_dataset.output_types,\n",
    "                                               maj_train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        maj_train_init_op = maj_iterator.make_initializer(maj_train_dataset)\n",
    "       \n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "        maj_dev_init_op = maj_iterator.make_initializer(maj_dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        maj_prior = tf.transpose(maj_iterator.get_next())\n",
    "        print(\"maj_label\",maj_prior)\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "        stpout= tf.squeeze(t_pout)\n",
    "        print(\"stpout\",stpout)\n",
    "        prod = tf.reduce_sum(maj_prior*stpout,axis=0)\n",
    "        print(\"prod\",prod)\n",
    "     \n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(maj_prior*tf.squeeze(t_pout),axis=1) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                sess.run(maj_train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                sess.run(maj_dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            sess.run(maj_dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maj_label Tensor(\"transpose:0\", shape=(2, ?), dtype=float64)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7a29e8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7a29e8>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31e7a29e8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "stpout Tensor(\"Squeeze_1:0\", dtype=float64)\n",
      "prod Tensor(\"Sum_1:0\", dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_2:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "1 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "2 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "3 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "4 loss 0.0\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.11754463 0.81421453 1.00955965 1.00062431 1.10637787 1.04133855\n",
      "  1.169206   1.03120785 1.14633471 1.0647233  0.98357064 1.06661405\n",
      "  0.98238184 1.07858159 1.04081247 1.04223612 0.85334134 0.91993454\n",
      "  1.05962482 0.83641602 1.03537624 0.94143537 0.99621468 1.16301113\n",
      "  0.87373343 1.17747756 1.06808572 1.06850778 1.05747627 0.99034372\n",
      "  0.82199156 0.83837747 0.96215054]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.581\n",
      "Neg. class accuracy: 0.799\n",
      "Precision            0.591\n",
      "Recall               0.581\n",
      "F1                   0.586\n",
      "----------------------------------------\n",
      "TP: 172 | FP: 119 | TN: 473 | FN: 124\n",
      "========================================\n",
      "\n",
      "{0: 597, 1: 291}\n",
      "acc 0.7263513513513513\n",
      "(array([0.79229481, 0.59106529]), array([0.79898649, 0.58108108]), array([0.79562658, 0.58603066]), array([592, 296]))\n",
      "(0.6916800497332021, 0.6900337837837838, 0.6908286206753274, None)\n",
      "[[473 119]\n",
      " [124 172]]\n",
      "prec: tp/(tp+fp) 0.5910652920962199 recall: tp/(tp+fn) 0.581081081081081\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n"
     ]
    }
   ],
   "source": [
    "train_unlmp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized loss with prior from other LFs\n",
    "\n",
    "def train_nlp(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        \n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        \n",
    "        \n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "#         ls_ = tf.multiply(l,s_)\n",
    "\n",
    "#         nls_ = tf.multiply(l,s_)*-1\n",
    "               \n",
    "        \n",
    "        pout = tf.map_fn(lambda li: tf.map_fn(lambda lij:li*lij,li ),l)\n",
    "#         print(\"nls\",nls_)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        \n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        \n",
    "\n",
    "        sumy = t_pout-logz\n",
    "        print(\"sumy\",sumy)\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_sum(t_pout-logz,axis=1) ))\n",
    "\n",
    "        \n",
    "        def index_along_every_row(array, index):\n",
    "            N, _ = array.shape\n",
    "            return array[np.arange(N), index]\n",
    "\n",
    "        #Best LF\n",
    "        blf = tf.argmax(t_pout,axis=1)\n",
    "        print(\"blf\",blf)\n",
    "        print(\"normloss\",normloss)\n",
    "        \n",
    "        \n",
    "        marginals = tf.py_func(index_along_every_row, [tf.squeeze(t_pout), tf.squeeze(blf)], [tf.float64])[0]\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict1 = tf.gather(k,tf.squeeze(blf))\n",
    "        \n",
    "        predict = tf.where(tf.equal(predict1,1),tf.ones_like(predict1),tf.zeros_like(predict1))\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "                print(\"blfs\")\n",
    "                unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "            \n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,b = sess.run([alphas,thetas,marginals,predict,blf])\n",
    "#             print(a)\n",
    "            print(t)\n",
    "\n",
    "#             MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "            print(\"blfs\")\n",
    "            unique, counts = np.unique(b.tolist(), return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "        \n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d4e95c0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d4e95c0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d4e95c0>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"sub:0\", shape=(?, 33, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 7786500.991153705\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.21092702 0.9079748  1.10199065 0.90247263 1.00824309 0.94324036\n",
      "  1.07090337 1.1235188  1.2375242  0.96658725 0.88550743 1.15869341\n",
      "  1.07658607 1.17200763 1.13447974 0.9440065  0.94815552 0.8219294\n",
      "  0.96139843 0.73833518 0.93738841 1.03379794 0.89801413 1.25616534\n",
      "  0.96708367 1.07922646 0.96984457 0.97027363 0.95925583 1.08451359\n",
      "  0.72392792 0.74032389 0.86403909]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "1 loss 7461453.1901169885\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.15369472 0.917885   1.07998007 0.80938249 0.91498167 0.85013375\n",
      "  0.97712107 1.09966941 1.20062707 0.87336857 0.7925665  1.12799642\n",
      "  1.03241013 1.1217056  1.08774363 0.85058571 0.92173995 0.72918238\n",
      "  0.86798609 0.6456306  0.84454906 1.0282505  0.80472107 1.1939616\n",
      "  0.96544151 0.98550882 0.8763339  0.87675713 0.86579571 1.03948456\n",
      "  0.63131025 0.64768603 0.77103463]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "2 loss 7258744.6239661025\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.04734456 0.82799416 0.9825643  0.71448193 0.81996089 0.75523862\n",
      "  0.88128207 1.00215457 1.10190608 0.77835699 0.69789624 1.02905092\n",
      "  0.92453976 1.01586514 0.98159338 0.75513949 0.81201328 0.63469438\n",
      "  0.77258213 0.55102196 0.75024745 0.93770523 0.7094074  1.08778988\n",
      "  0.87193258 0.88978986 0.78077964 0.78119608 0.77031188 0.93171223\n",
      "  0.53683755 0.55323295 0.67620465]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "3 loss 7084913.234376455\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.94370674 0.72897264 0.88224226 0.62234041 0.72748445 0.66309835\n",
      "  0.78697883 0.90185113 1.00143958 0.68592025 0.60634431 0.92830805\n",
      "  0.8203522  0.91242789 0.87803017 0.66168168 0.7072573  0.5436828\n",
      "  0.67922642 0.45968365 0.65952025 0.83962502 0.61625132 0.98421728\n",
      "  0.77225974 0.79574628 0.68707945 0.68747952 0.67677096 0.82755862\n",
      "  0.44589415 0.46234421 0.58425375]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "4 loss 6940041.820261128\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "{0.0: 388, 1.0: 500}\n",
      "acc 0.5990990990990991\n",
      "(array([0.80412371, 0.44      ]), array([0.52702703, 0.74324324]), array([0.63673469, 0.55276382]), array([592, 296]))\n",
      "(0.6220618556701031, 0.6351351351351351, 0.5947492564865142, None)\n",
      "[[312 280]\n",
      " [ 76 220]]\n",
      "prec: tp/(tp+fp) 0.44 recall: tp/(tp+fn) 0.7432432432432432\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n"
     ]
    }
   ],
   "source": [
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d652208>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d652208>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31d652208>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 33, 1), dtype=float64)\n",
      "t_k Tensor(\"mul:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "sumy Tensor(\"sub:0\", shape=(?, 33, 1), dtype=float64)\n",
      "blf Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"PyFunc:0\", dtype=float64)\n",
      "predict Tensor(\"Select:0\", dtype=float64)\n",
      "0 loss 7786500.991153705\n",
      "[[1.21092702 0.9079748  1.10199065 0.90247263 1.00824309 0.94324036\n",
      "  1.07090337 1.1235188  1.2375242  0.96658725 0.88550743 1.15869341\n",
      "  1.07658607 1.17200763 1.13447974 0.9440065  0.94815552 0.8219294\n",
      "  0.96139843 0.73833518 0.93738841 1.03379794 0.89801413 1.25616534\n",
      "  0.96708367 1.07922646 0.96984457 0.97027363 0.95925583 1.08451359\n",
      "  0.72392792 0.74032389 0.86403909]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "1 loss 7461453.1901169885\n",
      "[[1.15369472 0.917885   1.07998007 0.80938249 0.91498167 0.85013375\n",
      "  0.97712107 1.09966941 1.20062707 0.87336857 0.7925665  1.12799642\n",
      "  1.03241013 1.1217056  1.08774363 0.85058571 0.92173995 0.72918238\n",
      "  0.86798609 0.6456306  0.84454906 1.0282505  0.80472107 1.1939616\n",
      "  0.96544151 0.98550882 0.8763339  0.87675713 0.86579571 1.03948456\n",
      "  0.63131025 0.64768603 0.77103463]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "2 loss 7258744.6239661025\n",
      "[[1.04734456 0.82799416 0.9825643  0.71448193 0.81996089 0.75523862\n",
      "  0.88128207 1.00215457 1.10190608 0.77835699 0.69789624 1.02905092\n",
      "  0.92453976 1.01586514 0.98159338 0.75513949 0.81201328 0.63469438\n",
      "  0.77258213 0.55102196 0.75024745 0.93770523 0.7094074  1.08778988\n",
      "  0.87193258 0.88978986 0.78077964 0.78119608 0.77031188 0.93171223\n",
      "  0.53683755 0.55323295 0.67620465]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "3 loss 7084913.234376455\n",
      "[[0.94370674 0.72897264 0.88224226 0.62234041 0.72748445 0.66309835\n",
      "  0.78697883 0.90185113 1.00143958 0.68592025 0.60634431 0.92830805\n",
      "  0.8203522  0.91242789 0.87803017 0.66168168 0.7072573  0.5436828\n",
      "  0.67922642 0.45968365 0.65952025 0.83962502 0.61625132 0.98421728\n",
      "  0.77225974 0.79574628 0.68707945 0.68747952 0.67677096 0.82755862\n",
      "  0.44589415 0.46234421 0.58425375]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "4 loss 6940041.820261128\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n",
      "\n",
      "[[0.84191211 0.62984842 0.78245958 0.53690126 0.64050656 0.5775349\n",
      "  0.69464574 0.80208092 0.90157217 0.599194   0.52320662 0.82827791\n",
      "  0.71833274 0.81078397 0.77633438 0.57153865 0.60504544 0.46369258\n",
      "  0.58947253 0.37861228 0.57846    0.74112352 0.52698436 0.88243276\n",
      "  0.67281299 0.70414052 0.59623196 0.59657725 0.58640622 0.72555267\n",
      "  0.36754229 0.38404975 0.49969681]]\n",
      "blfs\n",
      "{0: 136, 1: 73, 3: 28, 4: 39, 5: 154, 6: 3, 32: 14, 8: 13, 13: 15, 15: 28, 17: 32, 18: 21, 19: 3, 20: 54, 21: 257, 23: 1, 24: 5, 26: 4, 28: 2, 30: 3, 31: 3}\n",
      "{0.0: 388, 1.0: 500}\n",
      "acc 0.5990990990990991\n",
      "(array([0.80412371, 0.44      ]), array([0.52702703, 0.74324324]), array([0.63673469, 0.55276382]), array([592, 296]))\n",
      "(0.6220618556701031, 0.6351351351351351, 0.5947492564865142, None)\n",
      "[[312 280]\n",
      " [ 76 220]]\n",
      "prec: tp/(tp+fp) 0.44 recall: tp/(tp+fn) 0.7432432432432432\n",
      "(0.44, 0.7432432432432432, 0.5527638190954774, None)\n"
     ]
    }
   ],
   "source": [
    "# print blf\n",
    "train_nlp(0.1/len(train_L_S),5,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Un normalized training with different params\n",
    "\n",
    "def train_unl(lr,ep,th):\n",
    "    \n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "        print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for it in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    while True:\n",
    "                        _,ls = sess.run([train_step,normloss])\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"loss\",tl)\n",
    "\n",
    "                sess.run(dev_init_op)\n",
    "                a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "                print(a)\n",
    "                print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "                print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf0b8>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf0b8>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf0b8>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -15869.846624424299\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.13216794 0.84431419 1.03577597 1.02109161 1.13526783 1.08015688\n",
      "  1.18145719 1.06028046 1.1751276  1.08588497 1.0068098  1.09248317\n",
      "  0.98336898 1.09600627 1.05379678 1.05549242 0.85541465 0.95083779\n",
      "  1.08461451 0.83868296 1.07003751 0.96766079 0.9986     1.17431885\n",
      "  0.8993755  1.19734509 1.07500128 1.07051312 1.06684234 0.99202436\n",
      "  0.83212316 0.84560774 0.98149621]]\n",
      "{0: 597, 1: 291}\n",
      "(0.5910652920962199, 0.581081081081081, 0.58603066439523, None)\n",
      "\n",
      "1 loss -16214.65212550975\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.14667932 0.87444394 1.0621771  1.04219316 1.16423881 1.11874316\n",
      "  1.19391738 1.0893804  1.2040518  1.10710219 1.02995553 1.11852526\n",
      "  0.98455667 1.11352671 1.06690143 1.06898106 0.85754985 0.9809955\n",
      "  1.11006471 0.84087479 1.10447784 0.99378945 1.00097057 1.1853921\n",
      "  0.92531294 1.21670697 1.08174958 1.07226547 1.07621178 0.99367552\n",
      "  0.8422764  0.85277171 1.00083577]]\n",
      "{0: 598, 1: 290}\n",
      "(0.5896551724137931, 0.5777027027027027, 0.5836177474402731, None)\n",
      "\n",
      "2 loss -16561.204610821405\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.16114906 0.90458204 1.08857823 1.06329845 1.19321574 1.15745091\n",
      "  1.20635982 1.11848117 1.23297601 1.12831942 1.05310373 1.14456737\n",
      "  0.98574439 1.13101501 1.08000609 1.08238921 0.85968747 1.01118612\n",
      "  1.13551144 0.8430677  1.13909544 1.0198811  1.00330216 1.19635177\n",
      "  0.95123975 1.23606015 1.08848559 1.0739708  1.08551093 0.99524307\n",
      "  0.85239606 0.85990408 1.02015758]]\n",
      "{0: 598, 1: 290}\n",
      "(0.5896551724137931, 0.5777027027027027, 0.5836177474402731, None)\n",
      "\n",
      "3 loss -16909.370373253547\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.17557735 0.93472783 1.11497933 1.08440726 1.22219832 1.19627504\n",
      "  1.21878436 1.14758265 1.2619002  1.14953663 1.07625426 1.17060947\n",
      "  0.98693211 1.14847091 1.09311076 1.09571581 0.86182743 1.04140786\n",
      "  1.16095438 0.84526168 1.17388574 1.04593168 1.00559409 1.20719777\n",
      "  0.97715462 1.25540463 1.09520911 1.07562957 1.09473977 0.9967292\n",
      "  0.86248187 0.86700551 1.03946073]]\n",
      "{0: 589, 1: 299}\n",
      "(0.5785953177257525, 0.5844594594594594, 0.5815126050420167, None)\n",
      "\n",
      "4 loss -17259.067448019156\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.18996435 0.96488071 1.14138042 1.10551945 1.25118627 1.23521082\n",
      "  1.23119086 1.17668474 1.29082439 1.17075384 1.09940701 1.19665158\n",
      "  0.98811983 1.16589421 1.10621544 1.10895989 0.86396965 1.07165904\n",
      "  1.18639337 0.84745674 1.20884454 1.07193723 1.00784581 1.21793007\n",
      "  1.00305638 1.27474035 1.10191991 1.07724231 1.10389827 0.99813616\n",
      "  0.8725336  0.87407662 1.05874442]]\n",
      "{0: 591, 1: 297}\n",
      "(0.5791245791245792, 0.581081081081081, 0.5801011804384485, None)\n",
      "\n",
      "5 loss -17610.21866257076\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.20431021 0.9950401  1.16778148 1.12663489 1.28017934 1.27425383\n",
      "  1.24357918 1.20578736 1.31974857 1.19197105 1.12256186 1.22269368\n",
      "  0.98930754 1.18328472 1.11932011 1.1221205  0.86611403 1.10193822\n",
      "  1.21182829 0.84965286 1.24396798 1.09789382 1.0100569  1.22854873\n",
      "  1.02894394 1.29406732 1.10861781 1.07880957 1.11298635 0.99946623\n",
      "  0.88255107 0.88111805 1.07800789]]\n",
      "{0: 612, 1: 276}\n",
      "(0.5978260869565217, 0.5574324324324325, 0.5769230769230769, None)\n",
      "\n",
      "6 loss -17962.751295727656\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.21861509 1.02520549 1.19418253 1.14775349 1.30917731 1.31339993\n",
      "  1.25594923 1.23489043 1.34867274 1.21318824 1.1457187  1.24873578\n",
      "  0.99049525 1.20064233 1.13242479 1.13519673 0.86826051 1.13224413\n",
      "  1.23725909 0.85185005 1.27925249 1.12379757 1.0122271  1.23905389\n",
      "  1.05481632 1.31338553 1.11530263 1.08033194 1.12200393 1.00072173\n",
      "  0.89253418 0.88813041 1.09725047]]\n",
      "{0: 607, 1: 281}\n",
      "(0.594306049822064, 0.5641891891891891, 0.5788561525129982, None)\n",
      "\n",
      "7 loss -18316.596780126394\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.23287916 1.05537641 1.22058357 1.16887514 1.33817997 1.35264522\n",
      "  1.26830087 1.26399391 1.37759691 1.23440544 1.16887744 1.27477788\n",
      "  0.99168296 1.21796692 1.14552947 1.14818773 0.87040901 1.16257567\n",
      "  1.26268579 0.85404829 1.31469477 1.14964466 1.01435626 1.24944582\n",
      "  1.08067259 1.332695   1.12197421 1.08181008 1.13095094 1.00190502\n",
      "  0.90248285 0.89511431 1.11647156]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "8 loss -18671.690429301383\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.24710257 1.08555241 1.2469846  1.18999973 1.36718713 1.39198601\n",
      "  1.28063403 1.29309773 1.40652107 1.25562263 1.19203799 1.30081998\n",
      "  0.99287066 1.23525843 1.15863416 1.16109268 0.87255945 1.19293191\n",
      "  1.28810845 0.85624759 1.35029177 1.17543128 1.01644438 1.25972491\n",
      "  1.10651191 1.35199578 1.1286324  1.08324468 1.1398273  1.00301849\n",
      "  0.91239704 0.90207034 1.13567062]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 loss -19027.97118752855\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.26128554 1.11573311 1.27338561 1.21112719 1.39619863 1.43141881\n",
      "  1.29294861 1.32220184 1.43544524 1.27683982 1.21520026 1.32686209\n",
      "  0.99405836 1.25251682 1.17173884 1.17391081 0.87471176 1.22331206\n",
      "  1.31352721 0.85844793 1.38604064 1.20115365 1.01849161 1.26989168\n",
      "  1.13233348 1.37128792 1.13527709 1.08463649 1.14863295 1.00406452\n",
      "  0.92227679 0.90899909 1.15484714]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "10 loss -19385.38140054044\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.27542826 1.14591812 1.29978662 1.23225741 1.42521429 1.47094027\n",
      "  1.30524454 1.35130623 1.46436939 1.29805701 1.23836417 1.35290419\n",
      "  0.99524606 1.26974212 1.18484352 1.18664141 0.87686586 1.25371543\n",
      "  1.33894224 0.86064932 1.42193873 1.22680803 1.02049819 1.27994681\n",
      "  1.15813657 1.39057148 1.14190818 1.0859863  1.15736784 1.00504551\n",
      "  0.93212216 0.91590114 1.17400072]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "11 loss -19743.8666052283\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.28953098 1.17610711 1.32618763 1.25339032 1.45423398 1.51054723\n",
      "  1.31752175 1.38041084 1.49329355 1.31927419 1.26152964 1.37894629\n",
      "  0.99643375 1.28693435 1.19794821 1.19928385 0.87902169 1.28414148\n",
      "  1.36435376 0.86285174 1.45798354 1.25239072 1.02246452 1.28989117\n",
      "  1.18392052 1.40984657 1.14852557 1.08729495 1.16603199 1.00596385\n",
      "  0.94193327 0.92277706 1.19313096]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "12 loss -20103.375336483656\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.30359398 1.20629975 1.35258863 1.27452584 1.48325755 1.55023664\n",
      "  1.32978017 1.40951565 1.5222177  1.34049137 1.28469661 1.40498839\n",
      "  0.99762144 1.30409359 1.21105289 1.21183753 0.88117919 1.31458975\n",
      "  1.38976203 0.86505519 1.49417274 1.27789802 1.02439107 1.29972576\n",
      "  1.20968467 1.42911325 1.15512922 1.08856334 1.17462541 1.00682191\n",
      "  0.95171027 0.92962742 1.21223755]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "13 loss -20463.85894941081\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.31761754 1.23649576 1.37898962 1.29566389 1.51228487 1.59000558\n",
      "  1.34201976 1.43862065 1.55114186 1.36170856 1.307865   1.43103049\n",
      "  0.99880914 1.32121995 1.22415757 1.22430193 0.88333827 1.34505988\n",
      "  1.41516736 0.86725966 1.53050411 1.30332629 1.02627845 1.30945179\n",
      "  1.23542845 1.44837165 1.16171908 1.0897924  1.18314818 1.00762204\n",
      "  0.96145335 0.93645277 1.2313202 ]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "14 loss -20825.27145525223\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.331602   1.26669486 1.40539061 1.31680439 1.54131584 1.62985126\n",
      "  1.35424048 1.46772579 1.58006601 1.38292574 1.33103475 1.4570726\n",
      "  0.99999682 1.33831358 1.23726225 1.23667661 0.8854989  1.37555159\n",
      "  1.4405701  0.86946515 1.56697556 1.32867193 1.02812733 1.31907064\n",
      "  1.26115131 1.46762187 1.16829513 1.0909831  1.19160042 1.00836655\n",
      "  0.97116276 0.94325368 1.2503787 ]]\n",
      "{0: 603, 1: 285}\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.331602   1.26669486 1.40539061 1.31680439 1.54131584 1.62985126\n",
      "  1.35424048 1.46772579 1.58006601 1.38292574 1.33103475 1.4570726\n",
      "  0.99999682 1.33831358 1.23726225 1.23667661 0.8854989  1.37555159\n",
      "  1.4405701  0.86946515 1.56697556 1.32867193 1.02812733 1.31907064\n",
      "  1.26115131 1.46762187 1.16829513 1.0909831  1.19160042 1.00836655\n",
      "  0.97116276 0.94325368 1.2503787 ]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.574\n",
      "Neg. class accuracy: 0.806\n",
      "Precision            0.596\n",
      "Recall               0.574\n",
      "F1                   0.585\n",
      "----------------------------------------\n",
      "TP: 170 | FP: 115 | TN: 477 | FN: 126\n",
      "========================================\n",
      "\n",
      "{0: 603, 1: 285}\n",
      "acc 0.7286036036036037\n",
      "(array([0.79104478, 0.59649123]), array([0.80574324, 0.57432432]), array([0.79832636, 0.58519793]), array([592, 296]))\n",
      "(0.6937680020947892, 0.6900337837837838, 0.6917621472140805, None)\n",
      "[[477 115]\n",
      " [126 170]]\n",
      "prec: tp/(tp+fp) 0.5964912280701754 recall: tp/(tp+fn) 0.5743243243243243\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n"
     ]
    }
   ],
   "source": [
    "train_unl(0.1/len(train_L_S),15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 33) dtype=float64_ref>\n",
      "k Tensor(\"Const_1:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf1d0>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf1d0>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31edaf1d0>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss -227408.91347570377\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 14.67744856  25.90583748  22.69587475  12.78045065  14.39716461\n",
      "    0.44035997  -3.2445657   25.08846134  24.96396903  13.96286426\n",
      "   11.82652493  22.46522151   1.79888645  16.7243985   11.77938823\n",
      "   -3.43412061   2.63100655   1.29551022  -6.77434535  -0.59566388\n",
      "  -10.15505727  61.98589432   1.58045045  12.01429343  28.43790004\n",
      "    3.19948727   1.48865339  -1.91094183  -4.65161603   5.80171757\n",
      "   -2.764758     1.47742811 -15.12888705]]\n",
      "{0: 186, 1: 702}\n",
      "(0.42165242165242167, 1.0, 0.5931863727454909, None)\n",
      "\n",
      "1 loss -795568.3743622616\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 29.1382024   50.98953277  44.5347361   11.90304728   7.97769311\n",
      "  -33.36295911 -13.78367756  49.16531977  48.89000862  10.12194851\n",
      "    5.60645624  44.00724617   2.78127445  32.43543394  22.61954728\n",
      "  -17.15336115   4.44616891 -22.67077969 -30.0618316   -3.05561389\n",
      "  -49.7209123  126.61153624   0.64822066  23.66716585  56.89541963\n",
      "  -11.66315922  -4.53226146  -6.20451658 -14.59742147  10.94639346\n",
      "  -14.72857165  -6.39233907 -41.37367621]]\n",
      "{0: 111, 1: 777}\n",
      "(0.38095238095238093, 1.0, 0.5517241379310345, None)\n",
      "\n",
      "2 loss -1497595.2865301068\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 43.82313598  76.0732257   66.37361624  -2.83511525 -13.74309888\n",
      "  -69.6174265  -24.53515019  73.24217802  72.81605934  -7.43529202\n",
      "  -12.32494795  65.54928463   3.76369901  48.14644276  33.45971014\n",
      "  -30.81955012   6.26133908 -49.74514454 -53.31057794  -5.5021359\n",
      "  -94.71278943 191.23319492  -0.91417375  35.31874093  85.40157055\n",
      "  -28.300968   -11.01874319 -10.49390156 -25.39489506  16.0910497\n",
      "  -26.65253492 -16.80352722 -68.29906796]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "3 loss -2229361.6521006986\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[  58.50780431  101.15691863   88.21249639  -20.37177447  -37.87423146\n",
      "  -105.87164134  -35.28655981   97.31903626   96.74211007  -24.98614902\n",
      "   -31.50792165   87.0913231     4.74612358   63.85745158   44.29987301\n",
      "   -44.48572589    8.07650926  -76.81881532  -76.55931453   -7.94837273\n",
      "  -139.70217543  255.85485259   -3.48628196   46.97031568  113.90771963\n",
      "   -44.93630527  -17.50395884  -14.7832795   -36.19236384   21.23570594\n",
      "   -38.57648808  -27.46786758  -95.22415065]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "4 loss -2962754.465933687\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[  73.19247258  126.24061155  110.05137654  -37.90572763  -62.00373818\n",
      "  -142.12585613  -46.03796941  121.3958945   120.66816079  -42.53700441\n",
      "   -50.6908199   108.63336157    5.72854814   79.5684604    55.14003587\n",
      "   -58.15190167    9.89167944 -103.89248593  -99.80805111  -10.394607\n",
      "  -184.6915608   320.47651026   -6.0035901    58.62189042  142.4138687\n",
      "   -61.5716419   -23.98917416  -19.07265745  -46.98983261   26.38036218\n",
      "   -50.50044124  -38.13220779 -122.14923326]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "5 loss -3696146.969814301\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[  87.87714084  151.32430448  131.89025669  -55.4396801   -86.13324448\n",
      "  -178.38007091  -56.789379    145.47275274  144.59421152  -60.08785979\n",
      "   -69.87371813  130.17540004    6.71097271   95.27946922   65.98019873\n",
      "   -71.81807744   11.70684961 -130.96615653 -123.05678769  -12.84084125\n",
      "  -229.68094617  385.09816792   -8.52040318   70.27346517  170.92001777\n",
      "   -78.20697853  -30.47438948  -23.36203539  -57.78730139   31.52501842\n",
      "   -62.4243944   -48.796548   -149.07431587]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "6 loss -4429539.47297085\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 102.56180911  176.40799741  153.72913684  -72.97363256 -110.26275078\n",
      "  -214.6342857   -67.5407886   169.54961099  168.52026224  -77.63871517\n",
      "   -89.05661637  151.71743851    7.69339727  110.99047804   76.82036159\n",
      "   -85.48425321   13.52201979 -158.03982714 -146.30552427  -15.2870755\n",
      "  -274.67033154  449.71982559  -11.03721297   81.92503992  199.42616684\n",
      "   -94.84231517  -36.95960479  -27.65141333  -68.58477017   36.66967466\n",
      "   -74.34834756  -59.46088821 -175.99939849]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "7 loss -5162931.976122954\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 117.24647737  201.49169034  175.56801699  -90.50758503 -134.39225709\n",
      "  -250.88850048  -78.2921982   193.62646923  192.44631297  -95.18957056\n",
      "  -108.2395146   173.25947698    8.67582184  126.70148685   87.66052445\n",
      "   -99.15042899   15.33718997 -185.11349774 -169.55426085  -17.73330975\n",
      "  -319.65971691  514.34148325  -13.55402274   93.57661466  227.93231591\n",
      "  -111.4776518   -43.44482011  -31.94079128  -79.38223894   41.8143309\n",
      "   -86.27230072  -70.12522842 -202.9244811 ]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss -5896324.47927504\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 131.93114564  226.57538327  197.40689714 -108.0415375  -158.52176339\n",
      "  -287.14271527  -89.0436078   217.70332747  216.37236369 -112.74042594\n",
      "  -127.42241284  194.80151545    9.6582464   142.41249567   98.50068732\n",
      "  -112.81660476   17.15236014 -212.18716835 -192.80299743  -20.179544\n",
      "  -364.64910227  578.96314092  -16.07083251  105.22818941  256.43846498\n",
      "  -128.11298843  -49.93003543  -36.23016922  -90.17970772   46.95898715\n",
      "   -98.19625388  -80.78956863 -229.84956371]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "9 loss -6629716.9824271165\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 146.6158139   251.6590762   219.24577729 -125.57548997 -182.65126969\n",
      "  -323.39693005  -99.7950174   241.78018572  240.29841441 -130.29128132\n",
      "  -146.60531107  216.34355392   10.64067097  158.12350449  109.34085018\n",
      "  -126.48278053   18.96753032 -239.26083895 -216.05173401  -22.62577825\n",
      "  -409.63848764  643.58479858  -18.58764228  116.87976416  284.94461405\n",
      "  -144.74832507  -56.41525074  -40.51954717 -100.9771765    52.10364339\n",
      "  -110.12020704  -91.45390884 -256.77464632]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "10 loss -7363109.485579201\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 161.30048217  276.74276913  241.08465744 -143.10944244 -206.780776\n",
      "  -359.65114484 -110.54642699  265.85704396  264.22446514 -147.8421367\n",
      "  -165.78820931  237.88559238   11.62309553  173.83451331  120.18101304\n",
      "  -140.1489563    20.7827005  -266.33450956 -239.3004706   -25.0720125\n",
      "  -454.62787301  708.20645625  -21.10445205  128.5313389   313.45076312\n",
      "  -161.3836617   -62.90046606  -44.80892511 -111.77464527   57.24829963\n",
      "  -122.0441602  -102.11824905 -283.69972894]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "11 loss -8096501.988731309\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 175.98515043  301.82646205  262.92353759 -160.64339491 -230.9102823\n",
      "  -395.90535962 -121.29783659  289.9339022   288.15051586 -165.39299209\n",
      "  -184.97110754  259.42763085   12.6055201   189.54552213  131.0211759\n",
      "  -153.81513208   22.59787068 -293.40818016 -262.54920718  -27.51824675\n",
      "  -499.61725838  772.82811392  -23.62126182  140.18291365  341.95691219\n",
      "  -178.01899834  -69.38568138  -49.09830305 -122.57211405   62.39295587\n",
      "  -133.96811336 -112.78258926 -310.62481155]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "12 loss -8829894.49188337\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 190.6698187   326.91015498  284.76241774 -178.17734738 -255.0397886\n",
      "  -432.1595744  -132.04924619  314.01076044  312.07656659 -182.94384747\n",
      "  -204.15400578  280.96966932   13.58794466  205.25653095  141.86133876\n",
      "  -167.48130785   24.41304085 -320.48185076 -285.79794376  -29.964481\n",
      "  -544.60664375  837.44977158  -26.13807159  151.8344884   370.46306126\n",
      "  -194.65433497  -75.87089669  -53.387681   -133.36958282   67.53761211\n",
      "  -145.89206652 -123.44692947 -337.54989416]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "13 loss -9563286.99503545\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 205.35448696  351.99384791  306.60129788 -195.71129985 -279.16929491\n",
      "  -468.41378919 -142.80065579  338.08761869  336.00261731 -200.49470285\n",
      "  -223.33690401  302.51170779   14.57036923  220.96753976  152.70150163\n",
      "  -181.14748362   26.22821103 -347.55552137 -309.04668034  -32.41071525\n",
      "  -589.59602912  902.07142925  -28.65488136  163.48606314  398.96921033\n",
      "  -211.2896716   -82.35611201  -57.67705894 -144.1670516    72.68226835\n",
      "  -157.81601968 -134.11126968 -364.47497677]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "14 loss -10296679.498187572\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 220.03915523  377.07754084  328.44017803 -213.24525232 -303.29880121\n",
      "  -504.66800397 -153.55206539  362.16447693  359.92866804 -218.04555823\n",
      "  -242.51980225  324.05374626   15.55279379  236.67854858  163.54166449\n",
      "  -194.8136594    28.04338121 -374.62919197 -332.29541692  -34.8569495\n",
      "  -634.58541449  966.69308691  -31.17169113  175.13763789  427.4753594\n",
      "  -227.92500824  -88.84132733  -61.96643688 -154.96452038   77.82692459\n",
      "  -169.73997284 -144.77560989 -391.40005938]]\n",
      "{0: 106, 1: 782}\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 220.03915523  377.07754084  328.44017803 -213.24525232 -303.29880121\n",
      "  -504.66800397 -153.55206539  362.16447693  359.92866804 -218.04555823\n",
      "  -242.51980225  324.05374626   15.55279379  236.67854858  163.54166449\n",
      "  -194.8136594    28.04338121 -374.62919197 -332.29541692  -34.8569495\n",
      "  -634.58541449  966.69308691  -31.17169113  175.13763789  427.4753594\n",
      "  -227.92500824  -88.84132733  -61.96643688 -154.96452038   77.82692459\n",
      "  -169.73997284 -144.77560989 -391.40005938]]\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 1.0\n",
      "Neg. class accuracy: 0.179\n",
      "Precision            0.379\n",
      "Recall               1.0\n",
      "F1                   0.549\n",
      "----------------------------------------\n",
      "TP: 296 | FP: 486 | TN: 106 | FN: 0\n",
      "========================================\n",
      "\n",
      "{0: 106, 1: 782}\n",
      "acc 0.4527027027027027\n",
      "(array([1.        , 0.37851662]), array([0.17905405, 1.        ]), array([0.30372493, 0.54916512]), array([592, 296]))\n",
      "(0.6892583120204604, 0.589527027027027, 0.42644502448022714, None)\n",
      "[[106 486]\n",
      " [  0 296]]\n",
      "prec: tp/(tp+fp) 0.37851662404092073 recall: tp/(tp+fn) 1.0\n",
      "(0.37851662404092073, 1.0, 0.549165120593692, None)\n"
     ]
    }
   ],
   "source": [
    "train_unl(0.01,15,tf.truncated_normal_initializer(1,0.1,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee21978>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee21978>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee21978>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 190172.47853772584\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.2053655  0.30936395 0.28997198 0.2367527  0.28647343 0.32020677\n",
      "  0.19884814 0.30623267 0.30723075 0.2419398  0.25581996 0.28299078\n",
      "  0.18634078 0.21796294 0.20752234 0.19268362 0.18442374 0.26176987\n",
      "  0.22819245 0.18263638 0.30204006 0.36018888 0.18171976 0.19192962\n",
      "  0.30156119 0.22449297 0.18345572 0.18210333 0.18615508 0.18474442\n",
      "  0.19665109 0.19592285 0.21894825]]\n",
      "dev loss 20432.207128112896\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.733\n",
      "Neg. class accuracy: 0.715\n",
      "Precision            0.562\n",
      "Recall               0.733\n",
      "F1                   0.636\n",
      "----------------------------------------\n",
      "TP: 217 | FP: 169 | TN: 423 | FN: 79\n",
      "========================================\n",
      "\n",
      "{0: 502, 1: 386}\n",
      "acc 0.7207207207207207\n",
      "(array([0.84262948, 0.56217617]), array([0.71452703, 0.73310811]), array([0.77330896, 0.63636364]), array([592, 296]))\n",
      "(0.7024028239374109, 0.7238175675675675, 0.7048362971580522, None)\n",
      "[[423 169]\n",
      " [ 79 217]]\n",
      "prec: tp/(tp+fp) 0.5621761658031088 recall: tp/(tp+fn) 0.7331081081081081\n",
      "(0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)\n",
      "un-norma thetas ep15 lr 0.1/len(train)\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee20278>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee20278>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31ee20278>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss 247125.96744431488\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[1.331602   1.26669486 1.40539061 1.31680439 1.54131584 1.62985126\n",
      "  1.35424048 1.46772579 1.58006601 1.38292574 1.33103475 1.4570726\n",
      "  0.99999682 1.33831358 1.23726225 1.23667661 0.8854989  1.37555159\n",
      "  1.4405701  0.86946515 1.56697556 1.32867193 1.02812733 1.31907064\n",
      "  1.26115131 1.46762187 1.16829513 1.0909831  1.19160042 1.00836655\n",
      "  0.97116276 0.94325368 1.2503787 ]]\n",
      "dev loss 26587.082646325307\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.574\n",
      "Neg. class accuracy: 0.806\n",
      "Precision            0.596\n",
      "Recall               0.574\n",
      "F1                   0.585\n",
      "----------------------------------------\n",
      "TP: 170 | FP: 115 | TN: 477 | FN: 126\n",
      "========================================\n",
      "\n",
      "{0: 603, 1: 285}\n",
      "acc 0.7286036036036037\n",
      "(array([0.79104478, 0.59649123]), array([0.80574324, 0.57432432]), array([0.79832636, 0.58519793]), array([592, 296]))\n",
      "(0.6937680020947892, 0.6900337837837838, 0.6917621472140805, None)\n",
      "[[477 115]\n",
      " [126 170]]\n",
      "prec: tp/(tp+fp) 0.5964912280701754 recall: tp/(tp+fn) 0.5743243243243243\n",
      "(0.5964912280701754, 0.5743243243243243, 0.585197934595525, None)\n",
      "[(190172.47853772584, 20432.207128112896, (0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)), (247125.96744431488, 26587.082646325307, (0.5964912280701754, 0.5743243243243243, 0.585197934595525, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value Normalized\n",
    "\n",
    "def getNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getNLObjValue(np.array([[0.2053655 , 0.30936395, 0.28997198, 0.2367527 , 0.28647343,\n",
    "       0.32020677, 0.19884814, 0.30623267, 0.30723075, 0.2419398 ,\n",
    "       0.25581996, 0.28299078, 0.18634078, 0.21796294, 0.20752234,\n",
    "       0.19268362, 0.18442374, 0.26176987, 0.22819245, 0.18263638,\n",
    "       0.30204006, 0.36018888, 0.18171976, 0.19192962, 0.30156119,\n",
    "       0.22449297, 0.18345572, 0.18210333, 0.18615508, 0.18474442,\n",
    "       0.19665109, 0.19592285, 0.21894825]])))\n",
    " \n",
    "            \n",
    "print(\"un-norma thetas ep15 lr 0.1/len(train)\")\n",
    "\n",
    "l_f1s.append(getNLObjValue(np.array([[1.331602,   1.26669486, 1.40539061, 1.31680439, 1.54131584, 1.62985126,\n",
    "  1.35424048, 1.46772579, 1.58006601, 1.38292574, 1.33103475, 1.4570726,\n",
    "  0.99999682, 1.33831358, 1.23726225, 1.23667661, 0.8854989 , 1.37555159,\n",
    "  1.4405701 , 0.86946515, 1.56697556, 1.32867193, 1.02812733, 1.31907064,\n",
    "  1.26115131, 1.46762187, 1.16829513, 1.0909831 , 1.19160042, 1.00836655,\n",
    "  0.97116276, 0.94325368, 1.2503787 ]])))\n",
    "\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Normalized loss plot loss vs f1s\n",
    "    \n",
    "y_loss=[190172.47,247125.96,182772.825,182772.62]\n",
    "x_f1s =[0.636,0.585,0.668,0.668]\n",
    "text=[\"snorkel-thetas\",\"unNormalized-thetas-ep7\",\"normalized-trained-thetas-ep7\",\"normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"CDR-Normalized-Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snorkel thetas\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31db88518>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31db88518>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31db88518>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -7326.533290003919\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[0.2053655  0.30936395 0.28997198 0.2367527  0.28647343 0.32020677\n",
      "  0.19884814 0.30623267 0.30723075 0.2419398  0.25581996 0.28299078\n",
      "  0.18634078 0.21796294 0.20752234 0.19268362 0.18442374 0.26176987\n",
      "  0.22819245 0.18263638 0.30204006 0.36018888 0.18171976 0.19192962\n",
      "  0.30156119 0.22449297 0.18345572 0.18210333 0.18615508 0.18474442\n",
      "  0.19665109 0.19592285 0.21894825]]\n",
      "dev loss -769.3308920775862\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.733\n",
      "Neg. class accuracy: 0.715\n",
      "Precision            0.562\n",
      "Recall               0.733\n",
      "F1                   0.636\n",
      "----------------------------------------\n",
      "TP: 217 | FP: 169 | TN: 423 | FN: 79\n",
      "========================================\n",
      "\n",
      "{0: 502, 1: 386}\n",
      "acc 0.7207207207207207\n",
      "(array([0.84262948, 0.56217617]), array([0.71452703, 0.73310811]), array([0.77330896, 0.63636364]), array([592, 296]))\n",
      "(0.7024028239374109, 0.7238175675675675, 0.7048362971580522, None)\n",
      "[[423 169]\n",
      " [ 79 217]]\n",
      "prec: tp/(tp+fp) 0.5621761658031088 recall: tp/(tp+fn) 0.7331081081081081\n",
      "(0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)\n",
      "normalized thetas ep15 lr0.01\n",
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 33), dtype=float64)\n",
      "k Tensor(\"Const_2:0\", shape=(33,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31eaf0710>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31eaf0710>\n",
      "<tf.Variable 'alphas:0' shape=(33,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 33), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 33), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb31eaf0710>\n",
      "nls Tensor(\"mul:0\", shape=(?, 33), dtype=float64)\n",
      "thetas Tensor(\"Const_1:0\", shape=(1, 33), dtype=float64)\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 33), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(33,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 train loss -10754.901547424994\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233  0.18357064 0.26661405\n",
      " 0.18238184 0.27858159 0.24081247 0.24223612 0.05334134 0.11993454\n",
      " 0.25962482 0.03641602 0.23537624 0.14143537 0.19621468 0.36301113\n",
      " 0.07373343 0.37747756 0.26808572 0.26850778 0.25747627 0.19034372\n",
      " 0.02199156 0.03837747 0.16215054]\n",
      "[[ 0.28575421  0.51761933  0.5207703   0.36454638  0.55417525  0.64604823\n",
      "   0.13772734  0.5908674   0.64182507  0.41374798  0.40126751  0.53715197\n",
      "  -0.05536051  0.32497394  0.21877467  0.14599839 -0.05084094  0.423559\n",
      "   0.32848078  0.05423492  0.65924925  1.0990666   0.05418686  0.22025056\n",
      "   0.55000554  0.33751599  0.06750857  0.05696271  0.11251414 -0.01585396\n",
      "   0.11384963  0.12932497  0.32449617]]\n",
      "dev loss -1083.2420373622351\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.851\n",
      "Neg. class accuracy: 0.657\n",
      "Precision            0.554\n",
      "Recall               0.851\n",
      "F1                   0.671\n",
      "----------------------------------------\n",
      "TP: 252 | FP: 203 | TN: 389 | FN: 44\n",
      "========================================\n",
      "\n",
      "{0: 433, 1: 455}\n",
      "acc 0.7218468468468469\n",
      "(array([0.89838337, 0.55384615]), array([0.65709459, 0.85135135]), array([0.75902439, 0.67110519]), array([592, 296]))\n",
      "(0.7261147628353171, 0.754222972972973, 0.7150647916599007, None)\n",
      "[[389 203]\n",
      " [ 44 252]]\n",
      "prec: tp/(tp+fp) 0.5538461538461539 recall: tp/(tp+fn) 0.8513513513513513\n",
      "(0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None)\n",
      "[(-7326.533290003919, -769.3308920775862, (0.5621761658031088, 0.7331081081081081, 0.6363636363636364, None)), (-10754.901547424994, -1083.2420373622351, (0.5538461538461539, 0.8513513513513513, 0.6711051930758989, None))]\n"
     ]
    }
   ],
   "source": [
    "## Objective value on snorkel thetas Unnormalized # remove logz from obj\n",
    "\n",
    "def getUNLObjValue(th):\n",
    "    BATCH_SIZE = 1\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "        labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "        print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "    #     thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "    #                              initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "    #                              dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.convert_to_tensor(th)\n",
    "\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "        print(\"k\",k)\n",
    "        print(alphas.graph)\n",
    "        print(thetas.graph)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "        print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "        print(s.graph)\n",
    "\n",
    "        s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "\n",
    "\n",
    "        ls_ = tf.multiply(l,s_)\n",
    "\n",
    "        nls_ = tf.multiply(l,s_)*-1\n",
    "\n",
    "        pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "        print(\"nls\",nls_)\n",
    "        print(\"thetas\",thetas)\n",
    "\n",
    "    #     lst = tf.matmul(ls_,thetas)\n",
    "    #     print(\"lst\",lst)\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "        print(\"pout\",pout)\n",
    "\n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t_k =  k*tf.squeeze(thetas)\n",
    "        print(\"t_k\",t_k)\n",
    "\n",
    "        zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "        print(\"zy\",zy)\n",
    "        print(\"logz\",logz)\n",
    "\n",
    "        lsp = tf.reduce_logsumexp(t_pout)\n",
    "        print(\"lsp\",lsp)\n",
    "\n",
    "    #     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "        normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) ))\n",
    "\n",
    "\n",
    "        print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "        print(\"predict\",predict)\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    #     train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            tl = 0\n",
    "            for it in range(1):\n",
    "                sess.run(train_init_op)\n",
    "                \n",
    "                try:\n",
    "                    while True:\n",
    "    #                     _,ls = sess.run([train_step,normloss])\n",
    "                        ls = sess.run(normloss) # to calculate loss on fixed thetas\n",
    "                        tl = tl + ls\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(it,\"train loss\",tl)\n",
    "\n",
    "#                 sess.run(dev_init_op)\n",
    "#                 a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "#                  print(a)\n",
    "#                 print(t)\n",
    "\n",
    "\n",
    "    #         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "\n",
    "            # Initialize an iterator over the validation dataset.\n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl,dl = sess.run([alphas,thetas,marginals,predict,normloss])\n",
    "            print(a)\n",
    "            print(t)\n",
    "            print(\"dev loss\",dl)\n",
    "\n",
    "            MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "\n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "\n",
    "            print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "            cf = confusion_matrix(gold_labels_dev,pl)\n",
    "            print(cf)\n",
    "            print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            res = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\")\n",
    "            return (tl,dl,res)\n",
    "\n",
    "print(\"snorkel thetas\")\n",
    "l_f1s = []\n",
    "#snorkel thetas\n",
    "l_f1s.append(getUNLObjValue(np.array([[0.2053655 , 0.30936395, 0.28997198, 0.2367527 , 0.28647343,\n",
    "       0.32020677, 0.19884814, 0.30623267, 0.30723075, 0.2419398 ,\n",
    "       0.25581996, 0.28299078, 0.18634078, 0.21796294, 0.20752234,\n",
    "       0.19268362, 0.18442374, 0.26176987, 0.22819245, 0.18263638,\n",
    "       0.30204006, 0.36018888, 0.18171976, 0.19192962, 0.30156119,\n",
    "       0.22449297, 0.18345572, 0.18210333, 0.18615508, 0.18474442,\n",
    "       0.19665109, 0.19592285, 0.21894825]])))\n",
    "\n",
    "            \n",
    "print(\"normalized thetas ep15 lr0.01\")\n",
    "\n",
    "l_f1s.append(getUNLObjValue(np.array([[ 0.28575421,  0.51761933 , 0.5207703 ,  0.36454638,  0.55417525,  0.64604823,\n",
    "   0.13772734,  0.5908674,   0.64182507,  0.41374798,  0.40126751,  0.53715197,\n",
    "  -0.05536051,  0.32497394,  0.21877467,  0.14599839, -0.05084094,  0.423559,\n",
    "   0.32848078,  0.05423492,  0.65924925,  1.0990666 ,  0.05418686,  0.22025056,\n",
    "   0.55000554,  0.33751599,  0.06750857,  0.05696271,  0.11251414, -0.01585396,\n",
    "   0.11384963,  0.12932497,  0.32449617]])))\n",
    "\n",
    "\n",
    "print(l_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## UN-Normalized loss plot loss vs f1s\n",
    "    \n",
    "y_loss=[-7326.53,-10754.9,-20825.271]\n",
    "x_f1s =[0.636,0.671,0.585]\n",
    "text=[\"snorkel-thetas\",\"normalized-thetas-ep15\",\"Un-normalized-trained-thetas-ep15\"]\n",
    "drawLossVsF1(y_loss,x_f1s,text,\"CDR-Un-Normalized-Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_element Tensor(\"IteratorGetNext_1:0\", shape=(?, 2, 10), dtype=float64)\n",
      "k Tensor(\"Const_1:0\", shape=(10,), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "<tf.Variable 'alphas:0' shape=(10,) dtype=float64_ref>\n",
      "Tensor(\"unstack:1\", shape=(?, 10), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 10), dtype=float64)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7fb519d31320>\n",
      "nls Tensor(\"mul:0\", shape=(?, 10), dtype=float64)\n",
      "thetas <tf.Variable 'thetas:0' shape=(1, 10) dtype=float64_ref>\n",
      "pout Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 10), dtype=float64)\n",
      "t_pout Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2, ?, 1), dtype=float64)\n",
      "t_k Tensor(\"mul_1:0\", shape=(10,), dtype=float64)\n",
      "zy Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(2,), dtype=float64)\n",
      "logz Tensor(\"Log:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(), dtype=float64)\n",
      "normloss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(2, ?, 1), dtype=float64)\n",
      "predict Tensor(\"ArgMax:0\", shape=(?, 1), dtype=int64)\n",
      "0 loss 145767.7856663791\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "1 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "2 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "3 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "4 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "5 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "6 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "7 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "8 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "9 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "10 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "11 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "12 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "13 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "14 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "15 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "16 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "17 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "18 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "19 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "21 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "22 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "23 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "24 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "25 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "26 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "27 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "28 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "29 loss 145734.55921424946\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n",
      "\n",
      "[0.31754463 0.01421453 0.20955965 0.20062431 0.30637787 0.24133855\n",
      " 0.369206   0.23120785 0.34633471 0.2647233 ]\n",
      "[[ 0.61788499  0.58264704  0.66905544  0.66786296  0.48311329  5.59198573\n",
      "   0.61793172 -0.2656293  -0.36412623 -0.67924585]]\n",
      "{0: 2362, 1: 452}\n",
      "acc 0.7949538024164889\n",
      "(array([0.93353091, 0.07079646]), array([0.84      , 0.16931217]), array([0.88429918, 0.09984399]), array([2625,  189]))\n",
      "(0.5021636830944227, 0.5046560846560846, 0.49207158581109633, None)\n",
      "[[2205  420]\n",
      " [ 157   32]]\n",
      "prec: tp/(tp+fp) 0.07079646017699115 recall: tp/(tp+fn) 0.1693121693121693\n",
      "(0.07079646017699115, 0.1693121693121693, 0.09984399375975038, None)\n"
     ]
    }
   ],
   "source": [
    "## same network that didn't train\n",
    "BATCH_SIZE = 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 12\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "    dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(dev_L_S.shape[0])\n",
    "\n",
    "    labels = tf.convert_to_tensor(gold_labels_dev)\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "   \n",
    "    next_element = iterator.get_next()\n",
    "    print(\"next_element\",next_element)\n",
    "\n",
    "    alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('thetas', [1,NoOfLFs],\\\n",
    "                             initializer=tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                             dtype=tf.float64)\n",
    "    \n",
    "    k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    print(\"k\",k)\n",
    "    print(alphas.graph)\n",
    "    print(thetas.graph)\n",
    "    l,s =  tf.unstack(next_element,axis=1)\n",
    "    print(alphas)\n",
    "    print(s)\n",
    "    print(\"l\",l)\n",
    "    print(s.graph)\n",
    "\n",
    "    s_ = tf.map_fn(lambda x : tf.maximum(tf.subtract(x,alphas), 0), s)\n",
    "\n",
    "            \n",
    "        \n",
    "    ls_ = tf.multiply(l,s_)\n",
    "\n",
    "    nls_ = tf.multiply(l,s_)*-1\n",
    "    \n",
    "    pout = tf.map_fn(lambda x: l*x,np.array([-1,1],dtype=np.float64))\n",
    "    print(\"nls\",nls_)\n",
    "    print(\"thetas\",thetas)\n",
    "    \n",
    "#     lst = tf.matmul(ls_,thetas)\n",
    "#     print(\"lst\",lst)\n",
    "    t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout)\n",
    "    print(\"pout\",pout)\n",
    "    \n",
    "    print(\"t_pout\",t_pout)\n",
    "\n",
    "    t_k =  k*tf.squeeze(thetas)\n",
    "    print(\"t_k\",t_k)\n",
    "    \n",
    "    zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t_k*y),axis=0),np.array([-1,1],dtype=np.float64))\n",
    "    \n",
    "    logz = tf.log(tf.reduce_sum(zy,axis=0))\n",
    "    print(\"zy\",zy)\n",
    "    print(\"logz\",logz)\n",
    "    \n",
    "    lsp = tf.reduce_logsumexp(t_pout)\n",
    "    print(\"lsp\",lsp)\n",
    "\n",
    "#     normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0)) - logz)  # add z\n",
    "\n",
    "    normloss = tf.negative(tf.reduce_sum(tf.reduce_logsumexp(t_pout,axis=0) - logz))\n",
    "\n",
    "\n",
    "    print(\"normloss\",normloss)\n",
    "    marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "      \n",
    "    print(\"marginals\",marginals)\n",
    "    predict = tf.argmax(marginals,axis=0)\n",
    "    print(\"predict\",predict)\n",
    "    \n",
    "#     pre = tf.metrics.precision(labels,predict)\n",
    "#     rec = tf.metrics.recall(labels,predict)\n",
    "#     print(\"loss\",loss)\n",
    "#     print(\"nls_\",nls_)\n",
    "\n",
    "#     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "#     starter_learning_rate = 1.0\n",
    "#     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            10, 0.96, staircase=True)\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "#     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "#     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "#     reg_constant = 5.0  # Choose an appropriate one.\n",
    "#     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "#     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        \n",
    "        # Initialize an iterator over the training dataset.\n",
    "        for it in range(30):\n",
    "            sess.run(train_init_op)\n",
    "            tl = 0\n",
    "            try:\n",
    "                while True:\n",
    "                    _,ls = sess.run([train_step,normloss])\n",
    "                    tl = tl + ls\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(it,\"loss\",tl)\n",
    "            \n",
    "            sess.run(dev_init_op)\n",
    "            a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "            print(a)\n",
    "            print(t)\n",
    "          \n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "            \n",
    "\n",
    "            unique, counts = np.unique(pl, return_counts=True)\n",
    "            print(dict(zip(unique, counts)))\n",
    "            print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "            print()\n",
    "            \n",
    "        # Initialize an iterator over the validation dataset.\n",
    "        sess.run(dev_init_op)\n",
    "        a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "        print(a)\n",
    "        print(t)\n",
    "                      \n",
    "#         MentionScorer(dev_cands, L_gold_dev).score(m[1::].flatten())\n",
    "    \n",
    "       \n",
    "        unique, counts = np.unique(pl, return_counts=True)\n",
    "        print(dict(zip(unique, counts)))\n",
    "       \n",
    "        print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "  \n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl)))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "        cf = confusion_matrix(gold_labels_dev,pl)\n",
    "        print(cf)\n",
    "        print(\"prec: tp/(tp+fp)\",cf[1][1]/(cf[1][1]+cf[0][1]),\"recall: tp/(tp+fn)\",cf[1][1]/(cf[1][1]+cf[1][0]))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "\n",
    "    \n",
    "      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 0 1 9 3 2 8 7 5]\n"
     ]
    }
   ],
   "source": [
    "#snorkel\n",
    "a =np.array([ 0.07472098,  0.07514459,  0.11910277,  0.11186369,  0.07306518,\n",
    "        0.69216714,  0.07467749,  0.16012659,  0.13682546,  0.08183363])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 16 29 14 13 23 20  0  5 11  2  8  7  1 17  6 32 18 31 24 25  9  3 28\n",
      " 30 15 22 19 27 26 10  4 21]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([-0.27082211, -0.01928787, -0.14063245,  0.37856253,  0.43681819, -0.15844807,\n",
    "   0.13280198, -0.01935702, -0.10775934,  0.34390113,  0.39762823, -0.14286955,\n",
    "  -0.39588527, -0.33699178, -0.37821404,  0.38378715, -0.39537146,  0.11504936,\n",
    "   0.21906794,  0.39699417, -0.27113816,  8.13838832,  0.39548336, -0.31328908,\n",
    "   0.25503373,  0.28019293,  0.39734506,  0.39700564,  0.37866251, -0.39156514,\n",
    "   0.38002959,  0.21917987,  0.13605525])\n",
    "\n",
    "temp = a.flatten().argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(a))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00017390038913676534\n",
      "[1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99962997 1.         1.         1.         1.\n",
      " 1.         1.         1.00055981 0.99944019 1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99966171\n",
      " 1.00037556 1.00038181 1.        ]\n",
      "0 502\n",
      "0  d  (0.6458827900831905, 0.6613175675675675, 0.6176945431062317, None)\n",
      "\n",
      "-2.852652261056169e+32\n",
      "[1.00000000e+00 4.19096332e+01 1.61282266e+01 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 6.53761651e-01 2.98818871e+01\n",
      " 2.73636591e+01 1.00000000e+00 1.00000000e+00 1.37726648e+01\n",
      " 1.00000000e+00 1.09325114e+17 1.00000000e+00 1.00000000e+00\n",
      " 1.06648029e+00 9.51584748e-01 9.51436912e-01 9.84135000e-01\n",
      " 5.96544835e-01 4.02272777e+10 9.60992051e-01 6.00949033e+02\n",
      " 1.18230742e+02 9.52113532e-01 8.06269559e-01 8.73388071e-01\n",
      " 6.56039488e-01 1.04801446e+17 8.47703035e-01 8.44365498e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.0622783407234913e+67\n",
      "[1.00000000e+00 1.43344200e+03 2.48781988e+02 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 7.89037703e+02\n",
      " 8.81714699e+02 1.00000000e+00 1.00000000e+00 2.34544600e+02\n",
      " 1.00000000e+00 2.10967076e+34 1.00000000e+00 1.00000000e+00\n",
      " 1.13949202e+00 9.51575033e-01 9.51436912e-01 9.04171121e-01\n",
      " 5.96544835e-01 9.36474848e+20 9.03578251e-01 3.91911816e+04\n",
      " 1.12698180e+04 9.52113532e-01 6.74689330e-01 8.03451390e-01\n",
      " 6.01198782e-01 2.02237656e+34 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "0 -9.30043372417304e+72\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-4.6948196355860187e+67\n",
      "[1.00000000e+00 1.83829184e+03 2.94633493e+02 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 1.01188714e+03\n",
      " 1.06522370e+03 1.00000000e+00 1.00000000e+00 2.77785173e+02\n",
      " 1.00000000e+00 3.15957251e+35 1.00000000e+00 1.00000000e+00\n",
      " 1.13949202e+00 9.51575033e-01 9.51436912e-01 9.04171121e-01\n",
      " 5.96544835e-01 5.78505513e+21 9.03578251e-01 5.44237193e+04\n",
      " 1.69470154e+04 9.52113532e-01 6.74689330e-01 8.03451390e-01\n",
      " 6.01198782e-01 3.02883535e+35 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-8.872685835358324e+103\n",
      "[1.00000000e+00 8.55982116e+04 5.27812973e+03 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 3.35935250e+04\n",
      " 3.43245806e+04 1.00000000e+00 1.00000000e+00 4.50517914e+03\n",
      " 1.00000000e+00 6.09709656e+52 1.00000000e+00 1.00000000e+00\n",
      " 1.21998165e+00 9.51575033e-01 9.51436912e-01 8.90777925e-01\n",
      " 5.96544835e-01 2.78447456e+32 8.70677135e-01 1.99574851e+06\n",
      " 2.28839216e+06 9.52113532e-01 6.11409487e-01 7.19909702e-01\n",
      " 6.01198782e-01 5.84481018e+52 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-3.3040346752451336e+138\n",
      "[1.00000000e+00 2.92787378e+06 8.14437614e+04 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 8.87130519e+05\n",
      " 1.10602441e+06 1.00000000e+00 1.00000000e+00 7.67897630e+04\n",
      " 1.00000000e+00 1.17657013e+70 1.00000000e+00 1.00000000e+00\n",
      " 1.30797411e+00 9.51575033e-01 9.51436912e-01 8.23758607e-01\n",
      " 5.96544835e-01 6.48214481e+42 8.22564466e-01 1.26509258e+08\n",
      " 2.18132110e+08 9.52113532e-01 5.98005309e-01 6.77570161e-01\n",
      " 6.01198782e-01 1.12788587e+70 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "1 -2.892740475020717e+144\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-1.4602431655937904e+139\n",
      "[1.00000000e+00 3.75479899e+06 9.64542072e+04 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 1.13768455e+06\n",
      " 1.33619800e+06 1.00000000e+00 1.00000000e+00 9.09424694e+04\n",
      " 1.00000000e+00 1.76210369e+71 1.00000000e+00 1.00000000e+00\n",
      " 1.30797411e+00 9.51575033e-01 9.51436912e-01 8.23758607e-01\n",
      " 5.96544835e-01 4.00433233e+43 8.22564466e-01 1.75682150e+08\n",
      " 3.28016674e+08 9.52113532e-01 5.98005309e-01 6.77570161e-01\n",
      " 6.01198782e-01 1.68919116e+71 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-2.7596968269741615e+175\n",
      "[1.00000000e+00 1.74838445e+08 1.72790374e+06 1.00000000e+00\n",
      " 1.00000000e+00 5.96595555e-01 5.96709500e-01 3.77698622e+07\n",
      " 4.30556588e+07 1.00000000e+00 1.00000000e+00 1.47486216e+06\n",
      " 1.00000000e+00 3.40037025e+88 1.00000000e+00 1.00000000e+00\n",
      " 1.40451095e+00 9.51575033e-01 9.51436912e-01 8.12608427e-01\n",
      " 5.96544835e-01 1.92737343e+54 7.95205777e-01 6.44265545e+09\n",
      " 4.42928013e+10 9.52113532e-01 5.98005309e-01 6.32078701e-01\n",
      " 6.01198782e-01 3.25966933e+88 8.47703035e-01 8.42419894e-01\n",
      " 5.96662195e-01]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-1.0276633455396451e+210\n",
      "[1.00000000e+000 5.98032235e+009 2.66622814e+007 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 9.97418324e+008\n",
      " 1.38736156e+009 1.00000000e+000 1.00000000e+000 2.51387068e+007\n",
      " 1.00000000e+000 6.56176925e+105 1.00000000e+000 1.00000000e+000\n",
      " 1.50971899e+000 9.51575033e-001 9.51436912e-001 7.57382372e-001\n",
      " 5.96544835e-001 4.48684783e+064 7.55568399e-001 4.08392478e+011\n",
      " 4.22203954e+012 9.52113532e-001 5.98005309e-001 6.13411799e-001\n",
      " 6.01198782e-001 6.29025559e+105 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "2 -8.997373352678785e+215\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-4.5418360409434207e+210\n",
      "[1.00000000e+000 7.66935668e+009 3.15762579e+007 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 1.27912116e+009\n",
      " 1.67608391e+009 1.00000000e+000 1.00000000e+000 2.97718818e+007\n",
      " 1.00000000e+000 9.82730866e+106 1.00000000e+000 1.00000000e+000\n",
      " 1.50971899e+000 9.51575033e-001 9.51436912e-001 7.57382372e-001\n",
      " 5.96544835e-001 2.77174151e+065 7.55568399e-001 5.67130581e+011\n",
      " 6.34890192e+012 9.52113532e-001 5.98005309e-001 6.13411799e-001\n",
      " 6.01198782e-001 9.42067313e+106 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-8.583563892751895e+246\n",
      "[1.00000000e+000 3.57115894e+011 5.65664638e+008 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 4.24654003e+010\n",
      " 5.40076366e+010 1.00000000e+000 1.00000000e+000 4.82826300e+008\n",
      " 1.00000000e+000 1.89639736e+124 1.00000000e+000 1.00000000e+000\n",
      " 1.62474903e+000 9.51575033e-001 9.51436912e-001 7.48284652e-001\n",
      " 5.96544835e-001 1.33410029e+076 7.33282203e-001 2.07979409e+013\n",
      " 8.57305964e+014 9.52113532e-001 5.98005309e-001 6.01745759e-001\n",
      " 6.01198782e-001 1.81792801e+124 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "4000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "\n",
      "-3.196370666683136e+281\n",
      "[1.00000000e+000 1.22150947e+013 8.72844328e+009 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 1.12141708e+012\n",
      " 1.74026181e+012 1.00000000e+000 1.00000000e+000 8.22966996e+009\n",
      " 1.00000000e+000 3.65951970e+141 1.00000000e+000 1.00000000e+000\n",
      " 1.74984525e+000 9.51575033e-001 9.51436912e-001 7.03901522e-001\n",
      " 5.96544835e-001 3.10573183e+086 7.01438698e-001 1.31835742e+015\n",
      " 8.17193669e+016 9.52113532e-001 5.98005309e-001 6.01692613e-001\n",
      " 6.01198782e-001 3.50809566e+141 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "8000  d  (0.16666666666666666, 0.5, 0.25, None)\n",
      "3 -2.7984787417514873e+287\n",
      "0 888\n",
      "(0.16666666666666666, 0.5, 0.25, None)\n",
      "(0.3333333333333333, 1.0, 0.5, None)\n",
      "\n",
      "-1.4126602410376465e+282\n",
      "[1.00000000e+000 1.56650282e+013 1.03371340e+010 1.00000000e+000\n",
      " 1.00000000e+000 5.96595555e-001 5.96709500e-001 1.43814113e+012\n",
      " 2.10242586e+012 1.00000000e+000 1.00000000e+000 9.74643457e+009\n",
      " 1.00000000e+000 5.48072147e+142 1.00000000e+000 1.00000000e+000\n",
      " 1.74984525e+000 9.51575033e-001 9.51436912e-001 7.03901522e-001\n",
      " 5.96544835e-001 1.91855979e+087 7.01438698e-001 1.83078987e+015\n",
      " 1.22885691e+017 9.52113532e-001 5.98005309e-001 6.01692613e-001\n",
      " 6.01198782e-001 5.25393954e+142 8.47703035e-001 8.42419894e-001\n",
      " 5.96662195e-001]\n",
      "0 888\n",
      "0  d  (0.16666666666666666, 0.5, 0.25, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/ipykernel_launcher.py:60: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "0 0\n",
      "4000  d  (0.3333333333333333, 0.5, 0.4, None)\n",
      "\n",
      "nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "0 0\n",
      "8000  d  (0.3333333333333333, 0.5, 0.4, None)\n",
      "4 nan\n",
      "0 0\n",
      "(0.3333333333333333, 0.5, 0.4, None)\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# rerun old network to get thetas\n",
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = pl\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = pl\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "# #stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# def train_NN():\n",
    "#     print()\n",
    "#     result_dir = \"./\"\n",
    "#     config = projector.ProjectorConfig()\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#     summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "#     tf.reset_default_graph()\n",
    "\n",
    "#     dim = 2 #(labels,scores)\n",
    "\n",
    "#     _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "#     alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "#                             dtype=tf.float64)\n",
    "\n",
    "#     l,s = tf.unstack(_x)\n",
    "\n",
    "#     prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "#     mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#     phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "#     phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "#     phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "#     predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "#     loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "#     check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "#     sess = tf.Session()\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess.run(init)\n",
    "\n",
    "#     for i in range(1):\n",
    "#         c = 0\n",
    "#         te_prev=1\n",
    "#         total_te = 0\n",
    "#         for L_S_i in train_L_S:\n",
    "\n",
    "#             a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "#             total_te+=te_curr\n",
    "\n",
    "#             if(abs(te_curr-te_prev)<1e-200):\n",
    "#                 break\n",
    "\n",
    "#             if(c%4000==0):\n",
    "#                 pl = []\n",
    "#                 for L_S_i in dev_L_S:\n",
    "#                     a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "#                     pl.append(p)\n",
    "#                 predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#                 print()\n",
    "#                 print(total_te/4000)\n",
    "#                 total_te=0\n",
    "# #                 print(a)\n",
    "# #                 print(t)\n",
    "# #                 print()\n",
    "#                 print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#                 print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "#             c+=1\n",
    "#             te_prev = te_curr\n",
    "#         pl = []\n",
    "#         for L_S_i in dev_L_S:\n",
    "#             p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "#             pl.append(p)\n",
    "#         predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "#         print(i,total_te)\n",
    "#         print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "#         print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "# train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
