{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37595, 161)\n",
      "42 161\n"
     ]
    }
   ],
   "source": [
    "import _pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "# pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "#pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "# val_L_S = pkl.load( open( \"val_L_S.p\", \"rb\" ) )\n",
    "# train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )\n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_Embeddings.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_Embeddings.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_TFIDF.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_TFIDF.p\", \"rb\" )) \n",
    "\n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_Keywords.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_Keywords.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_Keywords_regex.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_Keywords_regex.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_regex.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_regex.p\", \"rb\" )) \n",
    "\n",
    "train_L_S = pkl.load( open( \"train_L_S_Keywords_regex_extracts.p\", \"rb\" ) )\n",
    "test_L_S = pkl.load( open( \"test_L_S_Keywords_regex_extracts.p\", \"rb\" )) \n",
    "\n",
    "\n",
    "# LF_output_map = pkl.load(open(\"LF_output_map.p\",\"rb\"))\n",
    "# LF_Names = pkl.load(open(\"LF_Names.p\",\"rb\"))\n",
    "# LF_l = pkl.load(open(\"LF_l.p\",\"rb\"))\n",
    "\n",
    "# LF_output_map = pkl.load(open(\"LF_output_map_84.p\",\"rb\"))\n",
    "# LF_Names = pkl.load(open(\"LF_Names_84.p\",\"rb\"))\n",
    "# LF_l = pkl.load(open(\"LF_l_84.p\",\"rb\"))\n",
    "\n",
    "LF_output_map = pkl.load(open(\"LF_output_map_161.p\",\"rb\"))\n",
    "LF_Names = pkl.load(open(\"LF_Names_161.p\",\"rb\"))\n",
    "LF_l = pkl.load(open(\"LF_l_161.p\",\"rb\"))\n",
    "\n",
    "\n",
    "class_names = pkl.load(open(\"class_names.p\",\"rb\"))\n",
    "# print(LF_Names,len(LF_Names))\n",
    "# print(LF_output_map)\n",
    "# print(class_names)\n",
    "train_L_S_df = pd.DataFrame((np.array(train_L_S)[:,0,:]),columns=LF_Names) \n",
    "print(train_L_S_df.shape)\n",
    "\n",
    "# train_df=pd.read_csv('./complaints_train_data_clean.csv',usecols=[\"category_name\",\"complaint_title\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "train_df=pd.read_csv('./complaints_train_validation_data_clean.csv',usecols=[\"index\",\"category_name\",\"complaint_title\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "train_df['category_name'] = train_df[\"category_name\"].astype('category')\n",
    "train_df['true_label'] = train_df['category_name'].cat.codes\n",
    "\n",
    "train_L_S = np.array(train_L_S,dtype=np.float64)\n",
    "test_L_S = np.array(test_L_S,dtype=np.float64)\n",
    "dev_L_S = test_L_S\n",
    "NoOfClasses = len(class_names) \n",
    "NoOfLFs = len(LF_Names) \n",
    "\n",
    "print(NoOfClasses,NoOfLFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import _pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37595, 2, 245)\n",
      "(6667, 2, 245)\n",
      "42 245 245\n"
     ]
    }
   ],
   "source": [
    "def merge(a,b):\n",
    "    c = []\n",
    "    for i in range(len(a)):\n",
    "        ci = []\n",
    "        ci_l = a[i,0,:].tolist()+b[i,0,:].tolist()\n",
    "        ci_s = a[i,1,:].tolist()+b[i,1,:].tolist()\n",
    "        ci.append(ci_l)\n",
    "        ci.append(ci_s)\n",
    "        c.append(ci)\n",
    "    return c\n",
    "train_L_S_E = pkl.load( open( \"train_L_S_T-D_Embeddings.p\", \"rb\" ) )\n",
    "test_L_S_E = pkl.load( open( \"test_L_S_T-D_Embeddings.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S_K = pkl.load( open( \"train_L_S_Regex.p\", \"rb\" ) )\n",
    "# test_L_S_K = pkl.load( open( \"test_L_S_Regex.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S_K = pkl.load( open( \"train_L_S_Keywords_regex.p\", \"rb\" ) )\n",
    "# test_L_S_K = pkl.load( open( \"test_L_S_Keywords_regex.p\", \"rb\" )) \n",
    "\n",
    "train_L_S_K = pkl.load( open( \"train_L_S_Keywords_regex_extracts.p\", \"rb\" ) )\n",
    "test_L_S_K = pkl.load( open( \"test_L_S_Keywords_regex_extracts.p\", \"rb\" )) \n",
    "\n",
    "\n",
    "train_L_S = merge(np.array(train_L_S_K),np.array(train_L_S_E))\n",
    "test_L_S = merge(np.array(test_L_S_K),np.array(test_L_S_E))\n",
    "print(np.array(train_L_S).shape)\n",
    "print(np.array(test_L_S).shape)\n",
    "\n",
    "# LF_output_map = pkl.load(open(\"LF_output_map_84.p\",\"rb\"))\n",
    "# LF_Names = pkl.load(open(\"LF_Names_84.p\",\"rb\"))\n",
    "# LF_l = pkl.load(open(\"LF_l_84.p\",\"rb\"))\n",
    "\n",
    "LF_output_map = pkl.load(open(\"LF_output_map_245.p\",\"rb\"))\n",
    "LF_Names = pkl.load(open(\"LF_Names_245.p\",\"rb\"))\n",
    "LF_l = pkl.load(open(\"LF_l_245.p\",\"rb\"))\n",
    "\n",
    "class_names = pkl.load(open(\"class_names.p\",\"rb\"))\n",
    "train_L_S_df = pd.DataFrame((np.array(train_L_S)[:,0,:]),columns=LF_Names) \n",
    "\n",
    "NoOfLFs = len(LF_Names) \n",
    "NoOfClasses = len(class_names) \n",
    "\n",
    "train_L_S = np.array(train_L_S,dtype=np.float64)\n",
    "test_L_S = np.array(test_L_S,dtype=np.float64)\n",
    "dev_L_S = test_L_S\n",
    "\n",
    "print(NoOfClasses,NoOfLFs,len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load true test labels\n",
    "\n",
    "seed = 12\n",
    "test_df=pd.read_csv('./complaints_test_data_clean.csv',usecols=[\"category_name\",\"complaint_title\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "colsize = len(test_df['category_name'])\n",
    "\n",
    "test_df['category_name'] = test_df[\"category_name\"].astype('category')\n",
    "test_df['true_label'] = test_df['category_name'].cat.codes\n",
    "\n",
    "true_labels = test_df['true_label'].tolist()\n",
    "gold_labels_dev = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  load 500 gold labels\n",
    "# gold_df=pd.read_csv('./clean-gold-labels.tsv',sep='\\t',usecols=[\"category_name\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "# colsize = len(test_df['category_name'])\n",
    "\n",
    "# gold_df['category_name'] = test_df[\"category_name\"].astype('category')\n",
    "# gold_df['true_label'] = test_df['category_name'].cat.codes\n",
    "\n",
    "# true_labels = test_df['true_label'].tolist()\n",
    "                    \n",
    "# test_L_S = pkl.load( open( \"gold-labels-clean.p\", \"rb\" )) \n",
    "# print(np.array(test_L_S).shape)\n",
    "# for i,x in enumerate(gold_df.groupby(\"category_name\").agg({\"complaint_description\": np.count_nonzero}).index._data):\n",
    "#     print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(true_labels,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "#     draw2DArray(confusion_matrix(true_labels,pl))\n",
    "    return report2dict(classification_report(true_labels, pl, target_names=class_names))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(lr,ep,th,af,batch_size=32,LF_acc=None,LF_rec=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
    "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0,debug=True):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (to set a max value for alpha)\n",
    "    ## norm : use normalization or not\n",
    "    ## Gamma : penalty tuning parameter\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
    "#         test_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "#         test_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=af,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "        \n",
    "        \n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    \n",
    "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty in [4,5,6,7,8,9,10]):\n",
    "            LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
    "        \n",
    "        if(penalty == 6):\n",
    "            LF_r = tf.convert_to_tensor(LF_rec, dtype=tf.float64)\n",
    "        \n",
    "        if(debug):\n",
    "            print(\"k\",k)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "        if(debug):\n",
    "            print(\"s\",s)\n",
    "            print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "        if(smooth):\n",
    "            s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "            if(debug):\n",
    "                print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            if(debug):\n",
    "                print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "        if(smooth):\n",
    "            pout = tf.map_fn(lambda c: iskequalsy(l,c)*s_ ,pcl,name=\"pout\")\n",
    "        else:\n",
    "            pout = tf.map_fn(lambda c: iskequalsy(l,c) ,pcl,name=\"pout\")\n",
    "\n",
    "        if(debug):\n",
    "            print(\"pout\",pout)    \n",
    "\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                           name=\"t_pout\")\n",
    "    \n",
    "        if(debug):\n",
    "            print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        if(debug):\n",
    "            print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            if(debug):\n",
    "                print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            if(debug):\n",
    "                print(\"intsy\",out1)\n",
    "            return out1\n",
    "                \n",
    "\n",
    "        if(smooth):\n",
    "            #smooth normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "        else:\n",
    "            #discrete normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "            \n",
    "        ### for precision and recall t_pout\n",
    "               ### for precision and recall t_pout\n",
    "        def p_t_pout(j):\n",
    "            Lj = tf.map_fn(lambda li : tf.gather(li,j),l)\n",
    "            if(debug):\n",
    "                print(\"sft Lj\",Lj)\n",
    "            kj = tf.gather(k,j)\n",
    "            if(debug):\n",
    "                print(\"sft kj\",kj)\n",
    "            indices = tf.where(tf.equal(Lj,kj))\n",
    "            if(debug):\n",
    "                print(\"sft indices\",indices)\n",
    "            li_lij_eq_kj = tf.gather(l,tf.squeeze(indices,1))\n",
    "            if(smooth):\n",
    "                si_lij_eq_kj = tf.gather(s_,tf.squeeze(indices,1))\n",
    "            if(debug):\n",
    "                print(\"sft l_ij_eq_kj\",li_lij_eq_kj)\n",
    "            if(smooth):\n",
    "                prec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            else:\n",
    "                prec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            if(debug):\n",
    "                print(\"prec_z\",prec_z)\n",
    "            if(smooth):\n",
    "                prec_t_pout = (tf.matmul(li_lij_eq_kj*si_lij_eq_kj*kj, thetas,transpose_b=True))/prec_z\n",
    "            else:\n",
    "                prec_t_pout = (tf.matmul(li_lij_eq_kj*kj, thetas,transpose_b=True))/prec_z\n",
    "            if(debug):\n",
    "                print(\"prec_t_pout\",prec_t_pout)\n",
    "            return prec_t_pout\n",
    "        \n",
    "        def r_t_pout(j):\n",
    "            kj = tf.gather(k,j)\n",
    "            if(debug):\n",
    "                print(\"r_t_pout kj\",kj)\n",
    "\n",
    "            if(smooth):\n",
    "                rec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            else:\n",
    "                rec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            if(debug):\n",
    "                print(\"prec_z\",prec_z)\n",
    "            if(smooth):\n",
    "                rec_t_pout = (tf.matmul(l*s_*kj, thetas,transpose_b=True))/prec_z\n",
    "            else:\n",
    "                rec_t_pout = (tf.matmul(l*kj, thetas,transpose_b=True))/prec_z\n",
    "            if(debug):\n",
    "                print(\"prec_t_pout\",prec_t_pout)\n",
    "            return rec_t_pout\n",
    "            \n",
    "           \n",
    "        def softplus_p(j):\n",
    "            aj = tf.gather(LF_a,j)\n",
    "            if(debug):\n",
    "                print(\"sft aj\",aj)\n",
    "            f_p =  tf.reduce_sum(aj - p_t_pout(j))\n",
    "            if(debug):\n",
    "                print(\"f_p\",f_p)\n",
    "            sft_p = tf.nn.softplus(f_p,name=\"sft_p\")\n",
    "            if(debug):\n",
    "                print(\"sft_p\",sft_p)\n",
    "            return sft_p\n",
    "        \n",
    "        def softplus_r(j):\n",
    "            rj = tf.gather(LF_r,j)\n",
    "            if(debug):\n",
    "                print(\"sft aj\",rj)\n",
    "            f_r =  tf.reduce_sum(r_t_pout(j)) - rj\n",
    "            if(debug):\n",
    "                print(\"f_r\",f_r)\n",
    "            sft_r = tf.nn.softplus(f_r,name=\"sft_r\")\n",
    "            if(debug):\n",
    "                print(\"sft_r\",sft_r)\n",
    "            return sft_r\n",
    "        \n",
    "#         logsft = tf.map_fn(lambda j: tf.log(softplus(j)),np.arange(NoOfLFs),\\\n",
    "#                                              dtype=tf.float64)\n",
    "#         sft  =  tf.map_fn(lambda j: softplus(j),np.arange(NoOfLFs),\\\n",
    "#                                              dtype=tf.float64)\n",
    "        \n",
    "# \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        if(debug):\n",
    "            print(\"zy\",zy)\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        if(debug):\n",
    "            print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        if(debug):\n",
    "            print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk))))\n",
    "        elif(penalty == 4):\n",
    "            print(\"precision penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 5):\n",
    "            print(\"precision log(softplus) penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus_p(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 6):\n",
    "            print(\"recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 7):\n",
    "            print(\"precision and recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 8):\n",
    "            print(\"precision and sign 1 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 9):\n",
    "            print(\"precision and sign 2 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                - (g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 10):\n",
    "            print(\"precision and sign 3 penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-p3k))))\n",
    "        else:\n",
    "            print(\"normalized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "       \n",
    "        if(debug):\n",
    "            print(\"loss\",loss)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        if(debug):\n",
    "            print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,loss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "                print(\"dev set\")\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "                print(a)\n",
    "                print(t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "                print(\"macro\",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "                print(\"weighted\",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"weighted\"))\n",
    "#                 print()\n",
    "#                 print(\"test set\")\n",
    "#                 sess.run(test_init_op)\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(\"acc\",accuracy_score(gold_labels_test,pl))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_test),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "                \n",
    "#             # Initialize an iterator over the validation dataset.\n",
    "#             sess.run(dev_init_op)\n",
    "#             a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "#             print(t)\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "# #             predictAndPrint(pl)\n",
    "#             print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision and sign 2 penalty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 10471941.850797305\n",
      "dev set\n",
      "[ 1.27378614e-03 -1.75942767e-03  1.93944116e-04  1.04602824e-04\n",
      "  1.16212577e-03  5.11842836e-04  1.79040426e-03  4.10459259e-04\n",
      "  1.56168915e-03  7.45591400e-04 -6.59435440e-05  7.64431209e-04\n",
      " -7.78284827e-05  8.84124414e-04  5.05894425e-04  5.20745496e-04\n",
      " -1.36822336e-03 -7.02291247e-04  6.94633112e-04 -1.53747562e-03\n",
      "  4.52121073e-04 -4.87318718e-04  6.05014297e-05  1.72841843e-03\n",
      " -1.16436149e-03  1.87312657e-03  7.79231747e-04  7.83423168e-04\n",
      "  6.73119438e-04  1.79342432e-06 -1.68172822e-03 -1.51788226e-03\n",
      " -2.80164780e-04  9.39069191e-04  1.06510042e-04 -1.32300963e-03\n",
      "  7.69115738e-04 -4.47742286e-04 -8.53110367e-04  2.70156504e-04\n",
      "  9.30478582e-04 -6.39995657e-04  1.24129173e-03 -1.03763439e-03\n",
      "  3.72930924e-04  1.05522078e-03  1.60830737e-03  1.07953645e-03\n",
      "  1.61253364e-04 -5.11518392e-04 -1.30757272e-03 -8.24987648e-04\n",
      " -1.54259699e-04  1.89966663e-03  2.06844389e-03 -1.92889428e-04\n",
      "  5.81189575e-05 -2.15791453e-04  5.47461186e-04 -1.33764466e-03\n",
      "  7.83582780e-04 -1.11223935e-03  5.08426698e-04  9.24678023e-04\n",
      "  1.29355943e-03  1.94143656e-03  7.57568711e-04  5.51488647e-04\n",
      "  1.04382878e-03 -1.59867971e-04  4.58625799e-04  3.35447853e-04\n",
      "  7.26407546e-04 -3.57756824e-04  1.46717708e-03 -5.21375716e-04\n",
      "  4.22934945e-04  2.36811603e-04  1.35807693e-03  4.14952531e-04\n",
      "  1.57589482e-03 -4.76050321e-04 -8.25718247e-04  8.38321994e-04\n",
      " -6.18544207e-04 -4.74003843e-04  7.84176698e-05  3.37142476e-04\n",
      "  3.68676328e-04 -2.94135111e-04  8.60666039e-04 -1.28064371e-03\n",
      " -5.56535250e-04 -6.15642145e-04  4.31124778e-04  1.01079960e-04\n",
      " -4.48102817e-04  3.59995538e-05 -1.67649775e-03  1.02821992e-03\n",
      "  1.02476188e-03  6.57106560e-04  1.53390514e-03 -1.25378146e-03\n",
      " -1.06289401e-03 -8.55201967e-04  5.69805493e-04 -2.45834137e-05\n",
      " -5.54985601e-04  2.76427810e-04  1.83426740e-05 -2.86529055e-04\n",
      " -4.78650409e-04 -6.83959343e-04  2.89985392e-04 -8.25527904e-04\n",
      " -1.59593096e-04 -1.55631642e-03  1.55435541e-03  1.15042482e-04\n",
      "  8.91864444e-04  8.83561577e-04  6.19854059e-05  9.96357389e-04\n",
      " -4.08736985e-04 -5.56342330e-06 -1.26297627e-04 -3.62630854e-04\n",
      "  1.02494526e-03  8.54640585e-04  2.59106543e-05 -4.50955714e-05\n",
      "  2.56368755e-05 -1.23368495e-03 -1.69264243e-04  9.43865957e-04\n",
      "  8.24776690e-04  8.48141155e-04 -4.38929879e-04 -1.49389939e-03\n",
      " -1.69147592e-03 -6.56603463e-06  6.18654221e-04 -1.17143919e-03\n",
      "  3.39646207e-04  8.14454500e-04  2.05502697e-03 -3.57022897e-04\n",
      " -2.14357863e-04  8.54034118e-04 -7.33237634e-04  1.76173871e-03\n",
      "  6.15274003e-04  1.08242702e-04  5.45373767e-04 -1.35901895e-03\n",
      " -5.26935702e-04  2.37403430e-04  1.23286218e-03  1.33436448e-03\n",
      " -3.86335545e-04 -7.62271736e-05  1.84166891e-03 -4.49642722e-04\n",
      " -1.01998104e-03 -7.26624733e-04  6.89773549e-04 -9.73435979e-04\n",
      "  1.30029236e-03 -8.08413076e-04  7.03053100e-04  4.69392387e-04\n",
      " -5.70017249e-04  1.79472350e-03  1.54786053e-03 -1.85674616e-03\n",
      "  2.76726467e-04 -8.52600194e-05  6.05157944e-04  6.45126168e-04\n",
      "  3.84016877e-04 -2.52506712e-04 -1.39222776e-03 -1.14915629e-03\n",
      "  1.21282952e-03  8.32419313e-04  3.04716896e-04  1.81018274e-03\n",
      " -1.62001137e-03  5.02627970e-04  1.96237975e-04  1.27417295e-03\n",
      "  1.18227549e-04 -3.84462501e-04  1.25109516e-03 -9.81155883e-04\n",
      " -7.97847984e-04 -2.81735803e-05 -4.19733884e-04 -7.33657534e-04\n",
      "  1.42939528e-03  7.34987899e-04  8.26623514e-05  6.91373152e-04\n",
      "  7.62596784e-05 -1.80941861e-04 -1.16468237e-03  1.29402881e-03\n",
      "  1.51168961e-03 -1.55678884e-03 -7.82866566e-04 -2.22262607e-04\n",
      " -4.25848942e-04  4.59287762e-04 -8.11496890e-05 -1.83910812e-03\n",
      "  8.81579063e-04 -3.30599086e-04  7.74615981e-04  2.89885561e-04\n",
      "  8.29921642e-04  1.40913514e-03  2.57624763e-04 -1.43544761e-03\n",
      " -1.01945692e-03  9.77357716e-04  3.83873845e-04  4.89400371e-04\n",
      "  1.22757535e-04  4.67284049e-04 -2.27633235e-05  3.15551790e-04\n",
      " -2.34583675e-05 -1.41118217e-03  3.87920968e-04  1.19634350e-03\n",
      "  4.13416529e-04  1.80451597e-03  1.33371934e-03  1.13845509e-03\n",
      " -1.04682483e-03 -7.46526531e-05 -2.69502047e-04 -2.12936766e-04\n",
      " -8.74347050e-04]\n",
      "[[1.11744624 0.81411608 1.00946141 1.00052595 1.10627959 1.04124012\n",
      "  1.16910766 1.03110949 1.14623642 1.06462494 0.98347224 1.06651584\n",
      "  0.98228346 1.0784833  1.04071541 1.04213772 0.85324298 0.91983617\n",
      "  1.05952641 0.83631766 1.03527783 0.94133705 0.99611629 1.1629128\n",
      "  0.87363511 1.1773792  1.06798741 1.06840943 1.05737792 0.9902454\n",
      "  0.8218932  0.83827916 0.96205219 1.08397174 1.00071914 0.85775449\n",
      "  1.06697435 0.94527448 0.90475592 1.01708274 1.08311345 0.92606767\n",
      "  1.11419668 0.88629799 1.02735961 1.09558577 1.15090124 1.09801039\n",
      "  1.00619208 0.93891251 0.85931021 0.90756617 0.97463976 1.18004458\n",
      "  1.19690981 0.97077478 0.99593923 0.96848631 1.04481128 0.85630031\n",
      "  1.06842402 0.8788404  1.04090872 1.08253672 1.11942102 1.18421167\n",
      "  1.06582825 1.0452135  1.09444836 0.97407922 1.03592859 1.0236108\n",
      "  1.06270859 0.95428835 1.13678378 0.93792697 1.03235975 1.01374232\n",
      "  1.1258676  1.03155754 1.14765755 0.94246094 0.90749394 1.07390028\n",
      "  0.92821222 0.94265798 0.99790925 1.02378011 1.02693032 0.96063702\n",
      "  1.076134   0.86199903 0.93441461 0.92850137 1.03317898 1.00017748\n",
      "  0.94525665 0.9936672  0.822458   1.09288806 1.09254302 1.05577613\n",
      "  1.14345579 0.86468692 0.88377601 0.90454939 1.0470472  0.98760636\n",
      "  0.93457203 1.01770935 0.99189772 0.96141397 0.94220099 0.92167011\n",
      "  1.0190646  0.90751335 0.97410423 0.83443265 1.14549957 1.00155871\n",
      "  1.07924476 1.07840728 0.99626747 1.0897017  0.94919182 0.98951062\n",
      "  0.97743622 0.95380289 1.09256055 1.07552777 0.99265895 0.98555633\n",
      "  0.99262966 0.86669749 0.97313961 1.08446045 1.07254361 1.07488161\n",
      "  0.94620256 0.84067605 0.82091868 0.98940937 1.05193139 0.87292174\n",
      "  1.02402771 1.07151165 1.19556734 0.9543647  0.96863024 1.07546949\n",
      "  0.91674215 1.16624297 1.05159337 1.00087998 1.04460335 0.85416499\n",
      "  0.93737244 1.01380241 1.11334364 1.12350242 0.95142912 0.98244533\n",
      "  1.174226   0.94510306 0.88806793 0.91740516 1.05903377 0.89272406\n",
      "  1.12009317 0.90922676 1.06037149 1.03700616 0.93307128 1.16953914\n",
      "  1.14485768 0.80446076 1.01773625 0.98153967 1.05058131 1.05457747\n",
      "  1.02846731 0.9648154  0.85084712 0.87515125 1.11135435 1.07331368\n",
      "  1.0205387  1.17108138 0.82806625 1.04032912 1.00969027 1.11748382\n",
      "  1.00188984 0.95162598 1.11517463 0.89195324 0.91027142 0.9872459\n",
      "  0.94807674 0.9167035  1.13300693 1.06356471 0.99833378 1.05920532\n",
      "  0.99768485 0.97197314 0.87359774 1.11947039 1.14122531 0.83438876\n",
      "  0.9117772  0.96784184 0.94748126 1.03599571 0.98195795 0.80615606\n",
      "  1.07822959 0.95707007 1.06752521 1.01905423 1.07305765 1.13097833\n",
      "  1.01582805 0.84652134 0.88812416 1.08780258 1.02845886 1.03901181\n",
      "  1.00234273 1.03679149 0.98779099 1.0216215  0.98772059 0.84894839\n",
      "  1.02885916 1.10970612 1.03140678 1.17052027 1.12342844 1.10390844\n",
      "  0.8853675  0.98260392 0.96311726 0.9687722  0.90263283]]\n",
      "{0: 19, 1: 92, 2: 17, 3: 57, 4: 210, 5: 942, 6: 1, 7: 7, 8: 125, 9: 12, 10: 31, 11: 115, 12: 12, 13: 302, 14: 1255, 15: 15, 16: 15, 17: 2, 18: 109, 19: 29, 20: 1, 21: 7, 22: 57, 23: 204, 24: 25, 25: 60, 26: 240, 27: 37, 29: 1, 30: 106, 31: 61, 32: 628, 33: 26, 34: 67, 35: 303, 36: 546, 37: 773, 38: 99, 39: 15, 40: 42, 41: 2}\n",
      "acc 0.5776211189440528\n",
      "macro (0.34582249372457996, 0.3799190681738053, 0.31017847231706464, None)\n",
      "weighted (0.6455720863804449, 0.5776211189440528, 0.5842496177716572, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5],\n",
       "       [37],\n",
       "       [18],\n",
       "       ...,\n",
       "       [13],\n",
       "       [11],\n",
       "       [ 5]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.empty([len(LF_l)],dtype=np.float64)\n",
    "acc.fill(0.25)\n",
    "rec =  np.empty([len(LF_l)],dtype=np.float64)\n",
    "rec.fill(0.85*train_L_S.shape[0])\n",
    "# print(rec)\n",
    "train(0.1/len(train_L_S),1,batch_size = 1024, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = acc ,LF_rec = rec,\\\n",
    "                                pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=9,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k Tensor(\"Const:0\", shape=(84,), dtype=float64)\n",
      "s Tensor(\"unstack:1\", shape=(?, 84), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 84), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 84), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 84), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(84,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(84,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 2891530.938560508\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04]\n",
      "[[1.01797173 0.71431234 0.90958736 0.90071519 1.00750348 0.94252603\n",
      "  1.06921356 0.93126556 1.04720388 0.96493863 0.8835886  0.96770179\n",
      "  0.88239354 0.97969254 0.94927654 0.94223718 0.75338679 0.81995797\n",
      "  0.9616252  0.73649025 0.93538125 0.84154438 0.8964677  1.06360825\n",
      "  0.77386608 1.07767214 0.96895978 0.9687006  0.95747733 0.89034551\n",
      "  0.72231786 0.73884665 0.86600805 0.98412357 0.90082923 0.75918905\n",
      "  0.97272084 0.84730529 0.80711244 0.91718104 0.98331424 0.82633128\n",
      "  1.01478626 0.78650684 0.92750334 0.99583559 1.05210419 0.9993594\n",
      "  0.90629883 0.83907145 0.76032876 0.80788706 0.87476913 1.08127409\n",
      "  1.09705233 0.87199149 0.90453927 0.86858585 0.94496381 0.75642351\n",
      "  0.97066108 0.779021   0.94101139 0.98275095 1.01981567 1.08495868\n",
      "  0.96607127 0.94551182 0.99546263 0.87438069 0.93603619 0.92371175\n",
      "  0.96316055 0.85490725 1.04089469 0.83808424 0.93247509 0.91524765\n",
      "  1.03164215 0.93382792 1.05014374 0.8425593  0.80769482 0.97417055]]\n",
      "{0: 611, 1: 32, 2: 11, 3: 59, 4: 237, 5: 298, 6: 5, 7: 9, 8: 164, 9: 38, 10: 1, 11: 142, 12: 49, 13: 204, 14: 1186, 15: 1, 16: 20, 18: 441, 21: 26, 22: 63, 23: 185, 24: 32, 25: 25, 26: 270, 27: 28, 28: 9, 30: 109, 31: 57, 32: 699, 33: 9, 34: 1, 35: 219, 36: 529, 37: 493, 38: 338, 40: 26, 41: 41}\n",
      "acc 0.5017249137543123\n",
      "macro (0.37185405736958477, 0.37011207901765747, 0.2965660954350633, None)\n",
      "weighted (0.6520466478251481, 0.5017249137543123, 0.534905616152077, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [37],\n",
       "       [18],\n",
       "       ...,\n",
       "       [13],\n",
       "       [11],\n",
       "       [14]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 84\n",
    "train(0.1/len(train_L_S),1,batch_size = 1, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=False,smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k Tensor(\"Const:0\", shape=(161,), dtype=float64)\n",
      "s Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "unnormlized loss\n",
      "loss Tensor(\"Neg:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 5558252.154030701\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.01766771 0.71451864 0.90967454 0.90125504 1.00945075 0.94748484\n",
      "  1.06925826 0.9312861  1.04760723 0.96512827 0.88373718 0.96940625\n",
      "  0.8824901  0.98389568 0.9703818  0.94226115 0.75344896 0.81997675\n",
      "  0.96010973 0.73652628 0.93543582 0.84161514 0.89733215 1.06427228\n",
      "  0.77406267 1.07808702 0.96914886 0.96914848 0.95747627 0.89035627\n",
      "  0.72303547 0.73901581 0.87403703 0.98421998 0.90107883 0.7610906\n",
      "  0.98315947 0.86298641 0.80544427 0.91720298 0.98360504 0.82648122\n",
      "  1.01437664 0.78667656 0.92750898 0.9959006  1.05387092 1.00219347\n",
      "  0.90630633 0.83908417 0.76035166 0.80800801 0.87477681 1.08264045\n",
      "  1.0970308  0.87511335 0.92333    0.86858702 0.94499187 0.75643489\n",
      "  0.96957339 0.77902983 0.94102291 0.98281514 1.02020214 1.08538438\n",
      "  0.96621279 0.94573986 0.99556344 0.87472996 0.9360272  0.9237109\n",
      "  0.96377935 0.85496952 1.04696272 0.83811917 0.93252426 0.91686656\n",
      "  1.04132934 0.939908   1.04944477 0.8425593  0.80792213 0.97429386\n",
      "  0.82839963 0.84306425 0.89811312 0.92443791 0.92935883 0.86516401\n",
      "  0.97625481 0.76217333 0.83578999 0.82899116 0.93343363 0.90299293\n",
      "  0.84542386 0.89893147 0.75143251 0.9929896  0.99274198 0.95591637\n",
      "  1.04468529 0.76489104 0.78388456 0.80483463 0.94825916 0.88891814\n",
      "  0.83497336 0.91831782 0.89307959 0.86213388 0.84230181 0.82177201\n",
      "  0.92017253 0.80825789 0.88571829 0.73463433 1.04574115 0.90484168\n",
      "  0.99526348 0.99593734 0.89809657 0.98980006 0.8496632  0.88991174\n",
      "  0.87753457 0.85390125 0.99265936 0.97678663 0.89637739 0.88565512\n",
      "  0.89272802 0.76679585 0.87336189 0.98631746 0.97267171 0.98017952\n",
      "  0.87438109 0.74077441 0.72102187 0.88950773 0.95202975 0.77306093\n",
      "  0.92492568 0.97161484 1.09588649 0.85451219 0.8687289  0.97556938\n",
      "  0.81684034 1.06650733 0.95169173 0.90945146 0.94470171 0.75443736\n",
      "  0.83747543 0.92894053 1.02991783 1.02360077 0.8517935 ]]\n",
      "{0: 150, 1: 30, 2: 8, 3: 55, 4: 208, 5: 649, 6: 16, 7: 5, 8: 179, 9: 41, 10: 21, 11: 115, 12: 6, 13: 276, 14: 1277, 15: 7, 16: 8, 18: 222, 20: 1, 21: 4, 22: 78, 23: 224, 24: 21, 25: 94, 26: 183, 27: 35, 30: 77, 31: 47, 32: 623, 33: 23, 34: 12, 35: 216, 36: 547, 37: 966, 38: 155, 39: 7, 40: 42, 41: 39}\n",
      "acc 0.5561721913904305\n",
      "macro (0.3327161823876088, 0.35830051082025394, 0.29291334778106265, None)\n",
      "weighted (0.6330273786087535, 0.5561721913904305, 0.5646183404358545, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5],\n",
       "       [37],\n",
       "       [18],\n",
       "       ...,\n",
       "       [13],\n",
       "       [11],\n",
       "       [33]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 161\n",
    "train(0.1/len(train_L_S),1,batch_size = 1, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=False,smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k Tensor(\"Const:0\", shape=(245,), dtype=float64)\n",
      "s Tensor(\"unstack:1\", shape=(?, 245), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 245), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 245), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 245), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 245), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(245,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(245,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(245,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(245,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty1\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 9197259.507979987\n",
      "dev set\n",
      "[0.10064254 0.09660149 0.09959496 0.09802814 0.0929528  0.06875737\n",
      " 0.10149933 0.09987804 0.09877226 0.10006883 0.09927962 0.09394388\n",
      " 0.09936648 0.08764075 0.03966486 0.1001107  0.0979733  0.09891506\n",
      " 0.09899204 0.09739545 0.10008252 0.09887764 0.09756113 0.09709413\n",
      " 0.0975546  0.09976393 0.09688588 0.09885798 0.10041308 0.0997064\n",
      " 0.09447869 0.09588166 0.07182794 0.1000929  0.0984429  0.08757585\n",
      " 0.06720515 0.06335376 0.09724044 0.09988746 0.09972549 0.09848937\n",
      " 0.10075438 0.09751807 0.09998204 0.10026073 0.09412926 0.0873899\n",
      " 0.09987189 0.09893964 0.09626606 0.09850348 0.09942155 0.0960049\n",
      " 0.10177386 0.08872988 0.04303361 0.09951017 0.10002727 0.09827015\n",
      " 0.09882047 0.09790777 0.10021531 0.10033202 0.09954525 0.09841871\n",
      " 0.09966731 0.09913954 0.09735389 0.09808127 0.10019428 0.10007129\n",
      " 0.09739205 0.09736624 0.07765862 0.09882758 0.09994228 0.0905011\n",
      " 0.06968679 0.08171164 0.0988724  0.09925512 0.0980458  0.1000473\n",
      " 0.09881952 0.09798864 0.09951049 0.09843838 0.09414335 0.08357835\n",
      " 0.10057895 0.09813695 0.09649791 0.09867841 0.09980402 0.09336173\n",
      " 0.09910421 0.08707352 0.03660428 0.10077744 0.10047663 0.100309\n",
      " 0.09939449 0.09771778 0.09861143 0.09849178 0.09813263 0.09539793\n",
      " 0.09823657 0.09863447 0.09600363 0.09776877 0.09923683 0.09901989\n",
      " 0.09684722 0.09668115 0.07318702 0.09776061 0.10084392 0.0896755\n",
      " 0.06792554 0.06641154 0.09714177 0.10075955 0.09838995 0.09916071\n",
      " 0.09959633 0.09935938 0.10077577 0.09792331 0.08552555 0.09968347\n",
      " 0.09975981 0.09846551 0.09934993 0.09670644 0.10052686 0.08810179\n",
      " 0.04096019 0.09819568 0.09797214 0.09972864 0.10036426 0.09844973\n",
      " 0.09856567 0.10054085 0.10130356 0.09921721 0.09951378 0.10059652\n",
      " 0.09898011 0.10113193 0.10036241 0.08144688 0.10028564 0.0973072\n",
      " 0.09918275 0.06862645 0.06906688 0.10109009 0.09869892 0.09923055\n",
      " 0.10034105 0.09890067 0.09677175 0.09043938 0.06887979 0.09864726\n",
      " 0.10076935 0.09604129 0.1000071  0.09980522 0.09219926 0.1012636\n",
      " 0.0882866  0.03366706 0.09983277 0.09928388 0.10023367 0.09827213\n",
      " 0.09939152 0.09933407 0.09791075 0.09618941 0.09592363 0.09960761\n",
      " 0.09804048 0.09778128 0.09621564 0.100228   0.09987219 0.09775068\n",
      " 0.09760189 0.07085064 0.10038868 0.09720213 0.08815602 0.06529551\n",
      " 0.06290119 0.09616921 0.10104703 0.09948597 0.09921423 0.10002413\n",
      " 0.09847231 0.09917743 0.09661256 0.09297588 0.07037093 0.0980437\n",
      " 0.09862123 0.09669572 0.09883609 0.09979463 0.0927958  0.09750929\n",
      " 0.08737152 0.03702685 0.10034397 0.09967109 0.10046276 0.09909631\n",
      " 0.09925927 0.09811324 0.09829953 0.0985141  0.09495383 0.09924715\n",
      " 0.09784386 0.09626313 0.09794774 0.10003693 0.09964716 0.09472009\n",
      " 0.09789629 0.07355095 0.09952147 0.10017104 0.09099103 0.06735832\n",
      " 0.06169107 0.09690189 0.09930605 0.09848851 0.09821554]\n",
      "[[1.02030031 0.71778806 0.91411309 0.911152   1.0202954  0.97150895\n",
      "  1.07759046 0.93287778 1.05268155 0.96682209 0.88727914 0.97957997\n",
      "  0.89194712 1.00103127 1.01988502 0.94643371 0.75516152 0.82070039\n",
      "  0.96341531 0.73891692 0.94408413 0.84287806 0.90171855 1.07446992\n",
      "  0.78286118 1.08600467 0.97457621 0.97327002 0.9581418  0.89825088\n",
      "  0.72974925 0.74480714 0.903012   0.98895992 0.90824302 0.77375111\n",
      "  1.01880889 0.89439065 0.80895512 0.92182688 0.99087144 0.83403486\n",
      "  1.01536377 0.78913913 0.92831662 0.99727563 1.0639166  1.02120009\n",
      "  0.90701357 0.84011717 0.76404655 0.80881919 0.8755974  1.09155887\n",
      "  1.09776578 0.89089283 0.97308813 0.86925579 0.94605303 0.75708994\n",
      "  0.97464074 0.78039793 0.9417394  0.98371384 1.0229081  1.09051965\n",
      "  0.96812317 0.94820349 0.99928841 0.87753579 0.93673634 0.9243862\n",
      "  0.96848735 0.85997795 1.07431742 0.83936866 0.93348524 0.92722101\n",
      "  1.07605607 0.96350242 1.05395428 0.8432149  0.80947959 0.97503194\n",
      "  0.82963799 0.84686093 0.89954182 0.92741661 0.93863114 0.88679065\n",
      "  0.97716232 0.7638557  0.84386485 0.83076172 0.93531784 0.91356822\n",
      "  0.84721134 0.91616537 0.8023868  0.99366305 0.99433171 0.95662874\n",
      "  1.05275306 0.76730393 0.78469558 0.80620028 0.95267115 0.89604376\n",
      "  0.83749581 0.92105506 0.89857004 0.8656324  0.84388226 0.82254746\n",
      "  0.92617638 0.81652997 0.91384723 0.73674161 1.04712969 0.91684028\n",
      "  1.03037873 1.02560447 0.90589774 0.99045441 0.85134128 0.89087967\n",
      "  0.8782006  0.85456439 0.99332064 0.98287532 0.91572654 0.88633834\n",
      "  0.89338858 0.76745866 0.8835123  0.99312236 0.9778269  0.99793337\n",
      "  0.91952017 0.74143802 0.72171133 0.89016801 0.95269382 0.77382188\n",
      "  0.92766377 0.9723056  1.09732949 0.85537219 0.86940105 0.97630724\n",
      "  0.81751787 1.06778305 0.95235544 0.93094476 0.94536538 0.75660735\n",
      "  0.83815376 0.96342813 1.05948586 1.02426268 0.85326198 0.88420955\n",
      "  1.07650093 0.8475921  0.79216713 0.82900874 0.98932647 0.79545544\n",
      "  1.0213406  0.81236721 0.96179839 0.93842467 0.84276565 1.0724609\n",
      "  1.0611438  0.77324562 0.91914588 0.8831283  0.95183598 0.95780296\n",
      "  0.93007941 0.86770592 0.75255906 0.77878432 1.01738931 0.97560302\n",
      "  0.92420779 1.07577962 0.73113229 0.94110421 0.91207403 1.0220414\n",
      "  0.90518651 0.88675056 1.01697226 0.79471915 0.82149022 0.9296481\n",
      "  0.88624962 0.8203898  1.03441702 0.96681808 0.90148977 0.96105389\n",
      "  0.89980837 0.87392974 0.77739422 1.02864385 1.06724626 0.73710981\n",
      "  0.81313292 0.87100551 0.84868845 0.93737627 0.89084321 0.70943158\n",
      "  0.99355415 0.92006414 0.96909116 0.92048551 0.97402188 1.03381722\n",
      "  0.91736901 0.74964508 0.78974093 0.99114432 0.93404336 0.941068\n",
      "  0.90612848 0.94148114 0.8904292  0.92239684 0.89010663 0.75331249\n",
      "  0.93223072 1.03990966 0.93384481 1.07329103 1.03332484 1.04193737\n",
      "  0.82433318 0.88609436 0.86465446 0.87211113 0.80611653]]\n",
      "{0: 16, 1: 56, 2: 9, 3: 55, 4: 210, 5: 1049, 6: 1, 7: 5, 8: 106, 9: 11, 10: 10, 11: 114, 12: 10, 13: 289, 14: 1325, 15: 13, 16: 5, 17: 2, 18: 84, 19: 24, 20: 1, 21: 6, 22: 49, 23: 207, 24: 24, 25: 53, 26: 218, 27: 33, 29: 1, 30: 73, 31: 49, 32: 622, 33: 18, 34: 67, 35: 315, 36: 561, 37: 848, 38: 83, 39: 15, 40: 28, 41: 2}\n",
      "acc 0.584070796460177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro (0.35723459397522883, 0.35605849923403976, 0.30320977925150483, None)\n",
      "weighted (0.6373261103617502, 0.584070796460177, 0.5819657556725764, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5],\n",
       "       [37],\n",
       "       [18],\n",
       "       ...,\n",
       "       [13],\n",
       "       [11],\n",
       "       [ 5]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(0.1/len(train_L_S),1,batch_size = 1, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k Tensor(\"Const:0\", shape=(245,), dtype=float64)\n",
      "s Tensor(\"unstack:1\", shape=(?, 245), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 245), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 245), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 245), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 245), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(245,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(245,), dtype=float64)\n",
      "ky Tensor(\"zy/while/Select:0\", shape=(245,), dtype=float64)\n",
      "intsy Tensor(\"zy/while/add:0\", shape=(245,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "penalty1\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 7951687.772587098\n",
      "dev set\n",
      "[0.41731684 0.11293839 0.30908322 0.29864367 0.40299346 0.32722348\n",
      " 0.46910989 0.33083963 0.44435285 0.36442528 0.28323321 0.3621481\n",
      " 0.28192694 0.3715494  0.28747587 0.34204331 0.1528451  0.21946741\n",
      " 0.35902803 0.13506452 0.33501041 0.24083717 0.29483314 0.46163063\n",
      " 0.17287079 0.47667663 0.36720962 0.36721081 0.35731997 0.29015449\n",
      " 0.11879432 0.13587917 0.24168363 0.38372526 0.29963863 0.14892428\n",
      " 0.34162095 0.21700853 0.20344415 0.31700094 0.38255956 0.2254474\n",
      " 0.41406543 0.18533296 0.32719061 0.39516641 0.4479806  0.39107763\n",
      " 0.30610237 0.23859477 0.15698469 0.20730206 0.27446313 0.47664232\n",
      " 0.49707699 0.26378051 0.24586632 0.26840802 0.34456488 0.1559018\n",
      " 0.3679672  0.17776831 0.34081093 0.38215199 0.4186008  0.48329193\n",
      " 0.36529331 0.34450571 0.39371817 0.27288553 0.33586601 0.32353558\n",
      " 0.36059721 0.25247844 0.42181596 0.23763328 0.33217638 0.30711603\n",
      " 0.40244589 0.3173047  0.44645338 0.24238339 0.20680912 0.37344233\n",
      " 0.22798844 0.24171088 0.29757198 0.32225769 0.32382792 0.25113634\n",
      " 0.3760636  0.16163422 0.23190493 0.22822837 0.33296551 0.295531\n",
      " 0.24503201 0.28620006 0.06093395 0.39283984 0.39231366 0.35548645\n",
      " 0.44304991 0.16351555 0.18362762 0.20401983 0.34586145 0.28595828\n",
      " 0.2338869  0.31689026 0.29100382 0.2600677  0.2421209  0.22156221\n",
      " 0.31678492 0.20537314 0.25452081 0.13407705 0.44527859 0.29433019\n",
      " 0.35436186 0.35409896 0.29465058 0.38965907 0.24849612 0.28896971\n",
      " 0.27735927 0.25371602 0.39250946 0.37396582 0.28478382 0.28547888\n",
      " 0.29255282 0.1665844  0.27302981 0.38165551 0.37249018 0.36805795\n",
      " 0.19219562 0.14055604 0.12079297 0.28932788 0.35186857 0.17272102\n",
      " 0.32314506 0.37145566 0.49549119 0.2542326  0.26854587 0.37541777\n",
      " 0.21664487 0.46607114 0.35153043 0.28721352 0.34454154 0.15310548\n",
      " 0.23727487 0.28872455 0.39140992 0.42346312 0.25091596 0.28218617\n",
      " 0.47347864 0.24462897 0.18579601 0.21316168 0.34524551 0.19249355\n",
      " 0.41983415 0.20644636 0.36012904 0.33675256 0.22780134 0.46926122\n",
      " 0.43824889 0.03867932 0.31757704 0.28119376 0.35026662 0.35396828\n",
      " 0.32748472 0.26444771 0.15022061 0.17357597 0.40974601 0.37270153\n",
      " 0.31951804 0.47033962 0.12628514 0.34025599 0.30954894 0.41530882\n",
      " 0.29989687 0.23052786 0.41487847 0.19060461 0.20193505 0.25946081\n",
      " 0.21927201 0.21480468 0.43291614 0.36291638 0.29773955 0.35898147\n",
      " 0.29674308 0.27151606 0.17127657 0.41612206 0.42858104 0.13412856\n",
      " 0.21140939 0.26526866 0.24718204 0.33574095 0.27698825 0.1056189\n",
      " 0.37114047 0.1988091  0.3673837  0.31872759 0.37275103 0.43042696\n",
      " 0.31482783 0.1460694  0.18753304 0.38661233 0.32667727 0.33836692\n",
      " 0.30129643 0.33588986 0.28636164 0.32154366 0.28757041 0.14589624\n",
      " 0.32693117 0.39173396 0.33106721 0.46958629 0.41690984 0.37889526\n",
      " 0.15449793 0.28086477 0.26296718 0.268027   0.201943  ]\n",
      "[[1.04517709 0.7172694  0.92189099 0.92118883 1.0370064  0.96667543\n",
      "  1.12161762 0.93663923 1.09147591 0.97725777 0.89340935 0.99463131\n",
      "  0.90056333 1.01682003 1.04127076 0.95950417 0.75522849 0.82132479\n",
      "  0.9711942  0.73942212 0.95811611 0.8436923  0.90662459 1.11588013\n",
      "  0.78496616 1.13617108 0.98889296 0.98538729 0.95941269 0.90793318\n",
      "  0.72855597 0.74469691 0.90586578 1.01046706 0.91803099 0.77133562\n",
      "  1.04359403 0.89645955 0.80940511 0.93263649 1.01362208 0.83837384\n",
      "  1.01925383 0.78916943 0.92935992 1.00130704 1.10072495 1.03913354\n",
      "  0.90785396 0.84057425 0.76397227 0.8094276  0.8761745  1.167858\n",
      "  1.19561576 0.89274544 0.98861021 0.86975226 0.94776643 0.75744033\n",
      "  0.99320428 0.78117971 0.94300641 0.98711138 1.03482709 1.16070258\n",
      "  0.97148628 0.95049803 1.00970476 0.87891858 0.937918   0.9253171\n",
      "  0.97541497 0.86342253 1.11611717 0.83971462 0.93452833 0.92996558\n",
      "  1.11873769 0.98132378 1.09508061 0.84359303 0.80955441 0.97722981\n",
      "  0.8301262  0.84902798 0.90152998 0.93154636 0.94279834 0.88517492\n",
      "  0.98118607 0.76417736 0.84833839 0.83255962 0.94180724 0.92066823\n",
      "  0.84897928 0.91897831 0.80531603 0.9957558  1.00373812 0.95882183\n",
      "  1.08846739 0.76812998 0.78490602 0.80673582 0.96288856 0.89687999\n",
      "  0.83840767 0.92256986 0.90284851 0.8675503  0.84543706 0.82291473\n",
      "  0.93295272 0.81964467 0.91877767 0.73678022 1.07028983 0.92233719\n",
      "  1.05824111 1.04492881 0.9151899  0.99240478 0.85183144 0.89193665\n",
      "  0.87874123 0.85499023 0.99532854 0.99847546 0.91834498 0.88696943\n",
      "  0.89402513 0.76759893 0.89231546 1.00944852 0.99733712 1.01271228\n",
      "  0.92890006 0.74151897 0.72173331 0.89078712 0.9538969  0.77401395\n",
      "  0.93245419 0.973881   1.19404451 0.85569103 0.86991615 0.97857056\n",
      "  0.81780772 1.11288813 0.95357094 0.94050522 0.94646352 0.75656663\n",
      "  0.83851131 0.97957808 1.08978044 1.02775609 0.85365707 0.88543768\n",
      "  1.08110881 0.84871834 0.79260217 0.82489557 0.98202071 0.79620105\n",
      "  1.0239084  0.81257053 0.96375836 0.93984891 0.84285544 1.08349042\n",
      "  1.06931712 0.77520684 0.92059084 0.88408749 0.95403245 0.95869745\n",
      "  0.93200763 0.86947714 0.75290321 0.77830036 1.0188622  0.97833758\n",
      "  0.92649628 1.08670096 0.73100288 0.94216094 0.91442803 1.02616226\n",
      "  0.90647265 0.88570174 1.02181169 0.79491256 0.81974755 0.93391899\n",
      "  0.88648269 0.82014799 1.04009666 0.97149039 0.90445701 0.9637592\n",
      "  0.90024285 0.87494453 0.77772997 1.02932414 1.06251395 0.73726045\n",
      "  0.81347625 0.87163043 0.84897947 0.93861091 0.89135507 0.70929379\n",
      "  0.99331305 0.92585654 0.9713745  0.92149648 0.9755621  1.03539231\n",
      "  0.91873633 0.7497788  0.79018803 0.99355856 0.93247791 0.94227192\n",
      "  0.90784794 0.94188904 0.89101005 0.92326076 0.89174484 0.75283225\n",
      "  0.93388232 1.0468377  0.93667541 1.07987253 1.03631375 1.05108788\n",
      "  0.8208311  0.88625498 0.86563006 0.87369908 0.80697826]]\n",
      "{0: 9, 1: 36, 2: 25, 3: 123, 4: 158, 5: 856, 6: 11, 7: 7, 8: 149, 9: 1, 10: 10, 11: 116, 12: 13, 13: 236, 14: 1550, 15: 18, 16: 13, 17: 15, 18: 19, 19: 45, 20: 45, 21: 15, 22: 48, 23: 130, 24: 24, 25: 34, 26: 19, 27: 44, 29: 1, 30: 100, 31: 77, 32: 631, 33: 14, 34: 81, 35: 484, 36: 524, 37: 846, 38: 98, 39: 14, 40: 25, 41: 3}\n",
      "acc 0.5882705864706764\n",
      "macro (0.4091648197518863, 0.389882956476814, 0.337463050229747, None)\n",
      "weighted (0.6560543030783744, 0.5882705864706764, 0.5873520612414092, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5],\n",
       "       [37],\n",
       "       [17],\n",
       "       ...,\n",
       "       [13],\n",
       "       [11],\n",
       "       [14]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(0.1/len(train_L_S),1,batch_size = 1, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 512\n",
      "precision penalty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 9197632.259021793\n",
      "dev set\n",
      "[0.31774127 0.0144114  0.20975632 0.200821   0.3065745  0.2415356\n",
      " 0.36940266 0.23140461 0.34653136 0.26492    0.18376731 0.26681058\n",
      " 0.18257851 0.2787783  0.24100731 0.24243286 0.05353803 0.12013128\n",
      " 0.25982155 0.03661275 0.23557292 0.14163197 0.19641136 0.36320777\n",
      " 0.07392998 0.37767423 0.26828243 0.26870444 0.25767297 0.19054042\n",
      " 0.0221882  0.03857413 0.16234719 0.2842668  0.20101416 0.05804994\n",
      " 0.26726953 0.14557006 0.10505096 0.21737772 0.28340848 0.12636264\n",
      " 0.31449167 0.08659321 0.22765464 0.29588091 0.35119612 0.2983058\n",
      " 0.2064871  0.13920763 0.05960522 0.10786124 0.17493482 0.38033919\n",
      " 0.39720487 0.17106999 0.19623114 0.16878138 0.24510635 0.05659543\n",
      " 0.2687191  0.07913552 0.24120377 0.28283166 0.31971611 0.38450668\n",
      " 0.26612311 0.24550859 0.29474341 0.17437427 0.23622364 0.22390584\n",
      " 0.26300357 0.15458345 0.33707886 0.13822207 0.23265479 0.21403758\n",
      " 0.32616295 0.23185273 0.34795254 0.14275599 0.107789   0.27419526\n",
      " 0.12850724 0.1429533  0.19820424 0.22407515 0.22722534 0.1609328\n",
      " 0.27642902 0.06229416 0.13470959 0.12879644 0.23347398 0.20047232\n",
      " 0.14555168 0.19396231 0.02274991 0.29318311 0.29283802 0.25607121\n",
      " 0.34375089 0.06498202 0.08407108 0.10484429 0.24734225 0.18790144\n",
      " 0.13486688 0.21800438 0.19219273 0.16170898 0.142496   0.12196515\n",
      " 0.21935962 0.10780833 0.17439948 0.03472775 0.34579467 0.20185416\n",
      " 0.2795401  0.27870276 0.19656241 0.28999675 0.14948688 0.18980562\n",
      " 0.17773127 0.15409794 0.2928556  0.27582283 0.19295398 0.18585138\n",
      " 0.19292471 0.06699254 0.17343467 0.28475521 0.27283863 0.27517672\n",
      " 0.14649557 0.0409711  0.02121371 0.18970442 0.25222644 0.07321679\n",
      " 0.22432286 0.27180669 0.39586243 0.15465971 0.16892529 0.27576453\n",
      " 0.1170372  0.3665379  0.25188842 0.20117541 0.2448984  0.05446\n",
      " 0.13766749 0.2140976  0.31363897 0.32379747 0.15172429 0.18274029\n",
      " 0.37452121 0.145398   0.08836288 0.11770002 0.25932912 0.093019\n",
      " 0.32038826 0.10952173 0.2606665  0.23730114 0.133366   0.36983413\n",
      " 0.34515264 0.00475182 0.21803135 0.18183466 0.25087635 0.25487252\n",
      " 0.22876234 0.16511042 0.05114195 0.07544623 0.31164925 0.27360849\n",
      " 0.22083365 0.37137645 0.0283612  0.24062415 0.20998524 0.3177788\n",
      " 0.20218481 0.15192078 0.31546967 0.09224819 0.11056678 0.1875411\n",
      " 0.14837226 0.11699839 0.33330191 0.2638597  0.19862871 0.25950029\n",
      " 0.19798012 0.17226807 0.07389275 0.3197653  0.34152079 0.03468373\n",
      " 0.11207231 0.16813675 0.1477763  0.23629068 0.18225269 0.00645103\n",
      " 0.27852457 0.15736223 0.26782031 0.21934922 0.27335272 0.33127342\n",
      " 0.21612313 0.04681634 0.08841901 0.28809758 0.22875377 0.23930663\n",
      " 0.20263772 0.23708653 0.18808596 0.22191654 0.18801561 0.04924334\n",
      " 0.22915414 0.31000107 0.2317018  0.37081523 0.32372379 0.30420366\n",
      " 0.08566301 0.18289884 0.16341222 0.16906724 0.1029278 ]\n",
      "[[1.11734761 0.8140176  1.00936352 1.00042759 1.10618197 1.04114165\n",
      "  1.16900974 1.03101123 1.14613878 1.06452667 0.98337375 1.06641825\n",
      "  0.98218517 1.07838477 1.04069016 1.04203935 0.85314467 0.91973777\n",
      "  1.05942796 0.8362193  1.03517939 0.94123885 0.99601782 1.16281435\n",
      "  0.87353682 1.17728096 1.06788943 1.06831108 1.05727961 0.99014727\n",
      "  0.82179491 0.8381809  0.96195386 1.08387361 1.00062063 0.85765596\n",
      "  1.06687778 0.94517599 0.90465752 1.01698455 1.08301548 0.92596951\n",
      "  1.11409847 0.88619955 1.02726132 1.09548737 1.15080321 1.09791155\n",
      "  1.00609378 0.93881414 0.85921186 0.90746787 0.97454141 1.17994802\n",
      "  1.19681178 0.97067621 0.9958608  0.96838798 1.04471297 0.85620192\n",
      "  1.06832564 0.87874203 1.0408104  1.08243848 1.11932266 1.18411327\n",
      "  1.06573013 1.04511518 1.09435009 0.97398093 1.0358303  1.02351248\n",
      "  1.0626102  0.95419002 1.13668648 0.93782865 1.03226143 1.01364376\n",
      "  1.1257755  1.03145912 1.14755932 0.94236262 0.9073956  1.07380201\n",
      "  0.92811389 0.94255952 0.99781102 1.02368189 1.02683246 0.96053793\n",
      "  1.07603565 0.8619007  0.93431637 0.92840306 1.03308084 1.00007958\n",
      "  0.94515827 0.99356861 0.82236555 1.09278977 1.09244479 1.05567776\n",
      "  1.14335748 0.86458855 0.88367767 0.90445115 1.04694874 0.98750806\n",
      "  0.93447389 1.01761102 0.99179979 0.96131569 0.9421028  0.92157178\n",
      "  1.01896633 0.90741519 0.97400555 0.83433434 1.14540127 1.00146018\n",
      "  1.07914821 1.07830902 0.99616929 1.0896034  0.94909354 0.98941239\n",
      "  0.9773379  0.95370457 1.09246225 1.07542979 0.99256068 0.98545801\n",
      "  0.99253135 0.86659917 0.97304129 1.08436255 1.07244541 1.07478317\n",
      "  0.94611009 0.84057773 0.82082036 0.98931105 1.05183309 0.87282341\n",
      "  1.0239293  1.07141338 1.19546905 0.95426642 0.96853192 1.07537122\n",
      "  0.91664385 1.16614496 1.05149505 1.00078145 1.04450504 0.8540667\n",
      "  0.93727411 1.01370478 1.11324531 1.12340412 0.95133073 0.98234709\n",
      "  1.17412784 0.94500493 0.8879698  0.91730709 1.05893554 0.89262586\n",
      "  1.11999498 0.90912844 1.06027336 1.03690795 0.93297329 1.16944099\n",
      "  1.14475953 0.80436552 1.01763798 0.98144153 1.05048311 1.05447928\n",
      "  1.02836913 0.96471711 0.85074897 0.875053   1.11125619 1.07321568\n",
      "  1.02044064 1.17098338 0.82796804 1.04023081 1.00959216 1.11738562\n",
      "  1.00179169 0.95152795 1.11507654 0.89185493 0.91017296 0.98714776\n",
      "  0.9479783  0.9166053  1.13290885 1.06346667 0.9982357  1.05910704\n",
      "  0.99758647 0.971875   0.87349944 1.11937238 1.14112668 0.83429048\n",
      "  0.91167885 0.9677437  0.94738293 1.03589756 0.98185998 0.80605781\n",
      "  1.07813122 0.95697452 1.06742693 1.01895607 1.07295935 1.13087997\n",
      "  1.01572968 0.84642309 0.88802599 1.08770433 1.0283606  1.03891372\n",
      "  1.00224445 1.03669341 0.98769276 1.02152319 0.98762231 0.84885017\n",
      "  1.02876099 1.10960792 1.03130867 1.17042192 1.12333003 1.10381051\n",
      "  0.88526906 0.98250572 0.96301908 0.96867395 0.90253458]]\n",
      "{0: 9, 1: 40, 2: 39, 3: 141, 4: 160, 5: 839, 6: 45, 7: 8, 8: 247, 9: 1, 10: 11, 11: 115, 12: 20, 13: 241, 14: 1375, 15: 23, 16: 15, 17: 16, 18: 26, 19: 58, 20: 46, 21: 31, 22: 50, 23: 138, 24: 23, 25: 32, 26: 22, 27: 83, 29: 3, 30: 105, 31: 89, 32: 614, 33: 18, 34: 88, 35: 366, 36: 520, 37: 807, 38: 105, 39: 16, 40: 43, 41: 39}\n",
      "acc 0.5912704364781761\n",
      "macro (0.39711622525118856, 0.42667871530393403, 0.35191684533656364, None)\n",
      "weighted (0.6685476451270049, 0.5912704364781761, 0.6016232590834115, None)\n",
      "batch-size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision penalty\n",
      "0 loss 9196184.56975855\n",
      "dev set\n",
      "[0.31764297 0.01431294 0.209658   0.20072266 0.30647622 0.24143702\n",
      " 0.36930434 0.23130623 0.34643306 0.26482165 0.18366899 0.26671235\n",
      " 0.18248019 0.27867995 0.2409104  0.24233449 0.0534397  0.12003291\n",
      " 0.25972319 0.03651439 0.23547459 0.1415337  0.19631304 0.36310947\n",
      " 0.07383175 0.37757591 0.26818409 0.26860613 0.25757463 0.19044208\n",
      " 0.0220899  0.03847582 0.16224888 0.28416845 0.20091587 0.05795143\n",
      " 0.267171   0.14547148 0.10495264 0.21727939 0.28331012 0.12626432\n",
      " 0.31439336 0.08649479 0.22755631 0.29578252 0.35109788 0.29820732\n",
      " 0.20638877 0.13910925 0.05950692 0.10776288 0.17483648 0.38024103\n",
      " 0.39710653 0.17097157 0.19613418 0.16868303 0.24500801 0.05649706\n",
      " 0.26862075 0.07903714 0.24110543 0.28273338 0.31961775 0.38440836\n",
      " 0.26602486 0.24541023 0.29464508 0.17427593 0.2361253  0.22380751\n",
      " 0.26290529 0.15448508 0.33698047 0.13812371 0.23255645 0.21393915\n",
      " 0.32606438 0.23175435 0.34785423 0.14265766 0.10769067 0.27409696\n",
      " 0.12840893 0.14285484 0.19810592 0.22397679 0.22712701 0.16083416\n",
      " 0.2763307  0.06219577 0.13461129 0.12869809 0.23337565 0.20037405\n",
      " 0.14545337 0.19386393 0.02265315 0.29308477 0.29273971 0.25597285\n",
      " 0.34365255 0.06488365 0.08397273 0.10474603 0.24724393 0.1878031\n",
      " 0.13476863 0.21790605 0.19209438 0.16161065 0.14239766 0.12186682\n",
      " 0.21926131 0.10770999 0.1743011  0.03462939 0.34569632 0.20175565\n",
      " 0.27944154 0.27860423 0.19646412 0.28989842 0.14938853 0.18970729\n",
      " 0.17763293 0.15399961 0.29275726 0.27572449 0.19285562 0.18575304\n",
      " 0.19282638 0.0668942  0.17333633 0.28465699 0.2727403  0.27507834\n",
      " 0.14639826 0.04087276 0.02111538 0.18960608 0.2521281  0.07311846\n",
      " 0.22422449 0.27170836 0.39576408 0.15456139 0.16882695 0.27566619\n",
      " 0.11693886 0.36643961 0.25179008 0.20107691 0.24480006 0.05436168\n",
      " 0.13756915 0.21399911 0.31354055 0.32369913 0.1516259  0.182642\n",
      " 0.37442277 0.14529969 0.08826455 0.11760176 0.25923055 0.0929207\n",
      " 0.32028988 0.10942345 0.26056816 0.23720283 0.13326778 0.36973581\n",
      " 0.3450543  0.00465592 0.21793298 0.18173633 0.25077799 0.25477417\n",
      " 0.22866398 0.1650121  0.05104371 0.07534792 0.31155096 0.27351025\n",
      " 0.22073534 0.3712781  0.0282629  0.24052582 0.20988691 0.31768051\n",
      " 0.20208649 0.15182251 0.31537132 0.09214992 0.11046829 0.18744258\n",
      " 0.14827372 0.11690012 0.33320359 0.26376136 0.1985304  0.259402\n",
      " 0.19788167 0.17216977 0.07379442 0.31966703 0.34142223 0.03458544\n",
      " 0.11197394 0.16803846 0.14767797 0.23619237 0.18215447 0.00635273\n",
      " 0.27842624 0.15726562 0.26772195 0.21925089 0.27325436 0.33117507\n",
      " 0.21602477 0.04671802 0.08832077 0.28799927 0.22865549 0.23920839\n",
      " 0.2025394  0.23698818 0.18798765 0.22181821 0.18791728 0.04914505\n",
      " 0.22905582 0.30990278 0.23160345 0.37071695 0.32362531 0.30410514\n",
      " 0.08556446 0.18280056 0.1633139  0.1689689  0.10282949]\n",
      "[[1.11744615 0.8141161  1.00946149 1.00052597 1.10627973 1.04124011\n",
      "  1.16910774 1.03110951 1.14623656 1.06462495 0.98347222 1.06651599\n",
      "  0.98228347 1.07848316 1.04073269 1.04213773 0.85324299 0.91983615\n",
      "  1.0595264  0.83631766 1.03527781 0.94133707 0.99611626 1.16291275\n",
      "  0.8736351  1.17737925 1.06798751 1.06840943 1.05737792 0.99024542\n",
      "  0.82189322 0.83827915 0.96205217 1.08397179 1.00071912 0.85775449\n",
      "  1.06697489 0.94527448 0.90475591 1.01708276 1.08311351 0.92606768\n",
      "  1.1141967  0.88629799 1.02735962 1.09558576 1.15090122 1.09801027\n",
      "  1.00619208 0.9389125  0.8593102  0.90756618 0.97463976 1.18004503\n",
      "  1.1969099  0.9707747  0.99594204 0.96848631 1.04481129 0.8563003\n",
      "  1.06842402 0.87884039 1.04090872 1.08253673 1.119421   1.18421161\n",
      "  1.06582826 1.04521351 1.09444838 0.97407923 1.0359286  1.0236108\n",
      "  1.06270857 0.95428834 1.13678401 0.93792698 1.03235975 1.01374228\n",
      "  1.12586974 1.03155752 1.14765754 0.94246095 0.90749394 1.07390029\n",
      "  0.92821222 0.942658   0.99790927 1.02378013 1.02693042 0.96063695\n",
      "  1.07613399 0.86199903 0.93441461 0.92850138 1.03317901 1.00017756\n",
      "  0.94525665 0.99366709 0.82245824 1.09288807 1.09254305 1.05577611\n",
      "  1.14345577 0.86468692 0.88377601 0.9045494  1.04704716 0.98760636\n",
      "  0.93457203 1.01770934 0.99189781 0.96141397 0.94220101 0.92167011\n",
      "  1.01906462 0.90751335 0.97410419 0.83443266 1.14549959 1.00155871\n",
      "  1.07924536 1.07840733 0.99626748 1.08970171 0.94919183 0.98951063\n",
      "  0.97743622 0.9538029  1.09256055 1.07552785 0.99265892 0.98555633\n",
      "  0.99262967 0.86669749 0.97313961 1.08446053 1.07254365 1.07488153\n",
      "  0.94620318 0.84067605 0.82091868 0.98940937 1.0519314  0.87292174\n",
      "  1.02402771 1.07151166 1.19556734 0.9543647  0.96863024 1.0754695\n",
      "  0.91674216 1.16624303 1.05159337 1.00087999 1.04460335 0.854165\n",
      "  0.93737244 1.01380259 1.11334359 1.12350242 0.95142912 0.98244534\n",
      "  1.17422607 0.94510308 0.88806795 0.91740516 1.05903379 0.89272407\n",
      "  1.12009321 0.90922676 1.06037153 1.03700618 0.9330713  1.16953919\n",
      "  1.14485767 0.8044608  1.01773627 0.98153971 1.05058132 1.0545775\n",
      "  1.02846734 0.96481541 0.85084713 0.87515125 1.11135434 1.07331372\n",
      "  1.02053875 1.17108148 0.82806626 1.04032912 1.0096903  1.11748385\n",
      "  1.00188985 0.95162598 1.11517469 0.89195325 0.91027142 0.98724594\n",
      "  0.94807675 0.9167035  1.13300698 1.06356476 0.99833381 1.05920532\n",
      "  0.99768487 0.97197316 0.87359775 1.11947044 1.14122529 0.83438877\n",
      "  0.9117772  0.96784186 0.94748126 1.03599574 0.98195799 0.80615607\n",
      "  1.07822953 0.95707023 1.06752523 1.01905426 1.07305765 1.13097834\n",
      "  1.01582805 0.84652135 0.88812418 1.08780259 1.02845883 1.03901183\n",
      "  1.00234274 1.03679155 0.98779101 1.0216215  0.98772059 0.8489484\n",
      "  1.02885918 1.10970613 1.03140682 1.17052027 1.12342846 1.10390856\n",
      "  0.8853675  0.98260393 0.96311728 0.96877221 0.90263283]]\n",
      "{0: 9, 1: 40, 2: 39, 3: 141, 4: 160, 5: 839, 6: 45, 7: 8, 8: 247, 9: 1, 10: 11, 11: 115, 12: 20, 13: 241, 14: 1375, 15: 23, 16: 15, 17: 16, 18: 26, 19: 58, 20: 46, 21: 31, 22: 50, 23: 138, 24: 23, 25: 32, 26: 22, 27: 83, 29: 3, 30: 105, 31: 89, 32: 614, 33: 18, 34: 88, 35: 366, 36: 520, 37: 807, 38: 105, 39: 16, 40: 43, 41: 39}\n",
      "acc 0.5912704364781761\n",
      "macro (0.39711622525118856, 0.42667871530393403, 0.35191684533656364, None)\n",
      "weighted (0.6685476451270049, 0.5912704364781761, 0.6016232590834115, None)\n",
      "batch-size: 2048\n",
      "precision penalty\n",
      "0 loss 9195685.36070426\n",
      "dev set\n",
      "[0.31759502 0.01426495 0.20961005 0.20067471 0.30642827 0.24138897\n",
      " 0.36925639 0.23125826 0.34638511 0.2647737  0.18362104 0.26666443\n",
      " 0.18243224 0.27863199 0.24086274 0.24228652 0.05339174 0.11998494\n",
      " 0.25967523 0.03646642 0.23542664 0.14148576 0.19626508 0.36306153\n",
      " 0.07378382 0.37752796 0.26813612 0.26855818 0.25752667 0.19039412\n",
      " 0.02204196 0.03842787 0.16220093 0.28412048 0.20086793 0.0579034\n",
      " 0.26712303 0.14542341 0.1049047  0.21723144 0.28326217 0.12621637\n",
      " 0.31434542 0.08644681 0.22750836 0.29573455 0.35104994 0.2981593\n",
      " 0.20634082 0.13906128 0.05945898 0.10771492 0.17478853 0.38019314\n",
      " 0.39705857 0.17092358 0.19608654 0.16863507 0.24496005 0.05644909\n",
      " 0.26857279 0.07898916 0.24105748 0.28268545 0.31956979 0.38436041\n",
      " 0.26597693 0.24536227 0.29459712 0.17422798 0.23607735 0.22375955\n",
      " 0.26285735 0.15443711 0.33693249 0.13807575 0.2325085  0.21389116\n",
      " 0.32601639 0.23170637 0.34780628 0.1426097  0.10764271 0.27404902\n",
      " 0.12836097 0.14280684 0.19805798 0.22392884 0.22707904 0.1607861\n",
      " 0.27628276 0.0621478  0.13456335 0.12865013 0.2333277  0.20032612\n",
      " 0.14540542 0.19381596 0.02260547 0.29303682 0.29269176 0.25592489\n",
      " 0.34360459 0.06483569 0.08392477 0.10469811 0.24719598 0.18775513\n",
      " 0.1347207  0.2178581  0.19204641 0.1615627  0.1423497  0.12181886\n",
      " 0.21921336 0.10766203 0.17425313 0.03458143 0.34564835 0.20170763\n",
      " 0.27939355 0.27855618 0.19641618 0.28985046 0.14934057 0.18965934\n",
      " 0.17758497 0.15395165 0.2927093  0.27567652 0.19280766 0.18570508\n",
      " 0.19277842 0.06684625 0.17328837 0.28460908 0.27269234 0.27503037\n",
      " 0.14635045 0.04082481 0.02106743 0.18955813 0.25208015 0.0730705\n",
      " 0.22417652 0.2716604  0.39571612 0.15451344 0.168779   0.27561823\n",
      " 0.1168909  0.36639167 0.25174213 0.2010289  0.24475211 0.05431373\n",
      " 0.13752119 0.21395114 0.31349254 0.32365117 0.15157792 0.18259405\n",
      " 0.37437478 0.14525174 0.0882166  0.11755382 0.25918251 0.09287276\n",
      " 0.32024191 0.1093755  0.2605202  0.23715488 0.13321987 0.36968786\n",
      " 0.34500634 0.00460846 0.21788502 0.18168838 0.25073003 0.25472621\n",
      " 0.22861602 0.16496415 0.05099579 0.07529997 0.31150302 0.27346232\n",
      " 0.22068739 0.37123013 0.02821495 0.24047787 0.20983896 0.31763256\n",
      " 0.20203853 0.15177457 0.31532336 0.09210199 0.11042028 0.18739462\n",
      " 0.14822566 0.11685218 0.33315564 0.2637134  0.19848245 0.25935405\n",
      " 0.19783368 0.17212182 0.07374647 0.31961908 0.34137419 0.03453749\n",
      " 0.11192597 0.16799052 0.14763001 0.23614442 0.18210656 0.00630478\n",
      " 0.27837828 0.15721803 0.26767398 0.21920294 0.2732064  0.33112711\n",
      " 0.21597681 0.04667007 0.08827284 0.28795132 0.22860754 0.23916047\n",
      " 0.20249146 0.23694021 0.1879397  0.22177025 0.18786933 0.04909711\n",
      " 0.22900787 0.30985484 0.23155549 0.37066901 0.3235773  0.30405717\n",
      " 0.0855164  0.18275262 0.16326595 0.16892094 0.10278154]\n",
      "[[1.11749418 0.81416411 1.00950931 1.00057391 1.10632752 1.04128813\n",
      "  1.16915562 1.03115745 1.14628437 1.0646729  0.98352022 1.06656374\n",
      "  0.98233144 1.07853117 1.04076762 1.04218571 0.85329094 0.91988413\n",
      "  1.05957441 0.83636562 1.03532581 0.94138499 0.99616426 1.16296072\n",
      "  0.87368304 1.17742717 1.06803536 1.06845737 1.05742587 0.99029334\n",
      "  0.82194116 0.83832708 0.96210014 1.08401969 1.00076712 0.85780253\n",
      "  1.0670222  0.94532254 0.90480389 1.01713066 1.08316138 0.92611559\n",
      "  1.11424463 0.88634599 1.02740756 1.09563374 1.15094914 1.09805841\n",
      "  1.00624003 0.93896047 0.85935818 0.90761412 0.97468772 1.18009256\n",
      "  1.19695779 0.97082273 0.99598703 0.96853427 1.04485925 0.85634827\n",
      "  1.06847198 0.87888836 1.04095668 1.08258466 1.11946897 1.18425959\n",
      "  1.06587616 1.04526146 1.09449633 0.97412718 1.03597655 1.02365875\n",
      "  1.06275654 0.9543363  1.13683181 0.93797494 1.0324077  1.01379031\n",
      "  1.12591566 1.03160555 1.14770549 0.9425089  0.9075419  1.07394823\n",
      "  0.92826017 0.942706   0.99795719 1.02382805 1.02697826 0.96068516\n",
      "  1.07618195 0.86204699 0.93446255 0.92854933 1.03322691 1.00022539\n",
      "  0.94530462 0.99371512 0.82250521 1.09293602 1.09259097 1.05582408\n",
      "  1.14350375 0.86473488 0.88382397 0.90459733 1.04709516 0.98765433\n",
      "  0.93461993 1.0177573  0.99194566 0.96146191 0.94224892 0.92171806\n",
      "  1.01911256 0.90756125 0.97415228 0.83448062 1.14554755 1.00160675\n",
      "  1.07929269 1.07845533 0.9963154  1.08974966 0.94923977 0.98955855\n",
      "  0.97748417 0.95385085 1.09260851 1.07557574 0.99270687 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450836 1.07259156 1.07492954\n",
      "  0.94625018 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.07155961 1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.91679011 1.16629091 1.05164133 1.00092805 1.04465131 0.85421294\n",
      "  0.9374204  1.01385028 1.11339168 1.12355037 0.9514771  0.98249327\n",
      "  1.17427399 0.94515098 0.88811583 0.91745305 1.05908171 0.89277198\n",
      "  1.12014113 0.90927471 1.06041943 1.0370541  0.93311914 1.1695871\n",
      "  1.14490556 0.80450813 1.01778422 0.98158761 1.05062924 1.05462542\n",
      "  1.02851525 0.96486335 0.85089503 0.87519918 1.11140225 1.07336158\n",
      "  1.02058662 1.17112936 0.82811418 1.04037707 1.00973819 1.11753177\n",
      "  1.00193775 0.95167384 1.11522258 0.8920012  0.91031942 0.98729375\n",
      "  0.94812479 0.91675141 1.13305487 1.06361263 0.99838169 1.05925326\n",
      "  0.99773285 0.97202105 0.87364568 1.11951831 1.14127334 0.83443671\n",
      "  0.91182516 0.96788975 0.94752922 1.03604364 0.98200583 0.806204\n",
      "  1.07827748 0.95711763 1.06757318 1.01910216 1.0731056  1.13102631\n",
      "  1.01587601 0.84656928 0.88817208 1.08785053 1.02850676 1.03905971\n",
      "  1.00239067 1.03683944 0.98783892 1.02166945 0.98776853 0.84899633\n",
      "  1.02890709 1.10975406 1.03145471 1.17056823 1.12347645 1.10395633\n",
      "  0.88541553 0.98265185 0.96316518 0.96882014 0.90268075]]\n",
      "{0: 9, 1: 40, 2: 39, 3: 141, 4: 160, 5: 839, 6: 45, 7: 8, 8: 247, 9: 1, 10: 11, 11: 115, 12: 20, 13: 241, 14: 1375, 15: 23, 16: 15, 17: 16, 18: 26, 19: 58, 20: 46, 21: 31, 22: 50, 23: 138, 24: 23, 25: 32, 26: 22, 27: 83, 29: 3, 30: 105, 31: 89, 32: 614, 33: 18, 34: 88, 35: 366, 36: 520, 37: 807, 38: 105, 39: 16, 40: 43, 41: 39}\n",
      "acc 0.5912704364781761\n",
      "macro (0.39711622525118856, 0.42667871530393403, 0.35191684533656364, None)\n",
      "weighted (0.6685476451270049, 0.5912704364781761, 0.6016232590834115, None)\n"
     ]
    }
   ],
   "source": [
    "# 245 pen 4\n",
    "acc = np.empty([len(LF_l)],dtype=np.float64)\n",
    "acc.fill(0.25)\n",
    "rec =  np.empty([len(LF_l)],dtype=np.float64)\n",
    "rec.fill(0.85)\n",
    "for b in [512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                LF_acc = acc,LF_rec =rec ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=4,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 512\n",
      "precision penalty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 10473355.175136605\n",
      "dev set\n",
      "[ 1.37207500e-03 -1.66094411e-03  2.92264923e-04  2.02955952e-04\n",
      "  1.26036614e-03  6.10382928e-04  1.88871710e-03  5.08849414e-04\n",
      "  1.65997920e-03  8.43941932e-04  3.23756305e-05  8.62644484e-04\n",
      "  2.05078564e-05  9.82343197e-04  6.02278525e-04  6.19137061e-04\n",
      " -1.26987785e-03 -6.03940607e-04  7.93028289e-04 -1.43911250e-03\n",
      "  5.50465972e-04 -3.89043478e-04  1.58824469e-04  1.82662807e-03\n",
      " -1.06614411e-03  1.97145541e-03  8.77581889e-04  8.81742060e-04\n",
      "  7.71456261e-04  1.00133916e-04 -1.58340581e-03 -1.41958399e-03\n",
      " -1.81897199e-04  1.03743087e-03  2.04805625e-04 -1.22448610e-03\n",
      "  8.67604726e-04 -3.49169909e-04 -7.54785754e-04  3.68474611e-04\n",
      "  1.02882579e-03 -5.41681699e-04  1.33958982e-03 -9.39199512e-04\n",
      "  4.71261561e-04  1.15361230e-03  1.70648705e-03  1.17802500e-03\n",
      "  2.59576702e-04 -4.13144538e-04 -1.20928171e-03 -7.26622344e-04\n",
      " -5.59157767e-05  1.99777794e-03  2.16679271e-03 -9.45221336e-05\n",
      "  1.54579673e-04 -1.17445413e-04  6.45810884e-04 -1.23928396e-03\n",
      "  8.81937155e-04 -1.01386137e-03  6.06764426e-04  1.02295141e-03\n",
      "  1.39191280e-03  2.03970063e-03  8.55786537e-04  6.49853965e-04\n",
      "  1.14215174e-03 -6.15268491e-05  5.56962066e-04  4.33784560e-04\n",
      "  8.24692675e-04 -2.59391869e-04  1.56551210e-03 -4.23012859e-04\n",
      "  5.21266817e-04  3.35226935e-04  1.45662051e-03  5.13340919e-04\n",
      "  1.67418179e-03 -3.77713598e-04 -7.27383065e-04  9.36617672e-04\n",
      " -5.20232616e-04 -3.75507496e-04  1.76729925e-04  4.35492021e-04\n",
      "  4.67008881e-04 -1.95467849e-04  9.58975328e-04 -1.18225149e-03\n",
      " -4.58252686e-04 -5.17287173e-04  5.29449414e-04  1.99359679e-04\n",
      " -3.49783385e-04  1.34290459e-04 -1.57974620e-03  1.12655507e-03\n",
      "  1.12308258e-03  7.55453464e-04  1.63226500e-03 -1.15541490e-03\n",
      " -9.64546657e-04 -7.56940565e-04  6.68121159e-04  7.37423825e-05\n",
      " -4.56754060e-04  3.74754482e-04  1.16707010e-04 -1.88205749e-04\n",
      " -3.80313252e-04 -5.85622948e-04  3.88307209e-04 -7.27204647e-04\n",
      " -6.12088548e-05 -1.45795111e-03  1.65272398e-03  2.13586145e-04\n",
      "  9.90438601e-04  9.82089359e-04  1.60257256e-04  1.09469465e-03\n",
      " -3.10394896e-04  9.27546985e-05 -2.79611041e-05 -2.64294328e-04\n",
      "  1.12328079e-03  9.52998035e-04  1.24230237e-04  5.32426414e-05\n",
      "  1.23973591e-04 -1.13534880e-03 -7.09258578e-05  1.04207460e-03\n",
      "  9.23113315e-04  9.46429067e-04 -3.41870857e-04 -1.39556340e-03\n",
      " -1.59314669e-03  9.17707231e-05  7.16991216e-04 -1.07309912e-03\n",
      "  4.38030625e-04  9.12786104e-04  2.15338759e-03 -2.58706704e-04\n",
      " -1.16022341e-04  9.52369613e-04 -6.34899913e-04  1.86001196e-03\n",
      "  7.13610932e-04  2.06798640e-04  6.43710593e-04 -1.26069151e-03\n",
      " -4.28600266e-04  3.35874130e-04  1.33127362e-03  1.43270155e-03\n",
      " -2.87939121e-04  2.20537568e-05  1.94015014e-03 -3.51328661e-04\n",
      " -9.21633716e-04 -6.28399800e-04  7.88304159e-04 -8.75131777e-04\n",
      "  1.39867512e-03 -7.10133865e-04  8.01397067e-04  5.67706231e-04\n",
      " -4.71810772e-04  1.89305081e-03  1.64606441e-03 -1.76070694e-03\n",
      "  3.75109723e-04  1.30794106e-05  7.03505281e-04  7.43500075e-04\n",
      "  4.82373158e-04 -1.54168146e-04 -1.29397191e-03 -1.05084656e-03\n",
      "  1.31102444e-03  9.30630130e-04  4.03037262e-04  1.90854632e-03\n",
      " -1.52169769e-03  6.00958859e-04  2.94570218e-04  1.37248673e-03\n",
      "  2.16531771e-04 -2.86252411e-04  1.34945081e-03 -8.82874170e-04\n",
      " -6.99339561e-04  7.03048340e-05 -3.21194976e-04 -6.35391736e-04\n",
      "  1.52770690e-03  8.33328309e-04  1.80970441e-04  7.89655070e-04\n",
      "  1.74744419e-04 -8.26275734e-05 -1.06633511e-03  1.39225989e-03\n",
      "  1.61021984e-03 -1.45848528e-03 -6.84483022e-04 -1.23982390e-04\n",
      " -3.27505317e-04  5.57601600e-04  1.70582399e-05 -1.74078302e-03\n",
      "  9.79780732e-04 -2.34366233e-04  8.72999039e-04  3.88225124e-04\n",
      "  9.28269015e-04  1.50750890e-03  3.55981027e-04 -1.33710959e-03\n",
      " -9.21200347e-04  1.07566954e-03  4.82066140e-04  5.87610346e-04\n",
      "  2.21077764e-04  5.65647363e-04  7.55518685e-05  4.13882595e-04\n",
      "  7.48737699e-05 -1.31287145e-03  4.86225473e-04  1.29456376e-03\n",
      "  5.11772129e-04  1.90280147e-03  1.43222339e-03  1.23693774e-03\n",
      " -9.48285557e-04  2.36143586e-05 -1.71191746e-04 -1.14596730e-04\n",
      " -7.76039880e-04]\n",
      "[[1.11734787 0.81401753 1.00936327 1.00042756 1.10618152 1.04114168\n",
      "  1.16900939 1.03101116 1.14613826 1.06452661 0.98337381 1.06641781\n",
      "  0.98218513 1.07838513 1.04062157 1.04203931 0.85314463 0.91973782\n",
      "  1.05942795 0.83621931 1.03517941 0.94123879 0.99601789 1.16281455\n",
      "  0.87353686 1.17728083 1.06788917 1.06831108 1.05727959 0.99014717\n",
      "  0.82179485 0.83818093 0.96195392 1.08387344 1.0006207  0.85765596\n",
      "  1.06687612 0.945176   0.90465753 1.01698448 1.08301523 0.92596948\n",
      "  1.11409841 0.88619952 1.02726129 1.09548738 1.15080311 1.09791185\n",
      "  1.00609377 0.93881414 0.85921187 0.90746784 0.97454141 1.17994659\n",
      "  1.19681149 0.97067643 0.99584552 0.96838797 1.04471293 0.85620196\n",
      "  1.06832563 0.87874205 1.04081039 1.08243844 1.11932268 1.18411343\n",
      "  1.06573008 1.04511514 1.09435005 0.97398089 1.03583028 1.02351246\n",
      "  1.06261024 0.95419002 1.13668555 0.93782861 1.03226143 1.01364388\n",
      "  1.12576919 1.03145912 1.14755925 0.94236262 0.90739561 1.07380199\n",
      "  0.9281139  0.94255946 0.99781097 1.02368183 1.02683216 0.96053812\n",
      "  1.07603568 0.86190069 0.93431637 0.92840304 1.03308072 1.00007933\n",
      "  0.94515828 0.99356892 0.82236402 1.09278974 1.09244471 1.05567779\n",
      "  1.1433574  0.86458856 0.88367766 0.90445111 1.04694883 0.98750807\n",
      "  0.93447388 1.01761102 0.99179956 0.96131567 0.94210274 0.92157178\n",
      "  1.01896627 0.90741517 0.97400564 0.83433432 1.1454012  1.00146016\n",
      "  1.07914633 1.07830882 0.99616926 1.08960338 0.94909352 0.98941235\n",
      "  0.97733789 0.95370457 1.09246222 1.07542951 0.99256073 0.985458\n",
      "  0.99253134 0.86659916 0.97304127 1.0843623  1.0724453  1.07478332\n",
      "  0.94610702 0.84057772 0.82082036 0.98931104 1.05183307 0.8728234\n",
      "  1.0239293  1.07141334 1.19546898 0.95426641 0.96853191 1.07537117\n",
      "  0.91664384 1.16614472 1.05149504 1.00078136 1.04450502 0.85406668\n",
      "  0.93727411 1.01370416 1.11324519 1.12340409 0.95133073 0.98234708\n",
      "  1.17412762 0.94500485 0.88796973 0.91730711 1.05893549 0.89262583\n",
      "  1.11999486 0.90912846 1.06027324 1.03690789 0.93297324 1.16944085\n",
      "  1.14475956 0.80436522 1.01763791 0.98144144 1.05048305 1.05447915\n",
      "  1.02836905 0.96471706 0.85074893 0.87505299 1.11125622 1.07321556\n",
      "  1.0204405  1.17098309 0.82796801 1.04023079 1.00959206 1.11738552\n",
      "  1.00179163 0.95152796 1.11507635 0.89185491 0.91017296 0.98714763\n",
      "  0.94797825 0.91660529 1.1329087  1.06346652 0.99823561 1.05910704\n",
      "  0.99758641 0.97187493 0.87349941 1.11937225 1.14112676 0.83429047\n",
      "  0.91167884 0.96774366 0.94738292 1.03589748 0.98185986 0.80605778\n",
      "  1.0781314  0.95697376 1.06742686 1.01895598 1.07295935 1.13087994\n",
      "  1.01572968 0.84642306 0.88802594 1.08770429 1.02836069 1.03891367\n",
      "  1.00224442 1.03669326 0.98769272 1.02152318 0.98762229 0.84885012\n",
      "  1.02876094 1.10960787 1.03130856 1.17042193 1.12332998 1.10381015\n",
      "  0.88526906 0.98250569 0.96301905 0.96867392 0.90253458]]\n",
      "{0: 19, 1: 92, 2: 17, 3: 57, 4: 210, 5: 942, 6: 1, 7: 7, 8: 125, 9: 12, 10: 31, 11: 115, 12: 12, 13: 302, 14: 1255, 15: 15, 16: 15, 17: 2, 18: 109, 19: 29, 20: 1, 21: 7, 22: 57, 23: 204, 24: 25, 25: 60, 26: 240, 27: 37, 29: 1, 30: 106, 31: 61, 32: 628, 33: 26, 34: 67, 35: 303, 36: 546, 37: 773, 38: 99, 39: 15, 40: 42, 41: 2}\n",
      "acc 0.5776211189440528\n",
      "macro (0.34582249372457996, 0.3799190681738053, 0.31017847231706464, None)\n",
      "weighted (0.6455720863804449, 0.5776211189440528, 0.5842496177716572, None)\n",
      "batch-size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision penalty\n",
      "0 loss 10471941.850797305\n",
      "dev set\n",
      "[ 1.27378614e-03 -1.75942767e-03  1.93944116e-04  1.04602824e-04\n",
      "  1.16212577e-03  5.11842836e-04  1.79040426e-03  4.10459259e-04\n",
      "  1.56168915e-03  7.45591400e-04 -6.59435440e-05  7.64431209e-04\n",
      " -7.78284827e-05  8.84124414e-04  5.05894425e-04  5.20745496e-04\n",
      " -1.36822336e-03 -7.02291247e-04  6.94633112e-04 -1.53747562e-03\n",
      "  4.52121073e-04 -4.87318718e-04  6.05014297e-05  1.72841843e-03\n",
      " -1.16436149e-03  1.87312657e-03  7.79231747e-04  7.83423168e-04\n",
      "  6.73119438e-04  1.79342432e-06 -1.68172822e-03 -1.51788226e-03\n",
      " -2.80164780e-04  9.39069191e-04  1.06510042e-04 -1.32300963e-03\n",
      "  7.69115738e-04 -4.47742286e-04 -8.53110367e-04  2.70156504e-04\n",
      "  9.30478582e-04 -6.39995657e-04  1.24129173e-03 -1.03763439e-03\n",
      "  3.72930924e-04  1.05522078e-03  1.60830737e-03  1.07953645e-03\n",
      "  1.61253364e-04 -5.11518392e-04 -1.30757272e-03 -8.24987648e-04\n",
      " -1.54259699e-04  1.89966663e-03  2.06844389e-03 -1.92889428e-04\n",
      "  5.81189575e-05 -2.15791453e-04  5.47461186e-04 -1.33764466e-03\n",
      "  7.83582780e-04 -1.11223935e-03  5.08426698e-04  9.24678023e-04\n",
      "  1.29355943e-03  1.94143656e-03  7.57568711e-04  5.51488647e-04\n",
      "  1.04382878e-03 -1.59867971e-04  4.58625799e-04  3.35447853e-04\n",
      "  7.26407546e-04 -3.57756824e-04  1.46717708e-03 -5.21375716e-04\n",
      "  4.22934945e-04  2.36811603e-04  1.35807693e-03  4.14952531e-04\n",
      "  1.57589482e-03 -4.76050321e-04 -8.25718247e-04  8.38321994e-04\n",
      " -6.18544207e-04 -4.74003843e-04  7.84176698e-05  3.37142476e-04\n",
      "  3.68676328e-04 -2.94135111e-04  8.60666039e-04 -1.28064371e-03\n",
      " -5.56535250e-04 -6.15642145e-04  4.31124778e-04  1.01079960e-04\n",
      " -4.48102817e-04  3.59995538e-05 -1.67649775e-03  1.02821992e-03\n",
      "  1.02476188e-03  6.57106560e-04  1.53390514e-03 -1.25378146e-03\n",
      " -1.06289401e-03 -8.55201967e-04  5.69805493e-04 -2.45834137e-05\n",
      " -5.54985601e-04  2.76427810e-04  1.83426740e-05 -2.86529055e-04\n",
      " -4.78650409e-04 -6.83959343e-04  2.89985392e-04 -8.25527904e-04\n",
      " -1.59593096e-04 -1.55631642e-03  1.55435541e-03  1.15042482e-04\n",
      "  8.91864444e-04  8.83561577e-04  6.19854059e-05  9.96357389e-04\n",
      " -4.08736985e-04 -5.56342330e-06 -1.26297627e-04 -3.62630854e-04\n",
      "  1.02494526e-03  8.54640585e-04  2.59106543e-05 -4.50955714e-05\n",
      "  2.56368755e-05 -1.23368495e-03 -1.69264243e-04  9.43865957e-04\n",
      "  8.24776690e-04  8.48141155e-04 -4.38929879e-04 -1.49389939e-03\n",
      " -1.69147592e-03 -6.56603463e-06  6.18654221e-04 -1.17143919e-03\n",
      "  3.39646207e-04  8.14454500e-04  2.05502697e-03 -3.57022897e-04\n",
      " -2.14357863e-04  8.54034118e-04 -7.33237634e-04  1.76173871e-03\n",
      "  6.15274003e-04  1.08242702e-04  5.45373767e-04 -1.35901895e-03\n",
      " -5.26935702e-04  2.37403430e-04  1.23286218e-03  1.33436448e-03\n",
      " -3.86335545e-04 -7.62271736e-05  1.84166891e-03 -4.49642722e-04\n",
      " -1.01998104e-03 -7.26624733e-04  6.89773549e-04 -9.73435979e-04\n",
      "  1.30029236e-03 -8.08413076e-04  7.03053100e-04  4.69392387e-04\n",
      " -5.70017249e-04  1.79472350e-03  1.54786053e-03 -1.85674616e-03\n",
      "  2.76726467e-04 -8.52600194e-05  6.05157944e-04  6.45126168e-04\n",
      "  3.84016877e-04 -2.52506712e-04 -1.39222776e-03 -1.14915629e-03\n",
      "  1.21282952e-03  8.32419313e-04  3.04716896e-04  1.81018274e-03\n",
      " -1.62001137e-03  5.02627970e-04  1.96237975e-04  1.27417295e-03\n",
      "  1.18227549e-04 -3.84462501e-04  1.25109516e-03 -9.81155883e-04\n",
      " -7.97847984e-04 -2.81735803e-05 -4.19733884e-04 -7.33657534e-04\n",
      "  1.42939528e-03  7.34987899e-04  8.26623514e-05  6.91373152e-04\n",
      "  7.62596784e-05 -1.80941861e-04 -1.16468237e-03  1.29402881e-03\n",
      "  1.51168961e-03 -1.55678884e-03 -7.82866566e-04 -2.22262607e-04\n",
      " -4.25848942e-04  4.59287762e-04 -8.11496890e-05 -1.83910812e-03\n",
      "  8.81579063e-04 -3.30599086e-04  7.74615981e-04  2.89885561e-04\n",
      "  8.29921642e-04  1.40913514e-03  2.57624763e-04 -1.43544761e-03\n",
      " -1.01945692e-03  9.77357716e-04  3.83873845e-04  4.89400371e-04\n",
      "  1.22757535e-04  4.67284049e-04 -2.27633235e-05  3.15551790e-04\n",
      " -2.34583675e-05 -1.41118217e-03  3.87920968e-04  1.19634350e-03\n",
      "  4.13416529e-04  1.80451597e-03  1.33371934e-03  1.13845509e-03\n",
      " -1.04682483e-03 -7.46526531e-05 -2.69502047e-04 -2.12936766e-04\n",
      " -8.74347050e-04]\n",
      "[[1.11744624 0.81411608 1.00946141 1.00052595 1.10627959 1.04124012\n",
      "  1.16910766 1.03110949 1.14623642 1.06462494 0.98347224 1.06651584\n",
      "  0.98228346 1.0784833  1.04071541 1.04213772 0.85324298 0.91983617\n",
      "  1.05952641 0.83631766 1.03527783 0.94133705 0.99611629 1.1629128\n",
      "  0.87363511 1.1773792  1.06798741 1.06840943 1.05737792 0.9902454\n",
      "  0.8218932  0.83827916 0.96205219 1.08397174 1.00071914 0.85775449\n",
      "  1.06697435 0.94527448 0.90475592 1.01708274 1.08311345 0.92606767\n",
      "  1.11419668 0.88629799 1.02735961 1.09558577 1.15090124 1.09801039\n",
      "  1.00619208 0.93891251 0.85931021 0.90756617 0.97463976 1.18004458\n",
      "  1.19690981 0.97077478 0.99593923 0.96848631 1.04481128 0.85630031\n",
      "  1.06842402 0.8788404  1.04090872 1.08253672 1.11942102 1.18421167\n",
      "  1.06582825 1.0452135  1.09444836 0.97407922 1.03592859 1.0236108\n",
      "  1.06270859 0.95428835 1.13678378 0.93792697 1.03235975 1.01374232\n",
      "  1.1258676  1.03155754 1.14765755 0.94246094 0.90749394 1.07390028\n",
      "  0.92821222 0.94265798 0.99790925 1.02378011 1.02693032 0.96063702\n",
      "  1.076134   0.86199903 0.93441461 0.92850137 1.03317898 1.00017748\n",
      "  0.94525665 0.9936672  0.822458   1.09288806 1.09254302 1.05577613\n",
      "  1.14345579 0.86468692 0.88377601 0.90454939 1.0470472  0.98760636\n",
      "  0.93457203 1.01770935 0.99189772 0.96141397 0.94220099 0.92167011\n",
      "  1.0190646  0.90751335 0.97410423 0.83443265 1.14549957 1.00155871\n",
      "  1.07924476 1.07840728 0.99626747 1.0897017  0.94919182 0.98951062\n",
      "  0.97743622 0.95380289 1.09256055 1.07552777 0.99265895 0.98555633\n",
      "  0.99262966 0.86669749 0.97313961 1.08446045 1.07254361 1.07488161\n",
      "  0.94620256 0.84067605 0.82091868 0.98940937 1.05193139 0.87292174\n",
      "  1.02402771 1.07151165 1.19556734 0.9543647  0.96863024 1.07546949\n",
      "  0.91674215 1.16624297 1.05159337 1.00087998 1.04460335 0.85416499\n",
      "  0.93737244 1.01380241 1.11334364 1.12350242 0.95142912 0.98244533\n",
      "  1.174226   0.94510306 0.88806793 0.91740516 1.05903377 0.89272406\n",
      "  1.12009317 0.90922676 1.06037149 1.03700616 0.93307128 1.16953914\n",
      "  1.14485768 0.80446076 1.01773625 0.98153967 1.05058131 1.05457747\n",
      "  1.02846731 0.9648154  0.85084712 0.87515125 1.11135435 1.07331368\n",
      "  1.0205387  1.17108138 0.82806625 1.04032912 1.00969027 1.11748382\n",
      "  1.00188984 0.95162598 1.11517463 0.89195324 0.91027142 0.9872459\n",
      "  0.94807674 0.9167035  1.13300693 1.06356471 0.99833378 1.05920532\n",
      "  0.99768485 0.97197314 0.87359774 1.11947039 1.14122531 0.83438876\n",
      "  0.9117772  0.96784184 0.94748126 1.03599571 0.98195795 0.80615606\n",
      "  1.07822959 0.95707007 1.06752521 1.01905423 1.07305765 1.13097833\n",
      "  1.01582805 0.84652134 0.88812416 1.08780258 1.02845886 1.03901181\n",
      "  1.00234273 1.03679149 0.98779099 1.0216215  0.98772059 0.84894839\n",
      "  1.02885916 1.10970612 1.03140678 1.17052027 1.12342844 1.10390844\n",
      "  0.8853675  0.98260392 0.96311726 0.9687722  0.90263283]]\n",
      "{0: 19, 1: 92, 2: 17, 3: 57, 4: 210, 5: 942, 6: 1, 7: 7, 8: 125, 9: 12, 10: 31, 11: 115, 12: 12, 13: 302, 14: 1255, 15: 15, 16: 15, 17: 2, 18: 109, 19: 29, 20: 1, 21: 7, 22: 57, 23: 204, 24: 25, 25: 60, 26: 240, 27: 37, 29: 1, 30: 106, 31: 61, 32: 628, 33: 26, 34: 67, 35: 303, 36: 546, 37: 773, 38: 99, 39: 15, 40: 42, 41: 2}\n",
      "acc 0.5776211189440528\n",
      "macro (0.34582249372457996, 0.3799190681738053, 0.31017847231706464, None)\n",
      "weighted (0.6455720863804449, 0.5776211189440528, 0.5842496177716572, None)\n",
      "batch-size: 2048\n",
      "precision penalty\n",
      "0 loss 10471459.819781145\n",
      "dev set\n",
      "[ 1.22584039e-03 -1.80743438e-03  1.45993785e-04  5.66465633e-05\n",
      "  1.11417791e-03  4.63802944e-04  1.74245596e-03  3.62485518e-04\n",
      "  1.51374226e-03  6.97632497e-04 -1.13895309e-04  7.16519333e-04\n",
      " -1.25783217e-04  8.36202201e-04  4.58351595e-04  4.72770217e-04\n",
      " -1.41618430e-03 -7.50253152e-04  6.46656183e-04 -1.58543912e-03\n",
      "  4.04162939e-04 -5.35254867e-04  1.25467592e-05  1.68049540e-03\n",
      " -1.21228090e-03  1.82517593e-03  7.31261669e-04  7.35473958e-04\n",
      "  6.25162569e-04 -4.61631229e-05 -1.72968262e-03 -1.56582810e-03\n",
      " -3.28106305e-04  8.91104020e-04  5.85678908e-05 -1.37103611e-03\n",
      "  7.21174074e-04 -4.95822502e-04 -9.01059030e-04  2.22206419e-04\n",
      "  8.82522244e-04 -6.87943160e-04  1.19334510e-03 -1.08562322e-03\n",
      "  3.24977551e-04  1.00724869e-03  1.56038335e-03  1.03151207e-03\n",
      "  1.13301803e-04 -5.59486479e-04 -1.35551809e-03 -8.72952813e-04\n",
      " -2.02217544e-04  1.85179022e-03  2.02048324e-03 -2.40859242e-04\n",
      "  1.05837831e-05 -2.63751627e-04  4.99499130e-04 -1.38560988e-03\n",
      "  7.35623696e-04 -1.16020919e-03  4.60470163e-04  8.76741907e-04\n",
      "  1.24559692e-03  1.89349383e-03  7.09649892e-04  5.03524012e-04\n",
      "  9.95868655e-04 -2.07824217e-04  4.10669181e-04  2.87491225e-04\n",
      "  6.78465382e-04 -4.05726289e-04  1.41921377e-03 -5.69342513e-04\n",
      "  3.74979753e-04  1.88825011e-04  1.31011431e-03  3.66966948e-04\n",
      "  1.52795329e-03 -5.24007173e-04 -8.73673250e-04  7.90380801e-04\n",
      " -6.66496492e-04 -5.22013417e-04  3.04714276e-05  2.89187187e-04\n",
      "  3.20700251e-04 -3.42206320e-04  8.12719164e-04 -1.32861857e-03\n",
      " -6.04477923e-04 -6.63602487e-04  3.83171583e-04  5.31462347e-05\n",
      " -4.96053314e-04 -1.19474520e-05 -1.72417605e-03  9.80263791e-04\n",
      "  9.76811418e-04  6.09146096e-04  1.48594317e-03 -1.30174597e-03\n",
      " -1.11085454e-03 -9.03133476e-04  5.21853940e-04 -7.25497237e-05\n",
      " -6.02909962e-04  2.28476104e-04 -2.96332337e-05 -3.34480175e-04\n",
      " -5.26607190e-04 -7.31915784e-04  2.42030596e-04 -8.73482872e-04\n",
      " -2.07571734e-04 -1.60428424e-03  1.50638427e-03  6.70087316e-05\n",
      "  8.43889175e-04  8.35497736e-04  1.40502391e-05  9.48400447e-04\n",
      " -4.56693730e-04 -5.35127356e-05 -1.74254446e-04 -4.10587674e-04\n",
      "  9.76988717e-04  8.06670867e-04 -2.20353957e-05 -9.30531009e-05\n",
      " -2.23199756e-05 -1.28164171e-03 -2.17220517e-04  8.95963745e-04\n",
      "  7.76819375e-04  8.00195583e-04 -4.86695968e-04 -1.54185612e-03\n",
      " -1.73943065e-03 -5.45228929e-05  5.70697310e-04 -1.21939820e-03\n",
      "  2.91669958e-04  7.66499233e-04  2.00706100e-03 -4.04972620e-04\n",
      " -2.62314333e-04  8.06077875e-04 -7.81194918e-04  1.71380358e-03\n",
      "  5.67317118e-04  6.02122773e-05  4.97416897e-04 -1.40697118e-03\n",
      " -5.74892264e-04  1.89446198e-04  1.18484485e-03  1.28640757e-03\n",
      " -4.34312818e-04 -1.24170507e-04  1.79366363e-03 -4.97590616e-04\n",
      " -1.06793534e-03 -7.74568351e-04  6.41736857e-04 -1.02138161e-03\n",
      "  1.25232096e-03 -8.56355673e-04  6.55096449e-04  4.21442145e-04\n",
      " -6.17926954e-04  1.74677161e-03  1.49994322e-03 -1.90421739e-03\n",
      "  2.28753825e-04 -1.33218954e-04  5.57197227e-04  5.97160482e-04\n",
      "  3.36055724e-04 -3.00462934e-04 -1.44015748e-03 -1.19710641e-03\n",
      "  1.16490867e-03  7.84502684e-04  2.56768950e-04  1.76220482e-03\n",
      " -1.66795904e-03  4.54673380e-04  1.48283975e-04  1.22622158e-03\n",
      "  7.02794401e-05 -4.32381723e-04  1.20313184e-03 -1.02909368e-03\n",
      " -8.45869515e-04 -7.61104036e-05 -4.67804521e-04 -7.81590948e-04\n",
      "  1.38144734e-03  6.87033588e-04  3.47167035e-05  6.43429600e-04\n",
      "  2.82531220e-05 -2.28889816e-04 -1.21263664e-03  1.24608444e-03\n",
      "  1.46365364e-03 -1.60473430e-03 -8.30838342e-04 -2.70205433e-04\n",
      " -4.73805503e-04  4.11337521e-04 -1.29059866e-04 -1.88705942e-03\n",
      "  8.33662363e-04 -3.78107882e-04  7.26643437e-04  2.41926614e-04\n",
      "  7.81960926e-04  1.36116951e-03  2.09663610e-04 -1.48340372e-03\n",
      " -1.06738686e-03  9.29407194e-04  3.35953614e-04  4.41483992e-04\n",
      "  7.48096330e-05  4.19305803e-04 -7.07113851e-05  2.67597218e-04\n",
      " -7.14123429e-05 -1.45913296e-03  3.39972796e-04  1.14842214e-03\n",
      "  3.65453169e-04  1.75657703e-03  1.28569973e-03  1.09051462e-03\n",
      " -1.09489670e-03 -1.22586399e-04 -3.17449634e-04 -2.60890951e-04\n",
      " -9.22292438e-04]\n",
      "[[1.11749422 0.8141641  1.00950929 1.00057391 1.10632749 1.04128814\n",
      "  1.1691556  1.03115745 1.14628433 1.0646729  0.98352023 1.06656369\n",
      "  0.98233144 1.07853121 1.04076244 1.04218571 0.85329094 0.91988414\n",
      "  1.05957441 0.83636562 1.03532582 0.94138498 0.99616427 1.16296074\n",
      "  0.87368305 1.17742716 1.06803533 1.06845738 1.05742587 0.99029333\n",
      "  0.82194116 0.83832708 0.96210014 1.08401968 1.00076713 0.85780253\n",
      "  1.06702215 0.94532254 0.90480389 1.01713066 1.08316137 0.92611559\n",
      "  1.11424463 0.88634599 1.02740756 1.09563374 1.15094915 1.09805844\n",
      "  1.00624002 0.93896047 0.85935818 0.90761412 0.97468772 1.18009242\n",
      "  1.19695777 0.97082275 0.99598624 0.96853427 1.04485925 0.85634828\n",
      "  1.06847199 0.87888836 1.04095668 1.08258466 1.11946898 1.18425961\n",
      "  1.06587616 1.04526146 1.09449632 0.97412718 1.03597655 1.02365875\n",
      "  1.06275655 0.9543363  1.13683172 0.93797494 1.0324077  1.01379032\n",
      "  1.12591548 1.03160555 1.14770549 0.9425089  0.9075419  1.07394823\n",
      "  0.92826018 0.942706   0.99795719 1.02382804 1.02697824 0.96068519\n",
      "  1.07618196 0.86204699 0.93446256 0.92854933 1.03322691 1.00022536\n",
      "  0.94530462 0.99371515 0.82250514 1.09293602 1.09259096 1.05582409\n",
      "  1.14350377 0.86473488 0.88382397 0.90459732 1.04709517 0.98765433\n",
      "  0.93461993 1.0177573  0.99194563 0.96146191 0.94224892 0.92171806\n",
      "  1.01911256 0.90756125 0.97415229 0.83448062 1.14554754 1.00160675\n",
      "  1.07929264 1.07845531 0.9963154  1.08974966 0.94923977 0.98955855\n",
      "  0.97748417 0.95385085 1.09260851 1.07557572 0.99270687 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450833 1.07259155 1.07492956\n",
      "  0.94624998 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.0715596  1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.91679011 1.16629089 1.05164133 1.00092804 1.04465131 0.85421294\n",
      "  0.9374204  1.01385027 1.11339169 1.12355037 0.9514771  0.98249327\n",
      "  1.17427397 0.94515097 0.88811583 0.91745305 1.05908171 0.89277198\n",
      "  1.12014111 0.90927471 1.06041942 1.03705409 0.93311914 1.16958708\n",
      "  1.14490557 0.80450812 1.01778421 0.9815876  1.05062924 1.05462541\n",
      "  1.02851524 0.96486335 0.85089502 0.87519918 1.11140225 1.07336157\n",
      "  1.02058661 1.17112933 0.82811417 1.04037707 1.00973818 1.11753177\n",
      "  1.00193775 0.95167384 1.11522256 0.8920012  0.91031942 0.98729374\n",
      "  0.94812479 0.91675141 1.13305485 1.06361261 0.99838168 1.05925326\n",
      "  0.99773285 0.97202105 0.87364568 1.1195183  1.14127335 0.83443671\n",
      "  0.91182516 0.96788974 0.94752922 1.03604364 0.98200582 0.806204\n",
      "  1.0782775  0.95711758 1.06757318 1.01910215 1.0731056  1.13102631\n",
      "  1.01587601 0.84656928 0.88817207 1.08785052 1.02850677 1.0390597\n",
      "  1.00239067 1.03683942 0.98783892 1.02166945 0.98776853 0.84899632\n",
      "  1.02890708 1.10975406 1.0314547  1.17056823 1.12347645 1.10395631\n",
      "  0.88541553 0.98265184 0.96316518 0.96882014 0.90268075]]\n",
      "{0: 19, 1: 92, 2: 17, 3: 57, 4: 210, 5: 942, 6: 1, 7: 7, 8: 125, 9: 12, 10: 31, 11: 115, 12: 12, 13: 302, 14: 1255, 15: 15, 16: 15, 17: 2, 18: 109, 19: 29, 20: 1, 21: 7, 22: 57, 23: 204, 24: 25, 25: 60, 26: 240, 27: 37, 29: 1, 30: 106, 31: 61, 32: 628, 33: 26, 34: 67, 35: 303, 36: 546, 37: 773, 38: 99, 39: 15, 40: 42, 41: 2}\n",
      "acc 0.5776211189440528\n",
      "macro (0.34582249372457996, 0.3799190681738053, 0.31017847231706464, None)\n",
      "weighted (0.6455720863804449, 0.5776211189440528, 0.5842496177716572, None)\n"
     ]
    }
   ],
   "source": [
    "# 245 pen 4\n",
    "acc = np.empty([len(LF_l)],dtype=np.float64)\n",
    "acc.fill(0.25)\n",
    "rec =  np.empty([len(LF_l)],dtype=np.float64)\n",
    "rec.fill(0.85)\n",
    "for b in [512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0.2,0.1,seed),\\\n",
    "                                LF_acc = acc,LF_rec =rec ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=4,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 512\n",
      "precision and recall penalty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 10475522.981790131\n",
      "dev set\n",
      "[ 1.37207500e-03 -1.66094411e-03  2.92264923e-04  2.02955952e-04\n",
      "  1.26036614e-03  6.10382928e-04  1.88871710e-03  5.08849414e-04\n",
      "  1.65997920e-03  8.43941932e-04  3.23756305e-05  8.62644484e-04\n",
      "  2.05078564e-05  9.82343197e-04  6.02278525e-04  6.19137061e-04\n",
      " -1.26987785e-03 -6.03940607e-04  7.93028289e-04 -1.43911250e-03\n",
      "  5.50465972e-04 -3.89043478e-04  1.58824469e-04  1.82662807e-03\n",
      " -1.06614411e-03  1.97145541e-03  8.77581889e-04  8.81742060e-04\n",
      "  7.71456261e-04  1.00133916e-04 -1.58340581e-03 -1.41958399e-03\n",
      " -1.81897199e-04  1.03743087e-03  2.04805625e-04 -1.22448610e-03\n",
      "  8.67604726e-04 -3.49169909e-04 -7.54785754e-04  3.68474611e-04\n",
      "  1.02882579e-03 -5.41681699e-04  1.33958982e-03 -9.39199512e-04\n",
      "  4.71261561e-04  1.15361230e-03  1.70648705e-03  1.17802500e-03\n",
      "  2.59576702e-04 -4.13144538e-04 -1.20928171e-03 -7.26622344e-04\n",
      " -5.59157767e-05  1.99777794e-03  2.16679271e-03 -9.45221336e-05\n",
      "  1.54579673e-04 -1.17445413e-04  6.45810884e-04 -1.23928396e-03\n",
      "  8.81937155e-04 -1.01386137e-03  6.06764426e-04  1.02295141e-03\n",
      "  1.39191280e-03  2.03970063e-03  8.55786537e-04  6.49853965e-04\n",
      "  1.14215174e-03 -6.15268491e-05  5.56962066e-04  4.33784560e-04\n",
      "  8.24692675e-04 -2.59391869e-04  1.56551210e-03 -4.23012859e-04\n",
      "  5.21266817e-04  3.35226935e-04  1.45662051e-03  5.13340919e-04\n",
      "  1.67418179e-03 -3.77713598e-04 -7.27383065e-04  9.36617672e-04\n",
      " -5.20232616e-04 -3.75507496e-04  1.76729925e-04  4.35492021e-04\n",
      "  4.67008881e-04 -1.95467849e-04  9.58975328e-04 -1.18225149e-03\n",
      " -4.58252686e-04 -5.17287173e-04  5.29449414e-04  1.99359679e-04\n",
      " -3.49783385e-04  1.34290459e-04 -1.57974620e-03  1.12655507e-03\n",
      "  1.12308258e-03  7.55453464e-04  1.63226500e-03 -1.15541490e-03\n",
      " -9.64546657e-04 -7.56940565e-04  6.68121159e-04  7.37423825e-05\n",
      " -4.56754060e-04  3.74754482e-04  1.16707010e-04 -1.88205749e-04\n",
      " -3.80313252e-04 -5.85622948e-04  3.88307209e-04 -7.27204647e-04\n",
      " -6.12088548e-05 -1.45795111e-03  1.65272398e-03  2.13586145e-04\n",
      "  9.90438601e-04  9.82089359e-04  1.60257256e-04  1.09469465e-03\n",
      " -3.10394896e-04  9.27546985e-05 -2.79611041e-05 -2.64294328e-04\n",
      "  1.12328079e-03  9.52998035e-04  1.24230237e-04  5.32426414e-05\n",
      "  1.23973591e-04 -1.13534880e-03 -7.09258578e-05  1.04207460e-03\n",
      "  9.23113315e-04  9.46429067e-04 -3.41870857e-04 -1.39556340e-03\n",
      " -1.59314669e-03  9.17707231e-05  7.16991216e-04 -1.07309912e-03\n",
      "  4.38030625e-04  9.12786104e-04  2.15338759e-03 -2.58706704e-04\n",
      " -1.16022341e-04  9.52369613e-04 -6.34899913e-04  1.86001196e-03\n",
      "  7.13610932e-04  2.06798640e-04  6.43710593e-04 -1.26069151e-03\n",
      " -4.28600266e-04  3.35874130e-04  1.33127362e-03  1.43270155e-03\n",
      " -2.87939121e-04  2.20537568e-05  1.94015014e-03 -3.51328661e-04\n",
      " -9.21633716e-04 -6.28399800e-04  7.88304159e-04 -8.75131777e-04\n",
      "  1.39867512e-03 -7.10133865e-04  8.01397067e-04  5.67706231e-04\n",
      " -4.71810772e-04  1.89305081e-03  1.64606441e-03 -1.76070694e-03\n",
      "  3.75109723e-04  1.30794106e-05  7.03505281e-04  7.43500075e-04\n",
      "  4.82373158e-04 -1.54168146e-04 -1.29397191e-03 -1.05084656e-03\n",
      "  1.31102444e-03  9.30630130e-04  4.03037262e-04  1.90854632e-03\n",
      " -1.52169769e-03  6.00958859e-04  2.94570218e-04  1.37248673e-03\n",
      "  2.16531771e-04 -2.86252411e-04  1.34945081e-03 -8.82874170e-04\n",
      " -6.99339561e-04  7.03048340e-05 -3.21194976e-04 -6.35391736e-04\n",
      "  1.52770690e-03  8.33328309e-04  1.80970441e-04  7.89655070e-04\n",
      "  1.74744419e-04 -8.26275734e-05 -1.06633511e-03  1.39225989e-03\n",
      "  1.61021984e-03 -1.45848528e-03 -6.84483022e-04 -1.23982390e-04\n",
      " -3.27505317e-04  5.57601600e-04  1.70582399e-05 -1.74078302e-03\n",
      "  9.79780732e-04 -2.34366233e-04  8.72999039e-04  3.88225124e-04\n",
      "  9.28269015e-04  1.50750890e-03  3.55981027e-04 -1.33710959e-03\n",
      " -9.21200347e-04  1.07566954e-03  4.82066140e-04  5.87610346e-04\n",
      "  2.21077764e-04  5.65647363e-04  7.55518685e-05  4.13882595e-04\n",
      "  7.48737699e-05 -1.31287145e-03  4.86225473e-04  1.29456376e-03\n",
      "  5.11772129e-04  1.90280147e-03  1.43222339e-03  1.23693774e-03\n",
      " -9.48285557e-04  2.36143586e-05 -1.71191746e-04 -1.14596730e-04\n",
      " -7.76039880e-04]\n",
      "[[1.11734787 0.81401753 1.00936327 1.00042756 1.10618152 1.04114168\n",
      "  1.16900939 1.03101116 1.14613826 1.06452661 0.98337381 1.06641781\n",
      "  0.98218513 1.07838513 1.04062157 1.04203931 0.85314463 0.91973782\n",
      "  1.05942795 0.83621931 1.03517941 0.94123879 0.99601789 1.16281455\n",
      "  0.87353686 1.17728083 1.06788917 1.06831108 1.05727959 0.99014717\n",
      "  0.82179485 0.83818093 0.96195392 1.08387344 1.0006207  0.85765596\n",
      "  1.06687612 0.945176   0.90465753 1.01698448 1.08301523 0.92596948\n",
      "  1.11409841 0.88619952 1.02726129 1.09548738 1.15080311 1.09791185\n",
      "  1.00609377 0.93881414 0.85921187 0.90746784 0.97454141 1.17994659\n",
      "  1.19681149 0.97067643 0.99584552 0.96838797 1.04471293 0.85620196\n",
      "  1.06832563 0.87874205 1.04081039 1.08243844 1.11932268 1.18411343\n",
      "  1.06573008 1.04511514 1.09435005 0.97398089 1.03583028 1.02351246\n",
      "  1.06261024 0.95419002 1.13668555 0.93782861 1.03226143 1.01364388\n",
      "  1.12576919 1.03145912 1.14755925 0.94236262 0.90739561 1.07380199\n",
      "  0.9281139  0.94255946 0.99781097 1.02368183 1.02683216 0.96053812\n",
      "  1.07603568 0.86190069 0.93431637 0.92840304 1.03308072 1.00007933\n",
      "  0.94515828 0.99356892 0.82236402 1.09278974 1.09244471 1.05567779\n",
      "  1.1433574  0.86458856 0.88367766 0.90445111 1.04694883 0.98750807\n",
      "  0.93447388 1.01761102 0.99179956 0.96131567 0.94210274 0.92157178\n",
      "  1.01896627 0.90741517 0.97400564 0.83433432 1.1454012  1.00146016\n",
      "  1.07914633 1.07830882 0.99616926 1.08960338 0.94909352 0.98941235\n",
      "  0.97733789 0.95370457 1.09246222 1.07542951 0.99256073 0.985458\n",
      "  0.99253134 0.86659916 0.97304127 1.0843623  1.0724453  1.07478332\n",
      "  0.94610702 0.84057772 0.82082036 0.98931104 1.05183307 0.8728234\n",
      "  1.0239293  1.07141334 1.19546898 0.95426641 0.96853191 1.07537117\n",
      "  0.91664384 1.16614472 1.05149504 1.00078136 1.04450502 0.85406668\n",
      "  0.93727411 1.01370416 1.11324519 1.12340409 0.95133073 0.98234708\n",
      "  1.17412762 0.94500485 0.88796973 0.91730711 1.05893549 0.89262583\n",
      "  1.11999486 0.90912846 1.06027324 1.03690789 0.93297324 1.16944085\n",
      "  1.14475956 0.80436522 1.01763791 0.98144144 1.05048305 1.05447915\n",
      "  1.02836905 0.96471706 0.85074893 0.87505299 1.11125622 1.07321556\n",
      "  1.0204405  1.17098309 0.82796801 1.04023079 1.00959206 1.11738552\n",
      "  1.00179163 0.95152796 1.11507635 0.89185491 0.91017296 0.98714763\n",
      "  0.94797825 0.91660529 1.1329087  1.06346652 0.99823561 1.05910704\n",
      "  0.99758641 0.97187493 0.87349941 1.11937225 1.14112676 0.83429047\n",
      "  0.91167884 0.96774366 0.94738292 1.03589748 0.98185986 0.80605778\n",
      "  1.0781314  0.95697376 1.06742686 1.01895598 1.07295935 1.13087994\n",
      "  1.01572968 0.84642306 0.88802594 1.08770429 1.02836069 1.03891367\n",
      "  1.00224442 1.03669326 0.98769272 1.02152318 0.98762229 0.84885012\n",
      "  1.02876094 1.10960787 1.03130856 1.17042193 1.12332998 1.10381015\n",
      "  0.88526906 0.98250569 0.96301905 0.96867392 0.90253458]]\n",
      "{0: 19, 1: 92, 2: 17, 3: 57, 4: 210, 5: 942, 6: 1, 7: 7, 8: 125, 9: 12, 10: 31, 11: 115, 12: 12, 13: 302, 14: 1255, 15: 15, 16: 15, 17: 2, 18: 109, 19: 29, 20: 1, 21: 7, 22: 57, 23: 204, 24: 25, 25: 60, 26: 240, 27: 37, 29: 1, 30: 106, 31: 61, 32: 628, 33: 26, 34: 67, 35: 303, 36: 546, 37: 773, 38: 99, 39: 15, 40: 42, 41: 2}\n",
      "acc 0.5776211189440528\n",
      "macro (0.34582249372457996, 0.3799190681738053, 0.31017847231706464, None)\n",
      "weighted (0.6455720863804449, 0.5776211189440528, 0.5842496177716572, None)\n",
      "batch-size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision and recall penalty\n",
      "0 loss 10472802.45365209\n",
      "dev set\n",
      "[ 1.27378614e-03 -1.75942767e-03  1.93944116e-04  1.04602824e-04\n",
      "  1.16212577e-03  5.11842836e-04  1.79040426e-03  4.10459259e-04\n",
      "  1.56168915e-03  7.45591400e-04 -6.59435440e-05  7.64431209e-04\n",
      " -7.78284827e-05  8.84124414e-04  5.05894425e-04  5.20745496e-04\n",
      " -1.36822336e-03 -7.02291247e-04  6.94633112e-04 -1.53747562e-03\n",
      "  4.52121073e-04 -4.87318718e-04  6.05014297e-05  1.72841843e-03\n",
      " -1.16436149e-03  1.87312657e-03  7.79231747e-04  7.83423168e-04\n",
      "  6.73119438e-04  1.79342432e-06 -1.68172822e-03 -1.51788226e-03\n",
      " -2.80164780e-04  9.39069191e-04  1.06510042e-04 -1.32300963e-03\n",
      "  7.69115738e-04 -4.47742286e-04 -8.53110367e-04  2.70156504e-04\n",
      "  9.30478582e-04 -6.39995657e-04  1.24129173e-03 -1.03763439e-03\n",
      "  3.72930924e-04  1.05522078e-03  1.60830737e-03  1.07953645e-03\n",
      "  1.61253364e-04 -5.11518392e-04 -1.30757272e-03 -8.24987648e-04\n",
      " -1.54259699e-04  1.89966663e-03  2.06844389e-03 -1.92889428e-04\n",
      "  5.81189575e-05 -2.15791453e-04  5.47461186e-04 -1.33764466e-03\n",
      "  7.83582780e-04 -1.11223935e-03  5.08426698e-04  9.24678023e-04\n",
      "  1.29355943e-03  1.94143656e-03  7.57568711e-04  5.51488647e-04\n",
      "  1.04382878e-03 -1.59867971e-04  4.58625799e-04  3.35447853e-04\n",
      "  7.26407546e-04 -3.57756824e-04  1.46717708e-03 -5.21375716e-04\n",
      "  4.22934945e-04  2.36811603e-04  1.35807693e-03  4.14952531e-04\n",
      "  1.57589482e-03 -4.76050321e-04 -8.25718247e-04  8.38321994e-04\n",
      " -6.18544207e-04 -4.74003843e-04  7.84176698e-05  3.37142476e-04\n",
      "  3.68676328e-04 -2.94135111e-04  8.60666039e-04 -1.28064371e-03\n",
      " -5.56535250e-04 -6.15642145e-04  4.31124778e-04  1.01079960e-04\n",
      " -4.48102817e-04  3.59995538e-05 -1.67649775e-03  1.02821992e-03\n",
      "  1.02476188e-03  6.57106560e-04  1.53390514e-03 -1.25378146e-03\n",
      " -1.06289401e-03 -8.55201967e-04  5.69805493e-04 -2.45834137e-05\n",
      " -5.54985601e-04  2.76427810e-04  1.83426740e-05 -2.86529055e-04\n",
      " -4.78650409e-04 -6.83959343e-04  2.89985392e-04 -8.25527904e-04\n",
      " -1.59593096e-04 -1.55631642e-03  1.55435541e-03  1.15042482e-04\n",
      "  8.91864444e-04  8.83561577e-04  6.19854059e-05  9.96357389e-04\n",
      " -4.08736985e-04 -5.56342330e-06 -1.26297627e-04 -3.62630854e-04\n",
      "  1.02494526e-03  8.54640585e-04  2.59106543e-05 -4.50955714e-05\n",
      "  2.56368755e-05 -1.23368495e-03 -1.69264243e-04  9.43865957e-04\n",
      "  8.24776690e-04  8.48141155e-04 -4.38929879e-04 -1.49389939e-03\n",
      " -1.69147592e-03 -6.56603463e-06  6.18654221e-04 -1.17143919e-03\n",
      "  3.39646207e-04  8.14454500e-04  2.05502697e-03 -3.57022897e-04\n",
      " -2.14357863e-04  8.54034118e-04 -7.33237634e-04  1.76173871e-03\n",
      "  6.15274003e-04  1.08242702e-04  5.45373767e-04 -1.35901895e-03\n",
      " -5.26935702e-04  2.37403430e-04  1.23286218e-03  1.33436448e-03\n",
      " -3.86335545e-04 -7.62271736e-05  1.84166891e-03 -4.49642722e-04\n",
      " -1.01998104e-03 -7.26624733e-04  6.89773549e-04 -9.73435979e-04\n",
      "  1.30029236e-03 -8.08413076e-04  7.03053100e-04  4.69392387e-04\n",
      " -5.70017249e-04  1.79472350e-03  1.54786053e-03 -1.85674616e-03\n",
      "  2.76726467e-04 -8.52600194e-05  6.05157944e-04  6.45126168e-04\n",
      "  3.84016877e-04 -2.52506712e-04 -1.39222776e-03 -1.14915629e-03\n",
      "  1.21282952e-03  8.32419313e-04  3.04716896e-04  1.81018274e-03\n",
      " -1.62001137e-03  5.02627970e-04  1.96237975e-04  1.27417295e-03\n",
      "  1.18227549e-04 -3.84462501e-04  1.25109516e-03 -9.81155883e-04\n",
      " -7.97847984e-04 -2.81735803e-05 -4.19733884e-04 -7.33657534e-04\n",
      "  1.42939528e-03  7.34987899e-04  8.26623514e-05  6.91373152e-04\n",
      "  7.62596784e-05 -1.80941861e-04 -1.16468237e-03  1.29402881e-03\n",
      "  1.51168961e-03 -1.55678884e-03 -7.82866566e-04 -2.22262607e-04\n",
      " -4.25848942e-04  4.59287762e-04 -8.11496890e-05 -1.83910812e-03\n",
      "  8.81579063e-04 -3.30599086e-04  7.74615981e-04  2.89885561e-04\n",
      "  8.29921642e-04  1.40913514e-03  2.57624763e-04 -1.43544761e-03\n",
      " -1.01945692e-03  9.77357716e-04  3.83873845e-04  4.89400371e-04\n",
      "  1.22757535e-04  4.67284049e-04 -2.27633235e-05  3.15551790e-04\n",
      " -2.34583675e-05 -1.41118217e-03  3.87920968e-04  1.19634350e-03\n",
      "  4.13416529e-04  1.80451597e-03  1.33371934e-03  1.13845509e-03\n",
      " -1.04682483e-03 -7.46526531e-05 -2.69502047e-04 -2.12936766e-04\n",
      " -8.74347050e-04]\n",
      "[[1.11744624 0.81411608 1.00946141 1.00052595 1.10627959 1.04124012\n",
      "  1.16910766 1.03110949 1.14623642 1.06462494 0.98347224 1.06651584\n",
      "  0.98228346 1.0784833  1.04071541 1.04213772 0.85324298 0.91983617\n",
      "  1.05952641 0.83631766 1.03527783 0.94133705 0.99611629 1.1629128\n",
      "  0.87363511 1.1773792  1.06798741 1.06840943 1.05737792 0.9902454\n",
      "  0.8218932  0.83827916 0.96205219 1.08397174 1.00071914 0.85775449\n",
      "  1.06697435 0.94527448 0.90475592 1.01708274 1.08311345 0.92606767\n",
      "  1.11419668 0.88629799 1.02735961 1.09558577 1.15090124 1.09801039\n",
      "  1.00619208 0.93891251 0.85931021 0.90756617 0.97463976 1.18004458\n",
      "  1.19690981 0.97077478 0.99593923 0.96848631 1.04481128 0.85630031\n",
      "  1.06842402 0.8788404  1.04090872 1.08253672 1.11942102 1.18421167\n",
      "  1.06582825 1.0452135  1.09444836 0.97407922 1.03592859 1.0236108\n",
      "  1.06270859 0.95428835 1.13678378 0.93792697 1.03235975 1.01374232\n",
      "  1.1258676  1.03155754 1.14765755 0.94246094 0.90749394 1.07390028\n",
      "  0.92821222 0.94265798 0.99790925 1.02378011 1.02693032 0.96063702\n",
      "  1.076134   0.86199903 0.93441461 0.92850137 1.03317898 1.00017748\n",
      "  0.94525665 0.9936672  0.822458   1.09288806 1.09254302 1.05577613\n",
      "  1.14345579 0.86468692 0.88377601 0.90454939 1.0470472  0.98760636\n",
      "  0.93457203 1.01770935 0.99189772 0.96141397 0.94220099 0.92167011\n",
      "  1.0190646  0.90751335 0.97410423 0.83443265 1.14549957 1.00155871\n",
      "  1.07924476 1.07840728 0.99626747 1.0897017  0.94919182 0.98951062\n",
      "  0.97743622 0.95380289 1.09256055 1.07552777 0.99265895 0.98555633\n",
      "  0.99262966 0.86669749 0.97313961 1.08446045 1.07254361 1.07488161\n",
      "  0.94620256 0.84067605 0.82091868 0.98940937 1.05193139 0.87292174\n",
      "  1.02402771 1.07151165 1.19556734 0.9543647  0.96863024 1.07546949\n",
      "  0.91674215 1.16624297 1.05159337 1.00087998 1.04460335 0.85416499\n",
      "  0.93737244 1.01380241 1.11334364 1.12350242 0.95142912 0.98244533\n",
      "  1.174226   0.94510306 0.88806793 0.91740516 1.05903377 0.89272406\n",
      "  1.12009317 0.90922676 1.06037149 1.03700616 0.93307128 1.16953914\n",
      "  1.14485768 0.80446076 1.01773625 0.98153967 1.05058131 1.05457747\n",
      "  1.02846731 0.9648154  0.85084712 0.87515125 1.11135435 1.07331368\n",
      "  1.0205387  1.17108138 0.82806625 1.04032912 1.00969027 1.11748382\n",
      "  1.00188984 0.95162598 1.11517463 0.89195324 0.91027142 0.9872459\n",
      "  0.94807674 0.9167035  1.13300693 1.06356471 0.99833378 1.05920532\n",
      "  0.99768485 0.97197314 0.87359774 1.11947039 1.14122531 0.83438876\n",
      "  0.9117772  0.96784184 0.94748126 1.03599571 0.98195795 0.80615606\n",
      "  1.07822959 0.95707007 1.06752521 1.01905423 1.07305765 1.13097833\n",
      "  1.01582805 0.84652134 0.88812416 1.08780258 1.02845886 1.03901181\n",
      "  1.00234273 1.03679149 0.98779099 1.0216215  0.98772059 0.84894839\n",
      "  1.02885916 1.10970612 1.03140678 1.17052027 1.12342844 1.10390844\n",
      "  0.8853675  0.98260392 0.96311726 0.9687722  0.90263283]]\n",
      "{0: 19, 1: 92, 2: 17, 3: 57, 4: 210, 5: 942, 6: 1, 7: 7, 8: 125, 9: 12, 10: 31, 11: 115, 12: 12, 13: 302, 14: 1255, 15: 15, 16: 15, 17: 2, 18: 109, 19: 29, 20: 1, 21: 7, 22: 57, 23: 204, 24: 25, 25: 60, 26: 240, 27: 37, 29: 1, 30: 106, 31: 61, 32: 628, 33: 26, 34: 67, 35: 303, 36: 546, 37: 773, 38: 99, 39: 15, 40: 42, 41: 2}\n",
      "acc 0.5776211189440528\n",
      "macro (0.34582249372457996, 0.3799190681738053, 0.31017847231706464, None)\n",
      "weighted (0.6455720863804449, 0.5776211189440528, 0.5842496177716572, None)\n",
      "batch-size: 2048\n",
      "precision and recall penalty\n",
      "0 loss 10471824.857914379\n",
      "dev set\n",
      "[ 1.22584039e-03 -1.80743438e-03  1.45993785e-04  5.66465633e-05\n",
      "  1.11417791e-03  4.63802944e-04  1.74245596e-03  3.62485518e-04\n",
      "  1.51374226e-03  6.97632497e-04 -1.13895309e-04  7.16519333e-04\n",
      " -1.25783217e-04  8.36202201e-04  4.58351595e-04  4.72770217e-04\n",
      " -1.41618430e-03 -7.50253152e-04  6.46656183e-04 -1.58543912e-03\n",
      "  4.04162939e-04 -5.35254867e-04  1.25467592e-05  1.68049540e-03\n",
      " -1.21228090e-03  1.82517593e-03  7.31261669e-04  7.35473958e-04\n",
      "  6.25162569e-04 -4.61631229e-05 -1.72968262e-03 -1.56582810e-03\n",
      " -3.28106305e-04  8.91104020e-04  5.85678908e-05 -1.37103611e-03\n",
      "  7.21174074e-04 -4.95822502e-04 -9.01059030e-04  2.22206419e-04\n",
      "  8.82522244e-04 -6.87943160e-04  1.19334510e-03 -1.08562322e-03\n",
      "  3.24977551e-04  1.00724869e-03  1.56038335e-03  1.03151207e-03\n",
      "  1.13301803e-04 -5.59486479e-04 -1.35551809e-03 -8.72952813e-04\n",
      " -2.02217544e-04  1.85179022e-03  2.02048324e-03 -2.40859242e-04\n",
      "  1.05837831e-05 -2.63751627e-04  4.99499130e-04 -1.38560988e-03\n",
      "  7.35623696e-04 -1.16020919e-03  4.60470163e-04  8.76741907e-04\n",
      "  1.24559692e-03  1.89349383e-03  7.09649892e-04  5.03524012e-04\n",
      "  9.95868655e-04 -2.07824217e-04  4.10669181e-04  2.87491225e-04\n",
      "  6.78465382e-04 -4.05726289e-04  1.41921377e-03 -5.69342513e-04\n",
      "  3.74979753e-04  1.88825011e-04  1.31011431e-03  3.66966948e-04\n",
      "  1.52795329e-03 -5.24007173e-04 -8.73673250e-04  7.90380801e-04\n",
      " -6.66496492e-04 -5.22013417e-04  3.04714276e-05  2.89187187e-04\n",
      "  3.20700251e-04 -3.42206320e-04  8.12719164e-04 -1.32861857e-03\n",
      " -6.04477923e-04 -6.63602487e-04  3.83171583e-04  5.31462347e-05\n",
      " -4.96053314e-04 -1.19474520e-05 -1.72417605e-03  9.80263791e-04\n",
      "  9.76811418e-04  6.09146096e-04  1.48594317e-03 -1.30174597e-03\n",
      " -1.11085454e-03 -9.03133476e-04  5.21853940e-04 -7.25497237e-05\n",
      " -6.02909962e-04  2.28476104e-04 -2.96332337e-05 -3.34480175e-04\n",
      " -5.26607190e-04 -7.31915784e-04  2.42030596e-04 -8.73482872e-04\n",
      " -2.07571734e-04 -1.60428424e-03  1.50638427e-03  6.70087316e-05\n",
      "  8.43889175e-04  8.35497736e-04  1.40502391e-05  9.48400447e-04\n",
      " -4.56693730e-04 -5.35127356e-05 -1.74254446e-04 -4.10587674e-04\n",
      "  9.76988717e-04  8.06670867e-04 -2.20353957e-05 -9.30531009e-05\n",
      " -2.23199756e-05 -1.28164171e-03 -2.17220517e-04  8.95963745e-04\n",
      "  7.76819375e-04  8.00195583e-04 -4.86695968e-04 -1.54185612e-03\n",
      " -1.73943065e-03 -5.45228929e-05  5.70697310e-04 -1.21939820e-03\n",
      "  2.91669958e-04  7.66499233e-04  2.00706100e-03 -4.04972620e-04\n",
      " -2.62314333e-04  8.06077875e-04 -7.81194918e-04  1.71380358e-03\n",
      "  5.67317118e-04  6.02122773e-05  4.97416897e-04 -1.40697118e-03\n",
      " -5.74892264e-04  1.89446198e-04  1.18484485e-03  1.28640757e-03\n",
      " -4.34312818e-04 -1.24170507e-04  1.79366363e-03 -4.97590616e-04\n",
      " -1.06793534e-03 -7.74568351e-04  6.41736857e-04 -1.02138161e-03\n",
      "  1.25232096e-03 -8.56355673e-04  6.55096449e-04  4.21442145e-04\n",
      " -6.17926954e-04  1.74677161e-03  1.49994322e-03 -1.90421739e-03\n",
      "  2.28753825e-04 -1.33218954e-04  5.57197227e-04  5.97160482e-04\n",
      "  3.36055724e-04 -3.00462934e-04 -1.44015748e-03 -1.19710641e-03\n",
      "  1.16490867e-03  7.84502684e-04  2.56768950e-04  1.76220482e-03\n",
      " -1.66795904e-03  4.54673380e-04  1.48283975e-04  1.22622158e-03\n",
      "  7.02794401e-05 -4.32381723e-04  1.20313184e-03 -1.02909368e-03\n",
      " -8.45869515e-04 -7.61104036e-05 -4.67804521e-04 -7.81590948e-04\n",
      "  1.38144734e-03  6.87033588e-04  3.47167035e-05  6.43429600e-04\n",
      "  2.82531220e-05 -2.28889816e-04 -1.21263664e-03  1.24608444e-03\n",
      "  1.46365364e-03 -1.60473430e-03 -8.30838342e-04 -2.70205433e-04\n",
      " -4.73805503e-04  4.11337521e-04 -1.29059866e-04 -1.88705942e-03\n",
      "  8.33662363e-04 -3.78107882e-04  7.26643437e-04  2.41926614e-04\n",
      "  7.81960926e-04  1.36116951e-03  2.09663610e-04 -1.48340372e-03\n",
      " -1.06738686e-03  9.29407194e-04  3.35953614e-04  4.41483992e-04\n",
      "  7.48096330e-05  4.19305803e-04 -7.07113851e-05  2.67597218e-04\n",
      " -7.14123429e-05 -1.45913296e-03  3.39972796e-04  1.14842214e-03\n",
      "  3.65453169e-04  1.75657703e-03  1.28569973e-03  1.09051462e-03\n",
      " -1.09489670e-03 -1.22586399e-04 -3.17449634e-04 -2.60890951e-04\n",
      " -9.22292438e-04]\n",
      "[[1.11749422 0.8141641  1.00950929 1.00057391 1.10632749 1.04128814\n",
      "  1.1691556  1.03115745 1.14628433 1.0646729  0.98352023 1.06656369\n",
      "  0.98233144 1.07853121 1.04076244 1.04218571 0.85329094 0.91988414\n",
      "  1.05957441 0.83636562 1.03532582 0.94138498 0.99616427 1.16296074\n",
      "  0.87368305 1.17742716 1.06803533 1.06845738 1.05742587 0.99029333\n",
      "  0.82194116 0.83832708 0.96210014 1.08401968 1.00076713 0.85780253\n",
      "  1.06702215 0.94532254 0.90480389 1.01713066 1.08316137 0.92611559\n",
      "  1.11424463 0.88634599 1.02740756 1.09563374 1.15094915 1.09805844\n",
      "  1.00624002 0.93896047 0.85935818 0.90761412 0.97468772 1.18009242\n",
      "  1.19695777 0.97082275 0.99598624 0.96853427 1.04485925 0.85634828\n",
      "  1.06847199 0.87888836 1.04095668 1.08258466 1.11946898 1.18425961\n",
      "  1.06587616 1.04526146 1.09449632 0.97412718 1.03597655 1.02365875\n",
      "  1.06275655 0.9543363  1.13683172 0.93797494 1.0324077  1.01379032\n",
      "  1.12591548 1.03160555 1.14770549 0.9425089  0.9075419  1.07394823\n",
      "  0.92826018 0.942706   0.99795719 1.02382804 1.02697824 0.96068519\n",
      "  1.07618196 0.86204699 0.93446256 0.92854933 1.03322691 1.00022536\n",
      "  0.94530462 0.99371515 0.82250514 1.09293602 1.09259096 1.05582409\n",
      "  1.14350377 0.86473488 0.88382397 0.90459732 1.04709517 0.98765433\n",
      "  0.93461993 1.0177573  0.99194563 0.96146191 0.94224892 0.92171806\n",
      "  1.01911256 0.90756125 0.97415229 0.83448062 1.14554754 1.00160675\n",
      "  1.07929264 1.07845531 0.9963154  1.08974966 0.94923977 0.98955855\n",
      "  0.97748417 0.95385085 1.09260851 1.07557572 0.99270687 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450833 1.07259155 1.07492956\n",
      "  0.94624998 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.0715596  1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.91679011 1.16629089 1.05164133 1.00092804 1.04465131 0.85421294\n",
      "  0.9374204  1.01385027 1.11339169 1.12355037 0.9514771  0.98249327\n",
      "  1.17427397 0.94515097 0.88811583 0.91745305 1.05908171 0.89277198\n",
      "  1.12014111 0.90927471 1.06041942 1.03705409 0.93311914 1.16958708\n",
      "  1.14490557 0.80450812 1.01778421 0.9815876  1.05062924 1.05462541\n",
      "  1.02851524 0.96486335 0.85089502 0.87519918 1.11140225 1.07336157\n",
      "  1.02058661 1.17112933 0.82811417 1.04037707 1.00973818 1.11753177\n",
      "  1.00193775 0.95167384 1.11522256 0.8920012  0.91031942 0.98729374\n",
      "  0.94812479 0.91675141 1.13305485 1.06361261 0.99838168 1.05925326\n",
      "  0.99773285 0.97202105 0.87364568 1.1195183  1.14127335 0.83443671\n",
      "  0.91182516 0.96788974 0.94752922 1.03604364 0.98200582 0.806204\n",
      "  1.0782775  0.95711758 1.06757318 1.01910215 1.0731056  1.13102631\n",
      "  1.01587601 0.84656928 0.88817207 1.08785052 1.02850677 1.0390597\n",
      "  1.00239067 1.03683942 0.98783892 1.02166945 0.98776853 0.84899632\n",
      "  1.02890708 1.10975406 1.0314547  1.17056823 1.12347645 1.10395631\n",
      "  0.88541553 0.98265184 0.96316518 0.96882014 0.90268075]]\n",
      "{0: 19, 1: 92, 2: 17, 3: 57, 4: 210, 5: 942, 6: 1, 7: 7, 8: 125, 9: 12, 10: 31, 11: 115, 12: 12, 13: 302, 14: 1255, 15: 15, 16: 15, 17: 2, 18: 109, 19: 29, 20: 1, 21: 7, 22: 57, 23: 204, 24: 25, 25: 60, 26: 240, 27: 37, 29: 1, 30: 106, 31: 61, 32: 628, 33: 26, 34: 67, 35: 303, 36: 546, 37: 773, 38: 99, 39: 15, 40: 42, 41: 2}\n",
      "acc 0.5776211189440528\n",
      "macro (0.34582249372457996, 0.3799190681738053, 0.31017847231706464, None)\n",
      "weighted (0.6455720863804449, 0.5776211189440528, 0.5842496177716572, None)\n"
     ]
    }
   ],
   "source": [
    "# 245 pen 6\n",
    "acc = np.empty([len(LF_l)],dtype=np.float64)\n",
    "acc.fill(0.25)\n",
    "rec =  np.empty([len(LF_l)],dtype=np.float64)\n",
    "rec.fill(0.85)\n",
    "for b in [512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = acc,LF_rec =rec ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=6,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 32\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8158100.630830191\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11442135 0.81108692 1.006435   0.99749997 1.10326125 1.03821693\n",
      "  1.1660815  1.02808225 1.14321316 1.06159795 0.98044639 1.06349638\n",
      "  0.97925727 1.07546931 1.03782529 1.03911077 0.85021643 0.91680917\n",
      "  1.05649862 0.83329083 1.03225135 0.93831282 0.99309168 1.15988828\n",
      "  0.87061258 1.17435404 1.06496356 1.06538445 1.05435141 0.98721853\n",
      "  0.81886756 0.83525361 0.95905519 1.08094484 0.99769293 0.85473084\n",
      "  1.06398067 0.94229264 0.9017301  1.01405634 1.08008646 0.923042\n",
      "  1.11117145 0.88326964 1.02433296 1.09255746 1.14788407 1.09498575\n",
      "  1.00316598 0.93588566 0.85628632 0.9045385  0.97161295 1.17702694\n",
      "  1.19388301 0.9677562  0.99303655 0.96545969 1.04178463 0.85327305\n",
      "  1.06539757 0.87581317 1.03788235 1.07951245 1.11639521 1.18118658\n",
      "  1.06280514 1.04218674 1.09142441 0.97105379 1.0329021  1.02058432\n",
      "  1.05968376 0.95126084 1.13377679 0.93490018 1.02933359 1.01072006\n",
      "  1.12286896 1.02854738 1.14463424 0.93943447 0.90446785 1.07087515\n",
      "  0.92518688 0.93962835 0.99488295 1.02075347 1.02390819 0.95760933\n",
      "  1.07310818 0.85897182 0.9313923  0.92547432 1.03015269 0.99715663\n",
      "  0.94223108 0.99065237 0.81957851 1.0898616  1.08951685 1.05274923\n",
      "  1.1404293  0.86166001 0.8807493  0.90152567 1.0440228  0.98457995\n",
      "  0.93154882 1.01468379 0.98887332 0.95838898 0.93917438 0.91864367\n",
      "  1.01603872 0.90448647 0.97110328 0.83140584 1.14247299 0.99853386\n",
      "  1.07624879 1.07542216 0.99324492 1.08667517 0.94616534 0.98648507\n",
      "  0.97440974 0.95077643 1.08953409 1.07250295 0.98963787 0.98252975\n",
      "  0.98960319 0.86367107 0.97011317 1.08143875 1.06951737 1.07186636\n",
      "  0.94329808 0.83764964 0.81789242 0.9863829  1.04890489 0.86989536\n",
      "  1.02100091 1.0684852  1.19254071 0.95133889 0.96560381 1.07244301\n",
      "  0.91371566 1.16321823 1.04856688 0.99786743 1.04157685 0.85113823\n",
      "  0.9343461  1.01080718 1.11035886 1.12047589 0.94840151]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 64\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8097048.574518942\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11598163 0.81264915 1.007996   0.99906057 1.10481694 1.03977327\n",
      "  1.16764245 1.02964366 1.14477198 1.0631592  0.98200713 1.06505279\n",
      "  0.9808182  1.07702129 1.03928869 1.04067208 0.8517775  0.91837051\n",
      "  1.05806018 0.83485207 1.03381243 0.9398727  0.99465138 1.16144801\n",
      "  0.87217155 1.17591425 1.06652272 1.06694446 1.05591252 0.9887798\n",
      "  0.82042767 0.83681406 0.96059368 1.08250603 0.99925375 0.85628859\n",
      "  1.06551572 0.94381797 0.90329067 1.01561738 1.08164765 0.92460253\n",
      "  1.1127319  0.8848315  1.02589415 1.09411942 1.14943933 1.09654403\n",
      "  1.00472688 0.93744688 0.85784555 0.90610008 0.97317421 1.17858265\n",
      "  1.19544426 0.96931092 0.9945097  0.96702085 1.04334579 0.85483451\n",
      "  1.06695828 0.87737462 1.03944336 1.08107238 1.11795565 1.18274661\n",
      "  1.06436445 1.04374782 1.09298373 0.97261401 1.0344632  1.02214541\n",
      "  1.06124359 0.9528223  1.13532278 0.93646135 1.03089449 1.01227745\n",
      "  1.12440713 1.03009568 1.14619305 0.94099555 0.90602859 1.07243549\n",
      "  0.92674736 0.94119083 0.99644396 1.02231441 1.02546595 0.95916889\n",
      "  1.07466898 0.86053319 0.93295055 0.92703558 1.03171363 0.99871367\n",
      "  0.9437917  0.99220473 0.8210321  1.09142269 1.09107777 1.05431053\n",
      "  1.14199003 0.8632213  0.88231048 0.90308532 1.04558242 0.98614058\n",
      "  0.93310813 1.0162442  0.99043276 0.95994897 0.94073551 0.92020473\n",
      "  1.01759905 0.90604755 0.97264411 0.83296701 1.14403403 1.00009243\n",
      "  1.0777853  1.07695059 0.99480317 1.0882363  0.94772627 0.98804556\n",
      "  0.97597083 0.95233751 1.09109518 1.07406267 0.99119471 0.98409088\n",
      "  0.99116427 0.86523212 0.97167417 1.08299686 1.07107832 1.07341905\n",
      "  0.94477005 0.83921068 0.81945339 0.98794398 1.05046599 0.87145636\n",
      "  1.02256178 1.07004628 1.19410178 0.95289962 0.96716487 1.0740041\n",
      "  0.91527675 1.16477843 1.05012798 0.99941621 1.04313796 0.8526993\n",
      "  0.93590711 1.0123438  1.11188812 1.12203701 0.94996299]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 128\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8067026.012397951\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11676306 0.81343181 1.00877781 0.99984236 1.1055969  1.04055525\n",
      "  1.1684242  1.03042572 1.14555309 1.06394124 0.98278884 1.06583308\n",
      "  0.98159998 1.07780064 1.04004281 1.04145406 0.85255938 0.9191525\n",
      "  1.05884246 0.83563403 1.03459429 0.94065395 0.99543293 1.16222951\n",
      "  0.87295239 1.17669581 1.06730398 1.06772601 1.05669436 0.98956174\n",
      "  0.82120948 0.8375957  0.96136999 1.08328801 1.00003558 0.85707033\n",
      "  1.06629136 0.94459189 0.90407244 1.01639918 1.08242969 0.9253842\n",
      "  1.1135134  0.88561392 1.02667604 1.09490178 1.15021896 1.09732606\n",
      "  1.00550861 0.93822883 0.85862685 0.9068823  0.97395615 1.17936241\n",
      "  1.19622618 0.97009135 0.9952667  0.96780272 1.04412769 0.85561658\n",
      "  1.06774029 0.87815668 1.04022517 1.08185364 1.11873741 1.18352822\n",
      "  1.06514542 1.04452979 1.09376503 0.97339568 1.03524504 1.02292724\n",
      "  1.06202515 0.95360447 1.13610095 0.9372433  1.03167624 1.01305872\n",
      "  1.12518401 1.03087447 1.14697437 0.94177739 0.9068104  1.07321701\n",
      "  0.92752891 0.94197361 0.99722576 1.02309637 1.02624695 0.95995184\n",
      "  1.07545064 0.86131525 0.93373146 0.92781765 1.03249542 0.99949436\n",
      "  0.94457331 0.99298433 0.82178455 1.09220452 1.09185953 1.05509248\n",
      "  1.14277207 0.86400328 0.88309239 0.90386644 1.0463639  0.9869226\n",
      "  0.93388913 1.01702587 0.99121415 0.96073051 0.94151737 0.92098655\n",
      "  1.0183809  0.90682954 0.97342153 0.83374897 1.14481592 1.00087443\n",
      "  1.07856148 1.0777249  0.99558434 1.08901815 0.94850818 0.98882719\n",
      "  0.97675266 0.95311934 1.09187701 1.0748442  0.99197557 0.98487275\n",
      "  0.99194611 0.86601394 0.97245602 1.08377761 1.07186009 1.07419871\n",
      "  0.94552728 0.8399925  0.82023517 0.98872582 1.05124783 0.87223818\n",
      "  1.02334386 1.07082811 1.1948837  0.95368128 0.9679467  1.07478593\n",
      "  0.91605859 1.1655598  1.05090982 1.00019602 1.04391979 0.85348125\n",
      "  0.93668891 1.01311963 1.11266208 1.12281886 0.9507452 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 512\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8045414.838682952\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11734798 0.8140177  1.00936298 1.00042761 1.10618124 1.04114153\n",
      "  1.16900933 1.03101111 1.14613805 1.06452659 0.98337396 1.06641751\n",
      "  0.98218516 1.07838495 1.0406172  1.04203939 0.85314464 0.91973783\n",
      "  1.05942806 0.83621932 1.03517954 0.94123876 0.99601801 1.1628145\n",
      "  0.87353688 1.17728089 1.06788902 1.0683111  1.05727958 0.99014702\n",
      "  0.82179484 0.8381808  0.96195382 1.08387336 1.00062084 0.8576561\n",
      "  1.06687567 0.94517597 0.90465763 1.01698436 1.08301504 0.9259693\n",
      "  1.11409836 0.88619961 1.02726127 1.09548739 1.15080299 1.09791198\n",
      "  1.00609375 0.93881416 0.85921192 0.90746778 0.97454143 1.17994633\n",
      "  1.19681146 0.97067637 0.99584106 0.96838797 1.04471295 0.85620196\n",
      "  1.06832569 0.87874204 1.04081038 1.08243845 1.11932267 1.18411333\n",
      "  1.06572998 1.04511515 1.09435005 0.97398088 1.03583025 1.02351246\n",
      "  1.06261028 0.95418995 1.13668534 0.93782863 1.03226141 1.01364399\n",
      "  1.12576895 1.03145919 1.14755929 0.9423626  0.90739561 1.07380198\n",
      "  0.92811391 0.94255958 0.99781091 1.02368173 1.0268319  0.96053858\n",
      "  1.07603571 0.86190065 0.93431631 0.92840302 1.03308061 1.00007912\n",
      "  0.94515836 0.99356886 0.82235961 1.09278972 1.09244468 1.05567778\n",
      "  1.14335748 0.86458858 0.88367766 0.90445113 1.04694891 0.987508\n",
      "  0.93447374 1.01761102 0.9917993  0.96131562 0.94210261 0.92157176\n",
      "  1.01896624 0.9074149  0.97400592 0.8343343  1.14540122 1.0014603\n",
      "  1.07914611 1.07830881 0.9961692  1.08960336 0.94909346 0.98941227\n",
      "  0.97733788 0.95370455 1.09246221 1.07542939 0.99256056 0.98545798\n",
      "  0.99253132 0.86659915 0.97304126 1.08436217 1.07244525 1.07478327\n",
      "  0.94610429 0.84057771 0.82082034 0.98931103 1.05183305 0.8728234\n",
      "  1.02392935 1.07141331 1.195469   0.95426637 0.9685319  1.07537114\n",
      "  0.91664381 1.16614466 1.05149503 1.00078149 1.04450501 0.85406661\n",
      "  0.9372741  1.01370381 1.11324535 1.12340408 0.95133073]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 1024\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8042011.122739795\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11744628 0.81411613 1.0094613  1.00052595 1.10627952 1.04124009\n",
      "  1.16910765 1.03110948 1.14623636 1.06462494 0.98347229 1.06651574\n",
      "  0.98228349 1.07848325 1.04071445 1.04213775 0.85324298 0.91983618\n",
      "  1.05952644 0.83631767 1.03527788 0.94133704 0.99611633 1.16291279\n",
      "  0.87363512 1.17737921 1.06798736 1.06840943 1.05737792 0.99024536\n",
      "  0.82189319 0.83827912 0.96205217 1.08397171 1.00071918 0.85775455\n",
      "  1.06697418 0.94527448 0.90475595 1.01708269 1.0831134  0.92606762\n",
      "  1.11419667 0.88629802 1.02735961 1.09558577 1.15090122 1.09801045\n",
      "  1.00619207 0.93891252 0.85931023 0.90756615 0.97463977 1.1800445\n",
      "  1.19690981 0.97077477 0.99593829 0.96848631 1.04481129 0.85630032\n",
      "  1.06842404 0.8788404  1.04090872 1.08253673 1.11942102 1.18421165\n",
      "  1.06582823 1.0452135  1.09444837 0.97407922 1.03592859 1.02361079\n",
      "  1.0627086  0.95428832 1.13678372 0.93792698 1.03235974 1.01374236\n",
      "  1.12586751 1.03155757 1.14765756 0.94246094 0.90749395 1.07390028\n",
      "  0.92821223 0.94265802 0.99790923 1.02378007 1.02693025 0.96063718\n",
      "  1.07613402 0.86199902 0.93441461 0.92850137 1.03317894 1.00017741\n",
      "  0.94525668 0.99366719 0.82245706 1.09288806 1.09254301 1.05577613\n",
      "  1.14345583 0.86468693 0.88377601 0.9045494  1.04704723 0.98760635\n",
      "  0.93457199 1.01770935 0.99189764 0.96141395 0.94220095 0.9216701\n",
      "  1.01906459 0.90751326 0.97410434 0.83443265 1.14549958 1.00155877\n",
      "  1.07924467 1.07840728 0.99626746 1.0897017  0.9491918  0.98951059\n",
      "  0.97743622 0.95380289 1.09256055 1.07552774 0.9926589  0.98555632\n",
      "  0.99262966 0.86669749 0.97313961 1.0844604  1.07254359 1.0748816\n",
      "  0.94620192 0.84067605 0.82091867 0.98940937 1.05193139 0.87292174\n",
      "  1.02402773 1.07151165 1.19556735 0.95436469 0.96863024 1.07546948\n",
      "  0.91674215 1.16624295 1.05159337 1.00088003 1.04460335 0.85416496\n",
      "  0.93737244 1.01380229 1.11334371 1.12350242 0.95142912]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 2048\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8040435.076354949\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11749423 0.81416412 1.00950926 1.00057391 1.10632747 1.04128812\n",
      "  1.1691556  1.03115745 1.14628432 1.0646729  0.98352024 1.06656366\n",
      "  0.98233145 1.07853119 1.04076218 1.04218571 0.85329094 0.91988414\n",
      "  1.05957442 0.83636562 1.03532584 0.94138498 0.99616428 1.16296074\n",
      "  0.87368305 1.17742716 1.06803532 1.06845738 1.05742587 0.99029332\n",
      "  0.82194115 0.83832707 0.96210014 1.08401968 1.00076714 0.85780255\n",
      "  1.06702215 0.94532253 0.9048039  1.01713064 1.08316136 0.92611557\n",
      "  1.11424462 0.88634599 1.02740756 1.09563374 1.15094915 1.09805846\n",
      "  1.00624002 0.93896048 0.85935818 0.90761412 0.97468773 1.18009239\n",
      "  1.19695777 0.97082275 0.99598599 0.96853427 1.04485925 0.85634828\n",
      "  1.06847199 0.87888836 1.04095668 1.08258466 1.11946898 1.18425961\n",
      "  1.06587615 1.04526146 1.09449633 0.97412718 1.03597655 1.02365875\n",
      "  1.06275656 0.95433629 1.13683169 0.93797494 1.0324077  1.01379033\n",
      "  1.12591549 1.03160555 1.1477055  0.9425089  0.9075419  1.07394822\n",
      "  0.92826018 0.94270601 0.99795718 1.02382803 1.02697822 0.96068523\n",
      "  1.07618196 0.86204699 0.93446256 0.92854933 1.0332269  1.00022534\n",
      "  0.94530463 0.99371515 0.82250486 1.09293602 1.09259096 1.05582409\n",
      "  1.14350379 0.86473489 0.88382397 0.90459733 1.04709518 0.98765432\n",
      "  0.93461992 1.0177573  0.99194561 0.9614619  0.9422489  0.92171806\n",
      "  1.01911255 0.90756122 0.97415232 0.83448062 1.14554754 1.00160677\n",
      "  1.07929265 1.07845531 0.99631539 1.08974966 0.94923976 0.98955854\n",
      "  0.97748417 0.95385085 1.0926085  1.07557571 0.99270686 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450832 1.07259154 1.07492956\n",
      "  0.94624978 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.0715596  1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.9167901  1.16629089 1.05164133 1.00092805 1.04465131 0.85421293\n",
      "  0.9374204  1.01385026 1.11339171 1.12355037 0.9514771 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 161 pen 4\n",
    "\n",
    "for b in [512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 32\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8158100.630830191\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11442135 0.81108692 1.006435   0.99749997 1.10326125 1.03821693\n",
      "  1.1660815  1.02808225 1.14321316 1.06159795 0.98044639 1.06349638\n",
      "  0.97925727 1.07546931 1.03782529 1.03911077 0.85021643 0.91680917\n",
      "  1.05649862 0.83329083 1.03225135 0.93831282 0.99309168 1.15988828\n",
      "  0.87061258 1.17435404 1.06496356 1.06538445 1.05435141 0.98721853\n",
      "  0.81886756 0.83525361 0.95905519 1.08094484 0.99769293 0.85473084\n",
      "  1.06398067 0.94229264 0.9017301  1.01405634 1.08008646 0.923042\n",
      "  1.11117145 0.88326964 1.02433296 1.09255746 1.14788407 1.09498575\n",
      "  1.00316598 0.93588566 0.85628632 0.9045385  0.97161295 1.17702694\n",
      "  1.19388301 0.9677562  0.99303655 0.96545969 1.04178463 0.85327305\n",
      "  1.06539757 0.87581317 1.03788235 1.07951245 1.11639521 1.18118658\n",
      "  1.06280514 1.04218674 1.09142441 0.97105379 1.0329021  1.02058432\n",
      "  1.05968376 0.95126084 1.13377679 0.93490018 1.02933359 1.01072006\n",
      "  1.12286896 1.02854738 1.14463424 0.93943447 0.90446785 1.07087515\n",
      "  0.92518688 0.93962835 0.99488295 1.02075347 1.02390819 0.95760933\n",
      "  1.07310818 0.85897182 0.9313923  0.92547432 1.03015269 0.99715663\n",
      "  0.94223108 0.99065237 0.81957851 1.0898616  1.08951685 1.05274923\n",
      "  1.1404293  0.86166001 0.8807493  0.90152567 1.0440228  0.98457995\n",
      "  0.93154882 1.01468379 0.98887332 0.95838898 0.93917438 0.91864367\n",
      "  1.01603872 0.90448647 0.97110328 0.83140584 1.14247299 0.99853386\n",
      "  1.07624879 1.07542216 0.99324492 1.08667517 0.94616534 0.98648507\n",
      "  0.97440974 0.95077643 1.08953409 1.07250295 0.98963787 0.98252975\n",
      "  0.98960319 0.86367107 0.97011317 1.08143875 1.06951737 1.07186636\n",
      "  0.94329808 0.83764964 0.81789242 0.9863829  1.04890489 0.86989536\n",
      "  1.02100091 1.0684852  1.19254071 0.95133889 0.96560381 1.07244301\n",
      "  0.91371566 1.16321823 1.04856688 0.99786743 1.04157685 0.85113823\n",
      "  0.9343461  1.01080718 1.11035886 1.12047589 0.94840151]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 64\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8097048.574518942\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11598163 0.81264915 1.007996   0.99906057 1.10481694 1.03977327\n",
      "  1.16764245 1.02964366 1.14477198 1.0631592  0.98200713 1.06505279\n",
      "  0.9808182  1.07702129 1.03928869 1.04067208 0.8517775  0.91837051\n",
      "  1.05806018 0.83485207 1.03381243 0.9398727  0.99465138 1.16144801\n",
      "  0.87217155 1.17591425 1.06652272 1.06694446 1.05591252 0.9887798\n",
      "  0.82042767 0.83681406 0.96059368 1.08250603 0.99925375 0.85628859\n",
      "  1.06551572 0.94381797 0.90329067 1.01561738 1.08164765 0.92460253\n",
      "  1.1127319  0.8848315  1.02589415 1.09411942 1.14943933 1.09654403\n",
      "  1.00472688 0.93744688 0.85784555 0.90610008 0.97317421 1.17858265\n",
      "  1.19544426 0.96931092 0.9945097  0.96702085 1.04334579 0.85483451\n",
      "  1.06695828 0.87737462 1.03944336 1.08107238 1.11795565 1.18274661\n",
      "  1.06436445 1.04374782 1.09298373 0.97261401 1.0344632  1.02214541\n",
      "  1.06124359 0.9528223  1.13532278 0.93646135 1.03089449 1.01227745\n",
      "  1.12440713 1.03009568 1.14619305 0.94099555 0.90602859 1.07243549\n",
      "  0.92674736 0.94119083 0.99644396 1.02231441 1.02546595 0.95916889\n",
      "  1.07466898 0.86053319 0.93295055 0.92703558 1.03171363 0.99871367\n",
      "  0.9437917  0.99220473 0.8210321  1.09142269 1.09107777 1.05431053\n",
      "  1.14199003 0.8632213  0.88231048 0.90308532 1.04558242 0.98614058\n",
      "  0.93310813 1.0162442  0.99043276 0.95994897 0.94073551 0.92020473\n",
      "  1.01759905 0.90604755 0.97264411 0.83296701 1.14403403 1.00009243\n",
      "  1.0777853  1.07695059 0.99480317 1.0882363  0.94772627 0.98804556\n",
      "  0.97597083 0.95233751 1.09109518 1.07406267 0.99119471 0.98409088\n",
      "  0.99116427 0.86523212 0.97167417 1.08299686 1.07107832 1.07341905\n",
      "  0.94477005 0.83921068 0.81945339 0.98794398 1.05046599 0.87145636\n",
      "  1.02256178 1.07004628 1.19410178 0.95289962 0.96716487 1.0740041\n",
      "  0.91527675 1.16477843 1.05012798 0.99941621 1.04313796 0.8526993\n",
      "  0.93590711 1.0123438  1.11188812 1.12203701 0.94996299]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 128\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8067026.012397951\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11676306 0.81343181 1.00877781 0.99984236 1.1055969  1.04055525\n",
      "  1.1684242  1.03042572 1.14555309 1.06394124 0.98278884 1.06583308\n",
      "  0.98159998 1.07780064 1.04004281 1.04145406 0.85255938 0.9191525\n",
      "  1.05884246 0.83563403 1.03459429 0.94065395 0.99543293 1.16222951\n",
      "  0.87295239 1.17669581 1.06730398 1.06772601 1.05669436 0.98956174\n",
      "  0.82120948 0.8375957  0.96136999 1.08328801 1.00003558 0.85707033\n",
      "  1.06629136 0.94459189 0.90407244 1.01639918 1.08242969 0.9253842\n",
      "  1.1135134  0.88561392 1.02667604 1.09490178 1.15021896 1.09732606\n",
      "  1.00550861 0.93822883 0.85862685 0.9068823  0.97395615 1.17936241\n",
      "  1.19622618 0.97009135 0.9952667  0.96780272 1.04412769 0.85561658\n",
      "  1.06774029 0.87815668 1.04022517 1.08185364 1.11873741 1.18352822\n",
      "  1.06514542 1.04452979 1.09376503 0.97339568 1.03524504 1.02292724\n",
      "  1.06202515 0.95360447 1.13610095 0.9372433  1.03167624 1.01305872\n",
      "  1.12518401 1.03087447 1.14697437 0.94177739 0.9068104  1.07321701\n",
      "  0.92752891 0.94197361 0.99722576 1.02309637 1.02624695 0.95995184\n",
      "  1.07545064 0.86131525 0.93373146 0.92781765 1.03249542 0.99949436\n",
      "  0.94457331 0.99298433 0.82178455 1.09220452 1.09185953 1.05509248\n",
      "  1.14277207 0.86400328 0.88309239 0.90386644 1.0463639  0.9869226\n",
      "  0.93388913 1.01702587 0.99121415 0.96073051 0.94151737 0.92098655\n",
      "  1.0183809  0.90682954 0.97342153 0.83374897 1.14481592 1.00087443\n",
      "  1.07856148 1.0777249  0.99558434 1.08901815 0.94850818 0.98882719\n",
      "  0.97675266 0.95311934 1.09187701 1.0748442  0.99197557 0.98487275\n",
      "  0.99194611 0.86601394 0.97245602 1.08377761 1.07186009 1.07419871\n",
      "  0.94552728 0.8399925  0.82023517 0.98872582 1.05124783 0.87223818\n",
      "  1.02334386 1.07082811 1.1948837  0.95368128 0.9679467  1.07478593\n",
      "  0.91605859 1.1655598  1.05090982 1.00019602 1.04391979 0.85348125\n",
      "  0.93668891 1.01311963 1.11266208 1.12281886 0.9507452 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 512\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8045414.838682952\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11734798 0.8140177  1.00936298 1.00042761 1.10618124 1.04114153\n",
      "  1.16900933 1.03101111 1.14613805 1.06452659 0.98337396 1.06641751\n",
      "  0.98218516 1.07838495 1.0406172  1.04203939 0.85314464 0.91973783\n",
      "  1.05942806 0.83621932 1.03517954 0.94123876 0.99601801 1.1628145\n",
      "  0.87353688 1.17728089 1.06788902 1.0683111  1.05727958 0.99014702\n",
      "  0.82179484 0.8381808  0.96195382 1.08387336 1.00062084 0.8576561\n",
      "  1.06687567 0.94517597 0.90465763 1.01698436 1.08301504 0.9259693\n",
      "  1.11409836 0.88619961 1.02726127 1.09548739 1.15080299 1.09791198\n",
      "  1.00609375 0.93881416 0.85921192 0.90746778 0.97454143 1.17994633\n",
      "  1.19681146 0.97067637 0.99584106 0.96838797 1.04471295 0.85620196\n",
      "  1.06832569 0.87874204 1.04081038 1.08243845 1.11932267 1.18411333\n",
      "  1.06572998 1.04511515 1.09435005 0.97398088 1.03583025 1.02351246\n",
      "  1.06261028 0.95418995 1.13668534 0.93782863 1.03226141 1.01364399\n",
      "  1.12576895 1.03145919 1.14755929 0.9423626  0.90739561 1.07380198\n",
      "  0.92811391 0.94255958 0.99781091 1.02368173 1.0268319  0.96053858\n",
      "  1.07603571 0.86190065 0.93431631 0.92840302 1.03308061 1.00007912\n",
      "  0.94515836 0.99356886 0.82235961 1.09278972 1.09244468 1.05567778\n",
      "  1.14335748 0.86458858 0.88367766 0.90445113 1.04694891 0.987508\n",
      "  0.93447374 1.01761102 0.9917993  0.96131562 0.94210261 0.92157176\n",
      "  1.01896624 0.9074149  0.97400592 0.8343343  1.14540122 1.0014603\n",
      "  1.07914611 1.07830881 0.9961692  1.08960336 0.94909346 0.98941227\n",
      "  0.97733788 0.95370455 1.09246221 1.07542939 0.99256056 0.98545798\n",
      "  0.99253132 0.86659915 0.97304126 1.08436217 1.07244525 1.07478327\n",
      "  0.94610429 0.84057771 0.82082034 0.98931103 1.05183305 0.8728234\n",
      "  1.02392935 1.07141331 1.195469   0.95426637 0.9685319  1.07537114\n",
      "  0.91664381 1.16614466 1.05149503 1.00078149 1.04450501 0.85406661\n",
      "  0.9372741  1.01370381 1.11324535 1.12340408 0.95133073]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 1024\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8042011.122739795\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11744628 0.81411613 1.0094613  1.00052595 1.10627952 1.04124009\n",
      "  1.16910765 1.03110948 1.14623636 1.06462494 0.98347229 1.06651574\n",
      "  0.98228349 1.07848325 1.04071445 1.04213775 0.85324298 0.91983618\n",
      "  1.05952644 0.83631767 1.03527788 0.94133704 0.99611633 1.16291279\n",
      "  0.87363512 1.17737921 1.06798736 1.06840943 1.05737792 0.99024536\n",
      "  0.82189319 0.83827912 0.96205217 1.08397171 1.00071918 0.85775455\n",
      "  1.06697418 0.94527448 0.90475595 1.01708269 1.0831134  0.92606762\n",
      "  1.11419667 0.88629802 1.02735961 1.09558577 1.15090122 1.09801045\n",
      "  1.00619207 0.93891252 0.85931023 0.90756615 0.97463977 1.1800445\n",
      "  1.19690981 0.97077477 0.99593829 0.96848631 1.04481129 0.85630032\n",
      "  1.06842404 0.8788404  1.04090872 1.08253673 1.11942102 1.18421165\n",
      "  1.06582823 1.0452135  1.09444837 0.97407922 1.03592859 1.02361079\n",
      "  1.0627086  0.95428832 1.13678372 0.93792698 1.03235974 1.01374236\n",
      "  1.12586751 1.03155757 1.14765756 0.94246094 0.90749395 1.07390028\n",
      "  0.92821223 0.94265802 0.99790923 1.02378007 1.02693025 0.96063718\n",
      "  1.07613402 0.86199902 0.93441461 0.92850137 1.03317894 1.00017741\n",
      "  0.94525668 0.99366719 0.82245706 1.09288806 1.09254301 1.05577613\n",
      "  1.14345583 0.86468693 0.88377601 0.9045494  1.04704723 0.98760635\n",
      "  0.93457199 1.01770935 0.99189764 0.96141395 0.94220095 0.9216701\n",
      "  1.01906459 0.90751326 0.97410434 0.83443265 1.14549958 1.00155877\n",
      "  1.07924467 1.07840728 0.99626746 1.0897017  0.9491918  0.98951059\n",
      "  0.97743622 0.95380289 1.09256055 1.07552774 0.9926589  0.98555632\n",
      "  0.99262966 0.86669749 0.97313961 1.0844604  1.07254359 1.0748816\n",
      "  0.94620192 0.84067605 0.82091867 0.98940937 1.05193139 0.87292174\n",
      "  1.02402773 1.07151165 1.19556735 0.95436469 0.96863024 1.07546948\n",
      "  0.91674215 1.16624295 1.05159337 1.00088003 1.04460335 0.85416496\n",
      "  0.93737244 1.01380229 1.11334371 1.12350242 0.95142912]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 2048\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8040435.076354949\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11749423 0.81416412 1.00950926 1.00057391 1.10632747 1.04128812\n",
      "  1.1691556  1.03115745 1.14628432 1.0646729  0.98352024 1.06656366\n",
      "  0.98233145 1.07853119 1.04076218 1.04218571 0.85329094 0.91988414\n",
      "  1.05957442 0.83636562 1.03532584 0.94138498 0.99616428 1.16296074\n",
      "  0.87368305 1.17742716 1.06803532 1.06845738 1.05742587 0.99029332\n",
      "  0.82194115 0.83832707 0.96210014 1.08401968 1.00076714 0.85780255\n",
      "  1.06702215 0.94532253 0.9048039  1.01713064 1.08316136 0.92611557\n",
      "  1.11424462 0.88634599 1.02740756 1.09563374 1.15094915 1.09805846\n",
      "  1.00624002 0.93896048 0.85935818 0.90761412 0.97468773 1.18009239\n",
      "  1.19695777 0.97082275 0.99598599 0.96853427 1.04485925 0.85634828\n",
      "  1.06847199 0.87888836 1.04095668 1.08258466 1.11946898 1.18425961\n",
      "  1.06587615 1.04526146 1.09449633 0.97412718 1.03597655 1.02365875\n",
      "  1.06275656 0.95433629 1.13683169 0.93797494 1.0324077  1.01379033\n",
      "  1.12591549 1.03160555 1.1477055  0.9425089  0.9075419  1.07394822\n",
      "  0.92826018 0.94270601 0.99795718 1.02382803 1.02697822 0.96068523\n",
      "  1.07618196 0.86204699 0.93446256 0.92854933 1.0332269  1.00022534\n",
      "  0.94530463 0.99371515 0.82250486 1.09293602 1.09259096 1.05582409\n",
      "  1.14350379 0.86473489 0.88382397 0.90459733 1.04709518 0.98765432\n",
      "  0.93461992 1.0177573  0.99194561 0.9614619  0.9422489  0.92171806\n",
      "  1.01911255 0.90756122 0.97415232 0.83448062 1.14554754 1.00160677\n",
      "  1.07929265 1.07845531 0.99631539 1.08974966 0.94923976 0.98955854\n",
      "  0.97748417 0.95385085 1.0926085  1.07557571 0.99270686 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450832 1.07259154 1.07492956\n",
      "  0.94624978 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.0715596  1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.9167901  1.16629089 1.05164133 1.00092805 1.04465131 0.85421293\n",
      "  0.9374204  1.01385026 1.11339171 1.12355037 0.9514771 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 161 pen 4\n",
    "\n",
    "for b in [32,64,128,512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8007612.084087122\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11460664 0.81127227 1.00662067 0.99768491 1.10344211 1.03839401\n",
      "  1.16626717 1.02826777 1.14339651 1.0617835  0.98063181 1.06367831\n",
      "  0.97944282 1.07564592 1.03791907 1.03929629 0.85040195 0.91699482\n",
      "  1.05668387 0.83347653 1.03243685 0.93849798 0.99327603 1.16007292\n",
      "  0.87079746 1.17453889 1.0651471  1.06556903 1.05453701 0.98740422\n",
      "  0.81905174 0.8354388  0.95921634 1.08113021 0.9978781  0.85491096\n",
      "  1.06413594 0.9424368  0.90191526 1.01424198 1.08027191 0.92322727\n",
      "  1.11135685 0.88345502 1.02451868 1.09274318 1.14806523 1.09516616\n",
      "  1.00335156 0.93607114 0.85647003 0.90472414 0.97179863 1.17720931\n",
      "  1.1940686  0.96793429 0.99314328 0.96564527 1.04197019 0.85345873\n",
      "  1.0655824  0.87599892 1.03806784 1.07969765 1.11657987 1.18137119\n",
      "  1.06299023 1.042372   1.09160827 0.97123837 1.0330877  1.02076992\n",
      "  1.05986809 0.95144625 1.13394566 0.93508562 1.02951906 1.01090103\n",
      "  1.12302664 1.02871828 1.14481803 0.93962006 0.90465312 1.07106046\n",
      "  0.92537228 0.9398138  0.99506866 1.02093858 1.02409023 0.9577892\n",
      "  1.07329383 0.85915731 0.9315753  0.92565985 1.03033819 0.99733853\n",
      "  0.94241653 0.99082892 0.81965825 1.09004719 1.08970241 1.05293487\n",
      "  1.14061413 0.86184571 0.88093488 0.90171087 1.04420719 0.98476464\n",
      "  0.93173382 1.01486875 0.98905696 0.95857353 0.93936    0.91882926\n",
      "  1.01622313 0.90467176 0.97126609 0.83159129 1.14265823 0.99871468\n",
      "  1.07640498 1.07557041 0.99342824 1.08686077 0.94635066 0.98667029\n",
      "  0.97459534 0.95096203 1.08971968 1.07268685 0.98981886 0.98271535\n",
      "  0.98978878 0.86385666 0.9702986  1.08162251 1.06970289 1.0720432\n",
      "  0.94339488 0.83783523 0.81807801 0.98656849 1.04909048 0.87008086\n",
      "  1.0211856  1.06867081 1.19272601 0.95152439 0.9657894  1.07262861\n",
      "  0.91390126 1.1634036  1.04875247 0.99803715 1.04176245 0.85132349\n",
      "  0.93453166 1.01096468 1.11050982 1.12066149 0.94858676]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.5555722213889306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BMTC - Driver or Conductor</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.23</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cattle</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseases</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Government Land Encroachment</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lakes - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manholes</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Bus Shelters</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Sewage Drains</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.40</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overflow of Storm Water Drains</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parking Violations</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potholes</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.53</td>\n",
       "      <td>961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repair of streetlights</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.02</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sewage and Storm Water Drains - Others</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stray Dogs</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.98</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Pollution</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autorickshaws and Taxis</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.67</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Need new Bus Route</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Others</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.58</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bad Roads</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Broken Storm Water Drains</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clearing of Blockage of Under Ground Drainage Pipelines and Replacement of Damaged or Missing Manhole Cover</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.18</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desilting - Lakes</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.60</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electricity</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.55</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flooding of Roads and Footpaths</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Footpaths</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.63</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garbage</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawkers and Vendors</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoardings</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illegal posters and Hoardings</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.93</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maintenance of Roads and Footpaths - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mosquitos</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.93</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Streetlights</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.58</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Toilets</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noise Pollution</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.68</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parks and playgrounds</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Nuisance</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traffic</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.69</td>\n",
       "      <td>681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trees, Parks and Playgrounds - Others</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.73</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unauthorized Construction</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.10</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Leakage</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Supply</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.26</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>6667.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    f1-score  precision  \\\n",
       " BMTC - Driver or Conductor                             0.35       0.75   \n",
       " Cattle                                                 0.43       0.60   \n",
       " Diseases                                               0.14       0.24   \n",
       " Government Land Encroachment                           0.00       0.00   \n",
       " Lakes - Others                                         0.00       0.00   \n",
       " Manholes                                               0.12       0.25   \n",
       " New Bus Shelters                                       0.28       0.32   \n",
       " No Sewage Drains                                       0.28       0.21   \n",
       " Others                                                 0.00       0.00   \n",
       " Overflow of Storm Water Drains                         0.00       0.00   \n",
       " Parking Violations                                     0.40       0.42   \n",
       " Potholes                                               0.64       0.82   \n",
       " Repair of streetlights                                 0.04       0.46   \n",
       " Sewage and Storm Water Drains - Others                 0.03       0.02   \n",
       " Stray Dogs                                             0.95       0.92   \n",
       "Air Pollution                                           0.14       0.09   \n",
       "Autorickshaws and Taxis                                 0.50       0.40   \n",
       "BMTC - Need new Bus Route                               0.24       0.23   \n",
       "BMTC - Others                                           0.44       0.35   \n",
       "Bad Roads                                               0.47       0.59   \n",
       "Broken Storm Water Drains                               0.06       0.06   \n",
       "Clearing of Blockage of Under Ground Drainage P...      0.10       0.07   \n",
       "Desilting - Lakes                                       0.23       0.14   \n",
       "Electricity                                             0.67       0.87   \n",
       "Flooding of Roads and Footpaths                         0.03       0.17   \n",
       "Footpaths                                               0.53       0.46   \n",
       "Garbage                                                 0.89       0.91   \n",
       "Hawkers and Vendors                                     0.00       0.00   \n",
       "Hoardings                                               0.00       0.00   \n",
       "Illegal posters and Hoardings                           0.11       0.06   \n",
       "Maintenance of Roads and Footpaths - Others             0.00       0.00   \n",
       "Mosquitos                                               0.47       0.32   \n",
       "Need New Streetlights                                   0.21       0.12   \n",
       "Need New Toilets                                        0.57       0.57   \n",
       "Noise Pollution                                         0.63       0.58   \n",
       "Parks and playgrounds                                   0.44       0.38   \n",
       "Public Nuisance                                         0.10       0.13   \n",
       "Traffic                                                 0.59       0.51   \n",
       "Trees, Parks and Playgrounds - Others                   0.10       0.05   \n",
       "Unauthorized Construction                               0.14       0.29   \n",
       "Water Leakage                                           0.55       0.60   \n",
       "Water Supply                                            0.41       0.92   \n",
       "avg / total                                             0.57       0.63   \n",
       "\n",
       "                                                    recall  support  \n",
       " BMTC - Driver or Conductor                           0.23     26.0  \n",
       " Cattle                                               0.33      9.0  \n",
       " Diseases                                             0.10     49.0  \n",
       " Government Land Encroachment                         0.00      8.0  \n",
       " Lakes - Others                                       0.00     11.0  \n",
       " Manholes                                             0.08     13.0  \n",
       " New Bus Shelters                                     0.25    121.0  \n",
       " No Sewage Drains                                     0.40    100.0  \n",
       " Others                                               0.00    131.0  \n",
       " Overflow of Storm Water Drains                       0.00     41.0  \n",
       " Parking Violations                                   0.39     82.0  \n",
       " Potholes                                             0.53    961.0  \n",
       " Repair of streetlights                               0.02    259.0  \n",
       " Sewage and Storm Water Drains - Others               0.12     33.0  \n",
       " Stray Dogs                                           0.98    511.0  \n",
       "Air Pollution                                         0.31     45.0  \n",
       "Autorickshaws and Taxis                               0.67     18.0  \n",
       "BMTC - Need new Bus Route                             0.25     53.0  \n",
       "BMTC - Others                                         0.58    125.0  \n",
       "Bad Roads                                             0.39   1008.0  \n",
       "Broken Storm Water Drains                             0.05     20.0  \n",
       "Clearing of Blockage of Under Ground Drainage P...    0.18     71.0  \n",
       "Desilting - Lakes                                     0.60     10.0  \n",
       "Electricity                                           0.55    182.0  \n",
       "Flooding of Roads and Footpaths                       0.02     55.0  \n",
       "Footpaths                                             0.63    217.0  \n",
       "Garbage                                               0.87   1319.0  \n",
       "Hawkers and Vendors                                   0.00     23.0  \n",
       "Hoardings                                             0.00     27.0  \n",
       "Illegal posters and Hoardings                         0.93     14.0  \n",
       "Maintenance of Roads and Footpaths - Others           0.00     27.0  \n",
       "Mosquitos                                             0.93     27.0  \n",
       "Need New Streetlights                                 0.58     48.0  \n",
       "Need New Toilets                                      0.57     21.0  \n",
       "Noise Pollution                                       0.68     31.0  \n",
       "Parks and playgrounds                                 0.53     34.0  \n",
       "Public Nuisance                                       0.08     36.0  \n",
       "Traffic                                               0.69    681.0  \n",
       "Trees, Parks and Playgrounds - Others                 0.73     11.0  \n",
       "Unauthorized Construction                             0.10     21.0  \n",
       "Water Leakage                                         0.50     52.0  \n",
       "Water Supply                                          0.26    136.0  \n",
       "avg / total                                           0.56   6667.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 161 pen 4\n",
    "\n",
    "\n",
    "pl=train(0.00001,1,batch_size = 128, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=5)\n",
    "res = predictAndPrint(pl)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 84), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 84), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 84), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 84), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 84), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(84,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(84,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 84), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(84,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 4229288.894407854\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04]\n",
      "[[1.11460653 0.81127386 1.00662034 0.99768395 1.10344072 1.03839575\n",
      "  1.16626692 1.02826781 1.14339715 1.06178325 0.98063135 1.06367789\n",
      "  0.97944258 1.07564113 1.0378861  1.03929687 0.85040234 0.91699489\n",
      "  1.05668788 0.83347632 1.03243702 0.93849787 0.99327494 1.16007172\n",
      "  0.87079569 1.17453781 1.06514574 1.06556833 1.054537   0.98740448\n",
      "  0.81905371 0.83543661 0.95921095 1.08113057 0.99787858 0.85491192\n",
      "  1.06412996 0.9424338  0.90191627 1.01424179 1.08027293 0.92322752\n",
      "  1.11135723 0.88345524 1.02451864 1.09274394 1.14806066 1.09516534\n",
      "  1.00335165 0.9360706  0.85647168 0.90472465 0.97179886 1.17720515\n",
      "  1.19406947 0.9679328  0.99310752 0.96564552 1.04197058 0.85345922\n",
      "  1.06558542 0.87599918 1.03806774 1.0796976  1.11658052 1.18137024\n",
      "  1.06298847 1.04237251 1.09160589 0.97123806 1.03308722 1.02076991\n",
      "  1.0598684  0.95144555 1.13394354 0.93508597 1.02951905 1.01089879\n",
      "  1.12302323 1.02871523 1.14481721 0.93962008 0.90465347 1.07105976]]\n",
      "{0: 611, 1: 32, 2: 11, 3: 59, 4: 237, 5: 299, 6: 5, 7: 9, 8: 164, 9: 38, 10: 1, 11: 142, 12: 49, 13: 214, 14: 1175, 15: 1, 16: 20, 18: 442, 21: 26, 22: 63, 23: 185, 24: 32, 25: 25, 26: 270, 27: 28, 28: 9, 30: 109, 31: 57, 32: 699, 33: 9, 34: 1, 35: 219, 36: 528, 37: 493, 38: 338, 40: 26, 41: 41}\n",
      "acc 0.5002249887505624\n",
      "macro (0.37118047735176285, 0.36990302429894484, 0.2961447233669115, None)\n",
      "support (0.6510378262674855, 0.5002249887505624, 0.5337063114553743, None)\n",
      "\n",
      "acc 0.5002249887505624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BMTC - Driver or Conductor</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.31</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cattle</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseases</th>\n",
       "      <td>0.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Government Land Encroachment</th>\n",
       "      <td>0.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lakes - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manholes</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.46</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Bus Shelters</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Sewage Drains</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overflow of Storm Water Drains</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parking Violations</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.56</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potholes</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.56</td>\n",
       "      <td>961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repair of streetlights</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sewage and Storm Water Drains - Others</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stray Dogs</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.97</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Pollution</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.44</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autorickshaws and Taxis</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.78</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Need new Bus Route</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Others</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.64</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bad Roads</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Broken Storm Water Drains</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clearing of Blockage of Under Ground Drainage Pipelines and Replacement of Damaged or Missing Manhole Cover</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desilting - Lakes</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.70</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electricity</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.55</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flooding of Roads and Footpaths</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Footpaths</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garbage</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawkers and Vendors</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoardings</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illegal posters and Hoardings</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maintenance of Roads and Footpaths - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mosquitos</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.85</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Streetlights</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.56</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Toilets</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.67</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noise Pollution</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parks and playgrounds</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.62</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Nuisance</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traffic</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.52</td>\n",
       "      <td>681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trees, Parks and Playgrounds - Others</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unauthorized Construction</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Leakage</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.31</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Supply</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.26</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6667.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    f1-score  precision  \\\n",
       " BMTC - Driver or Conductor                             0.43       0.73   \n",
       " Cattle                                                 0.56       0.56   \n",
       " Diseases                                               0.04       1.00   \n",
       " Government Land Encroachment                           0.22       1.00   \n",
       " Lakes - Others                                         0.00       0.00   \n",
       " Manholes                                               0.31       0.23   \n",
       " New Bus Shelters                                       0.33       0.96   \n",
       " No Sewage Drains                                       0.21       0.14   \n",
       " Others                                                 0.00       0.00   \n",
       " Overflow of Storm Water Drains                         0.00       0.00   \n",
       " Parking Violations                                     0.48       0.42   \n",
       " Potholes                                               0.65       0.77   \n",
       " Repair of streetlights                                 0.00       0.00   \n",
       " Sewage and Storm Water Drains - Others                 0.05       0.03   \n",
       " Stray Dogs                                             0.96       0.94   \n",
       "Air Pollution                                           0.06       0.03   \n",
       "Autorickshaws and Taxis                                 0.56       0.44   \n",
       "BMTC - Need new Bus Route                               0.23       0.22   \n",
       "BMTC - Others                                           0.44       0.34   \n",
       "Bad Roads                                               0.28       0.62   \n",
       "Broken Storm Water Drains                               0.00       0.00   \n",
       "Clearing of Blockage of Under Ground Drainage P...      0.03       0.02   \n",
       "Desilting - Lakes                                       0.29       0.18   \n",
       "Electricity                                             0.62       0.71   \n",
       "Flooding of Roads and Footpaths                         0.04       0.04   \n",
       "Footpaths                                               0.55       0.55   \n",
       "Garbage                                                 0.87       0.92   \n",
       "Hawkers and Vendors                                     0.33       0.35   \n",
       "Hoardings                                               0.00       0.00   \n",
       "Illegal posters and Hoardings                           0.05       0.02   \n",
       "Maintenance of Roads and Footpaths - Others             0.00       0.00   \n",
       "Mosquitos                                               0.51       0.37   \n",
       "Need New Streetlights                                   0.23       0.15   \n",
       "Need New Toilets                                        0.53       0.44   \n",
       "Noise Pollution                                         0.58       0.61   \n",
       "Parks and playgrounds                                   0.46       0.37   \n",
       "Public Nuisance                                         0.09       0.22   \n",
       "Traffic                                                 0.60       0.72   \n",
       "Trees, Parks and Playgrounds - Others                   0.05       0.02   \n",
       "Unauthorized Construction                               0.00       0.00   \n",
       "Water Leakage                                           0.41       0.62   \n",
       "Water Supply                                            0.40       0.85   \n",
       "avg / total                                             0.53       0.65   \n",
       "\n",
       "                                                    recall  support  \n",
       " BMTC - Driver or Conductor                           0.31     26.0  \n",
       " Cattle                                               0.56      9.0  \n",
       " Diseases                                             0.02     49.0  \n",
       " Government Land Encroachment                         0.12      8.0  \n",
       " Lakes - Others                                       0.00     11.0  \n",
       " Manholes                                             0.46     13.0  \n",
       " New Bus Shelters                                     0.20    121.0  \n",
       " No Sewage Drains                                     0.38    100.0  \n",
       " Others                                               0.00    131.0  \n",
       " Overflow of Storm Water Drains                       0.00     41.0  \n",
       " Parking Violations                                   0.56     82.0  \n",
       " Potholes                                             0.56    961.0  \n",
       " Repair of streetlights                               0.00    259.0  \n",
       " Sewage and Storm Water Drains - Others               0.18     33.0  \n",
       " Stray Dogs                                           0.97    511.0  \n",
       "Air Pollution                                         0.44     45.0  \n",
       "Autorickshaws and Taxis                               0.78     18.0  \n",
       "BMTC - Need new Bus Route                             0.25     53.0  \n",
       "BMTC - Others                                         0.64    125.0  \n",
       "Bad Roads                                             0.18   1008.0  \n",
       "Broken Storm Water Drains                             0.00     20.0  \n",
       "Clearing of Blockage of Under Ground Drainage P...    0.06     71.0  \n",
       "Desilting - Lakes                                     0.70     10.0  \n",
       "Electricity                                           0.55    182.0  \n",
       "Flooding of Roads and Footpaths                       0.04     55.0  \n",
       "Footpaths                                             0.54    217.0  \n",
       "Garbage                                               0.82   1319.0  \n",
       "Hawkers and Vendors                                   0.30     23.0  \n",
       "Hoardings                                             0.00     27.0  \n",
       "Illegal posters and Hoardings                         0.79     14.0  \n",
       "Maintenance of Roads and Footpaths - Others           0.00     27.0  \n",
       "Mosquitos                                             0.85     27.0  \n",
       "Need New Streetlights                                 0.56     48.0  \n",
       "Need New Toilets                                      0.67     21.0  \n",
       "Noise Pollution                                       0.55     31.0  \n",
       "Parks and playgrounds                                 0.62     34.0  \n",
       "Public Nuisance                                       0.06     36.0  \n",
       "Traffic                                               0.52    681.0  \n",
       "Trees, Parks and Playgrounds - Others                 0.73     11.0  \n",
       "Unauthorized Construction                             0.00     21.0  \n",
       "Water Leakage                                         0.31     52.0  \n",
       "Water Supply                                          0.26    136.0  \n",
       "avg / total                                           0.50   6667.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 84 pen 4\n",
    "\n",
    "\n",
    "pl=train(0.00001,1,batch_size = 128, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=5)\n",
    "res = predictAndPrint(pl)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
