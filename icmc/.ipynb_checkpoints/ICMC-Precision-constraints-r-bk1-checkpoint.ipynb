{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37595, 84)\n",
      "42 84\n"
     ]
    }
   ],
   "source": [
    "import _pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "# pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "#pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "# val_L_S = pkl.load( open( \"val_L_S.p\", \"rb\" ) )\n",
    "# train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )\n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_Embeddings.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_Embeddings.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_TFIDF.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_TFIDF.p\", \"rb\" )) \n",
    "\n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_Keywords.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_Keywords.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_Keywords_regex.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_Keywords_regex.p\", \"rb\" )) \n",
    "\n",
    "train_L_S = pkl.load( open( \"train_L_S_regex.p\", \"rb\" ) )\n",
    "test_L_S = pkl.load( open( \"test_L_S_regex.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S = pkl.load( open( \"train_L_S_Keywords_regex_extracts.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S_Keywords_regex_extracts.p\", \"rb\" )) \n",
    "\n",
    "\n",
    "# LF_output_map = pkl.load(open(\"LF_output_map.p\",\"rb\"))\n",
    "# LF_Names = pkl.load(open(\"LF_Names.p\",\"rb\"))\n",
    "# LF_l = pkl.load(open(\"LF_l.p\",\"rb\"))\n",
    "\n",
    "LF_output_map = pkl.load(open(\"LF_output_map_84.p\",\"rb\"))\n",
    "LF_Names = pkl.load(open(\"LF_Names_84.p\",\"rb\"))\n",
    "LF_l = pkl.load(open(\"LF_l_84.p\",\"rb\"))\n",
    "\n",
    "# LF_output_map = pkl.load(open(\"LF_output_map_161.p\",\"rb\"))\n",
    "# LF_Names = pkl.load(open(\"LF_Names_161.p\",\"rb\"))\n",
    "# LF_l = pkl.load(open(\"LF_l_161.p\",\"rb\"))\n",
    "\n",
    "\n",
    "class_names = pkl.load(open(\"class_names.p\",\"rb\"))\n",
    "# print(LF_Names,len(LF_Names))\n",
    "# print(LF_output_map)\n",
    "# print(class_names)\n",
    "train_L_S_df = pd.DataFrame((np.array(train_L_S)[:,0,:]),columns=LF_Names) \n",
    "print(train_L_S_df.shape)\n",
    "\n",
    "# train_df=pd.read_csv('./complaints_train_data_clean.csv',usecols=[\"category_name\",\"complaint_title\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "train_df=pd.read_csv('./complaints_train_validation_data_clean.csv',usecols=[\"index\",\"category_name\",\"complaint_title\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "train_df['category_name'] = train_df[\"category_name\"].astype('category')\n",
    "train_df['true_label'] = train_df['category_name'].cat.codes\n",
    "\n",
    "train_L_S = np.array(train_L_S,dtype=np.float64)\n",
    "test_L_S = np.array(test_L_S,dtype=np.float64)\n",
    "dev_L_S = test_L_S\n",
    "NoOfClasses = len(class_names) \n",
    "NoOfLFs = len(LF_Names) \n",
    "\n",
    "print(NoOfClasses,NoOfLFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge(a,b):\n",
    "    c = []\n",
    "    for i in range(len(a)):\n",
    "        ci = []\n",
    "        ci_l = a[i,0,:].tolist()+b[i,0,:].tolist()\n",
    "        ci_s = a[i,1,:].tolist()+b[i,1,:].tolist()\n",
    "        ci.append(ci_l)\n",
    "        ci.append(ci_s)\n",
    "        c.append(ci)\n",
    "    return c\n",
    "train_L_S_E = pkl.load( open( \"train_L_S_T-D_Embeddings.p\", \"rb\" ) )\n",
    "test_L_S_E = pkl.load( open( \"test_L_S_T-D_Embeddings.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S_K = pkl.load( open( \"train_L_S_Regex.p\", \"rb\" ) )\n",
    "# test_L_S_K = pkl.load( open( \"test_L_S_Regex.p\", \"rb\" )) \n",
    "\n",
    "# train_L_S_K = pkl.load( open( \"train_L_S_Keywords_regex.p\", \"rb\" ) )\n",
    "# test_L_S_K = pkl.load( open( \"test_L_S_Keywords_regex.p\", \"rb\" )) \n",
    "\n",
    "train_L_S_K = pkl.load( open( \"train_L_S_Keywords_regex_extracts.p\", \"rb\" ) )\n",
    "test_L_S_K = pkl.load( open( \"test_L_S_Keywords_regex_extracts.p\", \"rb\" )) \n",
    "\n",
    "\n",
    "train_L_S = merge(np.array(train_L_S_K),np.array(train_L_S_E))\n",
    "test_L_S = merge(np.array(test_L_S_K),np.array(test_L_S_E))\n",
    "print(np.array(train_L_S).shape)\n",
    "print(np.array(test_L_S).shape)\n",
    "\n",
    "# LF_output_map = pkl.load(open(\"LF_output_map_84.p\",\"rb\"))\n",
    "# LF_Names = pkl.load(open(\"LF_Names_84.p\",\"rb\"))\n",
    "# LF_l = pkl.load(open(\"LF_l_84.p\",\"rb\"))\n",
    "\n",
    "LF_output_map = pkl.load(open(\"LF_output_map_245.p\",\"rb\"))\n",
    "LF_Names = pkl.load(open(\"LF_Names_245.p\",\"rb\"))\n",
    "LF_l = pkl.load(open(\"LF_l_245.p\",\"rb\"))\n",
    "\n",
    "class_names = pkl.load(open(\"class_names.p\",\"rb\"))\n",
    "train_L_S_df = pd.DataFrame((np.array(train_L_S)[:,0,:]),columns=LF_Names) \n",
    "\n",
    "NoOfLFs = len(LF_Names) \n",
    "NoOfClasses = len(class_names) \n",
    "\n",
    "train_L_S = np.array(train_L_S,dtype=np.float64)\n",
    "test_L_S = np.array(test_L_S,dtype=np.float64)\n",
    "dev_L_S = test_L_S\n",
    "\n",
    "print(NoOfClasses,NoOfLFs,len(LF_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load true test labels\n",
    "\n",
    "seed = 12\n",
    "test_df=pd.read_csv('./complaints_test_data_clean.csv',usecols=[\"category_name\",\"complaint_title\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "colsize = len(test_df['category_name'])\n",
    "\n",
    "test_df['category_name'] = test_df[\"category_name\"].astype('category')\n",
    "test_df['true_label'] = test_df['category_name'].cat.codes\n",
    "\n",
    "true_labels = test_df['true_label'].tolist()\n",
    "gold_labels_dev = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  load 500 gold labels\n",
    "# gold_df=pd.read_csv('./clean-gold-labels.tsv',sep='\\t',usecols=[\"category_name\",\"complaint_description\",],na_filter=False)\n",
    "\n",
    "# colsize = len(test_df['category_name'])\n",
    "\n",
    "# gold_df['category_name'] = test_df[\"category_name\"].astype('category')\n",
    "# gold_df['true_label'] = test_df['category_name'].cat.codes\n",
    "\n",
    "# true_labels = test_df['true_label'].tolist()\n",
    "                    \n",
    "# test_L_S = pkl.load( open( \"gold-labels-clean.p\", \"rb\" )) \n",
    "# print(np.array(test_L_S).shape)\n",
    "# for i,x in enumerate(gold_df.groupby(\"category_name\").agg({\"complaint_description\": np.count_nonzero}).index._data):\n",
    "#     print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def draw2DArray(a):\n",
    "    fig = plt.figure(figsize=(6, 3.2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('colorMap')\n",
    "    plt.imshow(np.array(a))\n",
    "    ax.set_aspect('equal')\n",
    "    cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "    cax.get_xaxis().set_visible(False)\n",
    "    cax.get_yaxis().set_visible(False)\n",
    "    cax.patch.set_alpha(0)\n",
    "    cax.set_frame_on(False)\n",
    "    plt.colorbar(orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "      \n",
    "def report2dict(cr):\n",
    "    # Parse rows\n",
    "    tmp = list()\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0:\n",
    "            tmp.append(parsed_row)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    measures = tmp[0]\n",
    "\n",
    "    D_class_data = defaultdict(dict)\n",
    "    for row in tmp[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            D_class_data[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return pd.DataFrame(D_class_data).T\n",
    "\n",
    "def predictAndPrint(pl):\n",
    "    print(\"acc\",accuracy_score(true_labels,pl))\n",
    "#     print(precision_recall_fscore_support(true_labels,pl,average='macro'))\n",
    "#     draw2DArray(confusion_matrix(true_labels,pl))\n",
    "    return report2dict(classification_report(true_labels, pl, target_names=class_names))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized model with smooth LFs + penalties\n",
    "\n",
    "def train(lr,ep,th,af,batch_size=32,LF_acc=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
    "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (to set a max value for alpha)\n",
    "    ## norm : use normalization or not\n",
    "    ## Gamma : penalty tuning parameter\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "       \n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=af,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "        \n",
    "        \n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    \n",
    "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
    "        \n",
    "        LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
    "        \n",
    "#         print(\"k\",k)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "        print(s)\n",
    "        print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "        \n",
    "        s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "        print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "        if(smooth):\n",
    "            pout = tf.map_fn(lambda c: iskequalsy(l,c)*s_ ,pcl,name=\"pout\")\n",
    "        else:\n",
    "            pout = tf.map_fn(lambda c: iskequalsy(l,c) ,pcl,name=\"pout\")\n",
    "\n",
    "        print(\"pout\",pout)    \n",
    "\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                           name=\"t_pout\")\n",
    "    \n",
    "        print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            print(\"intsy\",out1)\n",
    "            return out1\n",
    "                \n",
    "\n",
    "        if(smooth):\n",
    "            #smooth normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "        else:\n",
    "            #discrete normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "            \n",
    "        ### for precision penalty\n",
    "        def softplus(j):\n",
    "            Lj = tf.map_fn(lambda li : tf.gather(li,j),l)\n",
    "            print(\"sft Lj\",Lj)\n",
    "            kj = tf.gather(k,j)\n",
    "            print(\"sft kj\",kj)\n",
    "            aj = tf.gather(LF_a,j)\n",
    "            print(\"sft aj\",aj)\n",
    "            indices = tf.where(tf.equal(Lj,kj))\n",
    "            print(\"sft indices\",indices)\n",
    "            li_lij_eq_kj = tf.gather(l,tf.squeeze(indices,1))\n",
    "            print(\"sft l_ij_eq_kj\",li_lij_eq_kj)\n",
    "            prec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            print(\"prec_z\",prec_z)\n",
    "            prec_t_pout = (tf.matmul(iskequalsy(li_lij_eq_kj,kj), thetas,transpose_b=True))/prec_z\n",
    "            print(\"prec_t_pout\",prec_t_pout)\n",
    "            f =  tf.reduce_sum(aj - prec_t_pout)\n",
    "            print(\"f\",f)\n",
    "            sft = tf.nn.softplus(f,name=\"sft\")\n",
    "            print(\"sft\",sft)\n",
    "            return sft\n",
    "        \n",
    "\n",
    "        \n",
    "# \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        \n",
    "        print(\"zy\",zy)\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        \n",
    "        print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk))))\n",
    "        elif(penalty == 4):\n",
    "            print(\"precision penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 5):\n",
    "            print(\"precision log(softplus) penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(\"normalized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "       \n",
    "            \n",
    "        print(\"loss\",loss)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,loss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "                print(\"dev set\")\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "                print(a)\n",
    "                print(t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "                print(\"macro\",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"macro\"))\n",
    "                print(\"support\",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"weighted\"))\n",
    "                print()\n",
    "               \n",
    "                \n",
    "#             # Initialize an iterator over the validation dataset.\n",
    "#             sess.run(dev_init_op)\n",
    "#             a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "#             print(t)\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "# #             predictAndPrint(pl)\n",
    "#             print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(lr,ep,th,af,batch_size=32,LF_acc=None,LF_rec=None,pcl=np.array([-1,1],dtype=np.float64),norm=True,\\\n",
    "          smooth=True,penalty=0,p3k=3,alp=1,Gamma=1.0,debug=True):\n",
    "    \n",
    "    ## lr : learning rate\n",
    "    ## ep : no of epochs\n",
    "    ## th : thetas initializer\n",
    "    ## af : alphas initializer\n",
    "    ## penalty : {1,2,3} use one of the three penalties, 0: no-penalty\n",
    "    ## p3k : parameter for penalty-3 \n",
    "    ## smooth : flag if smooth lfs are used \n",
    "    ## make sure smooth/discrete LF data is loaded into train_L_S and test_L_S\n",
    "    ## pcl : all possible class labels  = [-1,1] for binary, \n",
    "    ##       np.arange(0,NoOfClasses) for multiclass\n",
    "    ## alp : alpha parameter (to set a max value for alpha)\n",
    "    ## norm : use normalization or not\n",
    "    ## Gamma : penalty tuning parameter\n",
    "    \n",
    "    BATCH_SIZE = batch_size\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "    seed = 12\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_L_S).batch(BATCH_SIZE)\n",
    "        dev_dataset = tf.data.Dataset.from_tensor_slices(dev_L_S).batch(len(dev_L_S))\n",
    "#         test_dataset = tf.data.Dataset.from_tensor_slices(test_L_S).batch(len(test_L_S))\n",
    "\n",
    "     \n",
    "        iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "        next_element = iterator.get_next()\n",
    "\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        dev_init_op = iterator.make_initializer(dev_dataset)\n",
    "#         test_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "        next_element = iterator.get_next()\n",
    "#         print(\"next_element\",next_element)\n",
    "\n",
    "        alphas = tf.get_variable('alphas', [NoOfLFs],\\\n",
    "                                 initializer=af,\\\n",
    "                                 dtype=tf.float64)\n",
    "\n",
    "        thetas = tf.get_variable('thetas',[1,NoOfLFs],\\\n",
    "                                initializer=th,\\\n",
    "                        dtype=tf.float64)\n",
    "        \n",
    "        \n",
    "\n",
    "#         print(\"thetas\",thetas)\n",
    "        k = tf.convert_to_tensor(LF_l, dtype=tf.float64)\n",
    "    \n",
    "        g = tf.convert_to_tensor(Gamma, dtype=tf.float64)\n",
    "        \n",
    "        LF_a = tf.convert_to_tensor(LF_acc, dtype=tf.float64)\n",
    "        \n",
    "        LF_r = tf.convert_to_tensor(LF_rec, dtype=tf.float64)\n",
    "        \n",
    "        if(debug):\n",
    "            print(\"k\",k)\n",
    "        l,s =  tf.unstack(next_element,axis=1)\n",
    "#         print(alphas)\n",
    "        if(debug):\n",
    "            print(\"s\",s)\n",
    "            print(\"l\",l)\n",
    "#         print(s.graph)\n",
    "        if(smooth):\n",
    "            s_ = tf.maximum(tf.subtract(s,tf.minimum(alphas,alp)), 0)\n",
    "        if(debug):\n",
    "            print(\"s_\",s_)\n",
    "\n",
    "       \n",
    "        def iskequalsy(v,s):\n",
    "            out = tf.where(tf.equal(v,s),tf.ones_like(v),-tf.ones_like(v))\n",
    "            if(debug):\n",
    "                print(\"out\",out)\n",
    "            return out\n",
    "\n",
    "        if(smooth):\n",
    "            pout = tf.map_fn(lambda c: iskequalsy(l,c)*s_ ,pcl,name=\"pout\")\n",
    "        else:\n",
    "            pout = tf.map_fn(lambda c: iskequalsy(l,c) ,pcl,name=\"pout\")\n",
    "\n",
    "        if(debug):\n",
    "            print(\"pout\",pout)    \n",
    "\n",
    "        t_pout = tf.map_fn(lambda x: tf.matmul(x,thetas,transpose_b=True),pout,\\\n",
    "                           name=\"t_pout\")\n",
    "    \n",
    "        if(debug):\n",
    "            print(\"t_pout\",t_pout)\n",
    "\n",
    "        t =  tf.squeeze(thetas)\n",
    "        if(debug):\n",
    "            print(\"t\",t)\n",
    "        \n",
    "        def ints(y):\n",
    "            ky = iskequalsy(k,y)\n",
    "            if(debug):\n",
    "                print(\"ky\",ky)\n",
    "            out1 = alphas+((tf.exp((t*ky*(1-alphas)))-1)/(t*ky))\n",
    "            if(debug):\n",
    "                print(\"intsy\",out1)\n",
    "            return out1\n",
    "                \n",
    "\n",
    "        if(smooth):\n",
    "            #smooth normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "        else:\n",
    "            #discrete normalizer\n",
    "            zy = tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"zy\")\n",
    "            \n",
    "        ### for precision and recall t_pout\n",
    "        def pr_t_pout(j):\n",
    "            Lj = tf.map_fn(lambda li : tf.gather(li,j),l)\n",
    "            if(debug):\n",
    "                print(\"sft Lj\",Lj)\n",
    "            kj = tf.gather(k,j)\n",
    "            if(debug):\n",
    "                print(\"sft kj\",kj)\n",
    "            indices = tf.where(tf.equal(Lj,kj))\n",
    "            if(debug):\n",
    "                print(\"sft indices\",indices)\n",
    "            li_lij_eq_kj = tf.gather(l,tf.squeeze(indices,1))\n",
    "            if(smooth):\n",
    "                si_lij_eq_kj = tf.gather(s_,tf.squeeze(indices,1))\n",
    "            if(debug):\n",
    "                print(\"sft l_ij_eq_kj\",li_lij_eq_kj)\n",
    "            if(smooth):\n",
    "                prec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            else:\n",
    "                prec_z = tf.reduce_sum(tf.map_fn(lambda y: tf.reduce_prod(1+tf.exp(t*iskequalsy(k,y)),axis=0),\\\n",
    "                           pcl,name=\"prec_zy\"))\n",
    "            if(debug):\n",
    "                print(\"prec_z\",prec_z)\n",
    "            if(smooth):\n",
    "                prec_t_pout = (tf.matmul(iskequalsy(li_lij_eq_kj,kj)*si_lij_eq_kj, thetas,transpose_b=True))/prec_z\n",
    "            else:\n",
    "                prec_t_pout = (tf.matmul(iskequalsy(li_lij_eq_kj,kj), thetas,transpose_b=True))/prec_z\n",
    "            if(debug):\n",
    "                print(\"prec_t_pout\",prec_t_pout)\n",
    "            return prec_t_pout\n",
    "           \n",
    "        def softplus_p(j):\n",
    "            aj = tf.gather(LF_a,j)\n",
    "            if(debug):\n",
    "                print(\"sft aj\",aj)\n",
    "            f_p =  tf.reduce_sum(aj - pr_t_pout(j))\n",
    "            if(debug):\n",
    "                print(\"f_p\",f_p)\n",
    "            sft_p = tf.nn.softplus(f_p,name=\"sft_p\")\n",
    "            if(debug):\n",
    "                print(\"sft_p\",sft_p)\n",
    "            return sft_p\n",
    "        \n",
    "        def softplus_r(j):\n",
    "            rj = tf.gather(LF_r,j)\n",
    "            if(debug):\n",
    "                print(\"sft aj\",rj)\n",
    "            f_r =  tf.reduce_sum( pr_t_pout(j) - rj)\n",
    "            if(debug):\n",
    "                print(\"f_r\",f_r)\n",
    "            sft_r = tf.nn.softplus(f_r,name=\"sft_r\")\n",
    "            if(debug):\n",
    "                print(\"sft_r\",sft_r)\n",
    "            return sft_r\n",
    "            \n",
    "        \n",
    "#         logsft = tf.map_fn(lambda j: tf.log(softplus(j)),np.arange(NoOfLFs),\\\n",
    "#                                              dtype=tf.float64)\n",
    "#         sft  =  tf.map_fn(lambda j: softplus(j),np.arange(NoOfLFs),\\\n",
    "#                                              dtype=tf.float64)\n",
    "        \n",
    "# \n",
    "#         zy = tf.map_fn(lambda y: tf.reduce_prod(1+ints(y),axis=0),\\\n",
    "#                        np.array(NoOfClasses,dtype=np.float64))\n",
    "        if(debug):\n",
    "            print(\"zy\",zy)\n",
    "        logz = tf.log(tf.reduce_sum(zy,axis=0),name=\"logz\")\n",
    "        if(debug):\n",
    "            print(\"logz\",logz)\n",
    "        tf.summary.scalar('logz', logz)\n",
    "        lsp = tf.reduce_logsumexp(t_pout,axis=0)\n",
    "        if(debug):\n",
    "            print(\"lsp\",lsp)\n",
    "        tf.summary.scalar('lsp', tf.reduce_sum(lsp))\n",
    "        \n",
    "        if(not norm):\n",
    "            print(\"unnormlized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  ))\n",
    "        elif(penalty == 1):\n",
    "            print(\"penalty1\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                      +(g*tf.reduce_sum(tf.maximum(tf.zeros_like(thetas),-thetas)))\n",
    "        elif(penalty == 2):\n",
    "            print(\"penalty2\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     -(g*tf.minimum( tf.reduce_min(thetas),0.0))\n",
    "        elif(penalty == 3):\n",
    "            print(\"penalty3\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                     +(g*tf.reduce_sum(tf.log(1+tf.exp(-thetas-pk))))\n",
    "        elif(penalty == 4):\n",
    "            print(\"precision penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 5):\n",
    "            print(\"precision log(softplus) penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: tf.log(softplus(j)),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "        elif(penalty == 6):\n",
    "            print(\"precision and recall penalty\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  )) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_p(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64))) \\\n",
    "                + (g*tf.reduce_sum(tf.map_fn(lambda j: softplus_r(j),np.arange(NoOfLFs),\\\n",
    "                                             dtype=tf.float64)))\n",
    "\n",
    "        else:\n",
    "            print(\"normalized loss\")\n",
    "            loss = tf.negative(tf.reduce_sum(lsp  - logz  ))\n",
    "       \n",
    "        if(debug):\n",
    "            print(\"loss\",loss)\n",
    "        tf.summary.scalar('loss', loss)\n",
    "#         tf.summary.histogram('thetas', t)\n",
    "#         tf.summary.histogram('alphas', alphas)\n",
    "#         print(\"normloss\",normloss)\n",
    "        marginals = tf.nn.softmax(t_pout,axis=0)\n",
    "\n",
    "        if(debug):\n",
    "            print(\"marginals\",marginals)\n",
    "        predict = tf.argmax(marginals,axis=0)\n",
    "\n",
    "\n",
    "    #     pre = tf.metrics.precision(labels,predict)\n",
    "    #     rec = tf.metrics.recall(labels,predict)\n",
    "    #     print(\"loss\",loss)\n",
    "    #     print(\"nls_\",nls_)\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False,dtype=tf.float64)\n",
    "    #     starter_learning_rate = 1.0\n",
    "    #     learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "    #                                            10, 0.96, staircase=True)\n",
    "    #     train_step = tf.train.AdamOptimizer(learning_rate).minimize(normloss, global_step=global_step) \n",
    "\n",
    "\n",
    "    #     train_step = tf.train.AdamOptimizer(0.001).minimize(normloss)\n",
    "    #     reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #     reg_constant = 5.0  # Choose an appropriate one.\n",
    "    #     totalloss = normloss + reg_constant * sum(reg_losses)\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss) \n",
    "    #     train_step = tf.train.AdagradOptimizer(0.01).minimize(normloss) \n",
    "    #     train_step = tf.train.MomentumOptimizer(0.01,0.2).minimize(normloss) \n",
    "\n",
    "    #     train_step = tf.train.GradientDescentOptimizer(0.1).minimize(normloss)\n",
    "\n",
    "        summary_merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./summary/train',\n",
    "                                      tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter('./summary/test')\n",
    "\n",
    "        init_g = tf.global_variables_initializer()\n",
    "        init_l = tf.local_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_g)\n",
    "            sess.run(init_l)\n",
    "\n",
    "            # Initialize an iterator over the training dataset.\n",
    "            for en in range(ep):\n",
    "                sess.run(train_init_op)\n",
    "                tl = 0\n",
    "                try:\n",
    "                    it = 0\n",
    "                    while True:\n",
    "                        sm,_,ls,t = sess.run([summary_merged,train_step,loss,thetas])\n",
    "#                         print(t)\n",
    "#                         print(tl)\n",
    "                        train_writer.add_summary(sm, it)\n",
    "#                         if(ls<1e-5):\n",
    "#                             break\n",
    "                        tl = tl + ls\n",
    "                        it = it + 1\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "                print(en,\"loss\",tl)\n",
    "                print(\"dev set\")\n",
    "                sess.run(dev_init_op)\n",
    "                sm,a,t,m,pl = sess.run([summary_merged,alphas,thetas,marginals,predict])\n",
    "                test_writer.add_summary(sm, en)\n",
    "                print(a)\n",
    "                print(t)\n",
    "                unique, counts = np.unique(pl, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                print(\"acc\",accuracy_score(gold_labels_dev,pl))\n",
    "                print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "#                 print(\"test set\")\n",
    "#                 sess.run(test_init_op)\n",
    "#                 a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#                 unique, counts = np.unique(pl, return_counts=True)\n",
    "#                 print(dict(zip(unique, counts)))\n",
    "#                 print(\"acc\",accuracy_score(gold_labels_test,pl))\n",
    "#                 print(precision_recall_fscore_support(np.array(gold_labels_test),np.array(pl),average=\"binary\"))\n",
    "#                 print()\n",
    "                \n",
    "#             # Initialize an iterator over the validation dataset.\n",
    "#             sess.run(dev_init_op)\n",
    "#             a,t,m,pl = sess.run([alphas,thetas,marginals,predict])\n",
    "#             print(a)\n",
    "#             print(t)\n",
    "\n",
    "#             unique, counts = np.unique(pl, return_counts=True)\n",
    "#             print(dict(zip(unique, counts)))\n",
    "\n",
    "#             print(\"acc\",accuracy_score(true_labels,pl))\n",
    "\n",
    "# #             predictAndPrint(pl)\n",
    "#             print(precision_recall_fscore_support(np.array(true_labels),np.array(pl),average=\"binary\"))\n",
    "\n",
    "#             cf = confusion_matrix(true_labels,pl)\n",
    "#             print(cf)\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 32\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8158100.630830191\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11442135 0.81108692 1.006435   0.99749997 1.10326125 1.03821693\n",
      "  1.1660815  1.02808225 1.14321316 1.06159795 0.98044639 1.06349638\n",
      "  0.97925727 1.07546931 1.03782529 1.03911077 0.85021643 0.91680917\n",
      "  1.05649862 0.83329083 1.03225135 0.93831282 0.99309168 1.15988828\n",
      "  0.87061258 1.17435404 1.06496356 1.06538445 1.05435141 0.98721853\n",
      "  0.81886756 0.83525361 0.95905519 1.08094484 0.99769293 0.85473084\n",
      "  1.06398067 0.94229264 0.9017301  1.01405634 1.08008646 0.923042\n",
      "  1.11117145 0.88326964 1.02433296 1.09255746 1.14788407 1.09498575\n",
      "  1.00316598 0.93588566 0.85628632 0.9045385  0.97161295 1.17702694\n",
      "  1.19388301 0.9677562  0.99303655 0.96545969 1.04178463 0.85327305\n",
      "  1.06539757 0.87581317 1.03788235 1.07951245 1.11639521 1.18118658\n",
      "  1.06280514 1.04218674 1.09142441 0.97105379 1.0329021  1.02058432\n",
      "  1.05968376 0.95126084 1.13377679 0.93490018 1.02933359 1.01072006\n",
      "  1.12286896 1.02854738 1.14463424 0.93943447 0.90446785 1.07087515\n",
      "  0.92518688 0.93962835 0.99488295 1.02075347 1.02390819 0.95760933\n",
      "  1.07310818 0.85897182 0.9313923  0.92547432 1.03015269 0.99715663\n",
      "  0.94223108 0.99065237 0.81957851 1.0898616  1.08951685 1.05274923\n",
      "  1.1404293  0.86166001 0.8807493  0.90152567 1.0440228  0.98457995\n",
      "  0.93154882 1.01468379 0.98887332 0.95838898 0.93917438 0.91864367\n",
      "  1.01603872 0.90448647 0.97110328 0.83140584 1.14247299 0.99853386\n",
      "  1.07624879 1.07542216 0.99324492 1.08667517 0.94616534 0.98648507\n",
      "  0.97440974 0.95077643 1.08953409 1.07250295 0.98963787 0.98252975\n",
      "  0.98960319 0.86367107 0.97011317 1.08143875 1.06951737 1.07186636\n",
      "  0.94329808 0.83764964 0.81789242 0.9863829  1.04890489 0.86989536\n",
      "  1.02100091 1.0684852  1.19254071 0.95133889 0.96560381 1.07244301\n",
      "  0.91371566 1.16321823 1.04856688 0.99786743 1.04157685 0.85113823\n",
      "  0.9343461  1.01080718 1.11035886 1.12047589 0.94840151]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 64\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8097048.574518942\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11598163 0.81264915 1.007996   0.99906057 1.10481694 1.03977327\n",
      "  1.16764245 1.02964366 1.14477198 1.0631592  0.98200713 1.06505279\n",
      "  0.9808182  1.07702129 1.03928869 1.04067208 0.8517775  0.91837051\n",
      "  1.05806018 0.83485207 1.03381243 0.9398727  0.99465138 1.16144801\n",
      "  0.87217155 1.17591425 1.06652272 1.06694446 1.05591252 0.9887798\n",
      "  0.82042767 0.83681406 0.96059368 1.08250603 0.99925375 0.85628859\n",
      "  1.06551572 0.94381797 0.90329067 1.01561738 1.08164765 0.92460253\n",
      "  1.1127319  0.8848315  1.02589415 1.09411942 1.14943933 1.09654403\n",
      "  1.00472688 0.93744688 0.85784555 0.90610008 0.97317421 1.17858265\n",
      "  1.19544426 0.96931092 0.9945097  0.96702085 1.04334579 0.85483451\n",
      "  1.06695828 0.87737462 1.03944336 1.08107238 1.11795565 1.18274661\n",
      "  1.06436445 1.04374782 1.09298373 0.97261401 1.0344632  1.02214541\n",
      "  1.06124359 0.9528223  1.13532278 0.93646135 1.03089449 1.01227745\n",
      "  1.12440713 1.03009568 1.14619305 0.94099555 0.90602859 1.07243549\n",
      "  0.92674736 0.94119083 0.99644396 1.02231441 1.02546595 0.95916889\n",
      "  1.07466898 0.86053319 0.93295055 0.92703558 1.03171363 0.99871367\n",
      "  0.9437917  0.99220473 0.8210321  1.09142269 1.09107777 1.05431053\n",
      "  1.14199003 0.8632213  0.88231048 0.90308532 1.04558242 0.98614058\n",
      "  0.93310813 1.0162442  0.99043276 0.95994897 0.94073551 0.92020473\n",
      "  1.01759905 0.90604755 0.97264411 0.83296701 1.14403403 1.00009243\n",
      "  1.0777853  1.07695059 0.99480317 1.0882363  0.94772627 0.98804556\n",
      "  0.97597083 0.95233751 1.09109518 1.07406267 0.99119471 0.98409088\n",
      "  0.99116427 0.86523212 0.97167417 1.08299686 1.07107832 1.07341905\n",
      "  0.94477005 0.83921068 0.81945339 0.98794398 1.05046599 0.87145636\n",
      "  1.02256178 1.07004628 1.19410178 0.95289962 0.96716487 1.0740041\n",
      "  0.91527675 1.16477843 1.05012798 0.99941621 1.04313796 0.8526993\n",
      "  0.93590711 1.0123438  1.11188812 1.12203701 0.94996299]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 128\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8067026.012397951\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11676306 0.81343181 1.00877781 0.99984236 1.1055969  1.04055525\n",
      "  1.1684242  1.03042572 1.14555309 1.06394124 0.98278884 1.06583308\n",
      "  0.98159998 1.07780064 1.04004281 1.04145406 0.85255938 0.9191525\n",
      "  1.05884246 0.83563403 1.03459429 0.94065395 0.99543293 1.16222951\n",
      "  0.87295239 1.17669581 1.06730398 1.06772601 1.05669436 0.98956174\n",
      "  0.82120948 0.8375957  0.96136999 1.08328801 1.00003558 0.85707033\n",
      "  1.06629136 0.94459189 0.90407244 1.01639918 1.08242969 0.9253842\n",
      "  1.1135134  0.88561392 1.02667604 1.09490178 1.15021896 1.09732606\n",
      "  1.00550861 0.93822883 0.85862685 0.9068823  0.97395615 1.17936241\n",
      "  1.19622618 0.97009135 0.9952667  0.96780272 1.04412769 0.85561658\n",
      "  1.06774029 0.87815668 1.04022517 1.08185364 1.11873741 1.18352822\n",
      "  1.06514542 1.04452979 1.09376503 0.97339568 1.03524504 1.02292724\n",
      "  1.06202515 0.95360447 1.13610095 0.9372433  1.03167624 1.01305872\n",
      "  1.12518401 1.03087447 1.14697437 0.94177739 0.9068104  1.07321701\n",
      "  0.92752891 0.94197361 0.99722576 1.02309637 1.02624695 0.95995184\n",
      "  1.07545064 0.86131525 0.93373146 0.92781765 1.03249542 0.99949436\n",
      "  0.94457331 0.99298433 0.82178455 1.09220452 1.09185953 1.05509248\n",
      "  1.14277207 0.86400328 0.88309239 0.90386644 1.0463639  0.9869226\n",
      "  0.93388913 1.01702587 0.99121415 0.96073051 0.94151737 0.92098655\n",
      "  1.0183809  0.90682954 0.97342153 0.83374897 1.14481592 1.00087443\n",
      "  1.07856148 1.0777249  0.99558434 1.08901815 0.94850818 0.98882719\n",
      "  0.97675266 0.95311934 1.09187701 1.0748442  0.99197557 0.98487275\n",
      "  0.99194611 0.86601394 0.97245602 1.08377761 1.07186009 1.07419871\n",
      "  0.94552728 0.8399925  0.82023517 0.98872582 1.05124783 0.87223818\n",
      "  1.02334386 1.07082811 1.1948837  0.95368128 0.9679467  1.07478593\n",
      "  0.91605859 1.1655598  1.05090982 1.00019602 1.04391979 0.85348125\n",
      "  0.93668891 1.01311963 1.11266208 1.12281886 0.9507452 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 512\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8045414.838682952\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11734798 0.8140177  1.00936298 1.00042761 1.10618124 1.04114153\n",
      "  1.16900933 1.03101111 1.14613805 1.06452659 0.98337396 1.06641751\n",
      "  0.98218516 1.07838495 1.0406172  1.04203939 0.85314464 0.91973783\n",
      "  1.05942806 0.83621932 1.03517954 0.94123876 0.99601801 1.1628145\n",
      "  0.87353688 1.17728089 1.06788902 1.0683111  1.05727958 0.99014702\n",
      "  0.82179484 0.8381808  0.96195382 1.08387336 1.00062084 0.8576561\n",
      "  1.06687567 0.94517597 0.90465763 1.01698436 1.08301504 0.9259693\n",
      "  1.11409836 0.88619961 1.02726127 1.09548739 1.15080299 1.09791198\n",
      "  1.00609375 0.93881416 0.85921192 0.90746778 0.97454143 1.17994633\n",
      "  1.19681146 0.97067637 0.99584106 0.96838797 1.04471295 0.85620196\n",
      "  1.06832569 0.87874204 1.04081038 1.08243845 1.11932267 1.18411333\n",
      "  1.06572998 1.04511515 1.09435005 0.97398088 1.03583025 1.02351246\n",
      "  1.06261028 0.95418995 1.13668534 0.93782863 1.03226141 1.01364399\n",
      "  1.12576895 1.03145919 1.14755929 0.9423626  0.90739561 1.07380198\n",
      "  0.92811391 0.94255958 0.99781091 1.02368173 1.0268319  0.96053858\n",
      "  1.07603571 0.86190065 0.93431631 0.92840302 1.03308061 1.00007912\n",
      "  0.94515836 0.99356886 0.82235961 1.09278972 1.09244468 1.05567778\n",
      "  1.14335748 0.86458858 0.88367766 0.90445113 1.04694891 0.987508\n",
      "  0.93447374 1.01761102 0.9917993  0.96131562 0.94210261 0.92157176\n",
      "  1.01896624 0.9074149  0.97400592 0.8343343  1.14540122 1.0014603\n",
      "  1.07914611 1.07830881 0.9961692  1.08960336 0.94909346 0.98941227\n",
      "  0.97733788 0.95370455 1.09246221 1.07542939 0.99256056 0.98545798\n",
      "  0.99253132 0.86659915 0.97304126 1.08436217 1.07244525 1.07478327\n",
      "  0.94610429 0.84057771 0.82082034 0.98931103 1.05183305 0.8728234\n",
      "  1.02392935 1.07141331 1.195469   0.95426637 0.9685319  1.07537114\n",
      "  0.91664381 1.16614466 1.05149503 1.00078149 1.04450501 0.85406661\n",
      "  0.9372741  1.01370381 1.11324535 1.12340408 0.95133073]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 1024\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8042011.122739795\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11744628 0.81411613 1.0094613  1.00052595 1.10627952 1.04124009\n",
      "  1.16910765 1.03110948 1.14623636 1.06462494 0.98347229 1.06651574\n",
      "  0.98228349 1.07848325 1.04071445 1.04213775 0.85324298 0.91983618\n",
      "  1.05952644 0.83631767 1.03527788 0.94133704 0.99611633 1.16291279\n",
      "  0.87363512 1.17737921 1.06798736 1.06840943 1.05737792 0.99024536\n",
      "  0.82189319 0.83827912 0.96205217 1.08397171 1.00071918 0.85775455\n",
      "  1.06697418 0.94527448 0.90475595 1.01708269 1.0831134  0.92606762\n",
      "  1.11419667 0.88629802 1.02735961 1.09558577 1.15090122 1.09801045\n",
      "  1.00619207 0.93891252 0.85931023 0.90756615 0.97463977 1.1800445\n",
      "  1.19690981 0.97077477 0.99593829 0.96848631 1.04481129 0.85630032\n",
      "  1.06842404 0.8788404  1.04090872 1.08253673 1.11942102 1.18421165\n",
      "  1.06582823 1.0452135  1.09444837 0.97407922 1.03592859 1.02361079\n",
      "  1.0627086  0.95428832 1.13678372 0.93792698 1.03235974 1.01374236\n",
      "  1.12586751 1.03155757 1.14765756 0.94246094 0.90749395 1.07390028\n",
      "  0.92821223 0.94265802 0.99790923 1.02378007 1.02693025 0.96063718\n",
      "  1.07613402 0.86199902 0.93441461 0.92850137 1.03317894 1.00017741\n",
      "  0.94525668 0.99366719 0.82245706 1.09288806 1.09254301 1.05577613\n",
      "  1.14345583 0.86468693 0.88377601 0.9045494  1.04704723 0.98760635\n",
      "  0.93457199 1.01770935 0.99189764 0.96141395 0.94220095 0.9216701\n",
      "  1.01906459 0.90751326 0.97410434 0.83443265 1.14549958 1.00155877\n",
      "  1.07924467 1.07840728 0.99626746 1.0897017  0.9491918  0.98951059\n",
      "  0.97743622 0.95380289 1.09256055 1.07552774 0.9926589  0.98555632\n",
      "  0.99262966 0.86669749 0.97313961 1.0844604  1.07254359 1.0748816\n",
      "  0.94620192 0.84067605 0.82091867 0.98940937 1.05193139 0.87292174\n",
      "  1.02402773 1.07151165 1.19556735 0.95436469 0.96863024 1.07546948\n",
      "  0.91674215 1.16624295 1.05159337 1.00088003 1.04460335 0.85416496\n",
      "  0.93737244 1.01380229 1.11334371 1.12350242 0.95142912]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 2048\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8040435.076354949\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11749423 0.81416412 1.00950926 1.00057391 1.10632747 1.04128812\n",
      "  1.1691556  1.03115745 1.14628432 1.0646729  0.98352024 1.06656366\n",
      "  0.98233145 1.07853119 1.04076218 1.04218571 0.85329094 0.91988414\n",
      "  1.05957442 0.83636562 1.03532584 0.94138498 0.99616428 1.16296074\n",
      "  0.87368305 1.17742716 1.06803532 1.06845738 1.05742587 0.99029332\n",
      "  0.82194115 0.83832707 0.96210014 1.08401968 1.00076714 0.85780255\n",
      "  1.06702215 0.94532253 0.9048039  1.01713064 1.08316136 0.92611557\n",
      "  1.11424462 0.88634599 1.02740756 1.09563374 1.15094915 1.09805846\n",
      "  1.00624002 0.93896048 0.85935818 0.90761412 0.97468773 1.18009239\n",
      "  1.19695777 0.97082275 0.99598599 0.96853427 1.04485925 0.85634828\n",
      "  1.06847199 0.87888836 1.04095668 1.08258466 1.11946898 1.18425961\n",
      "  1.06587615 1.04526146 1.09449633 0.97412718 1.03597655 1.02365875\n",
      "  1.06275656 0.95433629 1.13683169 0.93797494 1.0324077  1.01379033\n",
      "  1.12591549 1.03160555 1.1477055  0.9425089  0.9075419  1.07394822\n",
      "  0.92826018 0.94270601 0.99795718 1.02382803 1.02697822 0.96068523\n",
      "  1.07618196 0.86204699 0.93446256 0.92854933 1.0332269  1.00022534\n",
      "  0.94530463 0.99371515 0.82250486 1.09293602 1.09259096 1.05582409\n",
      "  1.14350379 0.86473489 0.88382397 0.90459733 1.04709518 0.98765432\n",
      "  0.93461992 1.0177573  0.99194561 0.9614619  0.9422489  0.92171806\n",
      "  1.01911255 0.90756122 0.97415232 0.83448062 1.14554754 1.00160677\n",
      "  1.07929265 1.07845531 0.99631539 1.08974966 0.94923976 0.98955854\n",
      "  0.97748417 0.95385085 1.0926085  1.07557571 0.99270686 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450832 1.07259154 1.07492956\n",
      "  0.94624978 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.0715596  1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.9167901  1.16629089 1.05164133 1.00092805 1.04465131 0.85421293\n",
      "  0.9374204  1.01385026 1.11339171 1.12355037 0.9514771 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 161 pen 6\n",
    "\n",
    "for b in [512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 32\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8158100.630830191\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11442135 0.81108692 1.006435   0.99749997 1.10326125 1.03821693\n",
      "  1.1660815  1.02808225 1.14321316 1.06159795 0.98044639 1.06349638\n",
      "  0.97925727 1.07546931 1.03782529 1.03911077 0.85021643 0.91680917\n",
      "  1.05649862 0.83329083 1.03225135 0.93831282 0.99309168 1.15988828\n",
      "  0.87061258 1.17435404 1.06496356 1.06538445 1.05435141 0.98721853\n",
      "  0.81886756 0.83525361 0.95905519 1.08094484 0.99769293 0.85473084\n",
      "  1.06398067 0.94229264 0.9017301  1.01405634 1.08008646 0.923042\n",
      "  1.11117145 0.88326964 1.02433296 1.09255746 1.14788407 1.09498575\n",
      "  1.00316598 0.93588566 0.85628632 0.9045385  0.97161295 1.17702694\n",
      "  1.19388301 0.9677562  0.99303655 0.96545969 1.04178463 0.85327305\n",
      "  1.06539757 0.87581317 1.03788235 1.07951245 1.11639521 1.18118658\n",
      "  1.06280514 1.04218674 1.09142441 0.97105379 1.0329021  1.02058432\n",
      "  1.05968376 0.95126084 1.13377679 0.93490018 1.02933359 1.01072006\n",
      "  1.12286896 1.02854738 1.14463424 0.93943447 0.90446785 1.07087515\n",
      "  0.92518688 0.93962835 0.99488295 1.02075347 1.02390819 0.95760933\n",
      "  1.07310818 0.85897182 0.9313923  0.92547432 1.03015269 0.99715663\n",
      "  0.94223108 0.99065237 0.81957851 1.0898616  1.08951685 1.05274923\n",
      "  1.1404293  0.86166001 0.8807493  0.90152567 1.0440228  0.98457995\n",
      "  0.93154882 1.01468379 0.98887332 0.95838898 0.93917438 0.91864367\n",
      "  1.01603872 0.90448647 0.97110328 0.83140584 1.14247299 0.99853386\n",
      "  1.07624879 1.07542216 0.99324492 1.08667517 0.94616534 0.98648507\n",
      "  0.97440974 0.95077643 1.08953409 1.07250295 0.98963787 0.98252975\n",
      "  0.98960319 0.86367107 0.97011317 1.08143875 1.06951737 1.07186636\n",
      "  0.94329808 0.83764964 0.81789242 0.9863829  1.04890489 0.86989536\n",
      "  1.02100091 1.0684852  1.19254071 0.95133889 0.96560381 1.07244301\n",
      "  0.91371566 1.16321823 1.04856688 0.99786743 1.04157685 0.85113823\n",
      "  0.9343461  1.01080718 1.11035886 1.12047589 0.94840151]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 64\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8097048.574518942\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11598163 0.81264915 1.007996   0.99906057 1.10481694 1.03977327\n",
      "  1.16764245 1.02964366 1.14477198 1.0631592  0.98200713 1.06505279\n",
      "  0.9808182  1.07702129 1.03928869 1.04067208 0.8517775  0.91837051\n",
      "  1.05806018 0.83485207 1.03381243 0.9398727  0.99465138 1.16144801\n",
      "  0.87217155 1.17591425 1.06652272 1.06694446 1.05591252 0.9887798\n",
      "  0.82042767 0.83681406 0.96059368 1.08250603 0.99925375 0.85628859\n",
      "  1.06551572 0.94381797 0.90329067 1.01561738 1.08164765 0.92460253\n",
      "  1.1127319  0.8848315  1.02589415 1.09411942 1.14943933 1.09654403\n",
      "  1.00472688 0.93744688 0.85784555 0.90610008 0.97317421 1.17858265\n",
      "  1.19544426 0.96931092 0.9945097  0.96702085 1.04334579 0.85483451\n",
      "  1.06695828 0.87737462 1.03944336 1.08107238 1.11795565 1.18274661\n",
      "  1.06436445 1.04374782 1.09298373 0.97261401 1.0344632  1.02214541\n",
      "  1.06124359 0.9528223  1.13532278 0.93646135 1.03089449 1.01227745\n",
      "  1.12440713 1.03009568 1.14619305 0.94099555 0.90602859 1.07243549\n",
      "  0.92674736 0.94119083 0.99644396 1.02231441 1.02546595 0.95916889\n",
      "  1.07466898 0.86053319 0.93295055 0.92703558 1.03171363 0.99871367\n",
      "  0.9437917  0.99220473 0.8210321  1.09142269 1.09107777 1.05431053\n",
      "  1.14199003 0.8632213  0.88231048 0.90308532 1.04558242 0.98614058\n",
      "  0.93310813 1.0162442  0.99043276 0.95994897 0.94073551 0.92020473\n",
      "  1.01759905 0.90604755 0.97264411 0.83296701 1.14403403 1.00009243\n",
      "  1.0777853  1.07695059 0.99480317 1.0882363  0.94772627 0.98804556\n",
      "  0.97597083 0.95233751 1.09109518 1.07406267 0.99119471 0.98409088\n",
      "  0.99116427 0.86523212 0.97167417 1.08299686 1.07107832 1.07341905\n",
      "  0.94477005 0.83921068 0.81945339 0.98794398 1.05046599 0.87145636\n",
      "  1.02256178 1.07004628 1.19410178 0.95289962 0.96716487 1.0740041\n",
      "  0.91527675 1.16477843 1.05012798 0.99941621 1.04313796 0.8526993\n",
      "  0.93590711 1.0123438  1.11188812 1.12203701 0.94996299]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 128\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8067026.012397951\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11676306 0.81343181 1.00877781 0.99984236 1.1055969  1.04055525\n",
      "  1.1684242  1.03042572 1.14555309 1.06394124 0.98278884 1.06583308\n",
      "  0.98159998 1.07780064 1.04004281 1.04145406 0.85255938 0.9191525\n",
      "  1.05884246 0.83563403 1.03459429 0.94065395 0.99543293 1.16222951\n",
      "  0.87295239 1.17669581 1.06730398 1.06772601 1.05669436 0.98956174\n",
      "  0.82120948 0.8375957  0.96136999 1.08328801 1.00003558 0.85707033\n",
      "  1.06629136 0.94459189 0.90407244 1.01639918 1.08242969 0.9253842\n",
      "  1.1135134  0.88561392 1.02667604 1.09490178 1.15021896 1.09732606\n",
      "  1.00550861 0.93822883 0.85862685 0.9068823  0.97395615 1.17936241\n",
      "  1.19622618 0.97009135 0.9952667  0.96780272 1.04412769 0.85561658\n",
      "  1.06774029 0.87815668 1.04022517 1.08185364 1.11873741 1.18352822\n",
      "  1.06514542 1.04452979 1.09376503 0.97339568 1.03524504 1.02292724\n",
      "  1.06202515 0.95360447 1.13610095 0.9372433  1.03167624 1.01305872\n",
      "  1.12518401 1.03087447 1.14697437 0.94177739 0.9068104  1.07321701\n",
      "  0.92752891 0.94197361 0.99722576 1.02309637 1.02624695 0.95995184\n",
      "  1.07545064 0.86131525 0.93373146 0.92781765 1.03249542 0.99949436\n",
      "  0.94457331 0.99298433 0.82178455 1.09220452 1.09185953 1.05509248\n",
      "  1.14277207 0.86400328 0.88309239 0.90386644 1.0463639  0.9869226\n",
      "  0.93388913 1.01702587 0.99121415 0.96073051 0.94151737 0.92098655\n",
      "  1.0183809  0.90682954 0.97342153 0.83374897 1.14481592 1.00087443\n",
      "  1.07856148 1.0777249  0.99558434 1.08901815 0.94850818 0.98882719\n",
      "  0.97675266 0.95311934 1.09187701 1.0748442  0.99197557 0.98487275\n",
      "  0.99194611 0.86601394 0.97245602 1.08377761 1.07186009 1.07419871\n",
      "  0.94552728 0.8399925  0.82023517 0.98872582 1.05124783 0.87223818\n",
      "  1.02334386 1.07082811 1.1948837  0.95368128 0.9679467  1.07478593\n",
      "  0.91605859 1.1655598  1.05090982 1.00019602 1.04391979 0.85348125\n",
      "  0.93668891 1.01311963 1.11266208 1.12281886 0.9507452 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 512\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8045414.838682952\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11734798 0.8140177  1.00936298 1.00042761 1.10618124 1.04114153\n",
      "  1.16900933 1.03101111 1.14613805 1.06452659 0.98337396 1.06641751\n",
      "  0.98218516 1.07838495 1.0406172  1.04203939 0.85314464 0.91973783\n",
      "  1.05942806 0.83621932 1.03517954 0.94123876 0.99601801 1.1628145\n",
      "  0.87353688 1.17728089 1.06788902 1.0683111  1.05727958 0.99014702\n",
      "  0.82179484 0.8381808  0.96195382 1.08387336 1.00062084 0.8576561\n",
      "  1.06687567 0.94517597 0.90465763 1.01698436 1.08301504 0.9259693\n",
      "  1.11409836 0.88619961 1.02726127 1.09548739 1.15080299 1.09791198\n",
      "  1.00609375 0.93881416 0.85921192 0.90746778 0.97454143 1.17994633\n",
      "  1.19681146 0.97067637 0.99584106 0.96838797 1.04471295 0.85620196\n",
      "  1.06832569 0.87874204 1.04081038 1.08243845 1.11932267 1.18411333\n",
      "  1.06572998 1.04511515 1.09435005 0.97398088 1.03583025 1.02351246\n",
      "  1.06261028 0.95418995 1.13668534 0.93782863 1.03226141 1.01364399\n",
      "  1.12576895 1.03145919 1.14755929 0.9423626  0.90739561 1.07380198\n",
      "  0.92811391 0.94255958 0.99781091 1.02368173 1.0268319  0.96053858\n",
      "  1.07603571 0.86190065 0.93431631 0.92840302 1.03308061 1.00007912\n",
      "  0.94515836 0.99356886 0.82235961 1.09278972 1.09244468 1.05567778\n",
      "  1.14335748 0.86458858 0.88367766 0.90445113 1.04694891 0.987508\n",
      "  0.93447374 1.01761102 0.9917993  0.96131562 0.94210261 0.92157176\n",
      "  1.01896624 0.9074149  0.97400592 0.8343343  1.14540122 1.0014603\n",
      "  1.07914611 1.07830881 0.9961692  1.08960336 0.94909346 0.98941227\n",
      "  0.97733788 0.95370455 1.09246221 1.07542939 0.99256056 0.98545798\n",
      "  0.99253132 0.86659915 0.97304126 1.08436217 1.07244525 1.07478327\n",
      "  0.94610429 0.84057771 0.82082034 0.98931103 1.05183305 0.8728234\n",
      "  1.02392935 1.07141331 1.195469   0.95426637 0.9685319  1.07537114\n",
      "  0.91664381 1.16614466 1.05149503 1.00078149 1.04450501 0.85406661\n",
      "  0.9372741  1.01370381 1.11324535 1.12340408 0.95133073]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 1024\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8042011.122739795\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11744628 0.81411613 1.0094613  1.00052595 1.10627952 1.04124009\n",
      "  1.16910765 1.03110948 1.14623636 1.06462494 0.98347229 1.06651574\n",
      "  0.98228349 1.07848325 1.04071445 1.04213775 0.85324298 0.91983618\n",
      "  1.05952644 0.83631767 1.03527788 0.94133704 0.99611633 1.16291279\n",
      "  0.87363512 1.17737921 1.06798736 1.06840943 1.05737792 0.99024536\n",
      "  0.82189319 0.83827912 0.96205217 1.08397171 1.00071918 0.85775455\n",
      "  1.06697418 0.94527448 0.90475595 1.01708269 1.0831134  0.92606762\n",
      "  1.11419667 0.88629802 1.02735961 1.09558577 1.15090122 1.09801045\n",
      "  1.00619207 0.93891252 0.85931023 0.90756615 0.97463977 1.1800445\n",
      "  1.19690981 0.97077477 0.99593829 0.96848631 1.04481129 0.85630032\n",
      "  1.06842404 0.8788404  1.04090872 1.08253673 1.11942102 1.18421165\n",
      "  1.06582823 1.0452135  1.09444837 0.97407922 1.03592859 1.02361079\n",
      "  1.0627086  0.95428832 1.13678372 0.93792698 1.03235974 1.01374236\n",
      "  1.12586751 1.03155757 1.14765756 0.94246094 0.90749395 1.07390028\n",
      "  0.92821223 0.94265802 0.99790923 1.02378007 1.02693025 0.96063718\n",
      "  1.07613402 0.86199902 0.93441461 0.92850137 1.03317894 1.00017741\n",
      "  0.94525668 0.99366719 0.82245706 1.09288806 1.09254301 1.05577613\n",
      "  1.14345583 0.86468693 0.88377601 0.9045494  1.04704723 0.98760635\n",
      "  0.93457199 1.01770935 0.99189764 0.96141395 0.94220095 0.9216701\n",
      "  1.01906459 0.90751326 0.97410434 0.83443265 1.14549958 1.00155877\n",
      "  1.07924467 1.07840728 0.99626746 1.0897017  0.9491918  0.98951059\n",
      "  0.97743622 0.95380289 1.09256055 1.07552774 0.9926589  0.98555632\n",
      "  0.99262966 0.86669749 0.97313961 1.0844604  1.07254359 1.0748816\n",
      "  0.94620192 0.84067605 0.82091867 0.98940937 1.05193139 0.87292174\n",
      "  1.02402773 1.07151165 1.19556735 0.95436469 0.96863024 1.07546948\n",
      "  0.91674215 1.16624295 1.05159337 1.00088003 1.04460335 0.85416496\n",
      "  0.93737244 1.01380229 1.11334371 1.12350242 0.95142912]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 2048\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8040435.076354949\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11749423 0.81416412 1.00950926 1.00057391 1.10632747 1.04128812\n",
      "  1.1691556  1.03115745 1.14628432 1.0646729  0.98352024 1.06656366\n",
      "  0.98233145 1.07853119 1.04076218 1.04218571 0.85329094 0.91988414\n",
      "  1.05957442 0.83636562 1.03532584 0.94138498 0.99616428 1.16296074\n",
      "  0.87368305 1.17742716 1.06803532 1.06845738 1.05742587 0.99029332\n",
      "  0.82194115 0.83832707 0.96210014 1.08401968 1.00076714 0.85780255\n",
      "  1.06702215 0.94532253 0.9048039  1.01713064 1.08316136 0.92611557\n",
      "  1.11424462 0.88634599 1.02740756 1.09563374 1.15094915 1.09805846\n",
      "  1.00624002 0.93896048 0.85935818 0.90761412 0.97468773 1.18009239\n",
      "  1.19695777 0.97082275 0.99598599 0.96853427 1.04485925 0.85634828\n",
      "  1.06847199 0.87888836 1.04095668 1.08258466 1.11946898 1.18425961\n",
      "  1.06587615 1.04526146 1.09449633 0.97412718 1.03597655 1.02365875\n",
      "  1.06275656 0.95433629 1.13683169 0.93797494 1.0324077  1.01379033\n",
      "  1.12591549 1.03160555 1.1477055  0.9425089  0.9075419  1.07394822\n",
      "  0.92826018 0.94270601 0.99795718 1.02382803 1.02697822 0.96068523\n",
      "  1.07618196 0.86204699 0.93446256 0.92854933 1.0332269  1.00022534\n",
      "  0.94530463 0.99371515 0.82250486 1.09293602 1.09259096 1.05582409\n",
      "  1.14350379 0.86473489 0.88382397 0.90459733 1.04709518 0.98765432\n",
      "  0.93461992 1.0177573  0.99194561 0.9614619  0.9422489  0.92171806\n",
      "  1.01911255 0.90756122 0.97415232 0.83448062 1.14554754 1.00160677\n",
      "  1.07929265 1.07845531 0.99631539 1.08974966 0.94923976 0.98955854\n",
      "  0.97748417 0.95385085 1.0926085  1.07557571 0.99270686 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450832 1.07259154 1.07492956\n",
      "  0.94624978 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.0715596  1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.9167901  1.16629089 1.05164133 1.00092805 1.04465131 0.85421293\n",
      "  0.9374204  1.01385026 1.11339171 1.12355037 0.9514771 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 161 pen 4\n",
    "\n",
    "for b in [512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=True,penalty=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 32\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8158100.630830191\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11442135 0.81108692 1.006435   0.99749997 1.10326125 1.03821693\n",
      "  1.1660815  1.02808225 1.14321316 1.06159795 0.98044639 1.06349638\n",
      "  0.97925727 1.07546931 1.03782529 1.03911077 0.85021643 0.91680917\n",
      "  1.05649862 0.83329083 1.03225135 0.93831282 0.99309168 1.15988828\n",
      "  0.87061258 1.17435404 1.06496356 1.06538445 1.05435141 0.98721853\n",
      "  0.81886756 0.83525361 0.95905519 1.08094484 0.99769293 0.85473084\n",
      "  1.06398067 0.94229264 0.9017301  1.01405634 1.08008646 0.923042\n",
      "  1.11117145 0.88326964 1.02433296 1.09255746 1.14788407 1.09498575\n",
      "  1.00316598 0.93588566 0.85628632 0.9045385  0.97161295 1.17702694\n",
      "  1.19388301 0.9677562  0.99303655 0.96545969 1.04178463 0.85327305\n",
      "  1.06539757 0.87581317 1.03788235 1.07951245 1.11639521 1.18118658\n",
      "  1.06280514 1.04218674 1.09142441 0.97105379 1.0329021  1.02058432\n",
      "  1.05968376 0.95126084 1.13377679 0.93490018 1.02933359 1.01072006\n",
      "  1.12286896 1.02854738 1.14463424 0.93943447 0.90446785 1.07087515\n",
      "  0.92518688 0.93962835 0.99488295 1.02075347 1.02390819 0.95760933\n",
      "  1.07310818 0.85897182 0.9313923  0.92547432 1.03015269 0.99715663\n",
      "  0.94223108 0.99065237 0.81957851 1.0898616  1.08951685 1.05274923\n",
      "  1.1404293  0.86166001 0.8807493  0.90152567 1.0440228  0.98457995\n",
      "  0.93154882 1.01468379 0.98887332 0.95838898 0.93917438 0.91864367\n",
      "  1.01603872 0.90448647 0.97110328 0.83140584 1.14247299 0.99853386\n",
      "  1.07624879 1.07542216 0.99324492 1.08667517 0.94616534 0.98648507\n",
      "  0.97440974 0.95077643 1.08953409 1.07250295 0.98963787 0.98252975\n",
      "  0.98960319 0.86367107 0.97011317 1.08143875 1.06951737 1.07186636\n",
      "  0.94329808 0.83764964 0.81789242 0.9863829  1.04890489 0.86989536\n",
      "  1.02100091 1.0684852  1.19254071 0.95133889 0.96560381 1.07244301\n",
      "  0.91371566 1.16321823 1.04856688 0.99786743 1.04157685 0.85113823\n",
      "  0.9343461  1.01080718 1.11035886 1.12047589 0.94840151]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch-size: 64\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8097048.574518942\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11598163 0.81264915 1.007996   0.99906057 1.10481694 1.03977327\n",
      "  1.16764245 1.02964366 1.14477198 1.0631592  0.98200713 1.06505279\n",
      "  0.9808182  1.07702129 1.03928869 1.04067208 0.8517775  0.91837051\n",
      "  1.05806018 0.83485207 1.03381243 0.9398727  0.99465138 1.16144801\n",
      "  0.87217155 1.17591425 1.06652272 1.06694446 1.05591252 0.9887798\n",
      "  0.82042767 0.83681406 0.96059368 1.08250603 0.99925375 0.85628859\n",
      "  1.06551572 0.94381797 0.90329067 1.01561738 1.08164765 0.92460253\n",
      "  1.1127319  0.8848315  1.02589415 1.09411942 1.14943933 1.09654403\n",
      "  1.00472688 0.93744688 0.85784555 0.90610008 0.97317421 1.17858265\n",
      "  1.19544426 0.96931092 0.9945097  0.96702085 1.04334579 0.85483451\n",
      "  1.06695828 0.87737462 1.03944336 1.08107238 1.11795565 1.18274661\n",
      "  1.06436445 1.04374782 1.09298373 0.97261401 1.0344632  1.02214541\n",
      "  1.06124359 0.9528223  1.13532278 0.93646135 1.03089449 1.01227745\n",
      "  1.12440713 1.03009568 1.14619305 0.94099555 0.90602859 1.07243549\n",
      "  0.92674736 0.94119083 0.99644396 1.02231441 1.02546595 0.95916889\n",
      "  1.07466898 0.86053319 0.93295055 0.92703558 1.03171363 0.99871367\n",
      "  0.9437917  0.99220473 0.8210321  1.09142269 1.09107777 1.05431053\n",
      "  1.14199003 0.8632213  0.88231048 0.90308532 1.04558242 0.98614058\n",
      "  0.93310813 1.0162442  0.99043276 0.95994897 0.94073551 0.92020473\n",
      "  1.01759905 0.90604755 0.97264411 0.83296701 1.14403403 1.00009243\n",
      "  1.0777853  1.07695059 0.99480317 1.0882363  0.94772627 0.98804556\n",
      "  0.97597083 0.95233751 1.09109518 1.07406267 0.99119471 0.98409088\n",
      "  0.99116427 0.86523212 0.97167417 1.08299686 1.07107832 1.07341905\n",
      "  0.94477005 0.83921068 0.81945339 0.98794398 1.05046599 0.87145636\n",
      "  1.02256178 1.07004628 1.19410178 0.95289962 0.96716487 1.0740041\n",
      "  0.91527675 1.16477843 1.05012798 0.99941621 1.04313796 0.8526993\n",
      "  0.93590711 1.0123438  1.11188812 1.12203701 0.94996299]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 128\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8067026.012397951\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11676306 0.81343181 1.00877781 0.99984236 1.1055969  1.04055525\n",
      "  1.1684242  1.03042572 1.14555309 1.06394124 0.98278884 1.06583308\n",
      "  0.98159998 1.07780064 1.04004281 1.04145406 0.85255938 0.9191525\n",
      "  1.05884246 0.83563403 1.03459429 0.94065395 0.99543293 1.16222951\n",
      "  0.87295239 1.17669581 1.06730398 1.06772601 1.05669436 0.98956174\n",
      "  0.82120948 0.8375957  0.96136999 1.08328801 1.00003558 0.85707033\n",
      "  1.06629136 0.94459189 0.90407244 1.01639918 1.08242969 0.9253842\n",
      "  1.1135134  0.88561392 1.02667604 1.09490178 1.15021896 1.09732606\n",
      "  1.00550861 0.93822883 0.85862685 0.9068823  0.97395615 1.17936241\n",
      "  1.19622618 0.97009135 0.9952667  0.96780272 1.04412769 0.85561658\n",
      "  1.06774029 0.87815668 1.04022517 1.08185364 1.11873741 1.18352822\n",
      "  1.06514542 1.04452979 1.09376503 0.97339568 1.03524504 1.02292724\n",
      "  1.06202515 0.95360447 1.13610095 0.9372433  1.03167624 1.01305872\n",
      "  1.12518401 1.03087447 1.14697437 0.94177739 0.9068104  1.07321701\n",
      "  0.92752891 0.94197361 0.99722576 1.02309637 1.02624695 0.95995184\n",
      "  1.07545064 0.86131525 0.93373146 0.92781765 1.03249542 0.99949436\n",
      "  0.94457331 0.99298433 0.82178455 1.09220452 1.09185953 1.05509248\n",
      "  1.14277207 0.86400328 0.88309239 0.90386644 1.0463639  0.9869226\n",
      "  0.93388913 1.01702587 0.99121415 0.96073051 0.94151737 0.92098655\n",
      "  1.0183809  0.90682954 0.97342153 0.83374897 1.14481592 1.00087443\n",
      "  1.07856148 1.0777249  0.99558434 1.08901815 0.94850818 0.98882719\n",
      "  0.97675266 0.95311934 1.09187701 1.0748442  0.99197557 0.98487275\n",
      "  0.99194611 0.86601394 0.97245602 1.08377761 1.07186009 1.07419871\n",
      "  0.94552728 0.8399925  0.82023517 0.98872582 1.05124783 0.87223818\n",
      "  1.02334386 1.07082811 1.1948837  0.95368128 0.9679467  1.07478593\n",
      "  0.91605859 1.1655598  1.05090982 1.00019602 1.04391979 0.85348125\n",
      "  0.93668891 1.01311963 1.11266208 1.12281886 0.9507452 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 512\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8045414.838682952\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11734798 0.8140177  1.00936298 1.00042761 1.10618124 1.04114153\n",
      "  1.16900933 1.03101111 1.14613805 1.06452659 0.98337396 1.06641751\n",
      "  0.98218516 1.07838495 1.0406172  1.04203939 0.85314464 0.91973783\n",
      "  1.05942806 0.83621932 1.03517954 0.94123876 0.99601801 1.1628145\n",
      "  0.87353688 1.17728089 1.06788902 1.0683111  1.05727958 0.99014702\n",
      "  0.82179484 0.8381808  0.96195382 1.08387336 1.00062084 0.8576561\n",
      "  1.06687567 0.94517597 0.90465763 1.01698436 1.08301504 0.9259693\n",
      "  1.11409836 0.88619961 1.02726127 1.09548739 1.15080299 1.09791198\n",
      "  1.00609375 0.93881416 0.85921192 0.90746778 0.97454143 1.17994633\n",
      "  1.19681146 0.97067637 0.99584106 0.96838797 1.04471295 0.85620196\n",
      "  1.06832569 0.87874204 1.04081038 1.08243845 1.11932267 1.18411333\n",
      "  1.06572998 1.04511515 1.09435005 0.97398088 1.03583025 1.02351246\n",
      "  1.06261028 0.95418995 1.13668534 0.93782863 1.03226141 1.01364399\n",
      "  1.12576895 1.03145919 1.14755929 0.9423626  0.90739561 1.07380198\n",
      "  0.92811391 0.94255958 0.99781091 1.02368173 1.0268319  0.96053858\n",
      "  1.07603571 0.86190065 0.93431631 0.92840302 1.03308061 1.00007912\n",
      "  0.94515836 0.99356886 0.82235961 1.09278972 1.09244468 1.05567778\n",
      "  1.14335748 0.86458858 0.88367766 0.90445113 1.04694891 0.987508\n",
      "  0.93447374 1.01761102 0.9917993  0.96131562 0.94210261 0.92157176\n",
      "  1.01896624 0.9074149  0.97400592 0.8343343  1.14540122 1.0014603\n",
      "  1.07914611 1.07830881 0.9961692  1.08960336 0.94909346 0.98941227\n",
      "  0.97733788 0.95370455 1.09246221 1.07542939 0.99256056 0.98545798\n",
      "  0.99253132 0.86659915 0.97304126 1.08436217 1.07244525 1.07478327\n",
      "  0.94610429 0.84057771 0.82082034 0.98931103 1.05183305 0.8728234\n",
      "  1.02392935 1.07141331 1.195469   0.95426637 0.9685319  1.07537114\n",
      "  0.91664381 1.16614466 1.05149503 1.00078149 1.04450501 0.85406661\n",
      "  0.9372741  1.01370381 1.11324535 1.12340408 0.95133073]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 1024\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8042011.122739795\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11744628 0.81411613 1.0094613  1.00052595 1.10627952 1.04124009\n",
      "  1.16910765 1.03110948 1.14623636 1.06462494 0.98347229 1.06651574\n",
      "  0.98228349 1.07848325 1.04071445 1.04213775 0.85324298 0.91983618\n",
      "  1.05952644 0.83631767 1.03527788 0.94133704 0.99611633 1.16291279\n",
      "  0.87363512 1.17737921 1.06798736 1.06840943 1.05737792 0.99024536\n",
      "  0.82189319 0.83827912 0.96205217 1.08397171 1.00071918 0.85775455\n",
      "  1.06697418 0.94527448 0.90475595 1.01708269 1.0831134  0.92606762\n",
      "  1.11419667 0.88629802 1.02735961 1.09558577 1.15090122 1.09801045\n",
      "  1.00619207 0.93891252 0.85931023 0.90756615 0.97463977 1.1800445\n",
      "  1.19690981 0.97077477 0.99593829 0.96848631 1.04481129 0.85630032\n",
      "  1.06842404 0.8788404  1.04090872 1.08253673 1.11942102 1.18421165\n",
      "  1.06582823 1.0452135  1.09444837 0.97407922 1.03592859 1.02361079\n",
      "  1.0627086  0.95428832 1.13678372 0.93792698 1.03235974 1.01374236\n",
      "  1.12586751 1.03155757 1.14765756 0.94246094 0.90749395 1.07390028\n",
      "  0.92821223 0.94265802 0.99790923 1.02378007 1.02693025 0.96063718\n",
      "  1.07613402 0.86199902 0.93441461 0.92850137 1.03317894 1.00017741\n",
      "  0.94525668 0.99366719 0.82245706 1.09288806 1.09254301 1.05577613\n",
      "  1.14345583 0.86468693 0.88377601 0.9045494  1.04704723 0.98760635\n",
      "  0.93457199 1.01770935 0.99189764 0.96141395 0.94220095 0.9216701\n",
      "  1.01906459 0.90751326 0.97410434 0.83443265 1.14549958 1.00155877\n",
      "  1.07924467 1.07840728 0.99626746 1.0897017  0.9491918  0.98951059\n",
      "  0.97743622 0.95380289 1.09256055 1.07552774 0.9926589  0.98555632\n",
      "  0.99262966 0.86669749 0.97313961 1.0844604  1.07254359 1.0748816\n",
      "  0.94620192 0.84067605 0.82091867 0.98940937 1.05193139 0.87292174\n",
      "  1.02402773 1.07151165 1.19556735 0.95436469 0.96863024 1.07546948\n",
      "  0.91674215 1.16624295 1.05159337 1.00088003 1.04460335 0.85416496\n",
      "  0.93737244 1.01380229 1.11334371 1.12350242 0.95142912]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n",
      "batch-size: 2048\n",
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 8040435.076354949\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11749423 0.81416412 1.00950926 1.00057391 1.10632747 1.04128812\n",
      "  1.1691556  1.03115745 1.14628432 1.0646729  0.98352024 1.06656366\n",
      "  0.98233145 1.07853119 1.04076218 1.04218571 0.85329094 0.91988414\n",
      "  1.05957442 0.83636562 1.03532584 0.94138498 0.99616428 1.16296074\n",
      "  0.87368305 1.17742716 1.06803532 1.06845738 1.05742587 0.99029332\n",
      "  0.82194115 0.83832707 0.96210014 1.08401968 1.00076714 0.85780255\n",
      "  1.06702215 0.94532253 0.9048039  1.01713064 1.08316136 0.92611557\n",
      "  1.11424462 0.88634599 1.02740756 1.09563374 1.15094915 1.09805846\n",
      "  1.00624002 0.93896048 0.85935818 0.90761412 0.97468773 1.18009239\n",
      "  1.19695777 0.97082275 0.99598599 0.96853427 1.04485925 0.85634828\n",
      "  1.06847199 0.87888836 1.04095668 1.08258466 1.11946898 1.18425961\n",
      "  1.06587615 1.04526146 1.09449633 0.97412718 1.03597655 1.02365875\n",
      "  1.06275656 0.95433629 1.13683169 0.93797494 1.0324077  1.01379033\n",
      "  1.12591549 1.03160555 1.1477055  0.9425089  0.9075419  1.07394822\n",
      "  0.92826018 0.94270601 0.99795718 1.02382803 1.02697822 0.96068523\n",
      "  1.07618196 0.86204699 0.93446256 0.92854933 1.0332269  1.00022534\n",
      "  0.94530463 0.99371515 0.82250486 1.09293602 1.09259096 1.05582409\n",
      "  1.14350379 0.86473489 0.88382397 0.90459733 1.04709518 0.98765432\n",
      "  0.93461992 1.0177573  0.99194561 0.9614619  0.9422489  0.92171806\n",
      "  1.01911255 0.90756122 0.97415232 0.83448062 1.14554754 1.00160677\n",
      "  1.07929265 1.07845531 0.99631539 1.08974966 0.94923976 0.98955854\n",
      "  0.97748417 0.95385085 1.0926085  1.07557571 0.99270686 0.98560428\n",
      "  0.99267762 0.86674545 0.97318757 1.08450832 1.07259154 1.07492956\n",
      "  0.94624978 0.84072401 0.82096663 0.98945733 1.05197935 0.8729697\n",
      "  1.0240757  1.0715596  1.19561531 0.95441264 0.9686782  1.07551744\n",
      "  0.9167901  1.16629089 1.05164133 1.00092805 1.04465131 0.85421293\n",
      "  0.9374204  1.01385026 1.11339171 1.12355037 0.9514771 ]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 161 pen 4\n",
    "\n",
    "for b in [32,64,128,512,1024,2048]:\n",
    "    print(\"batch-size:\",b)\n",
    "    train(0.1/len(train_L_S),1,batch_size = b, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 161), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 161), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 161), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 161), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(161,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 161), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(161,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 8007612.084087122\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04\n",
      " -7.16894246e-04 -5.72435782e-04 -1.99242063e-05  2.38784328e-04\n",
      "  2.70286318e-04 -3.92643368e-04  7.62323598e-04 -1.37902608e-03\n",
      " -6.54870489e-04 -7.14002677e-04  3.32772963e-04  2.75731693e-06\n",
      " -5.46449760e-04 -6.23445301e-05 -1.77444812e-03  9.29864162e-04\n",
      "  9.26413588e-04  5.58744880e-04  1.43554186e-03 -1.35214713e-03\n",
      " -1.16125629e-03 -9.53522822e-04  4.71455803e-04 -1.22952721e-04\n",
      " -6.53296910e-04  1.78077008e-04 -8.00399106e-05 -3.84877001e-04\n",
      " -5.77006965e-04 -7.82315407e-04  1.91629575e-04 -9.23883744e-04\n",
      " -2.57972702e-04 -1.65468978e-03  1.45597947e-03  1.65719480e-05\n",
      "  7.93431005e-04  7.85057410e-04 -3.63421495e-05  8.98000598e-04\n",
      " -5.07098334e-04 -1.03910585e-04 -2.24654272e-04 -4.60987500e-04\n",
      "  9.26589047e-04  7.56261165e-04 -7.24274370e-05 -1.43453192e-04\n",
      " -7.27198076e-05 -1.33204153e-03 -2.67620325e-04  8.45586987e-04\n",
      "  7.26419431e-04  7.49799607e-04 -5.36998767e-04 -1.59225593e-03\n",
      " -1.78982971e-03 -1.04922726e-04  5.20297462e-04 -1.26979902e-03\n",
      "  2.41261095e-04  7.16100011e-04  1.95665716e-03 -4.55369608e-04\n",
      " -3.12714021e-04  7.55678351e-04 -8.31594962e-04  1.66341280e-03\n",
      "  5.16917280e-04  9.78477331e-06  4.47017062e-04 -1.45736669e-03\n",
      " -6.25292051e-04  1.39006987e-04  1.13442119e-03  1.23600773e-03\n",
      " -4.84724901e-04]\n",
      "[[1.11460664 0.81127227 1.00662067 0.99768491 1.10344211 1.03839401\n",
      "  1.16626717 1.02826777 1.14339651 1.0617835  0.98063181 1.06367831\n",
      "  0.97944282 1.07564592 1.03791907 1.03929629 0.85040195 0.91699482\n",
      "  1.05668387 0.83347653 1.03243685 0.93849798 0.99327603 1.16007292\n",
      "  0.87079746 1.17453889 1.0651471  1.06556903 1.05453701 0.98740422\n",
      "  0.81905174 0.8354388  0.95921634 1.08113021 0.9978781  0.85491096\n",
      "  1.06413594 0.9424368  0.90191526 1.01424198 1.08027191 0.92322727\n",
      "  1.11135685 0.88345502 1.02451868 1.09274318 1.14806523 1.09516616\n",
      "  1.00335156 0.93607114 0.85647003 0.90472414 0.97179863 1.17720931\n",
      "  1.1940686  0.96793429 0.99314328 0.96564527 1.04197019 0.85345873\n",
      "  1.0655824  0.87599892 1.03806784 1.07969765 1.11657987 1.18137119\n",
      "  1.06299023 1.042372   1.09160827 0.97123837 1.0330877  1.02076992\n",
      "  1.05986809 0.95144625 1.13394566 0.93508562 1.02951906 1.01090103\n",
      "  1.12302664 1.02871828 1.14481803 0.93962006 0.90465312 1.07106046\n",
      "  0.92537228 0.9398138  0.99506866 1.02093858 1.02409023 0.9577892\n",
      "  1.07329383 0.85915731 0.9315753  0.92565985 1.03033819 0.99733853\n",
      "  0.94241653 0.99082892 0.81965825 1.09004719 1.08970241 1.05293487\n",
      "  1.14061413 0.86184571 0.88093488 0.90171087 1.04420719 0.98476464\n",
      "  0.93173382 1.01486875 0.98905696 0.95857353 0.93936    0.91882926\n",
      "  1.01622313 0.90467176 0.97126609 0.83159129 1.14265823 0.99871468\n",
      "  1.07640498 1.07557041 0.99342824 1.08686077 0.94635066 0.98667029\n",
      "  0.97459534 0.95096203 1.08971968 1.07268685 0.98981886 0.98271535\n",
      "  0.98978878 0.86385666 0.9702986  1.08162251 1.06970289 1.0720432\n",
      "  0.94339488 0.83783523 0.81807801 0.98656849 1.04909048 0.87008086\n",
      "  1.0211856  1.06867081 1.19272601 0.95152439 0.9657894  1.07262861\n",
      "  0.91390126 1.1634036  1.04875247 0.99803715 1.04176245 0.85132349\n",
      "  0.93453166 1.01096468 1.11050982 1.12066149 0.94858676]]\n",
      "{0: 152, 1: 30, 2: 8, 3: 57, 4: 208, 5: 657, 6: 16, 7: 5, 8: 184, 9: 42, 10: 21, 11: 115, 12: 6, 13: 294, 14: 1265, 15: 11, 16: 8, 18: 222, 20: 1, 21: 4, 22: 79, 23: 224, 24: 21, 25: 94, 26: 190, 27: 36, 30: 77, 31: 47, 32: 615, 33: 23, 34: 13, 35: 222, 36: 547, 37: 929, 38: 155, 39: 7, 40: 43, 41: 39}\n",
      "acc 0.5555722213889306\n",
      "macro (0.33073275717460604, 0.35962191280614225, 0.29242367786363743, None)\n",
      "support (0.6334119569135529, 0.5555722213889306, 0.5652202951283088, None)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.5555722213889306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BMTC - Driver or Conductor</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.23</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cattle</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseases</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Government Land Encroachment</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lakes - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manholes</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Bus Shelters</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Sewage Drains</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.40</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overflow of Storm Water Drains</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parking Violations</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potholes</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.53</td>\n",
       "      <td>961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repair of streetlights</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.02</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sewage and Storm Water Drains - Others</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stray Dogs</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.98</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Pollution</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autorickshaws and Taxis</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.67</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Need new Bus Route</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Others</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.58</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bad Roads</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Broken Storm Water Drains</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clearing of Blockage of Under Ground Drainage Pipelines and Replacement of Damaged or Missing Manhole Cover</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.18</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desilting - Lakes</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.60</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electricity</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.55</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flooding of Roads and Footpaths</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Footpaths</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.63</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garbage</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawkers and Vendors</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoardings</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illegal posters and Hoardings</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.93</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maintenance of Roads and Footpaths - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mosquitos</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.93</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Streetlights</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.58</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Toilets</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noise Pollution</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.68</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parks and playgrounds</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Nuisance</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traffic</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.69</td>\n",
       "      <td>681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trees, Parks and Playgrounds - Others</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.73</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unauthorized Construction</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.10</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Leakage</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Supply</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.26</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>6667.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    f1-score  precision  \\\n",
       " BMTC - Driver or Conductor                             0.35       0.75   \n",
       " Cattle                                                 0.43       0.60   \n",
       " Diseases                                               0.14       0.24   \n",
       " Government Land Encroachment                           0.00       0.00   \n",
       " Lakes - Others                                         0.00       0.00   \n",
       " Manholes                                               0.12       0.25   \n",
       " New Bus Shelters                                       0.28       0.32   \n",
       " No Sewage Drains                                       0.28       0.21   \n",
       " Others                                                 0.00       0.00   \n",
       " Overflow of Storm Water Drains                         0.00       0.00   \n",
       " Parking Violations                                     0.40       0.42   \n",
       " Potholes                                               0.64       0.82   \n",
       " Repair of streetlights                                 0.04       0.46   \n",
       " Sewage and Storm Water Drains - Others                 0.03       0.02   \n",
       " Stray Dogs                                             0.95       0.92   \n",
       "Air Pollution                                           0.14       0.09   \n",
       "Autorickshaws and Taxis                                 0.50       0.40   \n",
       "BMTC - Need new Bus Route                               0.24       0.23   \n",
       "BMTC - Others                                           0.44       0.35   \n",
       "Bad Roads                                               0.47       0.59   \n",
       "Broken Storm Water Drains                               0.06       0.06   \n",
       "Clearing of Blockage of Under Ground Drainage P...      0.10       0.07   \n",
       "Desilting - Lakes                                       0.23       0.14   \n",
       "Electricity                                             0.67       0.87   \n",
       "Flooding of Roads and Footpaths                         0.03       0.17   \n",
       "Footpaths                                               0.53       0.46   \n",
       "Garbage                                                 0.89       0.91   \n",
       "Hawkers and Vendors                                     0.00       0.00   \n",
       "Hoardings                                               0.00       0.00   \n",
       "Illegal posters and Hoardings                           0.11       0.06   \n",
       "Maintenance of Roads and Footpaths - Others             0.00       0.00   \n",
       "Mosquitos                                               0.47       0.32   \n",
       "Need New Streetlights                                   0.21       0.12   \n",
       "Need New Toilets                                        0.57       0.57   \n",
       "Noise Pollution                                         0.63       0.58   \n",
       "Parks and playgrounds                                   0.44       0.38   \n",
       "Public Nuisance                                         0.10       0.13   \n",
       "Traffic                                                 0.59       0.51   \n",
       "Trees, Parks and Playgrounds - Others                   0.10       0.05   \n",
       "Unauthorized Construction                               0.14       0.29   \n",
       "Water Leakage                                           0.55       0.60   \n",
       "Water Supply                                            0.41       0.92   \n",
       "avg / total                                             0.57       0.63   \n",
       "\n",
       "                                                    recall  support  \n",
       " BMTC - Driver or Conductor                           0.23     26.0  \n",
       " Cattle                                               0.33      9.0  \n",
       " Diseases                                             0.10     49.0  \n",
       " Government Land Encroachment                         0.00      8.0  \n",
       " Lakes - Others                                       0.00     11.0  \n",
       " Manholes                                             0.08     13.0  \n",
       " New Bus Shelters                                     0.25    121.0  \n",
       " No Sewage Drains                                     0.40    100.0  \n",
       " Others                                               0.00    131.0  \n",
       " Overflow of Storm Water Drains                       0.00     41.0  \n",
       " Parking Violations                                   0.39     82.0  \n",
       " Potholes                                             0.53    961.0  \n",
       " Repair of streetlights                               0.02    259.0  \n",
       " Sewage and Storm Water Drains - Others               0.12     33.0  \n",
       " Stray Dogs                                           0.98    511.0  \n",
       "Air Pollution                                         0.31     45.0  \n",
       "Autorickshaws and Taxis                               0.67     18.0  \n",
       "BMTC - Need new Bus Route                             0.25     53.0  \n",
       "BMTC - Others                                         0.58    125.0  \n",
       "Bad Roads                                             0.39   1008.0  \n",
       "Broken Storm Water Drains                             0.05     20.0  \n",
       "Clearing of Blockage of Under Ground Drainage P...    0.18     71.0  \n",
       "Desilting - Lakes                                     0.60     10.0  \n",
       "Electricity                                           0.55    182.0  \n",
       "Flooding of Roads and Footpaths                       0.02     55.0  \n",
       "Footpaths                                             0.63    217.0  \n",
       "Garbage                                               0.87   1319.0  \n",
       "Hawkers and Vendors                                   0.00     23.0  \n",
       "Hoardings                                             0.00     27.0  \n",
       "Illegal posters and Hoardings                         0.93     14.0  \n",
       "Maintenance of Roads and Footpaths - Others           0.00     27.0  \n",
       "Mosquitos                                             0.93     27.0  \n",
       "Need New Streetlights                                 0.58     48.0  \n",
       "Need New Toilets                                      0.57     21.0  \n",
       "Noise Pollution                                       0.68     31.0  \n",
       "Parks and playgrounds                                 0.53     34.0  \n",
       "Public Nuisance                                       0.08     36.0  \n",
       "Traffic                                               0.69    681.0  \n",
       "Trees, Parks and Playgrounds - Others                 0.73     11.0  \n",
       "Unauthorized Construction                             0.10     21.0  \n",
       "Water Leakage                                         0.50     52.0  \n",
       "Water Supply                                          0.26    136.0  \n",
       "avg / total                                           0.56   6667.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 161 pen 4\n",
    "\n",
    "\n",
    "pl=train(0.00001,1,batch_size = 128, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=5)\n",
    "res = predictAndPrint(pl)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 45, 1: 18, 2: 26, 3: 53, 4: 125, 5: 1008, 6: 20, 7: 9, 8: 71, 9: 10, 10: 49, 11: 182, 12: 55, 13: 217, 14: 1319, 15: 8, 16: 23, 17: 27, 18: 14, 19: 11, 20: 27, 21: 13, 22: 27, 23: 48, 24: 21, 25: 121, 26: 100, 27: 31, 28: 131, 29: 41, 30: 82, 31: 34, 32: 961, 33: 36, 34: 259, 35: 33, 36: 511, 37: 681, 38: 11, 39: 21, 40: 52, 41: 136}\n",
      "Tensor(\"unstack:1\", shape=(?, 84), dtype=float64)\n",
      "l Tensor(\"unstack:0\", shape=(?, 84), dtype=float64)\n",
      "s_ Tensor(\"Maximum:0\", shape=(?, 84), dtype=float64)\n",
      "out Tensor(\"pout/while/Select:0\", shape=(?, 84), dtype=float64)\n",
      "pout Tensor(\"pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 84), dtype=float64)\n",
      "t_pout Tensor(\"t_pout/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42, ?, 1), dtype=float64)\n",
      "t Tensor(\"Squeeze:0\", shape=(84,), dtype=float64)\n",
      "out Tensor(\"zy/while/Select:0\", shape=(84,), dtype=float64)\n",
      "zy Tensor(\"zy/TensorArrayStack/TensorArrayGatherV3:0\", shape=(42,), dtype=float64)\n",
      "logz Tensor(\"logz:0\", shape=(), dtype=float64)\n",
      "lsp Tensor(\"ReduceLogSumExp/add:0\", shape=(?, 1), dtype=float64)\n",
      "precision log(softplus) penalty\n",
      "sft Lj Tensor(\"map/while/map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?,), dtype=float64)\n",
      "sft kj Tensor(\"map/while/Gather:0\", shape=(), dtype=float64)\n",
      "sft aj Tensor(\"map/while/Gather_1:0\", shape=(), dtype=float64)\n",
      "sft indices Tensor(\"map/while/Where:0\", shape=(?, 1), dtype=int64)\n",
      "sft l_ij_eq_kj Tensor(\"map/while/Gather_2:0\", shape=(?, 84), dtype=float64)\n",
      "out Tensor(\"map/while/prec_zy/while/Select:0\", shape=(84,), dtype=float64)\n",
      "prec_z Tensor(\"map/while/Sum:0\", shape=(), dtype=float64)\n",
      "prec_t_pout Tensor(\"map/while/truediv:0\", shape=(?, 1), dtype=float64)\n",
      "f Tensor(\"map/while/Sum_1:0\", shape=(), dtype=float64)\n",
      "sft Tensor(\"map/while/sft:0\", shape=(), dtype=float64)\n",
      "loss Tensor(\"add:0\", shape=(), dtype=float64)\n",
      "marginals Tensor(\"transpose_1:0\", shape=(42, ?, 1), dtype=float64)\n",
      "0 loss 4229288.894407854\n",
      "dev set\n",
      "[ 1.17544629e-03 -1.85785468e-03  9.55965364e-05  6.24306244e-06\n",
      "  1.06377870e-03  4.13385488e-04  1.69205997e-03  3.12078490e-04\n",
      "  1.46334713e-03  6.47232968e-04 -1.64293569e-04  6.66140471e-04\n",
      " -1.76181555e-04  7.85815884e-04  4.08124727e-04  4.22361165e-04\n",
      " -1.46658659e-03 -8.00654606e-04  5.96248234e-04 -1.63583977e-03\n",
      "  3.53762393e-04 -5.85646273e-04 -3.78531600e-05  1.63011132e-03\n",
      " -1.26266566e-03  1.77477556e-03  6.80857197e-04  6.85077792e-04\n",
      "  5.74762734e-04 -9.65627854e-05 -1.78008441e-03 -1.61622533e-03\n",
      " -3.78494646e-04  8.40700815e-04  8.17543875e-06 -1.42147027e-03\n",
      "  6.70725883e-04 -5.46270315e-04 -9.51457003e-04  1.71810433e-04\n",
      "  8.32117608e-04 -7.38340312e-04  1.14295019e-03 -1.13603598e-03\n",
      "  2.74579585e-04  9.56841477e-04  1.50999543e-03  9.81088784e-04\n",
      "  6.29042106e-05 -6.09891204e-04 -1.40591418e-03 -9.23354820e-04\n",
      " -2.52618734e-04  1.80142764e-03  1.97008169e-03 -2.91268381e-04\n",
      " -3.96372740e-05 -3.14153275e-04  4.49096537e-04 -1.43601317e-03\n",
      "  6.85223922e-04 -1.21061239e-03  4.10070756e-04  8.26350550e-04\n",
      "  1.19519383e-03  1.84310007e-03  6.59265398e-04  4.53118651e-04\n",
      "  9.45467243e-04 -2.58224217e-04  3.60269454e-04  2.37091515e-04\n",
      "  6.28069553e-04 -4.56132989e-04  1.36882094e-03 -6.19746521e-04\n",
      "  3.24580976e-04  1.38407452e-04  1.25965941e-03  3.16559612e-04\n",
      "  1.47755887e-03 -5.74407005e-04 -9.24076949e-04  7.39986202e-04]\n",
      "[[1.11460653 0.81127386 1.00662034 0.99768395 1.10344072 1.03839575\n",
      "  1.16626692 1.02826781 1.14339715 1.06178325 0.98063135 1.06367789\n",
      "  0.97944258 1.07564113 1.0378861  1.03929687 0.85040234 0.91699489\n",
      "  1.05668788 0.83347632 1.03243702 0.93849787 0.99327494 1.16007172\n",
      "  0.87079569 1.17453781 1.06514574 1.06556833 1.054537   0.98740448\n",
      "  0.81905371 0.83543661 0.95921095 1.08113057 0.99787858 0.85491192\n",
      "  1.06412996 0.9424338  0.90191627 1.01424179 1.08027293 0.92322752\n",
      "  1.11135723 0.88345524 1.02451864 1.09274394 1.14806066 1.09516534\n",
      "  1.00335165 0.9360706  0.85647168 0.90472465 0.97179886 1.17720515\n",
      "  1.19406947 0.9679328  0.99310752 0.96564552 1.04197058 0.85345922\n",
      "  1.06558542 0.87599918 1.03806774 1.0796976  1.11658052 1.18137024\n",
      "  1.06298847 1.04237251 1.09160589 0.97123806 1.03308722 1.02076991\n",
      "  1.0598684  0.95144555 1.13394354 0.93508597 1.02951905 1.01089879\n",
      "  1.12302323 1.02871523 1.14481721 0.93962008 0.90465347 1.07105976]]\n",
      "{0: 611, 1: 32, 2: 11, 3: 59, 4: 237, 5: 299, 6: 5, 7: 9, 8: 164, 9: 38, 10: 1, 11: 142, 12: 49, 13: 214, 14: 1175, 15: 1, 16: 20, 18: 442, 21: 26, 22: 63, 23: 185, 24: 32, 25: 25, 26: 270, 27: 28, 28: 9, 30: 109, 31: 57, 32: 699, 33: 9, 34: 1, 35: 219, 36: 528, 37: 493, 38: 338, 40: 26, 41: 41}\n",
      "acc 0.5002249887505624\n",
      "macro (0.37118047735176285, 0.36990302429894484, 0.2961447233669115, None)\n",
      "support (0.6510378262674855, 0.5002249887505624, 0.5337063114553743, None)\n",
      "\n",
      "acc 0.5002249887505624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BMTC - Driver or Conductor</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.31</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cattle</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diseases</th>\n",
       "      <td>0.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Government Land Encroachment</th>\n",
       "      <td>0.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lakes - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manholes</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.46</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Bus Shelters</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.20</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Sewage Drains</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overflow of Storm Water Drains</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parking Violations</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.56</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potholes</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.56</td>\n",
       "      <td>961.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repair of streetlights</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sewage and Storm Water Drains - Others</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stray Dogs</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.97</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Pollution</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.44</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Autorickshaws and Taxis</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.78</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Need new Bus Route</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMTC - Others</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.64</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bad Roads</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1008.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Broken Storm Water Drains</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clearing of Blockage of Under Ground Drainage Pipelines and Replacement of Damaged or Missing Manhole Cover</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Desilting - Lakes</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.70</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electricity</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.55</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flooding of Roads and Footpaths</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Footpaths</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garbage</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hawkers and Vendors</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoardings</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illegal posters and Hoardings</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maintenance of Roads and Footpaths - Others</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mosquitos</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.85</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Streetlights</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.56</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Need New Toilets</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.67</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noise Pollution</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parks and playgrounds</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.62</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Public Nuisance</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Traffic</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.52</td>\n",
       "      <td>681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trees, Parks and Playgrounds - Others</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.73</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unauthorized Construction</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Leakage</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.31</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water Supply</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.26</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6667.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    f1-score  precision  \\\n",
       " BMTC - Driver or Conductor                             0.43       0.73   \n",
       " Cattle                                                 0.56       0.56   \n",
       " Diseases                                               0.04       1.00   \n",
       " Government Land Encroachment                           0.22       1.00   \n",
       " Lakes - Others                                         0.00       0.00   \n",
       " Manholes                                               0.31       0.23   \n",
       " New Bus Shelters                                       0.33       0.96   \n",
       " No Sewage Drains                                       0.21       0.14   \n",
       " Others                                                 0.00       0.00   \n",
       " Overflow of Storm Water Drains                         0.00       0.00   \n",
       " Parking Violations                                     0.48       0.42   \n",
       " Potholes                                               0.65       0.77   \n",
       " Repair of streetlights                                 0.00       0.00   \n",
       " Sewage and Storm Water Drains - Others                 0.05       0.03   \n",
       " Stray Dogs                                             0.96       0.94   \n",
       "Air Pollution                                           0.06       0.03   \n",
       "Autorickshaws and Taxis                                 0.56       0.44   \n",
       "BMTC - Need new Bus Route                               0.23       0.22   \n",
       "BMTC - Others                                           0.44       0.34   \n",
       "Bad Roads                                               0.28       0.62   \n",
       "Broken Storm Water Drains                               0.00       0.00   \n",
       "Clearing of Blockage of Under Ground Drainage P...      0.03       0.02   \n",
       "Desilting - Lakes                                       0.29       0.18   \n",
       "Electricity                                             0.62       0.71   \n",
       "Flooding of Roads and Footpaths                         0.04       0.04   \n",
       "Footpaths                                               0.55       0.55   \n",
       "Garbage                                                 0.87       0.92   \n",
       "Hawkers and Vendors                                     0.33       0.35   \n",
       "Hoardings                                               0.00       0.00   \n",
       "Illegal posters and Hoardings                           0.05       0.02   \n",
       "Maintenance of Roads and Footpaths - Others             0.00       0.00   \n",
       "Mosquitos                                               0.51       0.37   \n",
       "Need New Streetlights                                   0.23       0.15   \n",
       "Need New Toilets                                        0.53       0.44   \n",
       "Noise Pollution                                         0.58       0.61   \n",
       "Parks and playgrounds                                   0.46       0.37   \n",
       "Public Nuisance                                         0.09       0.22   \n",
       "Traffic                                                 0.60       0.72   \n",
       "Trees, Parks and Playgrounds - Others                   0.05       0.02   \n",
       "Unauthorized Construction                               0.00       0.00   \n",
       "Water Leakage                                           0.41       0.62   \n",
       "Water Supply                                            0.40       0.85   \n",
       "avg / total                                             0.53       0.65   \n",
       "\n",
       "                                                    recall  support  \n",
       " BMTC - Driver or Conductor                           0.31     26.0  \n",
       " Cattle                                               0.56      9.0  \n",
       " Diseases                                             0.02     49.0  \n",
       " Government Land Encroachment                         0.12      8.0  \n",
       " Lakes - Others                                       0.00     11.0  \n",
       " Manholes                                             0.46     13.0  \n",
       " New Bus Shelters                                     0.20    121.0  \n",
       " No Sewage Drains                                     0.38    100.0  \n",
       " Others                                               0.00    131.0  \n",
       " Overflow of Storm Water Drains                       0.00     41.0  \n",
       " Parking Violations                                   0.56     82.0  \n",
       " Potholes                                             0.56    961.0  \n",
       " Repair of streetlights                               0.00    259.0  \n",
       " Sewage and Storm Water Drains - Others               0.18     33.0  \n",
       " Stray Dogs                                           0.97    511.0  \n",
       "Air Pollution                                         0.44     45.0  \n",
       "Autorickshaws and Taxis                               0.78     18.0  \n",
       "BMTC - Need new Bus Route                             0.25     53.0  \n",
       "BMTC - Others                                         0.64    125.0  \n",
       "Bad Roads                                             0.18   1008.0  \n",
       "Broken Storm Water Drains                             0.00     20.0  \n",
       "Clearing of Blockage of Under Ground Drainage P...    0.06     71.0  \n",
       "Desilting - Lakes                                     0.70     10.0  \n",
       "Electricity                                           0.55    182.0  \n",
       "Flooding of Roads and Footpaths                       0.04     55.0  \n",
       "Footpaths                                             0.54    217.0  \n",
       "Garbage                                               0.82   1319.0  \n",
       "Hawkers and Vendors                                   0.30     23.0  \n",
       "Hoardings                                             0.00     27.0  \n",
       "Illegal posters and Hoardings                         0.79     14.0  \n",
       "Maintenance of Roads and Footpaths - Others           0.00     27.0  \n",
       "Mosquitos                                             0.85     27.0  \n",
       "Need New Streetlights                                 0.56     48.0  \n",
       "Need New Toilets                                      0.67     21.0  \n",
       "Noise Pollution                                       0.55     31.0  \n",
       "Parks and playgrounds                                 0.62     34.0  \n",
       "Public Nuisance                                       0.06     36.0  \n",
       "Traffic                                               0.52    681.0  \n",
       "Trees, Parks and Playgrounds - Others                 0.73     11.0  \n",
       "Unauthorized Construction                             0.00     21.0  \n",
       "Water Leakage                                         0.31     52.0  \n",
       "Water Supply                                          0.26    136.0  \n",
       "avg / total                                           0.50   6667.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 84 pen 4\n",
    "\n",
    "\n",
    "pl=train(0.00001,1,batch_size = 128, th = tf.truncated_normal_initializer(1,0.1,seed),\\\n",
    "                                af = tf.truncated_normal_initializer(0,0.001,seed),\\\n",
    "                                LF_acc = get_LF_acc(dev_L_S,gold_labels_dev) ,pcl=np.arange(NoOfClasses,dtype=np.float64),\\\n",
    "                                norm=True,smooth=False,penalty=5)\n",
    "res = predictAndPrint(pl)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
