{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "# %time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "session = SnorkelSession()\n",
    "dev_cands = session.query(Spouse).filter(Spouse.split == 1).all()\n",
    "test_cands = session.query(Spouse).filter(Spouse.split == 2).all()\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "# L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "gold_labels_dev = [L[0,0] if L[0,0]==1 else -1 for L in L_gold_dev]\n",
    "# gold_labels_dev = [L[0,0] for L in L_gold_dev]\n",
    "# gold_labels_test = [L[0,0] for L in L_gold_test]\n",
    "\n",
    "from snorkel.learning.utils import MentionScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 2625\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "# gold_labels_dev = []\n",
    "# for i,L in enumerate(L_gold_dev):\n",
    "#     gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "# gold_labels_test = []\n",
    "# for i,L in enumerate(L_gold_test):\n",
    "#     gold_labels_test.append(L[0,0])\n",
    "    \n",
    "\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(-1))\n",
    "print(len(gold_labels_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../../../snorkel/tutorials/glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "softmax_Threshold = 0.3\n",
    "LF_Threshold = 0.3\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    global LF_Threshold\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_other_relationship_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "    return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    s = s.decode(\"utf-8\") \n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def LF_Three_Lists_Left_Window(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife_left_window(c)\n",
    "    c2,s2 = LF_family_left_window(c)\n",
    "    c3,s3 = LF_other_relationship_left_window(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "\n",
    "def LF_Three_Lists_Between_Words(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife(c)\n",
    "    c2,s2 = LF_familial_relationship(c)\n",
    "    c3,s3 = LF_other_relationship(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    \n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'BW:',s1,s2,s3,prob_sc\n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [LF_distant_supervision, LF_distant_supervision_last_names,LF_same_last_name,\n",
    "       LF_and_married, LF_Three_Lists_Between_Words,LF_Three_Lists_Left_Window, LF_no_spouse_in_sentence\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def PHI(K,LAMDAi,SCOREi):\n",
    "    return [K*l*s for (l,s) in zip(LAMDAi,SCOREi)]\n",
    "\n",
    "def softmax(THETA,LAMDAi,SCOREi):\n",
    "    x = []\n",
    "    for k in [1,-1]:\n",
    "        product = np.dot(PHI(k,LAMDAi,SCOREi),THETA)\n",
    "        x.append(product)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def function_conf(THETA,LAMDA,P_cap,Confidence):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for LAMDAi in LAMDA:\n",
    "        s = s + Confidence[i]*np.dot(np.log(softmax(THETA,LAMDAi)),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def function(THETA,LAMDA,SCORE,P_cap):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for i in range(len(LAMDA)):\n",
    "        s = s + np.dot(np.log(softmax(THETA,LAMDA[i],SCORE[i])),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def P_K_Given_LAMDAi_THETA(K,THETA,LAMDAi,SCOREi):\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    if(K==1):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x[1]\n",
    "      \n",
    "\n",
    "np.random.seed(78)\n",
    "THETA = np.random.rand(len(LFs),1)\n",
    "\n",
    "def PHIj(j,K,LAMDAi,SCOREi):\n",
    "    return LAMDAi[j]*K*SCOREi[j]\n",
    "\n",
    "def RIGHT(j,LAMDAi,SCOREi,THETA):\n",
    "    phi = []\n",
    "    for k in [1,-1]:\n",
    "        phi.append(PHIj(j,k,LAMDAi,SCOREi))\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    return np.dot(phi,x)\n",
    "    \n",
    "\n",
    "def function_conf_der(THETA,LAMDA,P_cap,Confidence):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for LAMDAi in LAMDA:\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + Confidence[i]*(PHIj(j,K,LAMDAi)-RIGHT(j,LAMDAi,THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "def function_der(THETA,LAMDA,SCORE,P_cap):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for index in range(len(LAMDA)):\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + (PHIj(j,K,LAMDA[index],SCORE[index])-RIGHT(j,LAMDA[index],SCORE[index],THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_LAMDA(cands):\n",
    "    LAMDA = []\n",
    "    SCORE = []\n",
    "    for ci in cands:\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        LAMDA.append(L)\n",
    "        SCORE.append(S) \n",
    "    return LAMDA,SCORE\n",
    "\n",
    "def get_Confidence(LAMDA):\n",
    "    confidence = []\n",
    "    for L in LAMDA:\n",
    "        Total_L = float(len(L))\n",
    "        No_zeros = L.count(0)\n",
    "        No_Non_Zeros = Total_L - No_zeros\n",
    "        confidence.append(No_Non_Zeros/Total_L)\n",
    "    return confidence    \n",
    "    \n",
    "def get_Initial_P_cap(LAMDA):\n",
    "    P_cap = []\n",
    "    for L in LAMDA:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "    #print(np.array(LAMDA))\n",
    "    #print(np.array(P_cap))append(L)\n",
    "    #LAMDA=np.array(LAMDA).astype(int)\n",
    "    #P_cap=np.array(P_cap)\n",
    "    #print(np.array(LAMDA).shape)\n",
    "    #print(np.array(P_cap).shape)\n",
    "    #print(L)\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"No.Os\",L.count(0),\"No.1s\",L.count(1),\"No.-1s\",L.count(-1))\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"P(0):\",L.count(0)/len(L),\" P(1)\",L.count(1)/len(L),\"P(-1)\",L.count(-1)/len(L))\n",
    "\n",
    "        \n",
    "def get_P_cap(LAMDA,SCORE,THETA):\n",
    "    P_cap = []\n",
    "    for i in range(len(LAMDA)):\n",
    "        P_capi = softmax(THETA,LAMDA[i],SCORE[i])\n",
    "        P_cap.append(P_capi)\n",
    "    return P_cap\n",
    "\n",
    "\n",
    "def score(predicted_labels,gold_labels):\n",
    "    tp =0.0\n",
    "    tn =0.0\n",
    "    fp =0.0\n",
    "    fn =0.0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if(predicted_labels[i]==gold_labels[i]):\n",
    "            if(predicted_labels[i]==1):\n",
    "                tp=tp+1\n",
    "            else:\n",
    "                tn=tn+1\n",
    "        else:\n",
    "            if(predicted_labels[i]==1):\n",
    "                fp=fp+1\n",
    "            else:\n",
    "                fn=fn+1\n",
    "    print(\"tp\",tp,\"tn\",tn,\"fp\",fp,\"fn\",fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"F1 score:\",f1score)\n",
    "                \n",
    "           \n",
    "    \n",
    "from scipy.optimize import minimize\n",
    "import cPickle as pickle\n",
    "\n",
    "def get_marginals(P_cap):\n",
    "    marginals = []\n",
    "    for P_capi in P_cap:\n",
    "        marginals.append(P_capi[0])\n",
    "    return marginals\n",
    "\n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    plt.bar(range(0,2796),marginals)\n",
    "    plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "    \n",
    "    \n",
    "def train(No_Iter,Use_Confidence=True,theta_file_name=\"THETA\"):\n",
    "    global THETA\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    LAMDA,SCORE = get_LAMDA(train_cands)\n",
    "    P_cap = get_Initial_P_cap(LAMDA)\n",
    "    Confidence = get_Confidence(LAMDA)\n",
    "    for iteration in range(No_Iter):\n",
    "        if(Use_Confidence==True):\n",
    "            res = minimize(function_conf,THETA,args=(LAMDA,P_cap,Confidence), method='BFGS',jac=function_conf_der,options={'disp': True, 'maxiter':20}) #nelder-mead\n",
    "        else:\n",
    "            res = minimize(function,THETA,args=(LAMDA,SCORE,P_cap), method='BFGS',jac=function_der,options={'disp': True, 'maxiter':20}) #nelder-mead            \n",
    "        THETA = res.x # new THETA\n",
    "        print(THETA)\n",
    "        P_cap = get_P_cap(LAMDA,SCORE,THETA) #new p_cap \n",
    "        print_details(\"train iteration: \"+str(iteration),THETA,dev_LAMDA,dev_SCORE)\n",
    "        #score(predicted_labels,gold_labels)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Train_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Train_P_cap.p\",\"wb\"))\n",
    "    NP_THETA = np.array(THETA)\n",
    "    np.savetxt(theta_file_name+'.txt', NP_THETA, fmt='%f') \n",
    "    pickle.dump( NP_THETA, open( theta_file_name+'.p', \"wb\" )) # save the file as \"outfile_name.npy\" \n",
    "\n",
    "        \n",
    "def test(THETA):\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    P_cap = get_P_cap(dev_LAMDA,dev_SCORE,THETA)\n",
    "    print_details(\"test:\",THETA,dev_LAMDA,dev_SCORE)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Dev_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Dev_P_cap.p\",\"wb\"))\n",
    "                    \n",
    "def load_marginals(s):\n",
    "    marginals = []\n",
    "    if(s==\"train\"):\n",
    "        train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "        marginals = train_P_cap[:,0]\n",
    "    return marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "    return L_S\n",
    "\n",
    "def get_L_S(cands):  # sign gives label abs value gives score\n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        l_s=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            s= (s+1)/2  #to scale scores in [0,1] \n",
    "            l_s.append(l*s)\n",
    "        L_S.append(l_s)\n",
    "    return L_S\n",
    "\n",
    "def get_Initial_P_cap_L_S(L_S):\n",
    "    P_cap = []\n",
    "    for L,S in L_S:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "   \n",
    "    \n",
    "dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "train_L_S = get_L_S_Tensor(train_cands)\n",
    "\n",
    "\n",
    "# train_P_cap= get_Initial_P_cap_L_S(train_L_S) \n",
    "\n",
    "# dev_P_cap = get_Initial_P_cap_L_S(dev_L_S)\n",
    "\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "\n",
    "\n",
    "# pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare batch data\n",
    "# train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "# train_P_cap_batch,dev_P_cap_batch = get_P_cap_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import _pickle as pkl\n",
    "\n",
    "\n",
    "#pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "#pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "#pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "#pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))\n",
    "\n",
    "dev_L_S = pkl.load( open( \"dev_L_S.p\", \"rb\" ) )\n",
    "train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "# test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )\n",
    "\n",
    "# train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "# dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "# test_P_cap = pkl.load( open( \"test_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "def get_L_S_batch():\n",
    "    dev_L_batch = []\n",
    "    dev_S_batch = []\n",
    "    dev_L_S_batch = []\n",
    "    train_L_batch = []\n",
    "    train_S_batch = []\n",
    "    train_L_S_batch = []\n",
    "    for l,s in train_L_S:\n",
    "        train_L_batch.append(l)\n",
    "        train_S_batch.append(s)\n",
    "    train_L_S_batch = [train_L_batch, train_S_batch]\n",
    "    for l,s in dev_L_S:\n",
    "        dev_L_batch.append(l)\n",
    "        dev_S_batch.append(s)\n",
    "    dev_L_S_batch = [dev_L_batch, dev_S_batch]\n",
    "    return train_L_S_batch,dev_L_S_batch\n",
    "\n",
    "\n",
    "def get_P_cap_batch():\n",
    "    kp1_train= []\n",
    "    kn1_train = []\n",
    "    kp1_dev= []\n",
    "    kn1_dev = []\n",
    "    for pci in train_P_cap:\n",
    "        kp1_train.append(pci[0])\n",
    "        kn1_train.append(pci[1])\n",
    "    for pci in dev_P_cap:\n",
    "        kp1_dev.append(pci[0])\n",
    "        kn1_dev.append(pci[1])\n",
    "    return [kp1_train,kn1_train],[kp1_dev,kn1_dev]\n",
    "        \n",
    "def get_mini_batches(X,P_cap,bsize): #X : (train/dev/)_L_S_batch\n",
    "    for i in range(0, len(X[0]) - bsize + 1, bsize):\n",
    "        indices = slice(i, i + bsize)\n",
    "        #print(indices)\n",
    "        yield [X[0][indices],X[1][indices]],P_cap[indices]\n",
    "\n",
    "# train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "\n",
    "#for x in get_mini_batches(train_L_S_batch,200):\n",
    "#    print(len(x),len(x[0]),len(x[0][0]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stochastic + cross entropy logits func + remove min(theta,0) in loss\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def class_wise(y_test,y_score):\n",
    "    print(average_precision_score(y_test, y_score, average=None))\n",
    "    \n",
    "def draw_curve(y_test,y_score):\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AUC={0:0.2f}'.format(\n",
    "              average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stochastic + cross entropy logits func + remove min(theta,0) in loss + precision recall curve\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            break\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            \n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    probs = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p,prob = sess.run([loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "        probs.append(prob)\n",
    "    probs = np.array(probs)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(i,total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) + \\\n",
    "            tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "            + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            te_curr,p,prob = sess.run([loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(\"class wise:\")\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "\n",
    "for i in np.linspace(0,1,5):\n",
    "    train_NN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) + \\\n",
    "            tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "            + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            te_curr,p,prob = sess.run([loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(\"class wise:\")\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "\n",
    "for i in np.linspace(0,1,10):\n",
    "    train_NN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 find thresh + stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) \n",
    "            \n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        tuned_alphas = []\n",
    "        tuned_thetas = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            tuned_alphas,tuned_thetas,te_curr,p,prob = sess.run([alphas,thetas,loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(tuned_alphas)\n",
    "        print(tuned_thetas)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(\"class wise:\")\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "\n",
    "for i in np.linspace(0,1,10):\n",
    "    train_NN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8/12 find thresh + remove regular on alphas + stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight,reg):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    REG_CONST = tf.constant(reg,dtype=tf.float64)\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) \\\n",
    "            + (REG_CONST * tf.reduce_sum(tf.multiply(thetas,thetas))) \\\n",
    "            + (REG_CONST * tf.reduce_sum(tf.multiply(alphas,alphas))) \\\n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels)))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        tuned_alphas = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            tuned_alphas,te_curr,p,prob = sess.run([alphas,loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(tuned_alphas)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        p,r,f,_ = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')\n",
    "        print(\"ret\",p,r,f)\n",
    "        return (p,r)\n",
    "        #draw_pr_curve(precisionArr,recallArr,reg)\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precisionArr = []\n",
    "recallArr = []\n",
    "def draw_pr_curve(reg):\n",
    "    print(\"lambda\",reg)\n",
    "    print(\"precision\",precisionArr)\n",
    "    print(\"recall\",recallArr)\n",
    "    plt.step(recallArr, precisionArr, color='b', alpha=0.2,where='post')\n",
    "    plt.fill_between(recallArr, precisionArr, step='post', alpha=0.2,color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.title('Precision-Recall curve')\n",
    "    \n",
    "for reg in [1.2,1,4]:\n",
    "    for i in np.linspace(0,1,10):\n",
    "        p,r = train_NN(i,reg)\n",
    "        precisionArr.append(p)\n",
    "        recallArr.append(r)\n",
    "    draw_pr_curve(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reg in [0.6,0.8]:\n",
    "    for i in np.linspace(0,1,10):\n",
    "        p,r = train_NN(i,reg)\n",
    "        precisionArr.append(p)\n",
    "        recallArr.append(r)\n",
    "    draw_pr_curve(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = [0.164,0.164,0.224,0.234,0.234,0.233,0.233,0.233,0.233,0.233]\n",
    "recall = [0.607,0.607,0.561,0.551,0.551,0.551,0.551,0.551,0.551,0.551]\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 0.25])\n",
    "plt.xlim([0.55, 0.61])\n",
    "plt.title('Precision-Recall curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8/12 find thresh + remove regular on alphas + stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight,reg):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    REG_CONST = tf.constant(reg,dtype=tf.float64)\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) \\\n",
    "            + (REG_CONST * tf.reduce_sum(tf.multiply(thetas,thetas))) \\\n",
    "            #+tf.reduce_sum(tf.multiply(alphas,alphas)) \\\n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels)))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        tuned_alphas = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            tuned_alphas,te_curr,p,prob = sess.run([alphas,loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(tuned_alphas)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(\"class wise:\")\n",
    "        #print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        p,r,f,_ = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None)\n",
    "        precisionArr.append(p)\n",
    "        recallArr.append(r)\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        #print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        #print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "for reg in [0.5,1,1.5]:\n",
    "    for i in np.linspace(0,1,10):\n",
    "        train_NN(i,reg)\n",
    "        draw_pr_curve(precisionArr,recallArr,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "\n",
    "\n",
    "#print(train_P_cap)\n",
    "\n",
    "# discrete_labels = predict_labels(train_P_cap[:1])\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "#train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_out = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "# additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Batch with cross entropy logits function  + additional layer\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "dev_data_size = len(dev_L_S_batch[0])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "#train_P_cap = np.full([data_size,2],0.5)\n",
    "\n",
    "#print(train_P_cap)\n",
    "#print(train_P_cap.shape)\n",
    "#dev_P_cap = np.full([dev_data_size,2],0.5)\n",
    "\n",
    "\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.01),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "phi_p1 = tf.matmul(additional_layer_out,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(additional_layer_out),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "\n",
    "# phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "# phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "# pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "predict_2 = tf.where(tf.greater(tf.slice(tf.nn.softmax(phi_out),[0,1],[dev_data_size,1]),0.5),\n",
    "                    tf.ones((dev_data_size,1)),tf.negative(tf.ones((dev_data_size,1))))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "\n",
    "for i in range(100):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    a,t,te_curr,_, = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "    print(te_curr)\n",
    "    \n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    #print(train_P_cap[0:5])\n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        te_curr,pl,pl2,_ = sess.run([loss,predict,predict_2,train_step],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl2 = pl2.flatten().tolist()\n",
    "        pl = pl.flatten().tolist()\n",
    "        #print(te_curr)\n",
    "        #predicted_labels = pl2\n",
    "        predicted_labels = [-1 if x==0 else 1 for x in pl]\n",
    "        #for l,l2 in zip(predicted_labels,pl2):\n",
    "        #    print(l,l2)\n",
    "        \n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    " \n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All LF_Threshold =0.3 and softmax_Threshold=0.3 ,to be run\n",
    "train(2,Use_Confidence=False,theta_file_name=\"THETA\")\n",
    "\n",
    "test(THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    #plt.bar(range(0,2796),marginals)\n",
    "    #plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "#import cPickle as pickle\n",
    "#THETA = pickle.load( open( \"THETA.p\", \"rb\" ) )\n",
    "#test(THETA)\n",
    "#LAMDA,SCORE = get_LAMDA(dev_cands)\n",
    "#Confidence = get_Confidence(LAMDA)\n",
    "\n",
    "#P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "#marginals=get_marginals(P_cap)\n",
    "#plt.hist(marginals, bins=20)\n",
    "#plt.show()\n",
    "#plt.bar(range(0,888),train_marginals)\n",
    "#plt.show()\n",
    "\n",
    "print_details(\"dev set\",THETA,dev_LAMDA,dev_SCORE)\n",
    "predicted_labels=predict_labels(marginals)\n",
    "\n",
    "\n",
    "sorted_predicted_labels=[x for (y,x) in sorted(zip(Confidence,predicted_labels))] #sort Labels as per Confidence\n",
    "sorted_predicted_labels=list(reversed(sorted_predicted_labels))\n",
    "\n",
    "\n",
    "for i,j in enumerate(reversed(sorted(zip(Confidence,predicted_labels,gold_labels_dev)))):\n",
    "    if i>20:\n",
    "        break\n",
    "    print i,j\n",
    "#print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "#no_of_labels=186#int(len(predicted_labels)*0.1)  #54 - >0.2  , 108>= 0.15 , 186>= 0.12\n",
    "#print(len(sorted_predicted_labels[0:no_of_labels]))\n",
    "no_of_labels=2796\n",
    "score(predicted_labels[0:no_of_labels],gold_labels_dev[0:no_of_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00017328679513998633\n",
      "2313 501\n",
      "0  d  (0.3790162864931616, 0.4733986475956355, 0.3991855477883548, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.6931471805599453\n",
      "2313 501\n",
      "(0.3790162864931616, 0.4733986475956355, 0.3991855477883548, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss -- Marked\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "                print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "train_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.00017490705521919247\n",
      "[0.16566417 0.18358776 0.22931115 0.00638735 0.26661503 0.2486865\n",
      " 0.19304733]\n",
      "[1.10181039 1.03044105 0.97258876 0.90323371 0.89967656 1.07621387\n",
      " 0.98628538]\n",
      "2199 615\n",
      "0  d  (0.5766701050366574, 0.7089947089947091, 0.5835406301824212, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/ipykernel_launcher.py:59: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759 0.90080872        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "0 nan\n",
      "2814 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/snorkelEnv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "1 nan\n",
      "2814 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "2 nan\n",
      "2814 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "3 nan\n",
      "2814 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "0  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "4000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "8000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "12000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "16000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "\n",
      "nan\n",
      "[       nan        nan 1.01434759        nan        nan        nan\n",
      "        nan]\n",
      "[nan nan nan nan nan nan nan]\n",
      "2814 0\n",
      "20000  d  (0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "4 nan\n",
      "2814 0\n",
      "(0.4664179104477612, 0.5, 0.4826254826254826, None)\n",
      "(0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer= tf.truncated_normal_initializer(0.2,0.1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer= tf.truncated_normal_initializer(1,0.1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(5):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "                print(a)\n",
    "                print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "\n",
    "    \n",
    "train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
