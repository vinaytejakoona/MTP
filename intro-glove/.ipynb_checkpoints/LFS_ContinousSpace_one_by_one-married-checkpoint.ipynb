{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 0\n"
     ]
    }
   ],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "#%time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2697)\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "gold_labels_dev = []\n",
    "for i,L in enumerate(L_gold_dev):\n",
    "    gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "gold_labels_test = []\n",
    "for i,L in enumerate(L_gold_test):\n",
    "    gold_labels_test.append(L[0,0])\n",
    "    \n",
    "print(len(gold_labels_dev),len(gold_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.scripts.glove2word2vec.glove2word2vec('../glove.6B.300d.txt', '../glove_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    if sc<0.8:\n",
    "        return (0,0)\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    if sc<0.7:\n",
    "        return (0,0)\n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_other_relationship_left_window(c):\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "    return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def LF_Three_Lists_Left_Window(c):\n",
    "    c1,s1 = LF_husband_wife_left_window(c)\n",
    "    c2,s2 = LF_family_left_window(c)\n",
    "    c3,s3 = LF_other_relationship_left_window(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "    if s1==s2 or s3==s1 or np.max(sc)<0.5:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "\n",
    "def LF_Three_Lists_Between_Words(c):\n",
    "    c1,s1 = LF_husband_wife(c)\n",
    "    c2,s2 = LF_familial_relationship(c)\n",
    "    c3,s3 = LF_other_relationship(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    \n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'BW:',s1,s2,s3,prob_sc\n",
    "    #if s1==s2 or s3==s1 or np.max(sc)<0.5:\n",
    "    #    return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [LF_distant_supervision, LF_distant_supervision_last_names,LF_same_last_name,\n",
    "       LF_and_married, LF_Three_Lists_Between_Words,LF_Three_Lists_Left_Window\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    return (1,1) if len(spouses.intersection(get_between_tokens(c))) > 0 else (0,1)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    if len(spouses.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    elif len(spouses.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (1,1)\n",
    "    else:\n",
    "        return (0,1)\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,1)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,1) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,1)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    return (1,1) if 'and' in get_between_tokens(c) and 'married' in get_right_tokens(c) else (0,1)\n",
    "    \n",
    "def LF_familial_relationship(c):\n",
    "    return (-1,1) if len(family.intersection(get_between_tokens(c))) > 0 else (0,1)\n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    if len(family.intersection(get_left_tokens(c[0], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    elif len(family.intersection(get_left_tokens(c[1], window=2))) > 0:\n",
    "        return (-1,1)\n",
    "    else:\n",
    "        return (0,1)\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    return (-1,1) if len(other.intersection(get_between_tokens(c))) > 0 else (0,1)\n",
    "\n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,1)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    LF_distant_supervision, LF_distant_supervision_last_names, \n",
    "    LF_husband_wife, LF_husband_wife_left_window, LF_same_last_name,\n",
    "    LF_no_spouse_in_sentence, LF_and_married, LF_familial_relationship, \n",
    "    LF_family_left_window, LF_other_relationship\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def PHI(K,LAMDAi,SCOREi):\n",
    "    return [K*l*s for (l,s) in zip(LAMDAi,SCOREi)]\n",
    "\n",
    "def softmax(THETA,LAMDAi,SCOREi):\n",
    "    x = []\n",
    "    for k in [1,-1]:\n",
    "        product = np.dot(PHI(k,LAMDAi,SCOREi),THETA)\n",
    "        x.append(product)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def function_conf(THETA,LAMDA,P_cap,Confidence):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for LAMDAi in LAMDA:\n",
    "        s = s + Confidence[i]*np.dot(np.log(softmax(THETA,LAMDAi)),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def function(THETA,LAMDA,SCORE,P_cap):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for i in range(len(LAMDA)):\n",
    "        s = s + np.dot(np.log(softmax(THETA,LAMDA[i],SCORE[i])),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def P_K_Given_LAMDAi_THETA(K,THETA,LAMDAi,SCOREi):\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    if(K==1):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x[1]\n",
    "      \n",
    "\n",
    "np.random.seed(78)\n",
    "THETA = np.random.rand(len(LFs),1)\n",
    "\n",
    "def PHIj(j,K,LAMDAi,SCOREi):\n",
    "    return LAMDAi[j]*K*SCOREi[j]\n",
    "\n",
    "def RIGHT(j,LAMDAi,SCOREi,THETA):\n",
    "    phi = []\n",
    "    for k in [1,-1]:\n",
    "        phi.append(PHIj(j,k,LAMDAi,SCOREi))\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    return np.dot(phi,x)\n",
    "    \n",
    "\n",
    "def function_conf_der(THETA,LAMDA,P_cap,Confidence):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for LAMDAi in LAMDA:\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + Confidence[i]*(PHIj(j,K,LAMDAi)-RIGHT(j,LAMDAi,THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "def function_der(THETA,LAMDA,SCORE,P_cap):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for index in range(len(LAMDA)):\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + (PHIj(j,K,LAMDA[index],SCORE[index])-RIGHT(j,LAMDA[index],SCORE[index],THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_LAMDA(cands):\n",
    "    LAMDA = []\n",
    "    SCORE = []\n",
    "    for ci in cands:\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        LAMDA.append(L)\n",
    "        SCORE.append(S) \n",
    "    return LAMDA,SCORE\n",
    "\n",
    "def get_Confidence(LAMDA):\n",
    "    confidence = []\n",
    "    for L in LAMDA:\n",
    "        Total_L = float(len(L))\n",
    "        No_zeros = L.count(0)\n",
    "        No_Non_Zeros = Total_L - No_zeros\n",
    "        confidence.append(No_Non_Zeros/Total_L)\n",
    "    return confidence    \n",
    "    \n",
    "def get_Initial_P_cap(LAMDA):\n",
    "    P_cap = []\n",
    "    for L in LAMDA:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "    #print(np.array(LAMDA))\n",
    "    #print(np.array(P_cap))append(L)\n",
    "    #LAMDA=np.array(LAMDA).astype(int)\n",
    "    #P_cap=np.array(P_cap)\n",
    "    #print(np.array(LAMDA).shape)\n",
    "    #print(np.array(P_cap).shape)\n",
    "    #print(L)\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"No.Os\",L.count(0),\"No.1s\",L.count(1),\"No.-1s\",L.count(-1))\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"P(0):\",L.count(0)/len(L),\" P(1)\",L.count(1)/len(L),\"P(-1)\",L.count(-1)/len(L))\n",
    "\n",
    "        \n",
    "def get_P_cap(LAMDA,SCORE,THETA):\n",
    "    P_cap = []\n",
    "    for i in range(len(LAMDA)):\n",
    "        P_capi = softmax(THETA,LAMDA[i],SCORE[i])\n",
    "        P_cap.append(P_capi)\n",
    "    return P_cap\n",
    "\n",
    "\n",
    "def score(predicted_labels,gold_labels):\n",
    "    tp =0.0\n",
    "    tn =0.0\n",
    "    fp =0.0\n",
    "    fn =0.0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if(predicted_labels[i]==gold_labels[i]):\n",
    "            if(predicted_labels[i]==1):\n",
    "                tp=tp+1\n",
    "            else:\n",
    "                tn=tn+1\n",
    "        else:\n",
    "            if(predicted_labels[i]==1):\n",
    "                fp=fp+1\n",
    "            else:\n",
    "                fn=fn+1\n",
    "    print(\"tp\",tp,\"tn\",tn,\"fp\",fp,\"fn\",fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"F1 score:\",f1score)\n",
    "                \n",
    "           \n",
    "    \n",
    "from scipy.optimize import minimize\n",
    "import cPickle as pickle\n",
    "\n",
    "def get_marginals(P_cap):\n",
    "    marginals = []\n",
    "    for P_capi in P_cap:\n",
    "        marginals.append(P_capi[0])\n",
    "    return marginals\n",
    "\n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    plt.bar(range(0,2796),marginals)\n",
    "    plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "    \n",
    "    \n",
    "def train(No_Iter,Use_Confidence=True,theta_file_name):\n",
    "    global THETA\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    LAMDA,SCORE = get_LAMDA(train_cands)\n",
    "    P_cap = get_Initial_P_cap(LAMDA)\n",
    "    Confidence = get_Confidence(LAMDA)\n",
    "    for iteration in range(No_Iter):\n",
    "        if(Use_Confidence==True):\n",
    "            res = minimize(function_conf,THETA,args=(LAMDA,P_cap,Confidence), method='BFGS',jac=function_conf_der,options={'disp': True, 'maxiter':20}) #nelder-mead\n",
    "        else:\n",
    "            res = minimize(function,THETA,args=(LAMDA,SCORE,P_cap), method='BFGS',jac=function_der,options={'disp': True, 'maxiter':20}) #nelder-mead            \n",
    "        THETA = res.x # new THETA\n",
    "        print(THETA)\n",
    "        P_cap = get_P_cap(LAMDA,SCORE,THETA) #new p_cap \n",
    "        print_details(\"train iteration: \"+str(iteration),THETA,dev_LAMDA,dev_SCORE)\n",
    "        #score(predicted_labels,gold_labels)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Train_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Train_P_cap.p\",\"wb\"))\n",
    "    NP_THETA = np.array(THETA)\n",
    "    np.savetxt(theta_file_name+'.txt', NP_THETA, fmt='%f') \n",
    "    pickle.dump( NP_THETA, open( theta_file_name+'.p', \"wb\" )) # save the file as \"outfile_name.npy\" \n",
    "\n",
    "        \n",
    "def test(THETA):\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    P_cap = get_P_cap(dev_LAMDA,dev_SCORE,THETA)\n",
    "    print_details(\"test:\",THETA,dev_LAMDA,dev_SCORE)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Dev_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Dev_P_cap.p\",\"wb\"))\n",
    "                    \n",
    "def load_marginals(s):\n",
    "    marginals = []\n",
    "    if(s==\"train\"):\n",
    "        train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "        marginals = train_P_cap[:,0]\n",
    "    return marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "dev_LAMDA,dev_SCORE = get_LAMDA(dev_cands)\n",
    "\n",
    "write_to_file(wordvec_unavailable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with thresholds LF_and_married\n",
    "\n",
    "train(3,Use_Confidence=False,\"THETA\")\n",
    "\n",
    "test(THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev set\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXpIaZ9BLUuMMeklq3P8BAEHOYQ2u82AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0b2e92a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.082104228121927234, 0.85204081632653061, 0.14977578475336323, None)\n",
      "0 (1.0, 1, 1)\n",
      "1 (0.8333333333333334, 1, 1)\n",
      "2 (0.8333333333333334, 1, 1)\n",
      "3 (0.8333333333333334, 1, 1)\n",
      "4 (0.8333333333333334, 1, -1)\n",
      "5 (0.8333333333333334, 1, -1)\n",
      "6 (0.8333333333333334, 1, -1)\n",
      "7 (0.8333333333333334, -1, 1)\n",
      "8 (0.8333333333333334, -1, -1)\n",
      "9 (0.6666666666666666, 1, 1)\n",
      "10 (0.6666666666666666, 1, 1)\n",
      "11 (0.6666666666666666, 1, 1)\n",
      "12 (0.6666666666666666, 1, 1)\n",
      "13 (0.6666666666666666, 1, 1)\n",
      "14 (0.6666666666666666, 1, 1)\n",
      "15 (0.6666666666666666, 1, 1)\n",
      "16 (0.6666666666666666, 1, 1)\n",
      "17 (0.6666666666666666, 1, -1)\n",
      "18 (0.6666666666666666, 1, -1)\n",
      "19 (0.6666666666666666, 1, -1)\n",
      "20 (0.6666666666666666, 1, -1)\n",
      "('tp', 167.0, 'tn', 733.0, 'fp', 1867.0, 'fn', 29.0)\n",
      "('precision:', 0.08210422812192723)\n",
      "('recall:', 0.8520408163265306)\n",
      "('F1 score:', 0.14977578475336323)\n"
     ]
    }
   ],
   "source": [
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    #plt.bar(range(0,2796),marginals)\n",
    "    #plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "#import cPickle as pickle\n",
    "#THETA = pickle.load( open( \"THETA.p\", \"rb\" ) )\n",
    "#test(THETA)\n",
    "#LAMDA,SCORE = get_LAMDA(dev_cands)\n",
    "#Confidence = get_Confidence(LAMDA)\n",
    "\n",
    "#P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "#marginals=get_marginals(P_cap)\n",
    "#plt.hist(marginals, bins=20)\n",
    "#plt.show()\n",
    "#plt.bar(range(0,888),train_marginals)\n",
    "#plt.show()\n",
    "\n",
    "print_details(\"dev set\",THETA,dev_LAMDA,dev_SCORE)\n",
    "predicted_labels=predict_labels(marginals)\n",
    "\n",
    "\n",
    "sorted_predicted_labels=[x for (y,x) in sorted(zip(Confidence,predicted_labels))] #sort Labels as per Confidence\n",
    "sorted_predicted_labels=list(reversed(sorted_predicted_labels))\n",
    "\n",
    "\n",
    "for i,j in enumerate(reversed(sorted(zip(Confidence,predicted_labels,gold_labels_dev)))):\n",
    "    if i>20:\n",
    "        break\n",
    "    print i,j\n",
    "#print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "#no_of_labels=186#int(len(predicted_labels)*0.1)  #54 - >0.2  , 108>= 0.15 , 186>= 0.12\n",
    "#print(len(sorted_predicted_labels[0:no_of_labels]))\n",
    "no_of_labels=2796\n",
    "score(predicted_labels[0:no_of_labels],gold_labels_dev[0:no_of_labels])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
