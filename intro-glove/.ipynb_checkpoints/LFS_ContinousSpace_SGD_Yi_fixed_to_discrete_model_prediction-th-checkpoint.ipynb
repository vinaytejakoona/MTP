{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "#%time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2697)\n",
      "(196, 2600)\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "gold_labels_dev = []\n",
    "for i,L in enumerate(L_gold_dev):\n",
    "    gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "gold_labels_test = []\n",
    "for i,L in enumerate(L_gold_test):\n",
    "    gold_labels_test.append(L[0,0])\n",
    "    \n",
    "print(len(gold_labels_dev),len(gold_labels_test))\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "softmax_Threshold = 0.3\n",
    "LF_Threshold = 0.3\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    global LF_Threshold\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_other_relationship_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "    return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def LF_Three_Lists_Left_Window(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife_left_window(c)\n",
    "    c2,s2 = LF_family_left_window(c)\n",
    "    c3,s3 = LF_other_relationship_left_window(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "\n",
    "def LF_Three_Lists_Between_Words(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife(c)\n",
    "    c2,s2 = LF_familial_relationship(c)\n",
    "    c3,s3 = LF_other_relationship(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    \n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'BW:',s1,s2,s3,prob_sc\n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [LF_distant_supervision, LF_distant_supervision_last_names,LF_same_last_name,\n",
    "       LF_and_married, LF_Three_Lists_Between_Words,LF_Three_Lists_Left_Window, LF_no_spouse_in_sentence\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def PHI(K,LAMDAi,SCOREi):\n",
    "    return [K*l*s for (l,s) in zip(LAMDAi,SCOREi)]\n",
    "\n",
    "def softmax(THETA,LAMDAi,SCOREi):\n",
    "    x = []\n",
    "    for k in [1,-1]:\n",
    "        product = np.dot(PHI(k,LAMDAi,SCOREi),THETA)\n",
    "        x.append(product)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def function_conf(THETA,LAMDA,P_cap,Confidence):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for LAMDAi in LAMDA:\n",
    "        s = s + Confidence[i]*np.dot(np.log(softmax(THETA,LAMDAi)),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def function(THETA,LAMDA,SCORE,P_cap):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for i in range(len(LAMDA)):\n",
    "        s = s + np.dot(np.log(softmax(THETA,LAMDA[i],SCORE[i])),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def P_K_Given_LAMDAi_THETA(K,THETA,LAMDAi,SCOREi):\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    if(K==1):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x[1]\n",
    "      \n",
    "\n",
    "np.random.seed(78)\n",
    "THETA = np.random.rand(len(LFs),1)\n",
    "\n",
    "def PHIj(j,K,LAMDAi,SCOREi):\n",
    "    return LAMDAi[j]*K*SCOREi[j]\n",
    "\n",
    "def RIGHT(j,LAMDAi,SCOREi,THETA):\n",
    "    phi = []\n",
    "    for k in [1,-1]:\n",
    "        phi.append(PHIj(j,k,LAMDAi,SCOREi))\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    return np.dot(phi,x)\n",
    "    \n",
    "\n",
    "def function_conf_der(THETA,LAMDA,P_cap,Confidence):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for LAMDAi in LAMDA:\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + Confidence[i]*(PHIj(j,K,LAMDAi)-RIGHT(j,LAMDAi,THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "def function_der(THETA,LAMDA,SCORE,P_cap):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for index in range(len(LAMDA)):\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + (PHIj(j,K,LAMDA[index],SCORE[index])-RIGHT(j,LAMDA[index],SCORE[index],THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_LAMDA(cands):\n",
    "    LAMDA = []\n",
    "    SCORE = []\n",
    "    for ci in cands:\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        LAMDA.append(L)\n",
    "        SCORE.append(S) \n",
    "    return LAMDA,SCORE\n",
    "\n",
    "def get_Confidence(LAMDA):\n",
    "    confidence = []\n",
    "    for L in LAMDA:\n",
    "        Total_L = float(len(L))\n",
    "        No_zeros = L.count(0)\n",
    "        No_Non_Zeros = Total_L - No_zeros\n",
    "        confidence.append(No_Non_Zeros/Total_L)\n",
    "    return confidence    \n",
    "    \n",
    "def get_Initial_P_cap(LAMDA):\n",
    "    P_cap = []\n",
    "    for L in LAMDA:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "    #print(np.array(LAMDA))\n",
    "    #print(np.array(P_cap))append(L)\n",
    "    #LAMDA=np.array(LAMDA).astype(int)\n",
    "    #P_cap=np.array(P_cap)\n",
    "    #print(np.array(LAMDA).shape)\n",
    "    #print(np.array(P_cap).shape)\n",
    "    #print(L)\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"No.Os\",L.count(0),\"No.1s\",L.count(1),\"No.-1s\",L.count(-1))\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"P(0):\",L.count(0)/len(L),\" P(1)\",L.count(1)/len(L),\"P(-1)\",L.count(-1)/len(L))\n",
    "\n",
    "        \n",
    "def get_P_cap(LAMDA,SCORE,THETA):\n",
    "    P_cap = []\n",
    "    for i in range(len(LAMDA)):\n",
    "        P_capi = softmax(THETA,LAMDA[i],SCORE[i])\n",
    "        P_cap.append(P_capi)\n",
    "    return P_cap\n",
    "\n",
    "\n",
    "def score(predicted_labels,gold_labels):\n",
    "    tp =0.0\n",
    "    tn =0.0\n",
    "    fp =0.0\n",
    "    fn =0.0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if(predicted_labels[i]==gold_labels[i]):\n",
    "            if(predicted_labels[i]==1):\n",
    "                tp=tp+1\n",
    "            else:\n",
    "                tn=tn+1\n",
    "        else:\n",
    "            if(predicted_labels[i]==1):\n",
    "                fp=fp+1\n",
    "            else:\n",
    "                fn=fn+1\n",
    "    print(\"tp\",tp,\"tn\",tn,\"fp\",fp,\"fn\",fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"F1 score:\",f1score)\n",
    "                \n",
    "           \n",
    "    \n",
    "from scipy.optimize import minimize\n",
    "import cPickle as pickle\n",
    "\n",
    "def get_marginals(P_cap):\n",
    "    marginals = []\n",
    "    for P_capi in P_cap:\n",
    "        marginals.append(P_capi[0])\n",
    "    return marginals\n",
    "\n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    plt.bar(range(0,2796),marginals)\n",
    "    plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "    \n",
    "    \n",
    "def train(No_Iter,Use_Confidence=True,theta_file_name=\"THETA\"):\n",
    "    global THETA\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    LAMDA,SCORE = get_LAMDA(train_cands)\n",
    "    P_cap = get_Initial_P_cap(LAMDA)\n",
    "    Confidence = get_Confidence(LAMDA)\n",
    "    for iteration in range(No_Iter):\n",
    "        if(Use_Confidence==True):\n",
    "            res = minimize(function_conf,THETA,args=(LAMDA,P_cap,Confidence), method='BFGS',jac=function_conf_der,options={'disp': True, 'maxiter':20}) #nelder-mead\n",
    "        else:\n",
    "            res = minimize(function,THETA,args=(LAMDA,SCORE,P_cap), method='BFGS',jac=function_der,options={'disp': True, 'maxiter':20}) #nelder-mead            \n",
    "        THETA = res.x # new THETA\n",
    "        print(THETA)\n",
    "        P_cap = get_P_cap(LAMDA,SCORE,THETA) #new p_cap \n",
    "        print_details(\"train iteration: \"+str(iteration),THETA,dev_LAMDA,dev_SCORE)\n",
    "        #score(predicted_labels,gold_labels)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Train_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Train_P_cap.p\",\"wb\"))\n",
    "    NP_THETA = np.array(THETA)\n",
    "    np.savetxt(theta_file_name+'.txt', NP_THETA, fmt='%f') \n",
    "    pickle.dump( NP_THETA, open( theta_file_name+'.p', \"wb\" )) # save the file as \"outfile_name.npy\" \n",
    "\n",
    "        \n",
    "def test(THETA):\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    P_cap = get_P_cap(dev_LAMDA,dev_SCORE,THETA)\n",
    "    print_details(\"test:\",THETA,dev_LAMDA,dev_SCORE)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Dev_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Dev_P_cap.p\",\"wb\"))\n",
    "                    \n",
    "def load_marginals(s):\n",
    "    marginals = []\n",
    "    if(s==\"train\"):\n",
    "        train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "        marginals = train_P_cap[:,0]\n",
    "    return marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "    return L_S\n",
    "\n",
    "def get_L_S(cands):  # sign gives label abs value gives score\n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        l_s=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            s= (s+1)/2  #to scale scores in [0,1] \n",
    "            l_s.append(l*s)\n",
    "        L_S.append(l_s)\n",
    "    return L_S\n",
    "\n",
    "def get_Initial_P_cap_L_S(L_S):\n",
    "    P_cap = []\n",
    "    for L,S in L_S:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "   \n",
    "    \n",
    "# dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "# train_L_S = get_L_S_Tensor(train_cands)\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "\n",
    "\n",
    "# train_P_cap= get_Initial_P_cap_L_S(train_L_S) \n",
    "\n",
    "# dev_P_cap = get_Initial_P_cap_L_S(dev_L_S)\n",
    "\n",
    "# test_P_cap = get_Initial_P_cap_L_S(test_L_S)\n",
    "\n",
    "# import cPickle as pkl\n",
    "\n",
    "# pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "# pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "# pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare batch data\n",
    "train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "train_P_cap_batch,dev_P_cap_batch = get_P_cap_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "\n",
    "#pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "#pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "#pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "#pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))\n",
    "\n",
    "dev_L_S = pkl.load( open( \"dev_L_S.p\", \"rb\" ) )\n",
    "train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "test_P_cap = pkl.load( open( \"test_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "def get_L_S_batch():\n",
    "    dev_L_batch = []\n",
    "    dev_S_batch = []\n",
    "    dev_L_S_batch = []\n",
    "    train_L_batch = []\n",
    "    train_S_batch = []\n",
    "    train_L_S_batch = []\n",
    "    for l,s in train_L_S:\n",
    "        train_L_batch.append(l)\n",
    "        train_S_batch.append(s)\n",
    "    train_L_S_batch = [train_L_batch, train_S_batch]\n",
    "    for l,s in dev_L_S:\n",
    "        dev_L_batch.append(l)\n",
    "        dev_S_batch.append(s)\n",
    "    dev_L_S_batch = [dev_L_batch, dev_S_batch]\n",
    "    return train_L_S_batch,dev_L_S_batch\n",
    "\n",
    "\n",
    "def get_P_cap_batch():\n",
    "    kp1_train= []\n",
    "    kn1_train = []\n",
    "    kp1_dev= []\n",
    "    kn1_dev = []\n",
    "    for pci in train_P_cap:\n",
    "        kp1_train.append(pci[0])\n",
    "        kn1_train.append(pci[1])\n",
    "    for pci in dev_P_cap:\n",
    "        kp1_dev.append(pci[0])\n",
    "        kn1_dev.append(pci[1])\n",
    "    return [kp1_train,kn1_train],[kp1_dev,kn1_dev]\n",
    "        \n",
    "def get_mini_batches(X,P_cap,bsize): #X : (train/dev/)_L_S_batch\n",
    "    for i in range(0, len(X[0]) - bsize + 1, bsize):\n",
    "        indices = slice(i, i + bsize)\n",
    "        #print(indices)\n",
    "        yield [X[0][indices],X[1][indices]],P_cap[indices]\n",
    "\n",
    "train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "\n",
    "#for x in get_mini_batches(train_L_S_batch,200):\n",
    "#    print(len(x),len(x[0]),len(x[0][0]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "12169.569138\n",
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "14274.6893101\n",
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "15056.1713148\n",
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "15309.4902762\n",
      "2274 522\n",
      "0   (0.036059806508355323, 0.41836734693877553, 0.066396761133603238, None)\n",
      "15396.2089236\n",
      "2332 464\n",
      "0   (0.037735849056603772, 0.44897959183673469, 0.069620253164556958, None)\n",
      "15433.9701141\n",
      "2346 450\n",
      "0   (0.037510656436487641, 0.44897959183673469, 0.069236821400472076, None)\n",
      "15491.9463652\n",
      "2094 702\n",
      "0   (0.037249283667621778, 0.39795918367346939, 0.068122270742358076, None)\n",
      "15553.5927661\n",
      "2259 537\n",
      "0   (0.036299247454625941, 0.41836734693877553, 0.066802443991853366, None)\n",
      "15336.2876494\n",
      "2332 464\n",
      "0   (0.037735849056603772, 0.44897959183673469, 0.069620253164556958, None)\n",
      "15408.5408184\n"
     ]
    }
   ],
   "source": [
    "#stochastic\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "        tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "#          + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    New_P_cap = []\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        New_P_cap.append(newPcap)\n",
    "    train_P_cap = New_P_cap\n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    print(total_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-1.99821249928e+22\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-1.00643208716e+80\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-6.44163265644e+137\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-4.12842845955e+195\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16855524079320114, 0.6071428571428571, 0.26385809312638581, None)\n",
      "-2.48449820085e+253\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-315837f5ade0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m#print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mNew_P_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mnewPcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_p_cap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewPcap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[1;32m   1054\u001b[0m                          'graph before calling run().')\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mversion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2335\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2337\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2338\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-3.05101543903e+24\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-1.49612942222e+82\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-9.57592297318e+139\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-6.12903532452e+197\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.17638266068759342, 0.60204081632653061, 0.27283236994219651, None)\n",
      "-3.92286718621e+255\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + remove min(theta,0) in loss\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0]\n",
      "1139   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-3.05101543903e+24\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1158   (0.03796844181459566, 0.39285714285714285, 0.069244604316546762, None)\n",
      "7.66676795166e+71\n",
      "2028 768\n",
      "1   (0.03796844181459566, 0.39285714285714285, 0.069244604316546762, None)\n",
      "1259   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "3.59025920518e+80\n",
      "737 2059\n",
      "2   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "2053   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.24710434464e+87\n",
      "737 2059\n",
      "3   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "2095   (0.037829766052762566, 0.38775510204081631, 0.068934240362811788, None)\n",
      "6.31578642638e+64\n",
      "2009 787\n",
      "4   (0.037829766052762566, 0.38775510204081631, 0.068934240362811788, None)\n",
      "2263   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-2.07026305156e+107\n",
      "737 2059\n",
      "5   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "2303   (0.037810945273631838, 0.38775510204081631, 0.068902991840435177, None)\n",
      "3.99011622848e+132\n",
      "2010 786\n",
      "6   (0.037810945273631838, 0.38775510204081631, 0.068902991840435177, None)\n",
      "2318   (0.16234652114597545, 0.6071428571428571, 0.25618945102260493, None)\n",
      "2.82192303469e+161\n",
      "733 2063\n",
      "7   (0.16234652114597545, 0.6071428571428571, 0.25618945102260493, None)\n",
      "2331   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.24529363194e+170\n",
      "737 2059\n",
      "8   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "2420   (0.037717121588089333, 0.38775510204081631, 0.068747173224785171, None)\n",
      "1.89701609214e+138\n",
      "2015 781\n",
      "9   (0.037717121588089333, 0.38775510204081631, 0.068747173224785171, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + remove min(theta,0) in loss + c increasing\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "print(train_P_cap[1])\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 1\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c],_p_cap:train_P_cap[c]})\n",
    "        train_P_cap[c] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        c= (c+1)%len(train_P_cap) \n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08558725464e+20\n",
      "515 2281\n",
      "22195   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.33735334926e+29\n",
      "515 2281\n",
      "22195   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.64647103539e+38\n",
      "515 2281\n",
      "22195   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "2.02717297625e+47\n",
      "515 2281\n",
      "22195   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "2.4955518764e+56\n",
      "515 2281\n",
      "22195   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d583dbdcd7d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_cap_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mtotal_te\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m#print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (maybe)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "\n",
    "\n",
    "#print(train_P_cap)\n",
    "\n",
    "# discrete_labels = predict_labels(train_P_cap[:1])\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "#train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_out = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "# additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4069152.07579\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068476.87684\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068420.94977\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068591.38647\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068378.51704\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4070007.39575\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068513.23018\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068402.40557\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068384.80685\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4069037.53787\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (maybe make [1 0])\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "L_train = []\n",
    "for x in fixed_Y:\n",
    "    if(x==1):\n",
    "        L_train.append([1,0])\n",
    "    else:\n",
    "        L_train.append([0,1])\n",
    "\n",
    "train_P_cap = L_train\n",
    "\n",
    "#print(train_P_cap)\n",
    "\n",
    "# discrete_labels = predict_labels(train_P_cap[:1])\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "#train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_out = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "# additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4069627.94138\n",
      "731 2065\n",
      "0   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4069499.18281\n",
      "731 2065\n",
      "1   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068432.03412\n",
      "731 2065\n",
      "2   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068374.81688\n",
      "731 2065\n",
      "3   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068654.30589\n",
      "731 2065\n",
      "4   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068411.01086\n",
      "731 2065\n",
      "5   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068499.78138\n",
      "731 2065\n",
      "6   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n",
      "4068510.72666\n",
      "731 2065\n",
      "7   (0.16279069767441862, 0.6071428571428571, 0.2567421790722762, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-889f8b68c428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_cap_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtotal_te\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m#print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (maybe make [1 0])\n",
    "#c increasing\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "L_train = []\n",
    "for x in fixed_Y:\n",
    "    if(x==1):\n",
    "        L_train.append([1,0])\n",
    "    else:\n",
    "        L_train.append([0,1])\n",
    "\n",
    "train_P_cap = L_train\n",
    "\n",
    "#print(train_P_cap)\n",
    "\n",
    "# discrete_labels = predict_labels(train_P_cap[:1])\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "#train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_out = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "# additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (maybe make [1 0])\n",
    "#c increasing + remove negative of loss\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "L_train = []\n",
    "for x in fixed_Y:\n",
    "    if(x==1):\n",
    "        L_train.append([1,0])\n",
    "    else:\n",
    "        L_train.append([0,1])\n",
    "\n",
    "train_P_cap = L_train\n",
    "\n",
    "#print(train_P_cap)\n",
    "\n",
    "# discrete_labels = predict_labels(train_P_cap[:1])\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "#train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_out = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "# additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap)) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-1962.11789765\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-183.357448901\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-68.9589110513\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-127.752387409\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-125.808991087\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-8.99542101801\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-8.99550454315\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-8.99558525264\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n",
      "-36.3690969097\n",
      "2454 342\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n",
      "-8.99614832915\n",
      "2454 342\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (after)\n",
    "#precision recall\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "_fixed_yi = tf.placeholder(tf.float64,shape=())\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "Pp1,Pn1 = tf.unstack(tf.nn.softmax(phi_out))\n",
    "\n",
    "p_yi = tf.where(tf.equal(_fixed_yi,1), Pp1, Pn1)\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(p_yi))\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i,fixed_yi in zip(train_L_S,train_P_cap,fixed_Y):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11584.2439528\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.463982\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.471924\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4564419\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4552647\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.3795317\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.5599733\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4457846\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.5101232\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4389113\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (after)\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "_fixed_yi = tf.placeholder(tf.float64,shape=())\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "Pp1,Pn1 = tf.unstack(tf.nn.softmax(phi_out))\n",
    "\n",
    "p_yi = tf.where(tf.equal(_fixed_yi,1), Pp1, Pn1)\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(p_yi))\\\n",
    "        + tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i,fixed_yi in zip(train_L_S,train_P_cap,fixed_Y):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-3.05101543903e+24\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-1.33152845787e+28\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-1.72516700344e+29\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-9.82706689593e+32\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-1.27322337343e+34\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-2.07113091047e+35\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-3.36907359527e+36\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-5.74412077375e+37\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-8.91489830239e+38\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-1.45017141714e+40\n",
      "737 2059\n",
      "0   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (after)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "_fixed_yi = tf.placeholder(tf.float64,shape=())\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "Pp1,Pn1 = tf.unstack(tf.nn.softmax(phi_out))\n",
    "\n",
    "p_yi = tf.where(tf.equal(_fixed_yi,1), Pp1, Pn1)\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(p_yi))\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i,fixed_yi in zip(train_L_S,train_P_cap,fixed_Y):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "       \n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11584.2013332\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4615796\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4323907\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4636791\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4415633\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4637904\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4635694\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.4761557\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n",
      "-11584.5077691\n",
      "2065 731\n",
      "0   (0.03777239709443099, 0.39795918367346939, 0.068996019460415739, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-936d7f2ddb8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfixed_yi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfixed_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_fixed_yi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfixed_yi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mtotal_te\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m#print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1109\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    432\u001b[0m            fetch.op.type == 'GetSessionHandleV2')):\n\u001b[1;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_assert_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m         self.name, self.get_shape(), self._dtype.name)\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;31m# Necessary to support Python's collection membership operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + yi fixed to output of model on discrete lfs (after)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "_fixed_yi = tf.placeholder(tf.float64,shape=())\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "Pp1,Pn1 = tf.unstack(tf.nn.softmax(phi_out))\n",
    "\n",
    "p_yi = tf.where(tf.equal(_fixed_yi,1), Pp1, Pn1)\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(p_yi)) \\\n",
    "        + tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "        tf.reduce_sum(tf.multiply(thetas,thetas))\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i,fixed_yi in zip(train_L_S,train_P_cap,fixed_Y):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-1962.11789765\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-183.357448901\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-68.9589110513\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-127.752387409\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-125.808991087\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-8.99542101801\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-8.99550454315\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "-8.99558525264\n",
      "2455 341\n",
      "0   (0.039918533604887986, 0.5, 0.07393436439079594, None)\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n",
      "-36.3690969097\n",
      "2454 342\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n",
      "-8.99614832915\n",
      "2454 342\n",
      "0   (0.039934800325998367, 0.5, 0.073962264150943383, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs (after)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "_fixed_yi = tf.placeholder(tf.float64,shape=())\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "Pp1,Pn1 = tf.unstack(tf.nn.softmax(phi_out))\n",
    "\n",
    "p_yi = tf.where(tf.equal(_fixed_yi,1), Pp1, Pn1)\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(p_yi))\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i,fixed_yi in zip(train_L_S,train_P_cap,fixed_Y):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13131.7660982\n",
      "2378 418\n",
      "0   (0.037846930193439862, 0.45918367346938777, 0.069930069930069921, None)\n",
      "13124.5847753\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.3849331\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.2662261\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.2085258\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.520522\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.4996499\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b4a7a7ae4075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfixed_yi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfixed_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_fixed_yi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfixed_yi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mtotal_te\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m#print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs +log loss(after thought)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "_fixed_yi = tf.placeholder(tf.float64,shape=())\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "Pp1,Pn1 = tf.unstack(tf.nn.softmax(phi_out))\n",
    "\n",
    "p_yi = tf.where(tf.equal(_fixed_yi,1), Pp1, Pn1)\n",
    "\n",
    "loss = tf.negative(tf.log(tf.reduce_sum(p_yi)))\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i,fixed_yi in zip(train_L_S,train_P_cap,fixed_Y):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13131.7660982\n",
      "2378 418\n",
      "0   (0.037846930193439862, 0.45918367346938777, 0.069930069930069921, None)\n",
      "13124.5847753\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.3849331\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.2662261\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.2085258\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n",
      "13123.520522\n",
      "2384 412\n",
      "0   (0.038171140939597316, 0.4642857142857143, 0.070542635658914735, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs +log loss(after thought)\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "fixed_Y = pkl.load(open(\"discrete_predicted_labels_train_data.p\",\"rb\"))\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "_fixed_yi = tf.placeholder(tf.float64,shape=())\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "Pp1,Pn1 = tf.unstack(tf.nn.softmax(phi_out))\n",
    "\n",
    "p_yi = tf.where(tf.equal(_fixed_yi,1), Pp1, Pn1)\n",
    "\n",
    "loss = tf.negative(tf.log(tf.reduce_sum(p_yi)))\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "#         #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i,fixed_yi in zip(train_L_S,train_P_cap,fixed_Y):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i,_fixed_yi:fixed_yi})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22195\n",
      "-0.403062582953\n",
      "746 2050\n",
      "0   (0.16085790884718498, 0.61224489795918369, 0.25477707006369427, None)\n",
      "-0.491641866684\n",
      "810 1986\n",
      "1   (0.17037037037037037, 0.70408163265306123, 0.27435387673956263, None)\n",
      "-0.594699991089\n",
      "787 2009\n",
      "2   (0.15374841168996187, 0.61734693877551017, 0.24618514750762968, None)\n",
      "-0.584680700423\n",
      "1241 1555\n",
      "3   (0.11764705882352941, 0.74489795918367352, 0.20320111343075853, None)\n",
      "-0.629941925989\n",
      "1247 1549\n",
      "4   (0.10986367281475541, 0.69897959183673475, 0.18988218988218988, None)\n",
      "-0.610793114481\n",
      "899 1897\n",
      "5   (0.1546162402669633, 0.70918367346938771, 0.25388127853881282, None)\n",
      "-0.622043836336\n",
      "820 1976\n",
      "6   (0.14999999999999999, 0.62755102040816324, 0.24212598425196849, None)\n",
      "-0.617336477866\n",
      "1480 1316\n",
      "7   (0.095270270270270269, 0.71938775510204078, 0.16825775656324585, None)\n",
      "-0.618130318603\n",
      "818 1978\n",
      "8   (0.1687041564792176, 0.70408163265306123, 0.27218934911242604, None)\n",
      "-0.586582216415\n",
      "973 1823\n",
      "9   (0.14285714285714285, 0.70918367346938771, 0.23781009409751927, None)\n",
      "-0.607121678903\n",
      "786 2010\n",
      "10   (0.17557251908396945, 0.70408163265306123, 0.28105906313645623, None)\n",
      "-0.561990194009\n",
      "834 1962\n",
      "11   (0.14748201438848921, 0.62755102040816324, 0.23883495145631067, None)\n",
      "-0.572345405133\n",
      "983 1813\n",
      "12   (0.1353001017293998, 0.6785714285714286, 0.22561492790500426, None)\n",
      "-0.566924296964\n",
      "789 2007\n",
      "13   (0.15335868187579213, 0.61734693877551017, 0.24568527918781724, None)\n",
      "-0.435678966348\n",
      "779 2017\n",
      "14   (0.15661103979460847, 0.62244897959183676, 0.25025641025641027, None)\n",
      "-0.458495279874\n",
      "842 1954\n",
      "15   (0.13657957244655583, 0.58673469387755106, 0.22157996146435455, None)\n",
      "-0.449164983646\n",
      "809 1987\n",
      "16   (0.14956736711990112, 0.61734693877551017, 0.24079601990049751, None)\n",
      "-0.379867863066\n",
      "838 1958\n",
      "17   (0.13603818615751789, 0.58163265306122447, 0.22050290135396519, None)\n",
      "-0.369685197109\n",
      "750 2046\n",
      "18   (0.16, 0.61224489795918369, 0.2536997885835095, None)\n",
      "-0.227868806484\n",
      "745 2051\n",
      "19   (0.16107382550335569, 0.61224489795918369, 0.25504782146652494, None)\n",
      "-0.0355672342723\n",
      "752 2044\n",
      "20   (0.15957446808510639, 0.61224489795918369, 0.25316455696202533, None)\n",
      "0.108307225631\n",
      "750 2046\n",
      "21   (0.16, 0.61224489795918369, 0.2536997885835095, None)\n",
      "0.335109901527\n",
      "743 2053\n",
      "22   (0.16150740242261102, 0.61224489795918369, 0.25559105431309898, None)\n",
      "0.740705182006\n",
      "737 2059\n",
      "23   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.11050066556\n",
      "737 2059\n",
      "24   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.60991097477\n",
      "737 2059\n",
      "25   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "2.25530605435\n",
      "737 2059\n",
      "26   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "3.14536698583\n",
      "737 2059\n",
      "27   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "4.36727211905\n",
      "737 2059\n",
      "28   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "6.1403302575\n",
      "737 2059\n",
      "29   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "9.3665721913\n",
      "737 2059\n",
      "30   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "15.1515395344\n",
      "737 2059\n",
      "31   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "27.3734255678\n",
      "737 2059\n",
      "32   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "57.8044624384\n",
      "737 2059\n",
      "33   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "155.940639752\n",
      "737 2059\n",
      "34   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "639.915555032\n",
      "737 2059\n",
      "35   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "5813.47074629\n",
      "737 2059\n",
      "36   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "276713.418925\n",
      "737 2059\n",
      "37   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "467766797.392\n",
      "737 2059\n",
      "38   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.27592153959e+15\n",
      "737 2059\n",
      "39   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "9.48800648303e+27\n",
      "737 2059\n",
      "40   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "5.25972489458e+53\n",
      "737 2059\n",
      "41   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.61439546837e+105\n",
      "737 2059\n",
      "42   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "nan\n",
      "0 2796\n",
      "43   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "44   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "45   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "46   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "47   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "48   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "49   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "50   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "51   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "52   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "53   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "54   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "55   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "56   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "57   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "58   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "59   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "60   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "61   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "62   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "63   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "64   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "65   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "66   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "67   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "68   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "69   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "70   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "71   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "72   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "73   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "74   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "75   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "76   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "77   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "78   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "79   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "80   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "81   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "82   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "83   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "84   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "85   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "86   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "87   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "88   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "89   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "90   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "91   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "92   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "93   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "94   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "95   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "96   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "97   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "98   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "99   (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "# Batch with cross entropy logits function  + additional layer\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "dev_data_size = len(dev_L_S_batch[0])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "#train_P_cap = np.full([data_size,2],0.5)\n",
    "\n",
    "#print(train_P_cap)\n",
    "#print(train_P_cap.shape)\n",
    "#dev_P_cap = np.full([dev_data_size,2],0.5)\n",
    "\n",
    "\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.01),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "phi_p1 = tf.matmul(additional_layer_out,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(additional_layer_out),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "\n",
    "# phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "# phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "# pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "predict_2 = tf.where(tf.greater(tf.slice(tf.nn.softmax(phi_out),[0,1],[dev_data_size,1]),0.5),\n",
    "                    tf.ones((dev_data_size,1)),tf.negative(tf.ones((dev_data_size,1))))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "\n",
    "for i in range(100):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    a,t,te_curr,_, = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "    print(te_curr)\n",
    "    \n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    #print(train_P_cap[0:5])\n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        te_curr,pl,pl2,_ = sess.run([loss,predict,predict_2,train_step],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl2 = pl2.flatten().tolist()\n",
    "        pl = pl.flatten().tolist()\n",
    "        #print(te_curr)\n",
    "        #predicted_labels = pl2\n",
    "        predicted_labels = [-1 if x==0 else 1 for x in pl]\n",
    "        #for l,l2 in zip(predicted_labels,pl2):\n",
    "        #    print(l,l2)\n",
    "        \n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    " \n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 330.069609\n",
      "         Iterations: 20\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 23\n",
      "[ 3.29293033  0.80240776  3.54276763  3.57746115  2.57487472  2.54625167\n",
      "  4.16422647]\n",
      "train iteration: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c9a7610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c80f590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 4182.213294\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "[ 3.29293033  0.80240776  3.54276763  3.57746115  2.57487472  2.54625167\n",
      "  4.16422647]\n",
      "train iteration: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c9a7d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435bea9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n",
      "test:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f436ad14350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c117750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n"
     ]
    }
   ],
   "source": [
    "# All LF_Threshold =0.3 and softmax_Threshold=0.3 ,to be run\n",
    "train(2,Use_Confidence=False,theta_file_name=\"THETA\")\n",
    "\n",
    "test(THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    #plt.bar(range(0,2796),marginals)\n",
    "    #plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "#import cPickle as pickle\n",
    "#THETA = pickle.load( open( \"THETA.p\", \"rb\" ) )\n",
    "#test(THETA)\n",
    "#LAMDA,SCORE = get_LAMDA(dev_cands)\n",
    "#Confidence = get_Confidence(LAMDA)\n",
    "\n",
    "#P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "#marginals=get_marginals(P_cap)\n",
    "#plt.hist(marginals, bins=20)\n",
    "#plt.show()\n",
    "#plt.bar(range(0,888),train_marginals)\n",
    "#plt.show()\n",
    "\n",
    "print_details(\"dev set\",THETA,dev_LAMDA,dev_SCORE)\n",
    "predicted_labels=predict_labels(marginals)\n",
    "\n",
    "\n",
    "sorted_predicted_labels=[x for (y,x) in sorted(zip(Confidence,predicted_labels))] #sort Labels as per Confidence\n",
    "sorted_predicted_labels=list(reversed(sorted_predicted_labels))\n",
    "\n",
    "\n",
    "for i,j in enumerate(reversed(sorted(zip(Confidence,predicted_labels,gold_labels_dev)))):\n",
    "    if i>20:\n",
    "        break\n",
    "    print i,j\n",
    "#print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "#no_of_labels=186#int(len(predicted_labels)*0.1)  #54 - >0.2  , 108>= 0.15 , 186>= 0.12\n",
    "#print(len(sorted_predicted_labels[0:no_of_labels]))\n",
    "no_of_labels=2796\n",
    "score(predicted_labels[0:no_of_labels],gold_labels_dev[0:no_of_labels])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
