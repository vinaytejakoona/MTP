{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "#%time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2796 2697\n",
      "196 2600\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "gold_labels_dev = []\n",
    "for i,L in enumerate(L_gold_dev):\n",
    "    gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "gold_labels_test = []\n",
    "for i,L in enumerate(L_gold_test):\n",
    "    gold_labels_test.append(L[0,0])\n",
    "    \n",
    "print(len(gold_labels_dev),len(gold_labels_test))\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "softmax_Threshold = 0.3\n",
    "LF_Threshold = 0.3\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    global LF_Threshold\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_other_relationship_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "    return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def LF_Three_Lists_Left_Window(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife_left_window(c)\n",
    "    c2,s2 = LF_family_left_window(c)\n",
    "    c3,s3 = LF_other_relationship_left_window(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "\n",
    "def LF_Three_Lists_Between_Words(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife(c)\n",
    "    c2,s2 = LF_familial_relationship(c)\n",
    "    c3,s3 = LF_other_relationship(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    \n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'BW:',s1,s2,s3,prob_sc\n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [LF_distant_supervision, LF_distant_supervision_last_names,LF_same_last_name,\n",
    "       LF_and_married, LF_Three_Lists_Between_Words,LF_Three_Lists_Left_Window, LF_no_spouse_in_sentence\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def PHI(K,LAMDAi,SCOREi):\n",
    "    return [K*l*s for (l,s) in zip(LAMDAi,SCOREi)]\n",
    "\n",
    "def softmax(THETA,LAMDAi,SCOREi):\n",
    "    x = []\n",
    "    for k in [1,-1]:\n",
    "        product = np.dot(PHI(k,LAMDAi,SCOREi),THETA)\n",
    "        x.append(product)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def function_conf(THETA,LAMDA,P_cap,Confidence):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for LAMDAi in LAMDA:\n",
    "        s = s + Confidence[i]*np.dot(np.log(softmax(THETA,LAMDAi)),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def function(THETA,LAMDA,SCORE,P_cap):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for i in range(len(LAMDA)):\n",
    "        s = s + np.dot(np.log(softmax(THETA,LAMDA[i],SCORE[i])),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def P_K_Given_LAMDAi_THETA(K,THETA,LAMDAi,SCOREi):\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    if(K==1):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x[1]\n",
    "      \n",
    "\n",
    "np.random.seed(78)\n",
    "THETA = np.random.rand(len(LFs),1)\n",
    "\n",
    "def PHIj(j,K,LAMDAi,SCOREi):\n",
    "    return LAMDAi[j]*K*SCOREi[j]\n",
    "\n",
    "def RIGHT(j,LAMDAi,SCOREi,THETA):\n",
    "    phi = []\n",
    "    for k in [1,-1]:\n",
    "        phi.append(PHIj(j,k,LAMDAi,SCOREi))\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    return np.dot(phi,x)\n",
    "    \n",
    "\n",
    "def function_conf_der(THETA,LAMDA,P_cap,Confidence):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for LAMDAi in LAMDA:\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + Confidence[i]*(PHIj(j,K,LAMDAi)-RIGHT(j,LAMDAi,THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "def function_der(THETA,LAMDA,SCORE,P_cap):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for index in range(len(LAMDA)):\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + (PHIj(j,K,LAMDA[index],SCORE[index])-RIGHT(j,LAMDA[index],SCORE[index],THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_LAMDA(cands):\n",
    "    LAMDA = []\n",
    "    SCORE = []\n",
    "    for ci in cands:\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        LAMDA.append(L)\n",
    "        SCORE.append(S) \n",
    "    return LAMDA,SCORE\n",
    "\n",
    "def get_Confidence(LAMDA):\n",
    "    confidence = []\n",
    "    for L in LAMDA:\n",
    "        Total_L = float(len(L))\n",
    "        No_zeros = L.count(0)\n",
    "        No_Non_Zeros = Total_L - No_zeros\n",
    "        confidence.append(No_Non_Zeros/Total_L)\n",
    "    return confidence    \n",
    "    \n",
    "def get_Initial_P_cap(LAMDA):\n",
    "    P_cap = []\n",
    "    for L in LAMDA:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "    #print(np.array(LAMDA))\n",
    "    #print(np.array(P_cap))append(L)\n",
    "    #LAMDA=np.array(LAMDA).astype(int)\n",
    "    #P_cap=np.array(P_cap)\n",
    "    #print(np.array(LAMDA).shape)\n",
    "    #print(np.array(P_cap).shape)\n",
    "    #print(L)\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"No.Os\",L.count(0),\"No.1s\",L.count(1),\"No.-1s\",L.count(-1))\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"P(0):\",L.count(0)/len(L),\" P(1)\",L.count(1)/len(L),\"P(-1)\",L.count(-1)/len(L))\n",
    "\n",
    "        \n",
    "def get_P_cap(LAMDA,SCORE,THETA):\n",
    "    P_cap = []\n",
    "    for i in range(len(LAMDA)):\n",
    "        P_capi = softmax(THETA,LAMDA[i],SCORE[i])\n",
    "        P_cap.append(P_capi)\n",
    "    return P_cap\n",
    "\n",
    "\n",
    "def score(predicted_labels,gold_labels):\n",
    "    tp =0.0\n",
    "    tn =0.0\n",
    "    fp =0.0\n",
    "    fn =0.0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if(predicted_labels[i]==gold_labels[i]):\n",
    "            if(predicted_labels[i]==1):\n",
    "                tp=tp+1\n",
    "            else:\n",
    "                tn=tn+1\n",
    "        else:\n",
    "            if(predicted_labels[i]==1):\n",
    "                fp=fp+1\n",
    "            else:\n",
    "                fn=fn+1\n",
    "    print(\"tp\",tp,\"tn\",tn,\"fp\",fp,\"fn\",fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"F1 score:\",f1score)\n",
    "                \n",
    "           \n",
    "    \n",
    "from scipy.optimize import minimize\n",
    "import cPickle as pickle\n",
    "\n",
    "def get_marginals(P_cap):\n",
    "    marginals = []\n",
    "    for P_capi in P_cap:\n",
    "        marginals.append(P_capi[0])\n",
    "    return marginals\n",
    "\n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    plt.bar(range(0,2796),marginals)\n",
    "    plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "    \n",
    "    \n",
    "def train(No_Iter,Use_Confidence=True,theta_file_name=\"THETA\"):\n",
    "    global THETA\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    LAMDA,SCORE = get_LAMDA(train_cands)\n",
    "    P_cap = get_Initial_P_cap(LAMDA)\n",
    "    Confidence = get_Confidence(LAMDA)\n",
    "    for iteration in range(No_Iter):\n",
    "        if(Use_Confidence==True):\n",
    "            res = minimize(function_conf,THETA,args=(LAMDA,P_cap,Confidence), method='BFGS',jac=function_conf_der,options={'disp': True, 'maxiter':20}) #nelder-mead\n",
    "        else:\n",
    "            res = minimize(function,THETA,args=(LAMDA,SCORE,P_cap), method='BFGS',jac=function_der,options={'disp': True, 'maxiter':20}) #nelder-mead            \n",
    "        THETA = res.x # new THETA\n",
    "        print(THETA)\n",
    "        P_cap = get_P_cap(LAMDA,SCORE,THETA) #new p_cap \n",
    "        print_details(\"train iteration: \"+str(iteration),THETA,dev_LAMDA,dev_SCORE)\n",
    "        #score(predicted_labels,gold_labels)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Train_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Train_P_cap.p\",\"wb\"))\n",
    "    NP_THETA = np.array(THETA)\n",
    "    np.savetxt(theta_file_name+'.txt', NP_THETA, fmt='%f') \n",
    "    pickle.dump( NP_THETA, open( theta_file_name+'.p', \"wb\" )) # save the file as \"outfile_name.npy\" \n",
    "\n",
    "        \n",
    "def test(THETA):\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    P_cap = get_P_cap(dev_LAMDA,dev_SCORE,THETA)\n",
    "    print_details(\"test:\",THETA,dev_LAMDA,dev_SCORE)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Dev_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Dev_P_cap.p\",\"wb\"))\n",
    "                    \n",
    "def load_marginals(s):\n",
    "    marginals = []\n",
    "    if(s==\"train\"):\n",
    "        train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "        marginals = train_P_cap[:,0]\n",
    "    return marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "    return L_S\n",
    "\n",
    "def get_L_S(cands):  # sign gives label abs value gives score\n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        l_s=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            s= (s+1)/2  #to scale scores in [0,1] \n",
    "            l_s.append(l*s)\n",
    "        L_S.append(l_s)\n",
    "    return L_S\n",
    "\n",
    "def get_Initial_P_cap_L_S(L_S):\n",
    "    P_cap = []\n",
    "    for L,S in L_S:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "   \n",
    "    \n",
    "# dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "# train_L_S = get_L_S_Tensor(train_cands)\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "\n",
    "\n",
    "# train_P_cap= get_Initial_P_cap_L_S(train_L_S) \n",
    "\n",
    "# dev_P_cap = get_Initial_P_cap_L_S(dev_L_S)\n",
    "\n",
    "# test_P_cap = get_Initial_P_cap_L_S(test_L_S)\n",
    "\n",
    "# import cPickle as pkl\n",
    "\n",
    "# pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "# pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "# pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare batch data\n",
    "train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "train_P_cap_batch,dev_P_cap_batch = get_P_cap_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "\n",
    "#pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "#pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "#pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "#pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))\n",
    "\n",
    "dev_L_S = pkl.load( open( \"dev_L_S.p\", \"rb\" ) )\n",
    "train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "test_P_cap = pkl.load( open( \"test_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "def get_L_S_batch():\n",
    "    dev_L_batch = []\n",
    "    dev_S_batch = []\n",
    "    dev_L_S_batch = []\n",
    "    train_L_batch = []\n",
    "    train_S_batch = []\n",
    "    train_L_S_batch = []\n",
    "    for l,s in train_L_S:\n",
    "        train_L_batch.append(l)\n",
    "        train_S_batch.append(s)\n",
    "    train_L_S_batch = [train_L_batch, train_S_batch]\n",
    "    for l,s in dev_L_S:\n",
    "        dev_L_batch.append(l)\n",
    "        dev_S_batch.append(s)\n",
    "    dev_L_S_batch = [dev_L_batch, dev_S_batch]\n",
    "    return train_L_S_batch,dev_L_S_batch\n",
    "\n",
    "\n",
    "def get_P_cap_batch():\n",
    "    kp1_train= []\n",
    "    kn1_train = []\n",
    "    kp1_dev= []\n",
    "    kn1_dev = []\n",
    "    for pci in train_P_cap:\n",
    "        kp1_train.append(pci[0])\n",
    "        kn1_train.append(pci[1])\n",
    "    for pci in dev_P_cap:\n",
    "        kp1_dev.append(pci[0])\n",
    "        kn1_dev.append(pci[1])\n",
    "    return [kp1_train,kn1_train],[kp1_dev,kn1_dev]\n",
    "        \n",
    "def get_mini_batches(X,P_cap,bsize): #X : (train/dev/)_L_S_batch\n",
    "    for i in range(0, len(X[0]) - bsize + 1, bsize):\n",
    "        indices = slice(i, i + bsize)\n",
    "        #print(indices)\n",
    "        yield [X[0][indices],X[1][indices]],P_cap[indices]\n",
    "\n",
    "train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "\n",
    "#for x in get_mini_batches(train_L_S_batch,200):\n",
    "#    print(len(x),len(x[0]),len(x[0][0]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22195\n",
      "30749.8000609\n",
      "390 2406\n",
      "0   (0.26153846153846155, 0.52040816326530615, 0.34812286689419802, None)\n",
      "689753213.406\n",
      "0 2796\n",
      "1   (0.0, 0.0, 0.0, None)\n",
      "282743366.959\n",
      "0 2796\n",
      "2   (0.0, 0.0, 0.0, None)\n",
      "115829847.539\n",
      "0 2796\n",
      "3   (0.0, 0.0, 0.0, None)\n",
      "47462071.2546\n",
      "0 2796\n",
      "4   (0.0, 0.0, 0.0, None)\n",
      "19458631.0562\n",
      "0 2796\n",
      "5   (0.0, 0.0, 0.0, None)\n",
      "7988422.10162\n",
      "0 2796\n",
      "6   (0.0, 0.0, 0.0, None)\n",
      "3290223.4154\n",
      "0 2796\n",
      "7   (0.0, 0.0, 0.0, None)\n",
      "1365841.41805\n",
      "3 2793\n",
      "8   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "577614.374249\n",
      "0 2796\n",
      "9   (0.0, 0.0, 0.0, None)\n",
      "254756.598444\n",
      "1 2795\n",
      "10   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "122514.621049\n",
      "0 2796\n",
      "11   (0.0, 0.0, 0.0, None)\n",
      "68348.631791\n",
      "3 2793\n",
      "12   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "46162.1939876\n",
      "16 2780\n",
      "13   (0.375, 0.030612244897959183, 0.056603773584905655, None)\n",
      "37074.1858935\n",
      "19 2777\n",
      "14   (0.42105263157894735, 0.040816326530612242, 0.074418604651162776, None)\n",
      "33351.5694898\n",
      "0 2796\n",
      "15   (0.0, 0.0, 0.0, None)\n",
      "31827.7003037\n",
      "19 2777\n",
      "16   (0.42105263157894735, 0.040816326530612242, 0.074418604651162776, None)\n",
      "31205.9563236\n",
      "539 2257\n",
      "17   (0.08534322820037106, 0.23469387755102042, 0.12517006802721087, None)\n",
      "30949.3845834\n",
      "1201 1595\n",
      "18   (0.069941715237302249, 0.42857142857142855, 0.12025769506084466, None)\n",
      "30882.7677497\n",
      "70 2726\n",
      "19   (0.2857142857142857, 0.10204081632653061, 0.15037593984962405, None)\n",
      "32048.5020189\n",
      "16 2780\n",
      "20   (0.375, 0.030612244897959183, 0.056603773584905655, None)\n",
      "31293.276256\n",
      "1731 1065\n",
      "21   (0.036395147313691506, 0.32142857142857145, 0.065386611312921644, None)\n",
      "30984.0583107\n",
      "0 2796\n",
      "22   (0.0, 0.0, 0.0, None)\n",
      "31447.8383601\n",
      "736 2060\n",
      "23   (0.16032608695652173, 0.60204081632653061, 0.25321888412017168, None)\n",
      "31088.0135235\n",
      "1956 840\n",
      "24   (0.027096114519427401, 0.27040816326530615, 0.049256505576208177, None)\n",
      "38082.1187003\n",
      "53 2743\n",
      "25   (0.22641509433962265, 0.061224489795918366, 0.09638554216867469, None)\n",
      "33768.4224069\n",
      "3 2793\n",
      "26   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "32434.3634452\n",
      "15 2781\n",
      "27   (0.13333333333333333, 0.01020408163265306, 0.018957345971563979, None)\n",
      "379110.149794\n",
      "0 2796\n",
      "28   (0.0, 0.0, 0.0, None)\n",
      "173449.786103\n",
      "42 2754\n",
      "29   (0.11904761904761904, 0.025510204081632654, 0.042016806722689072, None)\n",
      "89211.2204178\n",
      "0 2796\n",
      "30   (0.0, 0.0, 0.0, None)\n",
      "54726.6395173\n",
      "16 2780\n",
      "31   (0.375, 0.030612244897959183, 0.056603773584905655, None)\n",
      "40583.7864539\n",
      "19 2777\n",
      "32   (0.42105263157894735, 0.040816326530612242, 0.074418604651162776, None)\n",
      "34789.2573404\n",
      "0 2796\n",
      "33   (0.0, 0.0, 0.0, None)\n",
      "32423.034747\n",
      "84 2712\n",
      "34   (0.17857142857142858, 0.076530612244897961, 0.10714285714285714, None)\n",
      "31453.5152928\n",
      "684 2112\n",
      "35   (0.16374269005847952, 0.5714285714285714, 0.25454545454545452, None)\n",
      "32132.9274309\n",
      "3 2793\n",
      "36   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "508338.205489\n",
      "785 2011\n",
      "37   (0.08025477707006369, 0.32142857142857145, 0.12844036697247707, None)\n",
      "226394.796736\n",
      "3 2793\n",
      "38   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "111054.728662\n",
      "0 2796\n",
      "39   (0.0, 0.0, 0.0, None)\n",
      "63654.1419215\n",
      "27 2769\n",
      "40   (0.33333333333333331, 0.045918367346938778, 0.080717488789237665, None)\n",
      "44240.9660985\n",
      "8 2788\n",
      "41   (0.125, 0.0051020408163265302, 0.0098039215686274491, None)\n",
      "36289.5428423\n",
      "723 2073\n",
      "42   (0.16320885200553251, 0.60204081632653061, 0.25680087051142547, None)\n",
      "33047.8079274\n",
      "3 2793\n",
      "43   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "32199.7974996\n",
      "51 2745\n",
      "44   (0.25490196078431371, 0.066326530612244902, 0.10526315789473685, None)\n",
      "31355.1961837\n",
      "0 2796\n",
      "45   (0.0, 0.0, 0.0, None)\n",
      "31012.2979369\n",
      "46 2750\n",
      "46   (0.15217391304347827, 0.035714285714285712, 0.057851239669421489, None)\n",
      "31058.5909022\n",
      "1230 1566\n",
      "47   (0.078861788617886175, 0.49489795918367346, 0.13604488078541374, None)\n",
      "470731.110796\n",
      "3 2793\n",
      "48   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "248794.437749\n",
      "0 2796\n",
      "49   (0.0, 0.0, 0.0, None)\n",
      "120073.418568\n",
      "0 2796\n",
      "50   (0.0, 0.0, 0.0, None)\n",
      "67456.7327902\n",
      "0 2796\n",
      "51   (0.0, 0.0, 0.0, None)\n",
      "57736.607035\n",
      "0 2796\n",
      "52   (0.0, 0.0, 0.0, None)\n",
      "42761.2455261\n",
      "3 2793\n",
      "53   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "35680.7632981\n",
      "0 2796\n",
      "54   (0.0, 0.0, 0.0, None)\n",
      "32780.6056236\n",
      "3 2793\n",
      "55   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "31595.8986482\n",
      "0 2796\n",
      "56   (0.0, 0.0, 0.0, None)\n",
      "31378.8600582\n",
      "3 2793\n",
      "57   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "68443.6128363\n",
      "0 2796\n",
      "58   (0.0, 0.0, 0.0, None)\n",
      "46200.273023\n",
      "3 2793\n",
      "59   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "37089.5751063\n",
      "1967 829\n",
      "60   (0.027961362480935434, 0.28061224489795916, 0.050855293573740176, None)\n",
      "35949.572149\n",
      "3 2793\n",
      "61   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "32891.8819921\n",
      "1995 801\n",
      "62   (0.036591478696741855, 0.37244897959183676, 0.066636239160200825, None)\n",
      "31685.1758084\n",
      "30 2766\n",
      "63   (0.33333333333333331, 0.051020408163265307, 0.088495575221238937, None)\n",
      "31310.8647609\n",
      "3 2793\n",
      "64   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "30991.3619856\n",
      "0 2796\n",
      "65   (0.0, 0.0, 0.0, None)\n",
      "30868.4340242\n",
      "19 2777\n",
      "66   (0.42105263157894735, 0.040816326530612242, 0.074418604651162776, None)\n",
      "30809.869552\n",
      "723 2073\n",
      "67   (0.15767634854771784, 0.58163265306122447, 0.24809575625680089, None)\n",
      "30790.0777951\n",
      "1 2795\n",
      "68   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "30965.5950156\n",
      "8 2788\n",
      "69   (0.125, 0.0051020408163265302, 0.0098039215686274491, None)\n",
      "98344.2844161\n",
      "494 2302\n",
      "70   (0.20647773279352227, 0.52040816326530615, 0.29565217391304349, None)\n",
      "4419934.70836\n",
      "0 2796\n",
      "71   (0.0, 0.0, 0.0, None)\n",
      "1832803.14456\n",
      "0 2796\n",
      "72   (0.0, 0.0, 0.0, None)\n",
      "768882.857675\n",
      "26 2770\n",
      "73   (0.30769230769230771, 0.040816326530612242, 0.072072072072072071, None)\n",
      "333103.135236\n",
      "18 2778\n",
      "74   (0.3888888888888889, 0.035714285714285712, 0.065420560747663545, None)\n",
      "154607.251492\n",
      "16 2780\n",
      "75   (0.375, 0.030612244897959183, 0.056603773584905655, None)\n",
      "81493.5808299\n",
      "29 2767\n",
      "76   (0.17241379310344829, 0.025510204081632654, 0.044444444444444446, None)\n",
      "51546.8577351\n",
      "3 2793\n",
      "77   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "39280.6057074\n",
      "66 2730\n",
      "78   (0.15151515151515152, 0.051020408163265307, 0.076335877862595422, None)\n",
      "34259.9161731\n",
      "1237 1559\n",
      "79   (0.079223928860145509, 0.5, 0.13677599441730634, None)\n",
      "32284.054796\n",
      "0 2796\n",
      "80   (0.0, 0.0, 0.0, None)\n",
      "31389.6858444\n",
      "1 2795\n",
      "81   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "31027.7040125\n",
      "1988 808\n",
      "82   (0.036217303822937627, 0.36734693877551022, 0.065934065934065936, None)\n",
      "30878.3302125\n",
      "723 2073\n",
      "83   (0.16320885200553251, 0.60204081632653061, 0.25680087051142547, None)\n",
      "74939.2834397\n",
      "1712 1084\n",
      "84   (0.032710280373831772, 0.2857142857142857, 0.058700209643605866, None)\n",
      "49039.2576632\n",
      "19 2777\n",
      "85   (0.42105263157894735, 0.040816326530612242, 0.074418604651162776, None)\n",
      "38310.0020661\n",
      "0 2796\n",
      "86   (0.0, 0.0, 0.0, None)\n",
      "33857.5305294\n",
      "3 2793\n",
      "87   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "32261.8767598\n",
      "2 2794\n",
      "88   (0.5, 0.0051020408163265302, 0.010101010101010102, None)\n",
      "85036.997534\n",
      "16 2780\n",
      "89   (0.375, 0.030612244897959183, 0.056603773584905655, None)\n",
      "52997.105163\n",
      "72 2724\n",
      "90   (0.27777777777777779, 0.10204081632653061, 0.1492537313432836, None)\n",
      "39873.684002\n",
      "69 2727\n",
      "91   (0.2608695652173913, 0.091836734693877556, 0.13584905660377361, None)\n",
      "34561.8942423\n",
      "72 2724\n",
      "92   (0.27777777777777779, 0.10204081632653061, 0.1492537313432836, None)\n",
      "35265.5988093\n",
      "0 2796\n",
      "93   (0.0, 0.0, 0.0, None)\n",
      "32610.6102397\n",
      "3 2793\n",
      "94   (0.66666666666666663, 0.01020408163265306, 0.020100502512562811, None)\n",
      "31523.1475385\n",
      "0 2796\n",
      "95   (0.0, 0.0, 0.0, None)\n",
      "31077.7859349\n",
      "723 2073\n",
      "96   (0.16320885200553251, 0.60204081632653061, 0.25680087051142547, None)\n",
      "30897.4463402\n",
      "8 2788\n",
      "97   (0.125, 0.0051020408163265302, 0.0098039215686274491, None)\n",
      "30972.7938503\n",
      "72 2724\n",
      "98   (0.27777777777777779, 0.10204081632653061, 0.1492537313432836, None)\n",
      "64744.6189722\n",
      "63 2733\n",
      "99   (0.15873015873015872, 0.051020408163265307, 0.077220077220077232, None)\n"
     ]
    }
   ],
   "source": [
    "# Batch \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "dev_data_size = len(dev_L_S_batch[0])\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.01),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "\n",
    "phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "        tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "predict_2 = tf.where(tf.greater(tf.slice(tf.nn.softmax(phi_out),[0,1],[dev_data_size,1]),0.5),\n",
    "                     tf.ones((dev_data_size,1)),tf.negative(tf.ones((dev_data_size,1))))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "\n",
    "for i in range(100):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    a,t,te_curr,_, = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "    print(te_curr)\n",
    "    \n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    #print(train_P_cap[0:5])\n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        de_curr,pl,pl2,_ = sess.run([loss,predict,predict_2,train_step],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl2 = pl2.flatten().tolist()\n",
    "        pl = pl.flatten().tolist()\n",
    "        predicted_labels = pl2\n",
    "        #predicted_labels = [-1 if x==0 else 1 for x in pl]\n",
    "        #for l,l2 in zip(predicted_labels,pl2):\n",
    "        #    print(l,l2)\n",
    "        \n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    " \n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22195\n",
      "0 -0.401594426868\n",
      "417 2379\n",
      "0   (0.24940047961630696, 0.53061224489795922, 0.33931484502446985, None)\n",
      "1 -0.500688143358\n",
      "2 -0.578497422522\n",
      "2381 415\n",
      "2   (0.038639227215455693, 0.46938775510204084, 0.071400853705859524, None)\n",
      "3 -0.636933421482\n",
      "4 -0.641951099343\n",
      "2259 537\n",
      "4   (0.036299247454625941, 0.41836734693877553, 0.066802443991853366, None)\n",
      "5 -0.680634831826\n",
      "6 -0.687206039868\n",
      "2043 753\n",
      "6   (0.036221243269701421, 0.37755102040816324, 0.066100937918713715, None)\n",
      "7 -0.662330001617\n",
      "8 -0.659663132481\n",
      "1896 900\n",
      "8   (0.036919831223628692, 0.35714285714285715, 0.066921606118546847, None)\n",
      "9 -0.689898574633\n",
      "10 -0.691601187187\n",
      "2306 490\n",
      "10   (0.034258456201214225, 0.40306122448979592, 0.063149480415667467, None)\n",
      "11 -0.656009155224\n",
      "12 -0.659744720069\n",
      "2182 614\n",
      "12   (0.037580201649862512, 0.41836734693877553, 0.068965517241379309, None)\n",
      "13 -0.6764219261\n",
      "14 -0.69048220233\n",
      "2059 737\n",
      "14   (0.037396794560466247, 0.39285714285714285, 0.068292682926829273, None)\n",
      "15 -0.661786407128\n",
      "16 -0.686124951202\n",
      "2286 510\n",
      "16   (0.036745406824146981, 0.42857142857142855, 0.0676873489121676, None)\n",
      "17 -0.689436848423\n",
      "18 -0.69244070044\n",
      "2058 738\n",
      "18   (0.037414965986394558, 0.39285714285714285, 0.068322981366459631, None)\n",
      "19 -0.6656604415\n",
      "20 -0.683553954767\n",
      "2315 481\n",
      "20   (0.038012958963282939, 0.44897959183673469, 0.070091596973317405, None)\n",
      "21 -0.686587462461\n",
      "22 -0.691299554719\n",
      "2057 739\n",
      "22   (0.037433155080213901, 0.39285714285714285, 0.068353306702174876, None)\n",
      "23 -0.666359622173\n",
      "24 -0.686592493879\n",
      "2208 588\n",
      "24   (0.036684782608695655, 0.41326530612244899, 0.067387687188019976, None)\n",
      "25 -0.689972623168\n",
      "26 -0.692323945354\n",
      "2043 753\n",
      "26   (0.036710719530102791, 0.38265306122448978, 0.066994193836534169, None)\n",
      "27 -0.669450713964\n",
      "28 -0.66954375221\n",
      "2275 521\n",
      "28   (0.036043956043956042, 0.41836734693877553, 0.066369890732496967, None)\n",
      "29 -0.687601348448\n",
      "30 -0.690547565508\n",
      "2054 742\n",
      "30   (0.037487828627069134, 0.39285714285714285, 0.068444444444444447, None)\n",
      "31 -0.669730454492\n",
      "32 -0.68744078371\n",
      "2148 648\n",
      "32   (0.036778398510242089, 0.40306122448979592, 0.067406143344709901, None)\n",
      "33 -0.689382133816\n",
      "34 -0.687553738244\n",
      "2199 597\n",
      "34   (0.037744429286039112, 0.42346938775510207, 0.06931106471816284, None)\n",
      "35 -0.66898567058\n",
      "36 -0.684949365437\n",
      "2248 548\n",
      "36   (0.036032028469750892, 0.41326530612244899, 0.0662847790507365, None)\n",
      "37 -0.688158809328\n",
      "38 -0.690297890213\n",
      "2053 743\n",
      "38   (0.037506088650754991, 0.39285714285714285, 0.068474877723432637, None)\n",
      "39 -0.674772360135\n",
      "40 -0.687796050865\n",
      "2115 681\n",
      "40   (0.036406619385342787, 0.39285714285714285, 0.066637819125919512, None)\n",
      "41 -0.675128442529\n",
      "42 -0.686691157061\n",
      "2197 599\n",
      "42   (0.037323623122439691, 0.41836734693877553, 0.068533221897200167, None)\n",
      "43 -0.673811190234\n",
      "44 -0.685826812263\n",
      "2199 597\n",
      "44   (0.035470668485675309, 0.39795918367346939, 0.065135699373695205, None)\n",
      "45 -0.68914816062\n",
      "46 -0.687790615485\n",
      "2041 755\n",
      "46   (0.036746692797648209, 0.38265306122448978, 0.067054090299508262, None)\n",
      "47 -0.678424123886\n",
      "48 -0.681369918435\n",
      "2239 557\n",
      "48   (0.033943724877177311, 0.38775510204081631, 0.062422997946611908, None)\n",
      "49 -0.68638409413\n",
      "50 -0.685836134788\n",
      "2051 745\n",
      "50   (0.037055095075572891, 0.38775510204081631, 0.067645749888740545, None)\n",
      "51 -0.680152013936\n",
      "52 -0.686258363804\n",
      "2117 679\n",
      "52   (0.033538025507794049, 0.36224489795918369, 0.061392131431041941, None)\n",
      "53 -0.689657349526\n",
      "54 -0.685619756776\n",
      "2028 768\n",
      "54   (0.035502958579881658, 0.36734693877551022, 0.064748201438848921, None)\n",
      "55 -0.672311509452\n",
      "56 -0.677539100801\n",
      "2171 625\n",
      "56   (0.032243205895900504, 0.35714285714285715, 0.059146599070553439, None)\n",
      "57 -0.685113212645\n",
      "58 -0.685515162188\n",
      "2008 788\n",
      "58   (0.035358565737051796, 0.36224489795918369, 0.064428312159709622, None)\n",
      "59 -0.685470085977\n",
      "60 -0.687094958385\n",
      "2101 695\n",
      "60   (0.029985721085197526, 0.32142857142857145, 0.054854157596865474, None)\n",
      "61 -0.684963858036\n",
      "62 -0.678190586152\n",
      "2128 668\n",
      "62   (0.036184210526315791, 0.39285714285714285, 0.066265060240963861, None)\n",
      "63 -0.685848606762\n",
      "64 -0.684840076519\n",
      "2195 601\n",
      "64   (0.03234624145785877, 0.36224489795918369, 0.059389376829778337, None)\n",
      "65 -0.681964944132\n",
      "66 -0.686243978156\n",
      "1969 827\n",
      "66   (0.035043169121381411, 0.35204081632653061, 0.063741339491916862, None)\n",
      "67 -0.689673849118\n",
      "68 -0.688205758795\n",
      "2008 788\n",
      "68   (0.025896414342629483, 0.26530612244897961, 0.047186932849364802, None)\n",
      "69 -0.668650813994\n",
      "70 -0.672867896596\n",
      "2126 670\n",
      "70   (0.036218250235183443, 0.39285714285714285, 0.06632213608957796, None)\n",
      "71 -0.687957988982\n",
      "72 -0.689846682789\n",
      "2135 661\n",
      "72   (0.030444964871194378, 0.33163265306122447, 0.055770055770055768, None)\n",
      "73 -0.675230474219\n",
      "74 -0.687518896721\n",
      "1899 897\n",
      "74   (0.032648762506582413, 0.31632653061224492, 0.059188544152744625, None)\n",
      "75 -0.67942642605\n",
      "76 -0.690198934224\n",
      "2211 585\n",
      "76   (0.032564450474898234, 0.36734693877551022, 0.059825508932280842, None)\n",
      "77 -0.668071794885\n",
      "78 -0.686048638883\n",
      "2067 729\n",
      "78   (0.032414126753749398, 0.34183673469387754, 0.059213433495360152, None)\n",
      "79 -0.689461181077\n",
      "80 -0.692074247059\n",
      "1316 1480\n",
      "80   (0.022796352583586626, 0.15306122448979592, 0.039682539682539687, None)\n",
      "81 -0.649621371861\n",
      "82 -0.653818094925\n",
      "1750 1046\n",
      "82   (0.022285714285714287, 0.19897959183673469, 0.040082219938335051, None)\n",
      "83 -0.689812452169\n",
      "84 -0.686820619946\n",
      "2070 726\n",
      "84   (0.026086956521739129, 0.27551020408163263, 0.047661076787290375, None)\n",
      "85 -0.652614333862\n",
      "86 -0.689045129555\n",
      "1961 835\n",
      "86   (0.02855685874553799, 0.2857142857142857, 0.05192396847473342, None)\n",
      "87 -0.69297099284\n",
      "88 -0.685063681781\n",
      "1986 810\n",
      "88   (0.025176233635448138, 0.25510204081632654, 0.045829514207149404, None)\n",
      "89 -0.642259971567\n",
      "90 -0.658588517483\n",
      "1922 874\n",
      "90   (0.029656607700312174, 0.29081632653061223, 0.053824362606232287, None)\n",
      "91 -0.685830400193\n",
      "92 -0.678363616484\n",
      "2059 737\n",
      "92   (0.032054395337542493, 0.33673469387755101, 0.058536585365853655, None)\n",
      "93 -0.68557805412\n",
      "94 -0.69054956206\n",
      "1513 1283\n",
      "94   (0.023793787177792465, 0.18367346938775511, 0.042129900526623756, None)\n",
      "95 -0.673446924716\n",
      "96 -0.669813499611\n",
      "1738 1058\n",
      "96   (0.031645569620253167, 0.28061224489795916, 0.056876938986556366, None)\n",
      "97 -0.6898162119\n",
      "98 -0.690187837642\n",
      "1707 1089\n",
      "98   (0.022847100175746926, 0.19897959183673469, 0.040987913820283758, None)\n",
      "99 -0.673708329493\n",
      "100 -0.6762805959\n",
      "1890 906\n",
      "100   (0.02433862433862434, 0.23469387755102042, 0.044103547459252164, None)\n",
      "101 -0.674788059792\n",
      "102 -0.689572165374\n",
      "1836 960\n",
      "102   (0.026143790849673203, 0.24489795918367346, 0.047244094488188976, None)\n",
      "103 -0.663479475807\n",
      "104 -0.673592707537\n",
      "2001 795\n",
      "104   (0.028485757121439279, 0.29081632653061223, 0.05188893946290396, None)\n",
      "105 -0.6881210735\n",
      "106 -0.690908744703\n",
      "1288 1508\n",
      "106   (0.024844720496894408, 0.16326530612244897, 0.043126684636118594, None)\n",
      "107 -0.653043194206\n",
      "108 -0.66821547364\n",
      "1725 1071\n",
      "108   (0.024347826086956521, 0.21428571428571427, 0.04372722540343571, None)\n",
      "109 -0.689873493223\n",
      "110 -0.686880292306\n",
      "1860 936\n",
      "110   (0.02903225806451613, 0.27551020408163263, 0.052529182879377433, None)\n",
      "111 -0.673356836509\n",
      "112 -0.6872946737\n",
      "1808 988\n",
      "112   (0.023783185840707963, 0.21938775510204081, 0.04291417165668663, None)\n",
      "113 -0.688697988971\n",
      "114 -0.680868810489\n",
      "1954 842\n",
      "114   (0.031218014329580348, 0.31122448979591838, 0.05674418604651163, None)\n",
      "115 -0.669867217894\n",
      "116 -0.686374623086\n",
      "1831 965\n",
      "116   (0.024030584380120151, 0.22448979591836735, 0.043413912185495798, None)\n",
      "117 -0.682465265903\n",
      "118 -0.687233214677\n",
      "1557 1239\n",
      "118   (0.023121387283236993, 0.18367346938775511, 0.041072447233314317, None)\n",
      "119 -0.669303295441\n",
      "120 -0.688486239548\n",
      "1273 1523\n",
      "120   (0.02513747054202671, 0.16326530612244897, 0.043567052416609936, None)\n",
      "121 -0.663607363798\n",
      "122 -0.66384147176\n",
      "1716 1080\n",
      "122   (0.026806526806526808, 0.23469387755102042, 0.048117154811715482, None)\n",
      "123 -0.661519548605\n",
      "124 -0.688590135549\n",
      "1284 1512\n",
      "124   (0.024922118380062305, 0.16326530612244897, 0.043243243243243239, None)\n",
      "125 -0.66204286077\n",
      "126 -0.688791823798\n",
      "1145 1651\n",
      "126   (0.020960698689956331, 0.12244897959183673, 0.035794183445190156, None)\n",
      "127 -0.65180362595\n",
      "128 -0.652574765485\n",
      "1385 1411\n",
      "128   (0.025270758122743681, 0.17857142857142858, 0.044275774826059454, None)\n",
      "129 -0.644066813479\n",
      "130 -0.684744539359\n",
      "1706 1090\n",
      "130   (0.022860492379835874, 0.19897959183673469, 0.041009463722397478, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-46a58c727ce5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mte_prev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mtotal_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_L_S_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch with cross entropy logits function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "dev_data_size = len(dev_L_S_batch[0])\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.01),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "\n",
    "phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "# pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=phi_out))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\n",
    "        # + tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "        \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "predict_2 = tf.where(tf.greater(tf.slice(tf.nn.softmax(phi_out),[0,1],[dev_data_size,1]),0.5),\n",
    "                    tf.ones((dev_data_size,1)),tf.negative(tf.ones((dev_data_size,1))))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "\n",
    "for i in range(1000):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    a,t,te_curr,_, = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "    print(i,te_curr)\n",
    "    \n",
    "    \n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    #print(train_P_cap[0:5])\n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        \n",
    "        te_curr,pl,pl2,_ = sess.run([loss,predict,predict_2,train_step],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl2 = pl2.flatten().tolist()\n",
    "        pl = pl.flatten().tolist()\n",
    "        #print(te_curr)\n",
    "        #predicted_labels = pl2\n",
    "        predicted_labels = [-1 if x==0 else 1 for x in pl]\n",
    "        #for l,l2 in zip(predicted_labels,pl2):\n",
    "        #    print(l,l2)\n",
    "        \n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    print()\n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22195\n",
      "290.346368803\n",
      "1219 1577\n",
      "0   (0.074651353568498766, 0.4642857142857143, 0.1286219081272085, None)\n",
      "284.002635079\n",
      "355 2441\n",
      "1   (0.27887323943661974, 0.50510204081632648, 0.3593466424682395, None)\n",
      "304.453121938\n",
      "2 2794\n",
      "2   (0.0, 0.0, 0.0, None)\n",
      "348.263158932\n",
      "1272 1524\n",
      "3   (0.080974842767295593, 0.52551020408163263, 0.14032697547683923, None)\n",
      "311.541316115\n",
      "2161 635\n",
      "4   (0.046274872744099957, 0.51020408163265307, 0.084853627492575315, None)\n",
      "321.637043561\n",
      "0 2796\n",
      "5   (0.0, 0.0, 0.0, None)\n",
      "286.048078\n",
      "2171 625\n",
      "6   (0.03408567480423768, 0.37755102040816324, 0.062526404731727922, None)\n",
      "280.253947714\n",
      "2049 747\n",
      "7   (0.03611517813567594, 0.37755102040816324, 0.065924276169265036, None)\n",
      "343.327110578\n",
      "19 2777\n",
      "8   (0.42105263157894735, 0.040816326530612242, 0.074418604651162776, None)\n",
      "279.757595639\n",
      "714 2082\n",
      "9   (0.16246498599439776, 0.59183673469387754, 0.25494505494505498, None)\n",
      "311.660362446\n",
      "1734 1062\n",
      "10   (0.03690888119953864, 0.32653061224489793, 0.066321243523316059, None)\n",
      "346.3228319\n",
      "766 2030\n",
      "11   (0.073107049608355096, 0.2857142857142857, 0.11642411642411642, None)\n",
      "293.311590152\n",
      "0 2796\n",
      "12   (0.0, 0.0, 0.0, None)\n",
      "281.239216973\n",
      "2184 612\n",
      "13   (0.037545787545787544, 0.41836734693877553, 0.068907563025210089, None)\n",
      "385.070217261\n",
      "72 2724\n",
      "14   (0.27777777777777779, 0.10204081632653061, 0.1492537313432836, None)\n",
      "280.350182139\n",
      "1402 1394\n",
      "15   (0.029243937232524966, 0.20918367346938777, 0.05131414267834794, None)\n",
      "287.283498597\n",
      "1675 1121\n",
      "16   (0.030447761194029851, 0.26020408163265307, 0.054516301443078563, None)\n",
      "349.622473157\n",
      "2 2794\n",
      "17   (0.0, 0.0, 0.0, None)\n",
      "281.796958176\n",
      "1706 1090\n",
      "18   (0.032825322391559206, 0.2857142857142857, 0.058885383806519462, None)\n",
      "300.740391411\n",
      "1232 1564\n",
      "19   (0.077110389610389615, 0.48469387755102039, 0.13305322128851543, None)\n",
      "295.964536627\n",
      "1637 1159\n",
      "20   (0.027489309712889431, 0.22959183673469388, 0.049099836333878891, None)\n",
      "328.06306921\n",
      "1682 1114\n",
      "21   (0.03151010701545779, 0.27040816326530615, 0.056443024494142707, None)\n",
      "280.623907187\n",
      "2397 399\n",
      "22   (0.037129745515227366, 0.45408163265306123, 0.068646355572695716, None)\n",
      "507.819953017\n",
      "0 2796\n",
      "23   (0.0, 0.0, 0.0, None)\n",
      "288.975777739\n",
      "1458 1338\n",
      "24   (0.033607681755829906, 0.25, 0.059250302297460707, None)\n",
      "28563.1074144\n",
      "2 2794\n",
      "25   (0.5, 0.0051020408163265302, 0.010101010101010102, None)\n",
      "357.638789855\n",
      "1264 1532\n",
      "26   (0.080696202531645569, 0.52040816326530615, 0.13972602739726028, None)\n",
      "332.127963432\n",
      "1081 1715\n",
      "27   (0.01757631822386679, 0.096938775510204078, 0.02975724353954581, None)\n",
      "324.455061665\n",
      "1232 1564\n",
      "28   (0.077110389610389615, 0.48469387755102039, 0.13305322128851543, None)\n",
      "301.024661825\n",
      "0 2796\n",
      "29   (0.0, 0.0, 0.0, None)\n",
      "473.075028045\n",
      "2035 761\n",
      "30   (0.039803439803439804, 0.41326530612244899, 0.07261317794710892, None)\n",
      "285.191274968\n",
      "505 2291\n",
      "31   (0.20396039603960395, 0.52551020408163263, 0.29386590584878747, None)\n",
      "288.45316364\n",
      "792 2004\n",
      "32   (0.095959595959595953, 0.38775510204081631, 0.15384615384615383, None)\n",
      "21686.5985593\n",
      "0 2796\n",
      "33   (0.0, 0.0, 0.0, None)\n",
      "286.103722032\n",
      "1998 798\n",
      "34   (0.037537537537537538, 0.38265306122448978, 0.068368277119416593, None)\n",
      "412.411580775\n",
      "1909 887\n",
      "35   (0.027239392352016764, 0.26530612244897961, 0.049406175771971497, None)\n",
      "444.777033225\n",
      "72 2724\n",
      "36   (0.27777777777777779, 0.10204081632653061, 0.1492537313432836, None)\n",
      "289.124967072\n",
      "1 2795\n",
      "37   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "282.323290396\n",
      "2029 767\n",
      "38   (0.037949728930507638, 0.39285714285714285, 0.069213483146067414, None)\n",
      "302.784599574\n",
      "71 2725\n",
      "39   (0.26760563380281688, 0.096938775510204078, 0.14232209737827714, None)\n",
      "282.154033596\n",
      "2109 687\n",
      "40   (0.034139402560455195, 0.36734693877551022, 0.062472885032537971, None)\n",
      "280.16513786\n",
      "409 2387\n",
      "41   (0.048899755501222497, 0.10204081632653061, 0.066115702479338845, None)\n",
      "289.236286814\n",
      "538 2258\n",
      "42   (0.098513011152416355, 0.27040816326530615, 0.1444141689373297, None)\n",
      "352.610633523\n",
      "1990 806\n",
      "43   (0.034170854271356785, 0.34693877551020408, 0.062214089661482161, None)\n",
      "341.340419123\n",
      "590 2206\n",
      "44   (0.083050847457627114, 0.25, 0.12468193384223918, None)\n",
      "1437.01685114\n",
      "71 2725\n",
      "45   (0.26760563380281688, 0.096938775510204078, 0.14232209737827714, None)\n",
      "524.40324839\n",
      "48 2748\n",
      "46   (0.22916666666666666, 0.056122448979591837, 0.090163934426229511, None)\n",
      "293.634479232\n",
      "638 2158\n",
      "47   (0.043887147335423198, 0.14285714285714285, 0.067146282973621102, None)\n",
      "316.547570793\n",
      "0 2796\n",
      "48   (0.0, 0.0, 0.0, None)\n",
      "323.538681419\n",
      "0 2796\n",
      "49   (0.0, 0.0, 0.0, None)\n",
      "828.360295602\n",
      "0 2796\n",
      "50   (0.0, 0.0, 0.0, None)\n",
      "281.799751722\n",
      "2227 569\n",
      "51   (0.033677593174674447, 0.38265306122448978, 0.061906727197688811, None)\n",
      "297.195111792\n",
      "981 1815\n",
      "52   (0.017329255861365953, 0.08673469387755102, 0.028887000849617671, None)\n",
      "285.628743231\n",
      "1208 1588\n",
      "53   (0.071192052980132453, 0.43877551020408162, 0.12250712250712251, None)\n",
      "285.692910128\n",
      "1082 1714\n",
      "54   (0.018484288354898338, 0.10204081632653061, 0.031298904538341159, None)\n",
      "301.059823614\n",
      "1 2795\n",
      "55   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "278.608662401\n",
      "462 2334\n",
      "56   (0.22294372294372294, 0.52551020408163263, 0.31306990881458963, None)\n",
      "283.605102213\n",
      "311 2485\n",
      "57   (0.30225080385852088, 0.47959183673469385, 0.3708086785009862, None)\n",
      "352.147371444\n",
      "72 2724\n",
      "58   (0.27777777777777779, 0.10204081632653061, 0.1492537313432836, None)\n",
      "298.411401439\n",
      "1254 1542\n",
      "59   (0.076555023923444973, 0.48979591836734693, 0.13241379310344828, None)\n",
      "282.866259851\n",
      "2006 790\n",
      "60   (0.033898305084745763, 0.34693877551020408, 0.061762034514078114, None)\n",
      "293.652375713\n",
      "0 2796\n",
      "61   (0.0, 0.0, 0.0, None)\n",
      "291.944918774\n",
      "719 2077\n",
      "62   (0.1835883171070932, 0.67346938775510201, 0.28852459016393445, None)\n",
      "363.205526158\n",
      "0 2796\n",
      "63   (0.0, 0.0, 0.0, None)\n",
      "387.366717578\n",
      "4 2792\n",
      "64   (0.25, 0.0051020408163265302, 0.0099999999999999985, None)\n",
      "280.008310781\n",
      "499 2297\n",
      "65   (0.20841683366733466, 0.53061224489795922, 0.29928057553956833, None)\n",
      "372.893996949\n",
      "1998 798\n",
      "66   (0.037037037037037035, 0.37755102040816324, 0.067456700091157701, None)\n",
      "414.619448205\n",
      "71 2725\n",
      "67   (0.26760563380281688, 0.096938775510204078, 0.14232209737827714, None)\n",
      "293.632205\n",
      "1965 831\n",
      "68   (0.02748091603053435, 0.27551020408163263, 0.049976862563627947, None)\n",
      "327.536473145\n",
      "451 2345\n",
      "69   (0.1130820399113082, 0.26020408163265307, 0.15765069551777433, None)\n",
      "338.921730566\n",
      "397 2399\n",
      "70   (0.17884130982367757, 0.36224489795918369, 0.23946037099494097, None)\n",
      "334.619844\n",
      "165 2631\n",
      "71   (0.14545454545454545, 0.12244897959183673, 0.1329639889196676, None)\n",
      "1199.31124795\n",
      "71 2725\n",
      "72   (0.26760563380281688, 0.096938775510204078, 0.14232209737827714, None)\n",
      "285.702292286\n",
      "720 2076\n",
      "73   (0.16250000000000001, 0.59693877551020413, 0.25545851528384284, None)\n",
      "282.411900941\n",
      "488 2308\n",
      "74   (0.20491803278688525, 0.51020408163265307, 0.29239766081871343, None)\n",
      "295.867433111\n",
      "0 2796\n",
      "75   (0.0, 0.0, 0.0, None)\n",
      "355.648156308\n",
      "72 2724\n",
      "76   (0.27777777777777779, 0.10204081632653061, 0.1492537313432836, None)\n",
      "285.305887851\n",
      "2437 359\n",
      "77   (0.03939269593762823, 0.48979591836734693, 0.072920622863653625, None)\n",
      "281.261578154\n",
      "2543 253\n",
      "78   (0.048368069209594966, 0.62755102040816324, 0.089813800657174148, None)\n",
      "299.432199212\n",
      "2424 372\n",
      "79   (0.040429042904290426, 0.5, 0.074809160305343514, None)\n",
      "388.723840176\n",
      "766 2030\n",
      "80   (0.16057441253263707, 0.62755102040816324, 0.25571725571725573, None)\n",
      "285.345353081\n",
      "360 2436\n",
      "81   (0.044444444444444446, 0.081632653061224483, 0.057553956834532377, None)\n",
      "308.847645071\n",
      "1273 1523\n",
      "82   (0.081696779261586805, 0.53061224489795922, 0.1415929203539823, None)\n",
      "330.247098535\n",
      "19 2777\n",
      "83   (0.42105263157894735, 0.040816326530612242, 0.074418604651162776, None)\n",
      "446.911552\n",
      "41 2755\n",
      "84   (0.21951219512195122, 0.045918367346938778, 0.075949367088607597, None)\n",
      "488.222296257\n",
      "1 2795\n",
      "85   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "383.053641516\n",
      "652 2144\n",
      "86   (0.081288343558282211, 0.27040816326530615, 0.12500000000000003, None)\n",
      "290.855701513\n",
      "1955 841\n",
      "87   (0.02710997442455243, 0.27040816326530615, 0.049279404927940494, None)\n",
      "287.174617707\n",
      "2183 613\n",
      "88   (0.044434264773247821, 0.49489795918367346, 0.081546868432114325, None)\n",
      "280.763071699\n",
      "2195 601\n",
      "89   (0.035079726651480639, 0.39285714285714285, 0.064408197406942702, None)\n",
      "293.658622485\n",
      "1171 1625\n",
      "90   (0.029888983774551667, 0.17857142857142858, 0.051207022677395762, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472.463619574\n",
      "53 2743\n",
      "91   (0.22641509433962265, 0.061224489795918366, 0.09638554216867469, None)\n",
      "281.904591202\n",
      "306 2490\n",
      "92   (0.29411764705882354, 0.45918367346938777, 0.35856573705179284, None)\n",
      "281.705339505\n",
      "2243 553\n",
      "93   (0.04101649576460098, 0.46938775510204084, 0.075440754407544081, None)\n",
      "283.972449163\n",
      "1220 1576\n",
      "94   (0.075409836065573776, 0.46938775510204084, 0.12994350282485875, None)\n",
      "297.019554735\n",
      "487 2309\n",
      "95   (0.20328542094455851, 0.50510204081632648, 0.28989751098096633, None)\n",
      "487.861798501\n",
      "1 2795\n",
      "96   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "300.84626108\n",
      "71 2725\n",
      "97   (0.26760563380281688, 0.096938775510204078, 0.14232209737827714, None)\n",
      "6530.06203551\n",
      "1 2795\n",
      "98   (1.0, 0.0051020408163265302, 0.01015228426395939, None)\n",
      "287.322629067\n",
      "1270 1526\n",
      "99   (0.080314960629921259, 0.52040816326530615, 0.13915416098226469, None)\n"
     ]
    }
   ],
   "source": [
    "# Mini Batch \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "bsize = 200 #batchsize\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "\n",
    "phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "        tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "    \n",
    "for i in range(100):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for mini_batch in get_mini_batches(train_L_S_batch,train_P_cap, bsize):\n",
    "        train_L_S_mini_batch, train_P_cap_mini_batch = mini_batch\n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_mini_batch,_p_cap:train_P_cap_mini_batch})\n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    total_te +=te_curr\n",
    "        #print(train_P_cap[0:5])\n",
    "    print(total_te)\n",
    "        \n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        de_curr,pl,_,_ = sess.run([loss,predict,train_step,check_op],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl = pl.flatten().tolist()\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22195\n",
      "-0.794671552905\n",
      "463 2333\n",
      "0   (0.23110151187904968, 0.54591836734693877, 0.32473444613050079, None)\n",
      "-2.24381679259\n",
      "2056 740\n",
      "1   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.64290641534\n",
      "737 2059\n",
      "2   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9364434017\n",
      "2056 740\n",
      "3   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49435299311\n",
      "737 2059\n",
      "4   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9108827454\n",
      "2056 740\n",
      "5   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49124703223\n",
      "737 2059\n",
      "6   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.911553435\n",
      "2056 740\n",
      "7   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117277983\n",
      "737 2059\n",
      "8   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115694847\n",
      "2056 740\n",
      "9   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117100278\n",
      "737 2059\n",
      "10   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698688\n",
      "2056 740\n",
      "11   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117096025\n",
      "737 2059\n",
      "12   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.911569878\n",
      "2056 740\n",
      "13   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095924\n",
      "737 2059\n",
      "14   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698782\n",
      "2056 740\n",
      "15   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "16   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "17   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "18   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "19   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "20   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "21   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "22   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "23   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "24   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "25   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "26   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "27   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "28   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "29   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "30   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "31   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "32   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "33   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "34   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "35   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "36   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "37   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "38   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "39   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "40   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "41   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "42   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "43   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "44   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "45   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "46   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "47   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "48   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "49   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "50   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "51   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "52   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "53   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "54   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "55   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "56   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "57   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "58   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "59   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "60   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "61   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "62   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "63   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "64   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "65   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "66   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "67   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "68   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "69   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "70   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "71   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "72   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "73   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "74   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "75   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "76   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "77   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "78   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "79   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "80   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "81   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "82   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.9115698783\n",
      "2056 740\n",
      "83   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "84   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "85   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "86   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "87   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "88   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "89   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "90   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "91   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "92   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "93   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "94   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "95   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "96   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "97   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n",
      "-2.49117095921\n",
      "737 2059\n",
      "98   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "-11.9115698783\n",
      "2056 740\n",
      "99   (0.037451361867704279, 0.39285714285714285, 0.068383658969804625, None)\n"
     ]
    }
   ],
   "source": [
    "# Mini Batch + cross entropy softmax func\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "bsize = 200 #batchsize\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "#additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "\n",
    "phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "    \n",
    "for i in range(100):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for mini_batch in get_mini_batches(train_L_S_batch,train_P_cap, bsize):\n",
    "        train_L_S_mini_batch, train_P_cap_mini_batch = mini_batch\n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_mini_batch,_p_cap:train_P_cap_mini_batch})\n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    total_te +=te_curr\n",
    "        #print(train_P_cap[0:5])\n",
    "    print(total_te)\n",
    "        \n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        de_curr,pl,_,_ = sess.run([loss,predict,train_step,check_op],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl = pl.flatten().tolist()\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "12169.569138\n",
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "14274.6893101\n",
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "15056.1713148\n",
      "2333 463\n",
      "0   (0.037719674239177027, 0.44897959183673469, 0.069592724396994862, None)\n",
      "15309.4902762\n",
      "2274 522\n",
      "0   (0.036059806508355323, 0.41836734693877553, 0.066396761133603238, None)\n",
      "15396.2089236\n",
      "2332 464\n",
      "0   (0.037735849056603772, 0.44897959183673469, 0.069620253164556958, None)\n",
      "15433.9701141\n",
      "2346 450\n",
      "0   (0.037510656436487641, 0.44897959183673469, 0.069236821400472076, None)\n",
      "15491.9463652\n",
      "2094 702\n",
      "0   (0.037249283667621778, 0.39795918367346939, 0.068122270742358076, None)\n",
      "15553.5927661\n",
      "2259 537\n",
      "0   (0.036299247454625941, 0.41836734693877553, 0.066802443991853366, None)\n",
      "15336.2876494\n",
      "2332 464\n",
      "0   (0.037735849056603772, 0.44897959183673469, 0.069620253164556958, None)\n",
      "15408.5408184\n"
     ]
    }
   ],
   "source": [
    "#stochastic\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "        tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "# loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "#          + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    New_P_cap = []\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        New_P_cap.append(newPcap)\n",
    "    train_P_cap = New_P_cap\n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    print(total_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-1.99821249928e+22\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-1.00643208716e+80\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-6.44163265644e+137\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-4.12842845955e+195\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16855524079320114, 0.6071428571428571, 0.26385809312638581, None)\n",
      "-2.48449820085e+253\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-315837f5ade0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m#print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mNew_P_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mnewPcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_p_cap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewPcap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[1;32m   1054\u001b[0m                          'graph before calling run().')\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mversion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2335\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2337\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2338\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-3.05101543903e+24\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-1.49612942222e+82\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-9.57592297318e+139\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-6.12903532452e+197\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.17638266068759342, 0.60204081632653061, 0.27283236994219651, None)\n",
      "-3.92286718621e+255\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + remove min(theta,0) in loss\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + additional layer\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    New_P_cap = []\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        New_P_cap.append(newPcap)\n",
    "    train_P_cap = New_P_cap\n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    print(total_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06887941393e+20\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.31678477484e+29\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.62112517295e+38\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.99590613383e+47\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "2.45715280858e+56\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "3.02509468656e+65\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "3.72430034577e+74\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-2ed4860f071d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_cap_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mtotal_te\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m#print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "\n",
    "\n",
    "#print(train_P_cap)\n",
    "\n",
    "# discrete_labels = predict_labels(train_P_cap[:1])\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "#train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_out = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "# additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22195\n",
      "-0.403062582953\n",
      "746 2050\n",
      "0   (0.16085790884718498, 0.61224489795918369, 0.25477707006369427, None)\n",
      "-0.491641866684\n",
      "810 1986\n",
      "1   (0.17037037037037037, 0.70408163265306123, 0.27435387673956263, None)\n",
      "-0.594699991089\n",
      "787 2009\n",
      "2   (0.15374841168996187, 0.61734693877551017, 0.24618514750762968, None)\n",
      "-0.584680700423\n",
      "1241 1555\n",
      "3   (0.11764705882352941, 0.74489795918367352, 0.20320111343075853, None)\n",
      "-0.629941925989\n",
      "1247 1549\n",
      "4   (0.10986367281475541, 0.69897959183673475, 0.18988218988218988, None)\n",
      "-0.610793114481\n",
      "899 1897\n",
      "5   (0.1546162402669633, 0.70918367346938771, 0.25388127853881282, None)\n",
      "-0.622043836336\n",
      "820 1976\n",
      "6   (0.14999999999999999, 0.62755102040816324, 0.24212598425196849, None)\n",
      "-0.617336477866\n",
      "1480 1316\n",
      "7   (0.095270270270270269, 0.71938775510204078, 0.16825775656324585, None)\n",
      "-0.618130318603\n",
      "818 1978\n",
      "8   (0.1687041564792176, 0.70408163265306123, 0.27218934911242604, None)\n",
      "-0.586582216415\n",
      "973 1823\n",
      "9   (0.14285714285714285, 0.70918367346938771, 0.23781009409751927, None)\n",
      "-0.607121678903\n",
      "786 2010\n",
      "10   (0.17557251908396945, 0.70408163265306123, 0.28105906313645623, None)\n",
      "-0.561990194009\n",
      "834 1962\n",
      "11   (0.14748201438848921, 0.62755102040816324, 0.23883495145631067, None)\n",
      "-0.572345405133\n",
      "983 1813\n",
      "12   (0.1353001017293998, 0.6785714285714286, 0.22561492790500426, None)\n",
      "-0.566924296964\n",
      "789 2007\n",
      "13   (0.15335868187579213, 0.61734693877551017, 0.24568527918781724, None)\n",
      "-0.435678966348\n",
      "779 2017\n",
      "14   (0.15661103979460847, 0.62244897959183676, 0.25025641025641027, None)\n",
      "-0.458495279874\n",
      "842 1954\n",
      "15   (0.13657957244655583, 0.58673469387755106, 0.22157996146435455, None)\n",
      "-0.449164983646\n",
      "809 1987\n",
      "16   (0.14956736711990112, 0.61734693877551017, 0.24079601990049751, None)\n",
      "-0.379867863066\n",
      "838 1958\n",
      "17   (0.13603818615751789, 0.58163265306122447, 0.22050290135396519, None)\n",
      "-0.369685197109\n",
      "750 2046\n",
      "18   (0.16, 0.61224489795918369, 0.2536997885835095, None)\n",
      "-0.227868806484\n",
      "745 2051\n",
      "19   (0.16107382550335569, 0.61224489795918369, 0.25504782146652494, None)\n",
      "-0.0355672342723\n",
      "752 2044\n",
      "20   (0.15957446808510639, 0.61224489795918369, 0.25316455696202533, None)\n",
      "0.108307225631\n",
      "750 2046\n",
      "21   (0.16, 0.61224489795918369, 0.2536997885835095, None)\n",
      "0.335109901527\n",
      "743 2053\n",
      "22   (0.16150740242261102, 0.61224489795918369, 0.25559105431309898, None)\n",
      "0.740705182006\n",
      "737 2059\n",
      "23   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.11050066556\n",
      "737 2059\n",
      "24   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.60991097477\n",
      "737 2059\n",
      "25   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "2.25530605435\n",
      "737 2059\n",
      "26   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "3.14536698583\n",
      "737 2059\n",
      "27   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "4.36727211905\n",
      "737 2059\n",
      "28   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "6.1403302575\n",
      "737 2059\n",
      "29   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "9.3665721913\n",
      "737 2059\n",
      "30   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "15.1515395344\n",
      "737 2059\n",
      "31   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "27.3734255678\n",
      "737 2059\n",
      "32   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "57.8044624384\n",
      "737 2059\n",
      "33   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "155.940639752\n",
      "737 2059\n",
      "34   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "639.915555032\n",
      "737 2059\n",
      "35   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "5813.47074629\n",
      "737 2059\n",
      "36   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "276713.418925\n",
      "737 2059\n",
      "37   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "467766797.392\n",
      "737 2059\n",
      "38   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.27592153959e+15\n",
      "737 2059\n",
      "39   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "9.48800648303e+27\n",
      "737 2059\n",
      "40   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "5.25972489458e+53\n",
      "737 2059\n",
      "41   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.61439546837e+105\n",
      "737 2059\n",
      "42   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "nan\n",
      "0 2796\n",
      "43   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "44   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "45   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "46   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "47   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "48   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "49   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "50   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "51   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "52   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "53   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "54   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "55   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "56   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "57   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "58   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "59   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "60   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "61   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "62   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "63   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "64   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "65   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "66   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "67   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "68   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "69   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "70   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "71   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "72   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "73   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "74   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "75   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "76   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "77   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "78   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "79   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "80   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "81   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "82   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "83   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "84   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "85   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "86   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "87   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "88   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "89   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "90   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "91   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "92   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "93   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "94   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "95   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "96   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "97   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "98   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "99   (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "# Batch with cross entropy logits function  + additional layer\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "dev_data_size = len(dev_L_S_batch[0])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "#train_P_cap = np.full([data_size,2],0.5)\n",
    "\n",
    "#print(train_P_cap)\n",
    "#print(train_P_cap.shape)\n",
    "#dev_P_cap = np.full([dev_data_size,2],0.5)\n",
    "\n",
    "\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.01),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "phi_p1 = tf.matmul(additional_layer_out,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(additional_layer_out),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "\n",
    "# phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "# phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "# pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "predict_2 = tf.where(tf.greater(tf.slice(tf.nn.softmax(phi_out),[0,1],[dev_data_size,1]),0.5),\n",
    "                    tf.ones((dev_data_size,1)),tf.negative(tf.ones((dev_data_size,1))))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "\n",
    "for i in range(100):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    a,t,te_curr,_, = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "    print(te_curr)\n",
    "    \n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    #print(train_P_cap[0:5])\n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        te_curr,pl,pl2,_ = sess.run([loss,predict,predict_2,train_step],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl2 = pl2.flatten().tolist()\n",
    "        pl = pl.flatten().tolist()\n",
    "        #print(te_curr)\n",
    "        #predicted_labels = pl2\n",
    "        predicted_labels = [-1 if x==0 else 1 for x in pl]\n",
    "        #for l,l2 in zip(predicted_labels,pl2):\n",
    "        #    print(l,l2)\n",
    "        \n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    " \n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 330.069609\n",
      "         Iterations: 20\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 23\n",
      "[ 3.29293033  0.80240776  3.54276763  3.57746115  2.57487472  2.54625167\n",
      "  4.16422647]\n",
      "train iteration: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c9a7610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c80f590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 4182.213294\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "[ 3.29293033  0.80240776  3.54276763  3.57746115  2.57487472  2.54625167\n",
      "  4.16422647]\n",
      "train iteration: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c9a7d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435bea9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n",
      "test:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f436ad14350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c117750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n"
     ]
    }
   ],
   "source": [
    "# All LF_Threshold =0.3 and softmax_Threshold=0.3 ,to be run\n",
    "train(2,Use_Confidence=False,theta_file_name=\"THETA\")\n",
    "\n",
    "test(THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    #plt.bar(range(0,2796),marginals)\n",
    "    #plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "#import cPickle as pickle\n",
    "#THETA = pickle.load( open( \"THETA.p\", \"rb\" ) )\n",
    "#test(THETA)\n",
    "#LAMDA,SCORE = get_LAMDA(dev_cands)\n",
    "#Confidence = get_Confidence(LAMDA)\n",
    "\n",
    "#P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "#marginals=get_marginals(P_cap)\n",
    "#plt.hist(marginals, bins=20)\n",
    "#plt.show()\n",
    "#plt.bar(range(0,888),train_marginals)\n",
    "#plt.show()\n",
    "\n",
    "print_details(\"dev set\",THETA,dev_LAMDA,dev_SCORE)\n",
    "predicted_labels=predict_labels(marginals)\n",
    "\n",
    "\n",
    "sorted_predicted_labels=[x for (y,x) in sorted(zip(Confidence,predicted_labels))] #sort Labels as per Confidence\n",
    "sorted_predicted_labels=list(reversed(sorted_predicted_labels))\n",
    "\n",
    "\n",
    "for i,j in enumerate(reversed(sorted(zip(Confidence,predicted_labels,gold_labels_dev)))):\n",
    "    if i>20:\n",
    "        break\n",
    "    print i,j\n",
    "#print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "#no_of_labels=186#int(len(predicted_labels)*0.1)  #54 - >0.2  , 108>= 0.15 , 186>= 0.12\n",
    "#print(len(sorted_predicted_labels[0:no_of_labels]))\n",
    "no_of_labels=2796\n",
    "score(predicted_labels[0:no_of_labels],gold_labels_dev[0:no_of_labels])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
