{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Here, we just set how many documents we'll process for automatic testing- you can safely ignore this!\n",
    "n_docs = 500 if 'CI' in os.environ else 2591\n",
    "\n",
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])\n",
    "\n",
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import load_external_labels\n",
    "\n",
    "#%time load_external_labels(session, Spouse, annotator_name='gold')\n",
    "\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "#L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, zero_one=True)\n",
    "#L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2697)\n",
      "(196, 2600)\n"
     ]
    }
   ],
   "source": [
    "#gold_labels_dev = [x[0,0] for x in L_gold_dev.todense()]\n",
    "#for i,L in enumerate(gold_labels_dev):\n",
    "#    print(i,gold_labels_dev[i])\n",
    "\n",
    "gold_labels_dev = []\n",
    "for i,L in enumerate(L_gold_dev):\n",
    "    gold_labels_dev.append(L[0,0])\n",
    "    \n",
    "gold_labels_test = []\n",
    "for i,L in enumerate(L_gold_test):\n",
    "    gold_labels_test.append(L[0,0])\n",
    "    \n",
    "print(len(gold_labels_dev),len(gold_labels_test))\n",
    "print(gold_labels_dev.count(1),gold_labels_dev.count(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.matutils as gm\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "model = KeyedVectors.load_word2vec_format('../glove_w2v.txt', binary=False)  # C binary format\n",
    "\n",
    "\n",
    "wordvec_unavailable= set()\n",
    "def write_to_file(wordvec_unavailable):\n",
    "    with open(\"wordvec_unavailable.txt\",\"w\") as f:\n",
    "        for word in wordvec_unavailable:\n",
    "            f.write(word+\"\\n\")\n",
    "\n",
    "def preprocess(tokens):\n",
    "    btw_words = [word for word in tokens if word not in STOPWORDS]\n",
    "    btw_words = [word for word in btw_words if word.isalpha()]\n",
    "    return btw_words\n",
    "\n",
    "def get_word_vectors(btw_words): # returns vector of embeddings of words\n",
    "    word_vectors= []\n",
    "    for word in btw_words:\n",
    "        try:\n",
    "            word_v = np.array(model[word])\n",
    "            word_v = word_v.reshape(len(word_v),1)\n",
    "            #print(word_v.shape)\n",
    "            word_vectors.append(model[word])\n",
    "        except:\n",
    "            wordvec_unavailable.add(word)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similarity(word_vectors,target_word): # sent(list of word vecs) to word similarity\n",
    "    similarity = 0\n",
    "    target_word_vector = 0\n",
    "    try:\n",
    "        target_word_vector = model[target_word]\n",
    "    except:\n",
    "        wordvec_unavailable.add(target_word+\" t\")\n",
    "        return similarity\n",
    "    target_word_sparse = gm.any2sparse(target_word_vector,eps=1e-09)\n",
    "    for wv in word_vectors:\n",
    "        wv_sparse = gm.any2sparse(wv, eps=1e-09)\n",
    "        similarity = max(similarity,gm.cossim(wv_sparse,target_word_sparse))\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Continuous ################\n",
    "\n",
    "softmax_Threshold = 0.3\n",
    "LF_Threshold = 0.3\n",
    "\n",
    "import re\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "spouses = {'spouse', 'wife', 'husband', 'ex-wife', 'ex-husband'}\n",
    "family = {'father', 'mother', 'sister', 'brother', 'son', 'daughter',\n",
    "              'grandfather', 'grandmother', 'uncle', 'aunt', 'cousin'}\n",
    "family = family | {f + '-in-law' for f in family}\n",
    "other = {'boyfriend', 'girlfriend' 'boss', 'employee', 'secretary', 'co-worker'}\n",
    "\n",
    "# Helper function to get last name\n",
    "def last_name(s):\n",
    "    name_parts = s.split(' ')\n",
    "    return name_parts[-1] if len(name_parts) > 1 else None    \n",
    "\n",
    "def LF_husband_wife(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for sw in spouses:\n",
    "        sc=max(sc,get_similarity(word_vectors,sw))\n",
    "    return (1,sc)\n",
    "\n",
    "def LF_husband_wife_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for sw in spouses:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,sw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for sw in spouses:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,sw))\n",
    "    return(1,max(sc_1,sc_2))\n",
    "    \n",
    "def LF_same_last_name(c):\n",
    "    p1_last_name = last_name(c.person1.get_span())\n",
    "    p2_last_name = last_name(c.person2.get_span())\n",
    "    if p1_last_name and p2_last_name and p1_last_name == p2_last_name:\n",
    "        if c.person1.get_span() != c.person2.get_span():\n",
    "            return (1,1)\n",
    "    return (0,0)\n",
    "\n",
    "def LF_no_spouse_in_sentence(c):\n",
    "    return (-1,0.75) if np.random.rand() < 0.75 and len(spouses.intersection(c.get_parent().words)) == 0 else (0,0)\n",
    "\n",
    "def LF_and_married(c):\n",
    "    global LF_Threshold\n",
    "    word_vectors = get_word_vectors(preprocess(get_right_tokens(c)))\n",
    "    sc = get_similarity(word_vectors,'married')\n",
    "    \n",
    "    if 'and' in get_between_tokens(c):\n",
    "        return (1,sc)\n",
    "    else:\n",
    "        return (0,0)\n",
    "\n",
    "def LF_familial_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for fw in family:\n",
    "        sc=max(sc,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_family_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc_1 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[0])))\n",
    "    for fw in family:\n",
    "        sc_1=max(sc_1,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    sc_2 = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c[1])))\n",
    "    for fw in family:\n",
    "        sc_2=max(sc_2,get_similarity(word_vectors,fw))\n",
    "        \n",
    "    return (-1,max(sc_1,sc_2))\n",
    "\n",
    "def LF_other_relationship(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_between_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "        \n",
    "    return (-1,sc) \n",
    "\n",
    "def LF_other_relationship_left_window(c):\n",
    "    global LF_Threshold\n",
    "    sc = 0\n",
    "    word_vectors = get_word_vectors(preprocess(get_left_tokens(c)))\n",
    "    for ow in other:\n",
    "        sc=max(sc,get_similarity(word_vectors,ow))\n",
    "    return (-1,sc) \n",
    "\n",
    "import bz2\n",
    "\n",
    "# Function to remove special characters from text\n",
    "def strip_special(s):\n",
    "    return ''.join(c for c in s if ord(c) < 128)\n",
    "\n",
    "# Read in known spouse pairs and save as set of tuples\n",
    "with bz2.BZ2File('data/spouses_dbpedia.csv.bz2', 'rb') as f:\n",
    "    known_spouses = set(\n",
    "        tuple(strip_special(x).strip().split(',')) for x in f.readlines()\n",
    "    )\n",
    "# Last name pairs for known spouses\n",
    "last_names = set([(last_name(x), last_name(y)) for x, y in known_spouses if last_name(x) and last_name(y)])\n",
    "    \n",
    "def LF_distant_supervision(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    return (1,1) if (p1, p2) in known_spouses or (p2, p1) in known_spouses else (0,0)\n",
    "\n",
    "def LF_distant_supervision_last_names(c):\n",
    "    p1, p2 = c.person1.get_span(), c.person2.get_span()\n",
    "    p1n, p2n = last_name(p1), last_name(p2)\n",
    "    return (1,1) if (p1 != p2) and ((p1n, p2n) in last_names or (p2n, p1n) in last_names) else (0,1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def LF_Three_Lists_Left_Window(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife_left_window(c)\n",
    "    c2,s2 = LF_family_left_window(c)\n",
    "    c3,s3 = LF_other_relationship_left_window(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'Left:',s1,s2,s3,prob_sc\n",
    "    \n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "\n",
    "def LF_Three_Lists_Between_Words(c):\n",
    "    global softmax_Threshold\n",
    "    c1,s1 = LF_husband_wife(c)\n",
    "    c2,s2 = LF_familial_relationship(c)\n",
    "    c3,s3 = LF_other_relationship(c)\n",
    "    sc = np.array([s1,s2,s3])\n",
    "    c = [c1,c2,c3]\n",
    "    sharp_param = 1.5\n",
    "    \n",
    "    prob_sc = np.exp(sc * sharp_param - np.max(sc))\n",
    "    prob_sc = prob_sc / np.sum(prob_sc)\n",
    "    #print 'BW:',s1,s2,s3,prob_sc\n",
    "    if s1==s2 or s3==s1:\n",
    "        return (0,0)\n",
    "    return c[np.argmax(prob_sc)],1\n",
    "    \n",
    "LFs = [LF_distant_supervision, LF_distant_supervision_last_names,LF_same_last_name,\n",
    "       LF_and_married, LF_Three_Lists_Between_Words,LF_Three_Lists_Left_Window, LF_no_spouse_in_sentence\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LFs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-53f30544a463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mTHETA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLFs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPHIj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLAMDAi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSCOREi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LFs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def PHI(K,LAMDAi,SCOREi):\n",
    "    return [K*l*s for (l,s) in zip(LAMDAi,SCOREi)]\n",
    "\n",
    "def softmax(THETA,LAMDAi,SCOREi):\n",
    "    x = []\n",
    "    for k in [1,-1]:\n",
    "        product = np.dot(PHI(k,LAMDAi,SCOREi),THETA)\n",
    "        x.append(product)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def function_conf(THETA,LAMDA,P_cap,Confidence):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for LAMDAi in LAMDA:\n",
    "        s = s + Confidence[i]*np.dot(np.log(softmax(THETA,LAMDAi)),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def function(THETA,LAMDA,SCORE,P_cap):\n",
    "    s = 0.0\n",
    "    i = 0\n",
    "    for i in range(len(LAMDA)):\n",
    "        s = s + np.dot(np.log(softmax(THETA,LAMDA[i],SCORE[i])),P_cap[i])\n",
    "        i = i+1\n",
    "    return -s\n",
    "\n",
    "def P_K_Given_LAMDAi_THETA(K,THETA,LAMDAi,SCOREi):\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    if(K==1):\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x[1]\n",
    "      \n",
    "\n",
    "np.random.seed(78)\n",
    "THETA = np.random.rand(len(LFs),1)\n",
    "\n",
    "def PHIj(j,K,LAMDAi,SCOREi):\n",
    "    return LAMDAi[j]*K*SCOREi[j]\n",
    "\n",
    "def RIGHT(j,LAMDAi,SCOREi,THETA):\n",
    "    phi = []\n",
    "    for k in [1,-1]:\n",
    "        phi.append(PHIj(j,k,LAMDAi,SCOREi))\n",
    "    x = softmax(THETA,LAMDAi,SCOREi)\n",
    "    return np.dot(phi,x)\n",
    "    \n",
    "\n",
    "def function_conf_der(THETA,LAMDA,P_cap,Confidence):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for LAMDAi in LAMDA:\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + Confidence[i]*(PHIj(j,K,LAMDAi)-RIGHT(j,LAMDAi,THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "def function_der(THETA,LAMDA,SCORE,P_cap):\n",
    "    der = []\n",
    "    for j in range(len(THETA)):\n",
    "        i = 0\n",
    "        s = 0.0\n",
    "        for index in range(len(LAMDA)):\n",
    "            p = 0\n",
    "            for K in [1,-1]:\n",
    "                s = s + (PHIj(j,K,LAMDA[index],SCORE[index])-RIGHT(j,LAMDA[index],SCORE[index],THETA))*P_cap[i][p]\n",
    "                p = p+1\n",
    "            i = i+1\n",
    "        der.append(-s)\n",
    "    return np.array(der)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_LAMDA(cands):\n",
    "    LAMDA = []\n",
    "    SCORE = []\n",
    "    for ci in cands:\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        LAMDA.append(L)\n",
    "        SCORE.append(S) \n",
    "    return LAMDA,SCORE\n",
    "\n",
    "def get_Confidence(LAMDA):\n",
    "    confidence = []\n",
    "    for L in LAMDA:\n",
    "        Total_L = float(len(L))\n",
    "        No_zeros = L.count(0)\n",
    "        No_Non_Zeros = Total_L - No_zeros\n",
    "        confidence.append(No_Non_Zeros/Total_L)\n",
    "    return confidence    \n",
    "    \n",
    "def get_Initial_P_cap(LAMDA):\n",
    "    P_cap = []\n",
    "    for L in LAMDA:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "    #print(np.array(LAMDA))\n",
    "    #print(np.array(P_cap))append(L)\n",
    "    #LAMDA=np.array(LAMDA).astype(int)\n",
    "    #P_cap=np.array(P_cap)\n",
    "    #print(np.array(LAMDA).shape)\n",
    "    #print(np.array(P_cap).shape)\n",
    "    #print(L)\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"No.Os\",L.count(0),\"No.1s\",L.count(1),\"No.-1s\",L.count(-1))\n",
    "    #print(ci.chemical.get_span(),ci.disease.get_span(),\"P(0):\",L.count(0)/len(L),\" P(1)\",L.count(1)/len(L),\"P(-1)\",L.count(-1)/len(L))\n",
    "\n",
    "        \n",
    "def get_P_cap(LAMDA,SCORE,THETA):\n",
    "    P_cap = []\n",
    "    for i in range(len(LAMDA)):\n",
    "        P_capi = softmax(THETA,LAMDA[i],SCORE[i])\n",
    "        P_cap.append(P_capi)\n",
    "    return P_cap\n",
    "\n",
    "\n",
    "def score(predicted_labels,gold_labels):\n",
    "    tp =0.0\n",
    "    tn =0.0\n",
    "    fp =0.0\n",
    "    fn =0.0\n",
    "    for i in range(len(gold_labels)):\n",
    "        if(predicted_labels[i]==gold_labels[i]):\n",
    "            if(predicted_labels[i]==1):\n",
    "                tp=tp+1\n",
    "            else:\n",
    "                tn=tn+1\n",
    "        else:\n",
    "            if(predicted_labels[i]==1):\n",
    "                fp=fp+1\n",
    "            else:\n",
    "                fn=fn+1\n",
    "    print(\"tp\",tp,\"tn\",tn,\"fp\",fp,\"fn\",fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = (2*precision*recall)/(precision+recall)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"F1 score:\",f1score)\n",
    "                \n",
    "           \n",
    "    \n",
    "from scipy.optimize import minimize\n",
    "import cPickle as pickle\n",
    "\n",
    "def get_marginals(P_cap):\n",
    "    marginals = []\n",
    "    for P_capi in P_cap:\n",
    "        marginals.append(P_capi[0])\n",
    "    return marginals\n",
    "\n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    plt.bar(range(0,2796),marginals)\n",
    "    plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "    \n",
    "    \n",
    "def train(No_Iter,Use_Confidence=True,theta_file_name=\"THETA\"):\n",
    "    global THETA\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    LAMDA,SCORE = get_LAMDA(train_cands)\n",
    "    P_cap = get_Initial_P_cap(LAMDA)\n",
    "    Confidence = get_Confidence(LAMDA)\n",
    "    for iteration in range(No_Iter):\n",
    "        if(Use_Confidence==True):\n",
    "            res = minimize(function_conf,THETA,args=(LAMDA,P_cap,Confidence), method='BFGS',jac=function_conf_der,options={'disp': True, 'maxiter':20}) #nelder-mead\n",
    "        else:\n",
    "            res = minimize(function,THETA,args=(LAMDA,SCORE,P_cap), method='BFGS',jac=function_der,options={'disp': True, 'maxiter':20}) #nelder-mead            \n",
    "        THETA = res.x # new THETA\n",
    "        print(THETA)\n",
    "        P_cap = get_P_cap(LAMDA,SCORE,THETA) #new p_cap \n",
    "        print_details(\"train iteration: \"+str(iteration),THETA,dev_LAMDA,dev_SCORE)\n",
    "        #score(predicted_labels,gold_labels)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Train_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Train_P_cap.p\",\"wb\"))\n",
    "    NP_THETA = np.array(THETA)\n",
    "    np.savetxt(theta_file_name+'.txt', NP_THETA, fmt='%f') \n",
    "    pickle.dump( NP_THETA, open( theta_file_name+'.p', \"wb\" )) # save the file as \"outfile_name.npy\" \n",
    "\n",
    "        \n",
    "def test(THETA):\n",
    "    global dev_LAMDA,dev_SCORE\n",
    "    P_cap = get_P_cap(dev_LAMDA,dev_SCORE,THETA)\n",
    "    print_details(\"test:\",THETA,dev_LAMDA,dev_SCORE)\n",
    "    NP_P_cap = np.array(P_cap)\n",
    "    np.savetxt('Dev_P_cap.txt', NP_P_cap, fmt='%f')\n",
    "    pickle.dump(NP_P_cap,open(\"Dev_P_cap.p\",\"wb\"))\n",
    "                    \n",
    "def load_marginals(s):\n",
    "    marginals = []\n",
    "    if(s==\"train\"):\n",
    "        train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "        marginals = train_P_cap[:,0]\n",
    "    return marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' output:\n",
    "\n",
    "    [[[L_x1],[S_x1]],\n",
    "     [[L_x2],[S_x2]],\n",
    "     ......\n",
    "     ......\n",
    "    ]\n",
    "\n",
    "'''\n",
    "def get_L_S_Tensor(cands): \n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        L_S_ci=[]\n",
    "        L=[]\n",
    "        S=[]\n",
    "        P_ik = []\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            L.append(l)\n",
    "            S.append((s+1)/2)  #to scale scores in [0,1] \n",
    "        L_S_ci.append(L)\n",
    "        L_S_ci.append(S)\n",
    "        L_S.append(L_S_ci) \n",
    "    return L_S\n",
    "\n",
    "def get_L_S(cands):  # sign gives label abs value gives score\n",
    "    \n",
    "    L_S = []\n",
    "    for ci in cands:\n",
    "        l_s=[]\n",
    "        for LF in LFs:\n",
    "            #print LF.__name__\n",
    "            l,s = LF(ci)\n",
    "            s= (s+1)/2  #to scale scores in [0,1] \n",
    "            l_s.append(l*s)\n",
    "        L_S.append(l_s)\n",
    "    return L_S\n",
    "\n",
    "def get_Initial_P_cap_L_S(L_S):\n",
    "    P_cap = []\n",
    "    for L,S in L_S:\n",
    "        P_ik = []\n",
    "        denominator=float(L.count(1)+L.count(-1))\n",
    "        if(denominator==0):\n",
    "            denominator=1\n",
    "        P_ik.append(L.count(1)/denominator)\n",
    "        P_ik.append(L.count(-1)/denominator)\n",
    "        P_cap.append(P_ik)\n",
    "    return P_cap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "   \n",
    "    \n",
    "# dev_L_S = get_L_S_Tensor(dev_cands)\n",
    "# train_L_S = get_L_S_Tensor(train_cands)\n",
    "# test_L_S = get_L_S_Tensor(test_cands)\n",
    "\n",
    "\n",
    "# train_P_cap= get_Initial_P_cap_L_S(train_L_S) \n",
    "\n",
    "# dev_P_cap = get_Initial_P_cap_L_S(dev_L_S)\n",
    "\n",
    "# test_P_cap = get_Initial_P_cap_L_S(test_L_S)\n",
    "\n",
    "# import cPickle as pkl\n",
    "\n",
    "# pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "# pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "# pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "# pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "# pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare batch data\n",
    "train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "train_P_cap_batch,dev_P_cap_batch = get_P_cap_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import cPickle as pkl\n",
    "\n",
    "\n",
    "#pkl.dump(dev_L_S,open(\"dev_L_S.p\",\"wb\"))\n",
    "#pkl.dump(train_L_S,open(\"train_L_S.p\",\"wb\"))\n",
    "#pkl.dump(test_L_S,open(\"test_L_S.p\",\"wb\"))\n",
    "\n",
    "#pkl.dump(train_P_cap,open(\"train_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(dev_P_cap,open(\"dev_P_cap.p\",\"wb\"))\n",
    "#pkl.dump(test_P_cap,open(\"test_P_cap.p\",\"wb\"))\n",
    "\n",
    "dev_L_S = pkl.load( open( \"dev_L_S.p\", \"rb\" ) )\n",
    "train_L_S = pkl.load( open( \"train_L_S.p\", \"rb\" ) )\n",
    "test_L_S = pkl.load( open( \"test_L_S.p\", \"rb\" ) )\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "test_P_cap = pkl.load( open( \"test_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "def get_L_S_batch():\n",
    "    dev_L_batch = []\n",
    "    dev_S_batch = []\n",
    "    dev_L_S_batch = []\n",
    "    train_L_batch = []\n",
    "    train_S_batch = []\n",
    "    train_L_S_batch = []\n",
    "    for l,s in train_L_S:\n",
    "        train_L_batch.append(l)\n",
    "        train_S_batch.append(s)\n",
    "    train_L_S_batch = [train_L_batch, train_S_batch]\n",
    "    for l,s in dev_L_S:\n",
    "        dev_L_batch.append(l)\n",
    "        dev_S_batch.append(s)\n",
    "    dev_L_S_batch = [dev_L_batch, dev_S_batch]\n",
    "    return train_L_S_batch,dev_L_S_batch\n",
    "\n",
    "\n",
    "def get_P_cap_batch():\n",
    "    kp1_train= []\n",
    "    kn1_train = []\n",
    "    kp1_dev= []\n",
    "    kn1_dev = []\n",
    "    for pci in train_P_cap:\n",
    "        kp1_train.append(pci[0])\n",
    "        kn1_train.append(pci[1])\n",
    "    for pci in dev_P_cap:\n",
    "        kp1_dev.append(pci[0])\n",
    "        kn1_dev.append(pci[1])\n",
    "    return [kp1_train,kn1_train],[kp1_dev,kn1_dev]\n",
    "        \n",
    "def get_mini_batches(X,P_cap,bsize): #X : (train/dev/)_L_S_batch\n",
    "    for i in range(0, len(X[0]) - bsize + 1, bsize):\n",
    "        indices = slice(i, i + bsize)\n",
    "        #print(indices)\n",
    "        yield [X[0][indices],X[1][indices]],P_cap[indices]\n",
    "\n",
    "train_L_S_batch,dev_L_S_batch = get_L_S_batch()\n",
    "\n",
    "#for x in get_mini_batches(train_L_S_batch,200):\n",
    "#    print(len(x),len(x[0]),len(x[0][0]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-3.05101543903e+24\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-1.49612942222e+82\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-9.57592297318e+139\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.16216216216216217, 0.61224489795918369, 0.25641025641025639, None)\n",
      "-6.12903532452e+197\n",
      "781 2015\n",
      "0   (0.15492957746478872, 0.61734693877551017, 0.24769703172978502, None)\n",
      "0   (0.17638266068759342, 0.60204081632653061, 0.27283236994219651, None)\n",
      "-3.92286718621e+255\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "0   (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + remove min(theta,0) in loss\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def class_wise(y_test,y_score):\n",
    "    print(average_precision_score(y_test, y_score, average=None))\n",
    "    \n",
    "def draw_curve(y_test,y_score):\n",
    "    average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('2-class Precision-Recall curve: AUC={0:0.2f}'.format(\n",
    "              average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -3.05101543903e+24\n",
      "737 2059\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56786ed050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + remove min(theta,0) in loss + precision recall curve\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        #print\n",
    "        New_P_cap = []\n",
    "        newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "        train_P_cap[c+1] = newPcap\n",
    "#         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "#             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#             New_P_cap.append(newPcap)\n",
    "#         train_P_cap = New_P_cap\n",
    "\n",
    "        \n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            break\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            \n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    \n",
    "    pl = []\n",
    "    probs = []\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p,prob = sess.run([loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "        probs.append(prob)\n",
    "    probs = np.array(probs)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(i,total_te)\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weight:  0.0\n",
      "0 -39819.3864128\n",
      "737 2059\n",
      "class wise:\n",
      "(array([ 0.96260321,  0.1614654 ]), array([ 0.76230769,  0.60714286]), array([ 0.85082636,  0.2550911 ]), array([2600,  196]))\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "macro (0.56203430285545208, 0.68472527472527478, 0.55295873077658353, None)\n",
      "micro (0.7514306151645207, 0.7514306151645207, 0.75143061516452081, None)\n",
      "\n",
      "weight:  0.25\n",
      "0 -25325.0749373\n",
      "460 2336\n",
      "class wise:\n",
      "(array([ 0.96232877,  0.23478261]), array([ 0.86461538,  0.55102041]), array([ 0.910859  ,  0.32926829]), array([2600,  196]))\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "macro (0.59855568790946989, 0.70781789638932491, 0.62006364391034507, None)\n",
      "micro (0.84263233190271813, 0.84263233190271813, 0.84263233190271813, None)\n",
      "\n",
      "weight:  0.5\n",
      "0 -19868.8896039\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  0.75\n",
      "0 -17633.1904198\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  1.0\n",
      "0 -16574.9117171\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) + \\\n",
    "            tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "            + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            te_curr,p,prob = sess.run([loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(\"class wise:\")\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "\n",
    "for i in np.linspace(0,1,5):\n",
    "    train_NN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weight:  0.0\n",
      "0 -39822.034118\n",
      "737 2059\n",
      "class wise:\n",
      "(array([ 0.96260321,  0.1614654 ]), array([ 0.76230769,  0.60714286]), array([ 0.85082636,  0.2550911 ]), array([2600,  196]))\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "macro (0.56203430285545208, 0.68472527472527478, 0.55295873077658353, None)\n",
      "micro (0.7514306151645207, 0.7514306151645207, 0.75143061516452081, None)\n",
      "\n",
      "weight:  0.111111111111\n",
      "0 -31784.1577186\n",
      "737 2059\n",
      "class wise:\n",
      "(array([ 0.96260321,  0.1614654 ]), array([ 0.76230769,  0.60714286]), array([ 0.85082636,  0.2550911 ]), array([2600,  196]))\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "macro (0.56203430285545208, 0.68472527472527478, 0.55295873077658353, None)\n",
      "micro (0.7514306151645207, 0.7514306151645207, 0.75143061516452081, None)\n",
      "\n",
      "weight:  0.222222222222\n",
      "0 -26322.1090592\n",
      "491 2305\n",
      "class wise:\n",
      "(array([ 0.9626898 ,  0.22403259]), array([ 0.85346154,  0.56122449]), array([ 0.90479103,  0.3202329 ]), array([2600,  196]))\n",
      "(0.22403258655804481, 0.56122448979591832, 0.32023289665211063, None)\n",
      "macro (0.59336119566513956, 0.70734301412872846, 0.61251196310689116, None)\n",
      "micro (0.8329756795422032, 0.8329756795422032, 0.8329756795422032, None)\n",
      "\n",
      "weight:  0.333333333333\n",
      "0 -22946.5888653\n",
      "460 2336\n",
      "class wise:\n",
      "(array([ 0.96232877,  0.23478261]), array([ 0.86461538,  0.55102041]), array([ 0.910859  ,  0.32926829]), array([2600,  196]))\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "macro (0.59855568790946989, 0.70781789638932491, 0.62006364391034507, None)\n",
      "micro (0.84263233190271813, 0.84263233190271813, 0.84263233190271813, None)\n",
      "\n",
      "weight:  0.444444444444\n",
      "0 -20703.9016997\n",
      "460 2336\n",
      "class wise:\n",
      "(array([ 0.96232877,  0.23478261]), array([ 0.86461538,  0.55102041]), array([ 0.910859  ,  0.32926829]), array([2600,  196]))\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "macro (0.59855568790946989, 0.70781789638932491, 0.62006364391034507, None)\n",
      "micro (0.84263233190271813, 0.84263233190271813, 0.84263233190271813, None)\n",
      "\n",
      "weight:  0.555555555556\n",
      "0 -19187.0478796\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  0.666666666667\n",
      "0 -18177.2020536\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  0.777777777778\n",
      "0 -17479.9791334\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  0.888888888889\n",
      "0 -16967.4423169\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  1.0\n",
      "0 -16574.8406263\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n"
     ]
    }
   ],
   "source": [
    "# 2 stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) + \\\n",
    "            tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "            + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            te_curr,p,prob = sess.run([loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(\"class wise:\")\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "\n",
    "for i in np.linspace(0,1,10):\n",
    "    train_NN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weight:  0.0\n",
      "0 -39822.1517585\n",
      "[ -1.26097008e-152  -7.47747109e-051  -2.08803298e-007   6.40263757e-003\n",
      "  -3.01411785e-001  -1.24984319e+000  -1.70970064e-001]\n",
      "[ -1.26339742e-51   6.89398501e-25   4.21954138e-03   4.11998867e-03\n",
      "  -5.61168528e-01  -1.66547829e+00  -4.36209288e-01]\n",
      "737 2059\n",
      "class wise:\n",
      "(array([ 0.96260321,  0.1614654 ]), array([ 0.76230769,  0.60714286]), array([ 0.85082636,  0.2550911 ]), array([2600,  196]))\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "macro (0.56203430285545208, 0.68472527472527478, 0.55295873077658353, None)\n",
      "micro (0.7514306151645207, 0.7514306151645207, 0.75143061516452081, None)\n",
      "\n",
      "weight:  0.111111111111\n",
      "0 -31783.4576562\n",
      "[ -1.12125844e-152  -6.24454658e-051  -1.75834245e-007   5.60012296e-003\n",
      "  -2.95319307e-001  -8.52565329e-001  -1.65802849e-001]\n",
      "[ -1.18954250e-51   6.50169339e-25   3.90956995e-03   3.76398249e-03\n",
      "  -5.61536200e-01  -1.24117506e+00  -4.27436316e-01]\n",
      "737 2059\n",
      "class wise:\n",
      "(array([ 0.96260321,  0.1614654 ]), array([ 0.76230769,  0.60714286]), array([ 0.85082636,  0.2550911 ]), array([2600,  196]))\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "macro (0.56203430285545208, 0.68472527472527478, 0.55295873077658353, None)\n",
      "micro (0.7514306151645207, 0.7514306151645207, 0.75143061516452081, None)\n",
      "\n",
      "weight:  0.222222222222\n",
      "0 -26323.0644357\n",
      "[ -9.36537694e-153  -4.02808660e-051  -7.57797909e-008   4.45260250e-003\n",
      "  -3.69937330e-001  -4.26323296e-001  -1.78990300e-001]\n",
      "[ -1.05699664e-51   6.02837719e-25   3.31264094e-03   3.12805045e-03\n",
      "  -6.86691690e-01  -7.40129591e-01  -4.46125111e-01]\n",
      "491 2305\n",
      "class wise:\n",
      "(array([ 0.9626898 ,  0.22403259]), array([ 0.85346154,  0.56122449]), array([ 0.90479103,  0.3202329 ]), array([2600,  196]))\n",
      "(0.22403258655804481, 0.56122448979591832, 0.32023289665211063, None)\n",
      "macro (0.59336119566513956, 0.70734301412872846, 0.61251196310689116, None)\n",
      "micro (0.8329756795422032, 0.8329756795422032, 0.8329756795422032, None)\n",
      "\n",
      "weight:  0.333333333333\n",
      "0 -22947.3387008\n",
      "[ -7.56493926e-153  -2.50353949e-051   1.33000951e-008   3.41580860e-003\n",
      "  -3.15983716e-001  -2.75636058e-001  -1.50078777e-001]\n",
      "[ -9.23293038e-52   5.35333263e-25   2.75167614e-03   1.73036503e-03\n",
      "  -6.29297508e-01  -5.57331321e-01  -4.04756388e-01]\n",
      "460 2336\n",
      "class wise:\n",
      "(array([ 0.96232877,  0.23478261]), array([ 0.86461538,  0.55102041]), array([ 0.910859  ,  0.32926829]), array([2600,  196]))\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "macro (0.59855568790946989, 0.70781789638932491, 0.62006364391034507, None)\n",
      "micro (0.84263233190271813, 0.84263233190271813, 0.84263233190271813, None)\n",
      "\n",
      "weight:  0.444444444444\n",
      "0 -20703.8917484\n",
      "[ -5.78059866e-153  -1.33502599e-051   4.20710696e-008   2.46592590e-003\n",
      "  -2.29176325e-001  -1.89065605e-001  -1.11525338e-001]\n",
      "[ -7.89666651e-52   4.49597330e-25   2.24084128e-03   2.33477174e-04\n",
      "  -5.18582132e-01  -4.47476945e-01  -3.44544082e-01]\n",
      "460 2336\n",
      "class wise:\n",
      "(array([ 0.96232877,  0.23478261]), array([ 0.86461538,  0.55102041]), array([ 0.910859  ,  0.32926829]), array([2600,  196]))\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "macro (0.59855568790946989, 0.70781789638932491, 0.62006364391034507, None)\n",
      "micro (0.84263233190271813, 0.84263233190271813, 0.84263233190271813, None)\n",
      "\n",
      "weight:  0.555555555556\n",
      "0 -19187.1344348\n",
      "[ -4.17014459e-153  -5.31838526e-052   4.53071508e-008   1.70564840e-003\n",
      "  -1.55345082e-001  -1.25649364e-001  -7.74634265e-002]\n",
      "[ -6.66696690e-52   3.52935430e-25   1.78854697e-03  -1.12867308e-03\n",
      "  -4.13966321e-01  -3.56828407e-01  -2.83772433e-01]\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  0.666666666667\n",
      "0 -18177.181355\n",
      "[ -2.95227451e-153  -9.10252677e-053   3.95823362e-008   1.18286409e-003\n",
      "  -1.04546523e-001  -8.37952932e-002  -5.28578071e-002]\n",
      "[ -5.67272892e-52   2.63616356e-25   1.43637414e-03  -2.15810584e-03\n",
      "  -3.31922633e-01  -2.87033535e-01  -2.32322735e-01]\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  0.777777777778\n",
      "0 -17479.8056937\n",
      "[ -2.11723654e-153   9.90915388e-053   3.23376994e-008   8.34144894e-004\n",
      "  -7.22153897e-002  -5.76307924e-002  -3.66435576e-002]\n",
      "[ -4.87819021e-52   1.90755523e-25   1.18460635e-03  -2.77772373e-03\n",
      "  -2.71440164e-01  -2.35478994e-01  -1.92196802e-01]\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  0.888888888889\n",
      "0 -16967.4461652\n",
      "[ -1.54102263e-153   1.50205789e-052   2.57415608e-008   5.93072269e-004\n",
      "  -5.09497415e-002  -4.05566006e-002  -2.58094893e-002]\n",
      "[ -4.21660588e-52   1.34619569e-25   9.96935032e-04  -3.07774743e-03\n",
      "  -2.25286301e-01  -1.95891666e-01  -1.60523363e-01]\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n",
      "\n",
      "weight:  1.0\n",
      "0 -16574.8696473\n",
      "[ -1.12657246e-153   1.36876580e-052   2.01121044e-008   4.21208662e-004\n",
      "  -3.62592906e-002  -2.88065629e-002  -1.82975832e-002]\n",
      "[ -3.64337378e-52   9.25640173e-26   8.46227272e-04  -3.15690622e-03\n",
      "  -1.88336241e-01  -1.64014088e-01  -1.34667945e-01]\n",
      "462 2334\n",
      "class wise:\n",
      "(array([ 0.96229649,  0.23376623]), array([ 0.86384615,  0.55102041]), array([ 0.91041751,  0.32826748]), array([2600,  196]))\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "macro (0.5980313602421572, 0.70743328100470948, 0.61934249417539478, None)\n",
      "micro (0.84191702432045779, 0.84191702432045779, 0.8419170243204579, None)\n"
     ]
    }
   ],
   "source": [
    "# 2 find thresh + stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) \n",
    "            \n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        tuned_alphas = []\n",
    "        tuned_thetas = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            tuned_alphas,tuned_thetas,te_curr,p,prob = sess.run([alphas,thetas,loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(tuned_alphas)\n",
    "        print(tuned_thetas)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(\"class wise:\")\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "\n",
    "for i in np.linspace(0,1,10):\n",
    "    train_NN(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8/12 find thresh + remove regular on alphas + stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight,reg):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    REG_CONST = tf.constant(reg,dtype=tf.float64)\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) \\\n",
    "            + (REG_CONST * tf.reduce_sum(tf.multiply(thetas,thetas))) \\\n",
    "            + (REG_CONST * tf.reduce_sum(tf.multiply(alphas,alphas))) \\\n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels)))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        tuned_alphas = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            tuned_alphas,te_curr,p,prob = sess.run([alphas,loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(tuned_alphas)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        p,r,f,_ = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')\n",
    "        print(\"ret\",p,r,f)\n",
    "        return (p,r)\n",
    "        #draw_pr_curve(precisionArr,recallArr,reg)\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weight:  0.0\n",
      "0 -26032.5921151\n",
      "[ -2.92406223e-187  -7.11476382e-062  -2.98833416e-009   4.10257357e-003\n",
      "  -3.49866648e-001  -2.83415211e-001  -1.66901056e-001]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.111111111111\n",
      "0 -23361.3472335\n",
      "[ -2.43173912e-187  -5.33328824e-062   2.53067220e-009   3.29352050e-003\n",
      "  -2.85121665e-001  -2.17229072e-001  -1.38385004e-001]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.222222222222\n",
      "0 -21346.7219654\n",
      "[ -1.93887656e-187  -3.66348084e-062   4.48620675e-009   2.50911940e-003\n",
      "  -2.18243762e-001  -1.64787655e-001  -1.08561334e-001]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.333333333333\n",
      "0 -19845.933481\n",
      "[ -1.48412447e-187  -2.26570670e-062   4.85739816e-009   1.83526600e-003\n",
      "  -1.60014857e-001  -1.21120434e-001  -8.13085347e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.444444444444\n",
      "0 -18759.7370586\n",
      "[ -1.11050612e-187  -1.24312346e-062   4.51420008e-009   1.31929754e-003\n",
      "  -1.14913910e-001  -8.73428915e-002  -5.92001942e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.555555555556\n",
      "0 -17977.3570298\n",
      "[ -8.29356486e-188  -5.88112678e-063   3.91399336e-009   9.48527103e-004\n",
      "  -8.26081208e-002  -6.30015525e-002  -4.28210051e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.666666666667\n",
      "0 -17397.1053994\n",
      "[ -6.23543544e-188  -2.17020150e-063   3.28003261e-009   6.85096690e-004\n",
      "  -5.99525102e-002  -4.58248820e-002  -3.11091775e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.777777777778\n",
      "0 -16949.811898\n",
      "[ -4.71133577e-188  -3.27661316e-064   2.69231620e-009   4.95889851e-004\n",
      "  -4.38605495e-002  -3.35645444e-002  -2.27190262e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.888888888889\n",
      "0 -16595.7559688\n",
      "[ -3.55803284e-188   4.22426662e-064   2.17176913e-009   3.58242802e-004\n",
      "  -3.21916127e-002  -2.46455880e-002  -1.66232099e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  1.0\n",
      "0 -16311.363284\n",
      "[ -2.66922937e-188   6.02279202e-064   1.71956668e-009   2.57109503e-004\n",
      "  -2.35644973e-002  -1.80403016e-002  -1.21243977e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "lambda 1.2\n",
      "precision [0.23478260869565218, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376]\n",
      "recall [0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525]\n",
      "\n",
      "weight:  0.0\n",
      "0 -39822.4680886\n",
      "[ -1.26097008e-152  -7.47747109e-051  -2.08803298e-007   6.40263757e-003\n",
      "  -3.01411785e-001  -1.24984319e+000  -1.70970064e-001]\n",
      "737 2059\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "ret 0.161465400271 0.607142857143 0.255091103966\n",
      "\n",
      "weight:  0.111111111111\n",
      "0 -31784.1045452\n",
      "[ -1.12125844e-152  -6.24454658e-051  -1.75834245e-007   5.60012296e-003\n",
      "  -2.95319307e-001  -8.52565329e-001  -1.65802849e-001]\n",
      "737 2059\n",
      "(0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "ret 0.161465400271 0.607142857143 0.255091103966\n",
      "\n",
      "weight:  0.222222222222\n",
      "0 -26322.378299\n",
      "[ -9.36537694e-153  -4.02808660e-051  -7.57797909e-008   4.45260250e-003\n",
      "  -3.69937330e-001  -4.26323296e-001  -1.78990300e-001]\n",
      "491 2305\n",
      "(0.22403258655804481, 0.56122448979591832, 0.32023289665211063, None)\n",
      "ret 0.224032586558 0.561224489796 0.320232896652\n",
      "\n",
      "weight:  0.333333333333\n",
      "0 -22946.6533249\n",
      "[ -7.56493926e-153  -2.50353949e-051   1.33000951e-008   3.41580860e-003\n",
      "  -3.15983716e-001  -2.75636058e-001  -1.50078777e-001]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.444444444444\n",
      "0 -20703.8799233\n",
      "[ -5.78059866e-153  -1.33502599e-051   4.20710696e-008   2.46592590e-003\n",
      "  -2.29176325e-001  -1.89065605e-001  -1.11525338e-001]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.555555555556\n",
      "0 -19187.12837\n",
      "[ -4.17014459e-153  -5.31838526e-052   4.53071508e-008   1.70564840e-003\n",
      "  -1.55345082e-001  -1.25649364e-001  -7.74634265e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.666666666667\n",
      "0 -18177.2029377\n",
      "[ -2.95227451e-153  -9.10252677e-053   3.95823362e-008   1.18286409e-003\n",
      "  -1.04546523e-001  -8.37952932e-002  -5.28578071e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.777777777778\n",
      "0 -17479.9040381\n",
      "[ -2.11723654e-153   9.90915388e-053   3.23376994e-008   8.34144894e-004\n",
      "  -7.22153897e-002  -5.76307924e-002  -3.66435576e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  0.888888888889\n",
      "0 -16967.4424217\n",
      "[ -1.54102263e-153   1.50205789e-052   2.57415608e-008   5.93072269e-004\n",
      "  -5.09497415e-002  -4.05566006e-002  -2.58094893e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "\n",
      "weight:  1.0\n",
      "0 -16574.8519925\n",
      "[ -1.12657246e-153   1.36876580e-052   2.01121044e-008   4.21208662e-004\n",
      "  -3.62592906e-002  -2.88065629e-002  -1.82975832e-002]\n",
      "462 2334\n",
      "(0.23376623376623376, 0.55102040816326525, 0.32826747720364741, None)\n",
      "ret 0.233766233766 0.551020408163 0.328267477204\n",
      "lambda 1\n",
      "precision [0.23478260869565218, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.16146540027137041, 0.16146540027137041, 0.22403258655804481, 0.23478260869565218, 0.23478260869565218, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376]\n",
      "recall [0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.6071428571428571, 0.6071428571428571, 0.56122448979591832, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525]\n",
      "\n",
      "weight:  0.0\n",
      "0 -16117.9302474\n",
      "[ -2.66525857e-308  -2.25941662e-308   3.55374415e-036   1.81120186e-007\n",
      "  -2.46215777e-003   6.78394128e-004  -1.91814324e-003]\n",
      "411 2385\n",
      "(0.24817518248175183, 0.52040816326530615, 0.3360790774299835, None)\n",
      "ret 0.248175182482 0.520408163265 0.33607907743\n",
      "\n",
      "weight:  0.111111111111\n",
      "0 -16022.6343747\n",
      "[ -2.41579939e-308   0.00000000e+000   3.17885673e-036   1.47780419e-007\n",
      "  -2.05235739e-003   5.40329817e-004  -1.62268314e-003]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.222222222222\n",
      "0 -15936.1842186\n",
      "[  0.00000000e+00   0.00000000e+00   2.82037163e-36   1.20363009e-07\n",
      "  -1.70831504e-03   4.30314998e-04  -1.36883989e-03]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.333333333333\n",
      "0 -15857.8929441\n",
      "[  0.00000000e+00   0.00000000e+00   2.48037974e-36   9.77749875e-08\n",
      "  -1.41818506e-03   3.42314077e-04  -1.15020544e-03]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -15787.1672282\n",
      "[  0.00000000e+00   0.00000000e+00   2.16041073e-36   7.91327787e-08\n",
      "  -1.17259908e-03   2.71682571e-04  -9.61517505e-04]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.555555555556\n",
      "0 -15723.4771238\n",
      "[  0.00000000e+00   0.00000000e+00   1.86153133e-36   6.37236892e-08\n",
      "  -9.64110544e-04   2.14827730e-04  -7.98455014e-04]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.666666666667\n",
      "0 -15666.3417871\n",
      "[  0.00000000e+00   0.00000000e+00   1.58444604e-36   5.09730637e-08\n",
      "  -7.86769699e-04   1.68956140e-04  -6.57466300e-04]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.777777777778\n",
      "0 -15615.3232395\n",
      "[  0.00000000e+00   0.00000000e+00   1.32958601e-36   4.04173035e-08\n",
      "  -6.35799566e-04   1.31885700e-04  -5.35628197e-04]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  0.888888888889\n",
      "0 -15570.0237172\n",
      "[  0.00000000e+00   0.00000000e+00   1.09718191e-36   3.16821335e-08\n",
      "  -5.07348698e-04   1.01905006e-04  -4.30532099e-04]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "\n",
      "weight:  1.0\n",
      "0 -15530.0843903\n",
      "[  0.00000000e+00   0.00000000e+00   8.87321626e-37   2.44652726e-08\n",
      "  -3.98302103e-04   7.76676650e-05  -3.40192483e-04]\n",
      "460 2336\n",
      "(0.23478260869565218, 0.55102040816326525, 0.32926829268292684, None)\n",
      "ret 0.234782608696 0.551020408163 0.329268292683\n",
      "lambda 4\n",
      "precision [0.23478260869565218, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.16146540027137041, 0.16146540027137041, 0.22403258655804481, 0.23478260869565218, 0.23478260869565218, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.23376623376623376, 0.24817518248175183, 0.23478260869565218, 0.23478260869565218, 0.23478260869565218, 0.23478260869565218, 0.23478260869565218, 0.23478260869565218, 0.23478260869565218, 0.23478260869565218, 0.23478260869565218]\n",
      "recall [0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.6071428571428571, 0.6071428571428571, 0.56122448979591832, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.52040816326530615, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525, 0.55102040816326525]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e586d7d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "precisionArr = []\n",
    "recallArr = []\n",
    "def draw_pr_curve(reg):\n",
    "    print(\"lambda\",reg)\n",
    "    print(\"precision\",precisionArr)\n",
    "    print(\"recall\",recallArr)\n",
    "    plt.step(recallArr, precisionArr, color='b', alpha=0.2,where='post')\n",
    "    plt.fill_between(recallArr, precisionArr, step='post', alpha=0.2,color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, 1])\n",
    "    plt.title('Precision-Recall curve')\n",
    "    \n",
    "for reg in [1.2,1,4]:\n",
    "    for i in np.linspace(0,1,10):\n",
    "        p,r = train_NN(i,reg)\n",
    "        precisionArr.append(p)\n",
    "        recallArr.append(r)\n",
    "    draw_pr_curve(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for reg in [0.6,0.8]:\n",
    "    for i in np.linspace(0,1,10):\n",
    "        p,r = train_NN(i,reg)\n",
    "        precisionArr.append(p)\n",
    "        recallArr.append(r)\n",
    "    draw_pr_curve(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f5554268110>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyXnv7aIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55447e3610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision = [0.164,0.164,0.224,0.234,0.234,0.233,0.233,0.233,0.233,0.233]\n",
    "recall = [0.607,0.607,0.561,0.551,0.551,0.551,0.551,0.551,0.551,0.551]\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 0.25])\n",
    "plt.xlim([0.55, 0.61])\n",
    "plt.title('Precision-Recall curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weight:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/ipykernel_launcher.py:98: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan\n",
      "[ nan  nan  nan  nan  nan  nan  nan]\n",
      "0 2796\n",
      "class wise:\n",
      "(array([ 0.92989986,  0.        ]), array([ 1.,  0.]), array([ 0.9636768,  0.       ]), array([2600,  196]))\n",
      "(0.0, 0.0, 0.0, None)\n",
      "('lambda', 0.5)\n",
      "('precision', [array([ 0.92989986,  0.        ])])\n",
      "('recall', [array([ 1.,  0.])])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'recall' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fbe882043c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mtrain_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mdraw_pr_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecisionArr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecallArr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-727810f7dd88>\u001b[0m in \u001b[0;36mdraw_pr_curve\u001b[0;34m(p, r, reg)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_between\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'recall' is not defined"
     ]
    }
   ],
   "source": [
    "# 8/12 find thresh + remove regular on alphas + stochastic + cross entropy logits func + weighted logits+remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN(weight,reg):\n",
    "    print()\n",
    "    print(\"weight: \",weight)\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    REG_CONST = tf.constant(reg,dtype=tf.float64)\n",
    "    train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "    dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "    _p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "    #for k = 1\n",
    "\n",
    "    k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "    k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_p1,phi_n1])\n",
    "    \n",
    "    class_weights = tf.constant([weight, 1.0 - weight],dtype=tf.float64)\n",
    "    \n",
    "    weighted_logits = tf.multiply(phi_out, class_weights) \n",
    "\n",
    "    # loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "    #         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "    #         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "    \n",
    "    loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=_p_cap,logits=weighted_logits))) \\\n",
    "            + (REG_CONST * tf.reduce_sum(tf.multiply(thetas,thetas))) \\\n",
    "            #+tf.reduce_sum(tf.multiply(alphas,alphas)) \\\n",
    "            #- tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "    predict = tf.argmax(tf.nn.softmax(weighted_logits))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "    new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    '''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "            _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            print(_los)\n",
    "            print(_l)\n",
    "            print(_s)\n",
    "            print(_a)\n",
    "            print(_os)        \n",
    "            print(_t)\n",
    "            print()'''\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            total_te+=te_curr\n",
    "            #print(a)\n",
    "            #print(t)\n",
    "            #print\n",
    "            New_P_cap = []\n",
    "            newPcap = sess.run(new_p_cap,feed_dict={_x:train_L_S[c+1],_p_cap:train_P_cap[c+1]})\n",
    "            train_P_cap[c+1] = newPcap\n",
    "    #         for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "    #             newPcap = sess.run(new_p_cap,feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #             New_P_cap.append(newPcap)\n",
    "    #         train_P_cap = New_P_cap\n",
    "\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-300):\n",
    "                break\n",
    "                predicted_labels = []\n",
    "                for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                    de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                    predicted_labels.append(p)\n",
    "                print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels)))\n",
    "\n",
    "\n",
    "    #         if(c%20==0):\n",
    "    #             predicted_labels = []\n",
    "    #             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "    #                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "    #                 predicted_labels.append(p)\n",
    "    #             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "    #             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "    #         c+=1\n",
    "            te_prev = te_curr\n",
    "\n",
    "        pl = []\n",
    "        probs = []\n",
    "        tuned_alphas = []\n",
    "        for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "            tuned_alphas,te_curr,p,prob = sess.run([alphas,loss,predict,new_p_cap],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "            pl.append(p)\n",
    "            probs.append(prob)\n",
    "        probs = np.array(probs)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(tuned_alphas)\n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(\"class wise:\")\n",
    "        #print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None))\n",
    "        p,r,f,_ = precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average=None)\n",
    "        precisionArr.append(p)\n",
    "        recallArr.append(r)\n",
    "        #draw_curve(np.array(gold_labels_dev),probs[:,1])\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "        #print('macro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "        #print('micro',precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='micro'))\n",
    "\n",
    "    \n",
    "for reg in [0.5,1,1.5]:\n",
    "    for i in np.linspace(0,1,10):\n",
    "        train_NN(i,reg)\n",
    "        draw_pr_curve(precisionArr,recallArr,reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06887941393e+20\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.31678477484e+29\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.62112517295e+38\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "1.99590613383e+47\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "2.45715280858e+56\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "3.02509468656e+65\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n",
      "3.72430034577e+74\n",
      "515 2281\n",
      "0   (0.20194174757281552, 0.53061224489795922, 0.29254571026722925, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-2ed4860f071d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP_cap_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_L_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_P_cap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthetas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL_S_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_p_cap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mP_cap_i\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mtotal_te\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mte_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m#print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vinay/anaconda3/envs/en27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#stochastic + cross entropy logits func + yi fixed to output of model on discrete lfs\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_P_cap = np.load(\"Train_P_cap.npy\")\n",
    "\n",
    "\n",
    "#print(train_P_cap)\n",
    "\n",
    "# discrete_labels = predict_labels(train_P_cap[:1])\n",
    "\n",
    "# for i in range of discrete_labels:\n",
    "#     print(train_P_cap[i],discrete_labels[i])\n",
    "\n",
    "#train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(2))\n",
    "\n",
    "#W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "#b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "#for k = 1\n",
    "\n",
    "k_p1 = tf.ones(shape=(dim,len(LFs)),dtype=tf.float64)\n",
    "\n",
    "k_n1 = tf.negative(k_p1)\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "# phi_out = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(mul_L_S,k_n1),thetas))\n",
    "\n",
    "\n",
    "# additional_layer_out = tf.matmul(tf.expand_dims(mul_L_S,0),W) + b\n",
    "\n",
    "# phi_p1 = tf.reduce_sum(tf.multiply(tf.squeeze(additional_layer_out),thetas))\n",
    "\n",
    "# phi_n1 = tf.reduce_sum(tf.multiply(tf.multiply(tf.squeeze(additional_layer_out),k_n1),thetas))\n",
    "\n",
    "phi_out = tf.stack([phi_p1,phi_n1])\n",
    "\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.multiply(tf.log(tf.nn.softmax(phi_out)),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) + \\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        #- tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "'''for L_S_i,P_cap_i in zip(train_L_S,dev_P_cap):\n",
    "        _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        print(_los)\n",
    "        print(_l)\n",
    "        print(_s)\n",
    "        print(_a)\n",
    "        print(_os)        \n",
    "        print(_t)\n",
    "        print()'''\n",
    "    \n",
    "for i in range(10):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    for L_S_i,P_cap_i in zip(train_L_S,train_P_cap):\n",
    "        \n",
    "        a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        total_te+=te_curr\n",
    "        #print(a)\n",
    "        #print(t)\n",
    "        print\n",
    "        if(abs(te_curr-te_prev)<1e-300):\n",
    "            predicted_labels = []\n",
    "            for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "                de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "                predicted_labels.append(p)\n",
    "            print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "            break\n",
    "        \n",
    "#         if(c%20==0):\n",
    "#             predicted_labels = []\n",
    "#             for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "#                 de_curr,p,_ = sess.run([loss,predict,train_step],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "#                 predicted_labels.append(p)\n",
    "#             print(predicted_labels.count(0),predicted_labels.count(1))\n",
    "#             print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "#         c+=1\n",
    "        te_prev = te_curr\n",
    "    pl = []\n",
    "    print(total_te)\n",
    "    for L_S_i,P_cap_i in zip(dev_L_S,dev_P_cap):\n",
    "        te_curr,p = sess.run([loss,predict],feed_dict={_x:L_S_i,_p_cap:P_cap_i})\n",
    "        pl.append(p)\n",
    "    predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "    print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "    print(c,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22195\n",
      "-0.403062582953\n",
      "746 2050\n",
      "0   (0.16085790884718498, 0.61224489795918369, 0.25477707006369427, None)\n",
      "-0.491641866684\n",
      "810 1986\n",
      "1   (0.17037037037037037, 0.70408163265306123, 0.27435387673956263, None)\n",
      "-0.594699991089\n",
      "787 2009\n",
      "2   (0.15374841168996187, 0.61734693877551017, 0.24618514750762968, None)\n",
      "-0.584680700423\n",
      "1241 1555\n",
      "3   (0.11764705882352941, 0.74489795918367352, 0.20320111343075853, None)\n",
      "-0.629941925989\n",
      "1247 1549\n",
      "4   (0.10986367281475541, 0.69897959183673475, 0.18988218988218988, None)\n",
      "-0.610793114481\n",
      "899 1897\n",
      "5   (0.1546162402669633, 0.70918367346938771, 0.25388127853881282, None)\n",
      "-0.622043836336\n",
      "820 1976\n",
      "6   (0.14999999999999999, 0.62755102040816324, 0.24212598425196849, None)\n",
      "-0.617336477866\n",
      "1480 1316\n",
      "7   (0.095270270270270269, 0.71938775510204078, 0.16825775656324585, None)\n",
      "-0.618130318603\n",
      "818 1978\n",
      "8   (0.1687041564792176, 0.70408163265306123, 0.27218934911242604, None)\n",
      "-0.586582216415\n",
      "973 1823\n",
      "9   (0.14285714285714285, 0.70918367346938771, 0.23781009409751927, None)\n",
      "-0.607121678903\n",
      "786 2010\n",
      "10   (0.17557251908396945, 0.70408163265306123, 0.28105906313645623, None)\n",
      "-0.561990194009\n",
      "834 1962\n",
      "11   (0.14748201438848921, 0.62755102040816324, 0.23883495145631067, None)\n",
      "-0.572345405133\n",
      "983 1813\n",
      "12   (0.1353001017293998, 0.6785714285714286, 0.22561492790500426, None)\n",
      "-0.566924296964\n",
      "789 2007\n",
      "13   (0.15335868187579213, 0.61734693877551017, 0.24568527918781724, None)\n",
      "-0.435678966348\n",
      "779 2017\n",
      "14   (0.15661103979460847, 0.62244897959183676, 0.25025641025641027, None)\n",
      "-0.458495279874\n",
      "842 1954\n",
      "15   (0.13657957244655583, 0.58673469387755106, 0.22157996146435455, None)\n",
      "-0.449164983646\n",
      "809 1987\n",
      "16   (0.14956736711990112, 0.61734693877551017, 0.24079601990049751, None)\n",
      "-0.379867863066\n",
      "838 1958\n",
      "17   (0.13603818615751789, 0.58163265306122447, 0.22050290135396519, None)\n",
      "-0.369685197109\n",
      "750 2046\n",
      "18   (0.16, 0.61224489795918369, 0.2536997885835095, None)\n",
      "-0.227868806484\n",
      "745 2051\n",
      "19   (0.16107382550335569, 0.61224489795918369, 0.25504782146652494, None)\n",
      "-0.0355672342723\n",
      "752 2044\n",
      "20   (0.15957446808510639, 0.61224489795918369, 0.25316455696202533, None)\n",
      "0.108307225631\n",
      "750 2046\n",
      "21   (0.16, 0.61224489795918369, 0.2536997885835095, None)\n",
      "0.335109901527\n",
      "743 2053\n",
      "22   (0.16150740242261102, 0.61224489795918369, 0.25559105431309898, None)\n",
      "0.740705182006\n",
      "737 2059\n",
      "23   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.11050066556\n",
      "737 2059\n",
      "24   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.60991097477\n",
      "737 2059\n",
      "25   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "2.25530605435\n",
      "737 2059\n",
      "26   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "3.14536698583\n",
      "737 2059\n",
      "27   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "4.36727211905\n",
      "737 2059\n",
      "28   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "6.1403302575\n",
      "737 2059\n",
      "29   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "9.3665721913\n",
      "737 2059\n",
      "30   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "15.1515395344\n",
      "737 2059\n",
      "31   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "27.3734255678\n",
      "737 2059\n",
      "32   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "57.8044624384\n",
      "737 2059\n",
      "33   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "155.940639752\n",
      "737 2059\n",
      "34   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "639.915555032\n",
      "737 2059\n",
      "35   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "5813.47074629\n",
      "737 2059\n",
      "36   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "276713.418925\n",
      "737 2059\n",
      "37   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "467766797.392\n",
      "737 2059\n",
      "38   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.27592153959e+15\n",
      "737 2059\n",
      "39   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "9.48800648303e+27\n",
      "737 2059\n",
      "40   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "5.25972489458e+53\n",
      "737 2059\n",
      "41   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "1.61439546837e+105\n",
      "737 2059\n",
      "42   (0.16146540027137041, 0.6071428571428571, 0.25509110396570206, None)\n",
      "nan\n",
      "0 2796\n",
      "43   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "44   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "45   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "46   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "47   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "48   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "49   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "50   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "51   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "52   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "53   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "54   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "55   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "56   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "57   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "58   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "59   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "60   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "61   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "62   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "63   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "64   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "65   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "66   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "67   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "68   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "69   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "70   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "71   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "72   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "73   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "74   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "75   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "76   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "77   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "78   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "79   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "80   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "81   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "82   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "83   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "84   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "85   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "86   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "87   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "88   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "89   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "90   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "91   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "92   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "93   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "94   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "95   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "96   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "97   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "98   (0.0, 0.0, 0.0, None)\n",
      "nan\n",
      "0 2796\n",
      "99   (0.0, 0.0, 0.0, None)\n"
     ]
    }
   ],
   "source": [
    "# Batch with cross entropy logits function  + additional layer\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "result_dir = \"./\"\n",
    "config = projector.ProjectorConfig()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "summary_writer = tf.summary.FileWriter(result_dir)\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "dim = 2 #(labels,scores)\n",
    "\n",
    "data_size = len(train_L_S_batch[0])\n",
    "\n",
    "dev_data_size = len(dev_L_S_batch[0])\n",
    "\n",
    "train_P_cap = pkl.load( open( \"train_P_cap.p\", \"rb\" ) )\n",
    "dev_P_cap = pkl.load( open( \"dev_P_cap.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "#train_P_cap = np.full([data_size,2],0.5)\n",
    "\n",
    "#print(train_P_cap)\n",
    "#print(train_P_cap.shape)\n",
    "#dev_P_cap = np.full([dev_data_size,2],0.5)\n",
    "\n",
    "\n",
    "\n",
    "print(data_size)\n",
    "\n",
    "_x = tf.placeholder(tf.float64,shape=(dim,None,len(LFs)))\n",
    "_p_cap = tf.placeholder(tf.float64,shape=(None,2))\n",
    "\n",
    "W =tf.Variable(tf.truncated_normal([len(LFs),len(LFs)], stddev=0.8,dtype = tf.float64))\n",
    "\n",
    "b =tf.Variable(tf.truncated_normal([len(LFs)], stddev=0.01,dtype = tf.float64))\n",
    "\n",
    "alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(0.01),\n",
    "                        dtype=tf.float64)\n",
    "\n",
    "#print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "\n",
    "\n",
    "\n",
    "l,s = tf.unstack(_x)\n",
    "\n",
    "prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "\n",
    "mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "additional_layer_out = tf.matmul(mul_L_S,W) + b\n",
    "\n",
    "phi_p1 = tf.matmul(additional_layer_out,tf.expand_dims(thetas,-1))\n",
    "\n",
    "phi_n1 = tf.matmul(tf.negative(additional_layer_out),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "\n",
    "# phi_p1 = tf.matmul(mul_L_S,tf.expand_dims(thetas,-1))\n",
    "\n",
    "# phi_n1 = tf.matmul(tf.negative(mul_L_S),tf.expand_dims(thetas,-1))\n",
    "\n",
    "\n",
    "phi_out = tf.concat([phi_p1,phi_n1],1)\n",
    "\n",
    "# pio = tf.Print(phi_out,[phi_out])\n",
    "\n",
    "# loss = tf.negative(tf.reduce_sum(tf.matmul(tf.transpose(tf.log(tf.nn.softmax(phi_out))),_p_cap))) + \\\n",
    "#         tf.reduce_sum(tf.multiply(alphas,alphas)) +\\\n",
    "#         tf.reduce_sum(tf.multiply(thetas,thetas)) +\\\n",
    "#         - tf.minimum( tf.reduce_min(thetas),0)\n",
    "\n",
    "\n",
    "loss = tf.negative(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=phi_out,labels=_p_cap))) + \\\n",
    "        tf.reduce_sum(tf.multiply(alphas,alphas))\\\n",
    "        - tf.minimum( tf.reduce_min(thetas),0)\\\n",
    "         + tf.reduce_sum(tf.multiply(thetas,thetas)) \n",
    "        \n",
    "\n",
    "predict = tf.argmax(tf.nn.softmax(phi_out),1)\n",
    "\n",
    "predict_2 = tf.where(tf.greater(tf.slice(tf.nn.softmax(phi_out),[0,1],[dev_data_size,1]),0.5),\n",
    "                    tf.ones((dev_data_size,1)),tf.negative(tf.ones((dev_data_size,1))))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) \n",
    "\n",
    "new_p_cap = tf.nn.softmax(phi_out)\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#print(sess.run([phi_out,predict],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}))\n",
    "\n",
    "# for i in range(100):\n",
    "#     _l,_s,_os,_a,_t,_los,_ = sess.run([l,s,prelu_out_s,alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "#     print(_los)\n",
    "#     print(_l)\n",
    "#     print(_s)\n",
    "#     print(_a)\n",
    "#     print(_os)        \n",
    "#     print(_t)\n",
    "#     print()\n",
    "\n",
    "for i in range(100):\n",
    "    c = 0\n",
    "    te_prev=1\n",
    "    total_te = 0\n",
    "    a,t,te_curr,_, = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap})\n",
    "    print(te_curr)\n",
    "    \n",
    "    train_P_cap = sess.run(new_p_cap,feed_dict={_x:train_L_S_batch,_p_cap:train_P_cap}) \n",
    "    #print(train_P_cap[0:5])\n",
    "    #print(a)\n",
    "    #print(t)\n",
    "    #print()   \n",
    "    if(i%1 == 0):\n",
    "        te_curr,pl,pl2,_ = sess.run([loss,predict,predict_2,train_step],feed_dict={_x:dev_L_S_batch,_p_cap:dev_P_cap})\n",
    "        pl2 = pl2.flatten().tolist()\n",
    "        pl = pl.flatten().tolist()\n",
    "        #print(te_curr)\n",
    "        #predicted_labels = pl2\n",
    "        predicted_labels = [-1 if x==0 else 1 for x in pl]\n",
    "        #for l,l2 in zip(predicted_labels,pl2):\n",
    "        #    print(l,l2)\n",
    "        \n",
    "        print(predicted_labels.count(1),predicted_labels.count(-1))\n",
    "        #print(predicted_labels,gold_labels_dev)\n",
    "        print(i,\" \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary'))\n",
    " \n",
    "    if(abs(te_curr-te_prev)<1e-20):\n",
    "          break\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 330.069609\n",
      "         Iterations: 20\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 23\n",
      "[ 3.29293033  0.80240776  3.54276763  3.57746115  2.57487472  2.54625167\n",
      "  4.16422647]\n",
      "train iteration: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c9a7610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c80f590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 4182.213294\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "[ 3.29293033  0.80240776  3.54276763  3.57746115  2.57487472  2.54625167\n",
      "  4.16422647]\n",
      "train iteration: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c9a7d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435bea9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n",
      "test:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLDDAFJatj/AeqXOAnMd9m+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f436ad14350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAT6uTcRRQ78SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f435c117750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2796, 2796, 2796)\n",
      "(0.13274336283185842, 0.68877551020408168, 0.22258862324814513, None)\n"
     ]
    }
   ],
   "source": [
    "# All LF_Threshold =0.3 and softmax_Threshold=0.3 ,to be run\n",
    "train(2,Use_Confidence=False,theta_file_name=\"THETA\")\n",
    "\n",
    "test(THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_details(label,THETA,LAMDA,SCORE):\n",
    "    print(label)\n",
    "    P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "    marginals=get_marginals(P_cap)\n",
    "    plt.hist(marginals, bins=20)\n",
    "    plt.show()\n",
    "    #plt.bar(range(0,2796),marginals)\n",
    "    #plt.show()\n",
    "    predicted_labels=predict_labels(marginals)\n",
    "    print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "    #score(predicted_labels,gold_labels_dev)\n",
    "    print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='binary')) \n",
    "    \n",
    "def predict_labels(marginals):\n",
    "    predicted_labels=[]\n",
    "    for i in marginals:\n",
    "        if(i<0.5):\n",
    "            predicted_labels.append(-1)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    return predicted_labels\n",
    "\n",
    "#import cPickle as pickle\n",
    "#THETA = pickle.load( open( \"THETA.p\", \"rb\" ) )\n",
    "#test(THETA)\n",
    "#LAMDA,SCORE = get_LAMDA(dev_cands)\n",
    "#Confidence = get_Confidence(LAMDA)\n",
    "\n",
    "#P_cap = get_P_cap(LAMDA,SCORE,THETA)\n",
    "#marginals=get_marginals(P_cap)\n",
    "#plt.hist(marginals, bins=20)\n",
    "#plt.show()\n",
    "#plt.bar(range(0,888),train_marginals)\n",
    "#plt.show()\n",
    "\n",
    "print_details(\"dev set\",THETA,dev_LAMDA,dev_SCORE)\n",
    "predicted_labels=predict_labels(marginals)\n",
    "\n",
    "\n",
    "sorted_predicted_labels=[x for (y,x) in sorted(zip(Confidence,predicted_labels))] #sort Labels as per Confidence\n",
    "sorted_predicted_labels=list(reversed(sorted_predicted_labels))\n",
    "\n",
    "\n",
    "for i,j in enumerate(reversed(sorted(zip(Confidence,predicted_labels,gold_labels_dev)))):\n",
    "    if i>20:\n",
    "        break\n",
    "    print i,j\n",
    "#print(len(marginals),len(predicted_labels),len(gold_labels_dev))\n",
    "#no_of_labels=186#int(len(predicted_labels)*0.1)  #54 - >0.2  , 108>= 0.15 , 186>= 0.12\n",
    "#print(len(sorted_predicted_labels[0:no_of_labels]))\n",
    "no_of_labels=2796\n",
    "score(predicted_labels[0:no_of_labels],gold_labels_dev[0:no_of_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-0.000226377142841\n",
      "2251 545\n",
      "0  d  (0.58865213829531426, 0.71341836734693875, 0.60408179957052133, None)\n",
      "\n",
      "-1.94518369934e+28\n",
      "2232 564\n",
      "4000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-5.04415736866e+58\n",
      "2232 564\n",
      "8000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-1.33431810995e+89\n",
      "2232 564\n",
      "12000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-3.67295517678e+119\n",
      "2232 564\n",
      "16000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "\n",
      "-9.52453175821e+149\n",
      "2232 564\n",
      "20000  d  (0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n",
      "0 -1.71979948062e+170\n",
      "2232 564\n",
      "(0.57825249752154351, 0.6933045525902668, 0.58885935866155448, None)\n"
     ]
    }
   ],
   "source": [
    "#stochastic + weighted cross entropy logits func + remove min(theta,0) in loss \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def train_NN():\n",
    "    print()\n",
    "    result_dir = \"./\"\n",
    "    config = projector.ProjectorConfig()\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    summary_writer = tf.summary.FileWriter(result_dir)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    dim = 2 #(labels,scores)\n",
    "\n",
    "    _x = tf.placeholder(tf.float64,shape=(dim,len(LFs)))\n",
    "\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],initializer=tf.constant_initializer(0.2),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    thetas = tf.get_variable('theta', _x.get_shape()[-1],initializer=tf.constant_initializer(1),\n",
    "                            dtype=tf.float64)\n",
    "\n",
    "    l,s = tf.unstack(_x)\n",
    "\n",
    "    prelu_out_s = tf.maximum(tf.subtract(s,alphas), tf.zeros(shape=(len(LFs)),dtype=tf.float64))        \n",
    "\n",
    "    mul_L_S = tf.multiply(l,prelu_out_s)\n",
    "\n",
    "    phi_p1 = tf.reduce_sum(tf.multiply(mul_L_S,thetas))\n",
    "\n",
    "    phi_n1 = tf.reduce_sum(tf.multiply(tf.negative(mul_L_S),thetas))\n",
    "\n",
    "    phi_out = tf.stack([phi_n1,phi_p1])\n",
    "    \n",
    "    predict = tf.argmax(tf.nn.softmax(phi_out))\n",
    "\n",
    "    loss = tf.negative(tf.reduce_logsumexp(phi_out))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "\n",
    "    check_op = tf.add_check_numerics_ops()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1):\n",
    "        c = 0\n",
    "        te_prev=1\n",
    "        total_te = 0\n",
    "        for L_S_i in train_L_S:\n",
    "\n",
    "            a,t,te_curr,_ = sess.run([alphas,thetas,loss,train_step],feed_dict={_x:L_S_i})\n",
    "            total_te+=te_curr\n",
    "\n",
    "            if(abs(te_curr-te_prev)<1e-200):\n",
    "                break\n",
    "\n",
    "            if(c%4000==0):\n",
    "                pl = []\n",
    "                for L_S_i in dev_L_S:\n",
    "                    a,t,de_curr,p = sess.run([alphas,thetas,loss,predict],feed_dict={_x:L_S_i})\n",
    "                    pl.append(p)\n",
    "                predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "                print()\n",
    "                print(total_te/4000)\n",
    "                total_te=0\n",
    "#                 print(a)\n",
    "#                 print(t)\n",
    "#                 print()\n",
    "                print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "                print(c,\" d \",precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "            c+=1\n",
    "            te_prev = te_curr\n",
    "        pl = []\n",
    "        for L_S_i in dev_L_S:\n",
    "            p = sess.run(predict,feed_dict={_x:L_S_i})\n",
    "            pl.append(p)\n",
    "        predicted_labels = [-1 if x==0 else x for x in pl]\n",
    "        print(i,total_te)\n",
    "        print(predicted_labels.count(-1),predicted_labels.count(1))\n",
    "        print(precision_recall_fscore_support(np.array(gold_labels_dev),np.array(predicted_labels),average='macro'))\n",
    "    \n",
    "train_NN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
